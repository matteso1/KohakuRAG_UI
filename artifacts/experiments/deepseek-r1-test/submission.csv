"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context snippet from ref_id=wu2021b explicitly states that the total energy consumption of U.S. data centers increased by about 4% from 2010-2014. This directly answers the question about the average increase in electricity consumption during that period.","4","percent","[""wu2021b""]","is_blank","is_blank","The context snippet from ref_id=wu2021b explicitly states that the total energy consumption of U.S. data centers increased by about 4% from 2010-2014. This directly answers the question about the average increase in electricity consumption during that period."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e annually, equivalent to taking more than 13,900 cars off the road.","13900","cars","[""amazon2023""]","is_blank","is_blank","The context states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e annually, equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context explicitly states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24. This indicates that AWS began using recycled water for cooling in 4 additional data centers during that year.","4","data centers","[""amazon2023""]","is_blank","is_blank","The context explicitly states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24. This indicates that AWS began using recycled water for cooling in 4 additional data centers during that year."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context indicates that NVIDIA does not release embodied carbon emissions data for its GPUs. However, ref_id=morrison2025 provides an estimated embodied carbon emission of 0.013 kg of CO2eq per GPU hour when amortized over a 4-year lifespan. Additionally, they mention an assumption of 463 kg per GPU based on Luccioni et al. (2023).","0.013 kg/GPU hour and 463 kg/GPU","kg/GPU","[""morrison2025""]","is_blank","is_blank","The context indicates that NVIDIA does not release embodied carbon emissions data for its GPUs. However, ref_id=morrison2025 provides an estimated embodied carbon emission of 0.013 kg of CO2eq per GPU hour when amortized over a 4-year lifespan. Additionally, they mention an assumption of 463 kg per GPU based on Luccioni et al. (2023)."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The context states that the estimated amortized training cost of GPT-4 is $40 million (from ref_id=cottier2024) and the total training budget for FLM-101B is $100,000 (from ref_id=li2025a). The factor is calculated by dividing $40,000,000 by $100,000, resulting in 400.","400","ratio","[""cottier2024"", ""li2025a""]","is_blank","is_blank","The context states that the estimated amortized training cost of GPT-4 is $40 million (from ref_id=cottier2024) and the total training budget for FLM-101B is $100,000 (from ref_id=li2025a). The factor is calculated by dividing $40,000,000 by $100,000, resulting in 400."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context snippet from ref_id=strubell2019 provides a direct value for CO2e emissions of a round trip air travel from NY to SF as 1984 lbs. This matches the question's request for San Francisco (SF) to New York (NY) round trip, confirming the answer.","1984","tCO2e","[""strubell2019""]","is_blank","is_blank","The context snippet from ref_id=strubell2019 provides a direct value for CO2e emissions of a round trip air travel from NY to SF as 1984 lbs. This matches the question's request for San Francisco (SF) to New York (NY) round trip, confirming the answer."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context explicitly states that FLM-101B achieved an average score of 43.94 on the Open LLM Leaderboard, as shown in Table 4 and mentioned in the results section under Open LLM Evaluation.","43.94","score","[""li2025a""]","is_blank","is_blank","The context explicitly states that FLM-101B achieved an average score of 43.94 on the Open LLM Leaderboard, as shown in Table 4 and mentioned in the results section under Open LLM Evaluation."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context explicitly states that there was a 'more than 6,750 fold improvement in processor clock speed' between microprocessors manufactured in 1971 (Intel 4004) and those in 2021.","6750","fold","[""wu2021b""]","is_blank","is_blank","The context explicitly states that there was a 'more than 6,750 fold improvement in processor clock speed' between microprocessors manufactured in 1971 (Intel 4004) and those in 2021."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context snippet from ref_id=luccioni2023 states that the total training time of GPT-3 was estimated to be over 3.5 million hours, which is equivalent to 14.8 days with 10,000 GPUs. This directly answers the question about the number of days using 10,000 V100 GPUs.","14.8","days","[""luccioni2023""]","is_blank","is_blank","The context snippet from ref_id=luccioni2023 states that the total training time of GPT-3 was estimated to be over 3.5 million hours, which is equivalent to 14.8 days with 10,000 GPUs. This directly answers the question about the number of days using 10,000 V100 GPUs."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context from morrison2025's Table 3 explicitly lists the GPU Power Usage for the Llama 3.2 1B model at 8 requests per second as 0.036 kWh. This directly answers the question about the estimated GPU power usage under the specified conditions.","0.036","kWh","[""morrison2025""]","is_blank","is_blank","The context from morrison2025's Table 3 explicitly lists the GPU Power Usage for the Llama 3.2 1B model at 8 requests per second as 0.036 kWh. This directly answers the question about the estimated GPU power usage under the specified conditions."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context explicitly states that the total permitted annual emission limits for diesel generators in Northern Virginia are approximately 13,000 tons of NOx. The time frame mentioned in the question (January 1, 2023 to December 1, 2024) aligns with the permits discussed, as the permits issued since 2023 are referenced.","13000","tons","[""han2024""]","is_blank","is_blank","The context explicitly states that the total permitted annual emission limits for diesel generators in Northern Virginia are approximately 13,000 tons of NOx. The time frame mentioned in the question (January 1, 2023 to December 1, 2024) aligns with the permits discussed, as the permits issued since 2023 are referenced."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated).","72","percent","[""li2025a""]","is_blank","is_blank","The context states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated)."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context states that U.S. data centers could contribute to approximately 1,300 premature deaths in 2028 due to scope-2 pollutants, as projected by the EPA's COBRA modeling tool.","1300","deaths","[""han2024""]","is_blank","is_blank","The context states that U.S. data centers could contribute to approximately 1,300 premature deaths in 2028 due to scope-2 pollutants, as projected by the EPA's COBRA modeling tool."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The context snippet from ref_id=dodge2022 states that a full training run of the 6 billion parameter model would take approximately 60 days. This is calculated by noting that the training run was only 13% complete after 8 days, leading to an estimated total of 60 days (8 days / 0.13 ≈ 61.5, rounded to 60).","60","days","[""dodge2022""]","is_blank","is_blank","The context snippet from ref_id=dodge2022 states that a full training run of the 6 billion parameter model would take approximately 60 days. This is calculated by noting that the training run was only 13% complete after 8 days, leading to an estimated total of 60 days (8 days / 0.13 ≈ 61.5, rounded to 60)."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context from fernandez2025 discusses energy consumption for different decoding strategies and models, including OLMo 1B, OLMo 7B, and OLMoE 1B-7B. Figure 4 in the appendix shows that OLMoE 1B-7B (a Mixture-of-Experts model) requires more energy than dense models like OLMo 7B, especially at larger batch sizes. The question asks about GPU energy consumption for 1,000 queries, which aligns with the energy measurements discussed in the context.","OLMoE 1B-7B","is_blank","[""fernandez2025""]","is_blank","is_blank","The context from fernandez2025 discusses energy consumption for different decoding strategies and models, including OLMo 1B, OLMo 7B, and OLMoE 1B-7B. Figure 4 in the appendix shows that OLMoE 1B-7B (a Mixture-of-Experts model) requires more energy than dense models like OLMo 7B, especially at larger batch sizes. The question asks about GPU energy consumption for 1,000 queries, which aligns with the energy measurements discussed in the context."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This information is repeated multiple times in the provided context snippets from ref_id=stone2022.","2014","year","[""stone2022""]","is_blank","is_blank","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This information is repeated multiple times in the provided context snippets from ref_id=stone2022."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The context explicitly states that the UN's Global E-Waste Monitor 2024 reported about 22% of e-waste being formally collected and recycled. This information is directly provided in the snippet with ref_id=luccioni2025a, which references the UN report.","22","percent","[""luccioni2025a""]","is_blank","is_blank","The context explicitly states that the UN's Global E-Waste Monitor 2024 reported about 22% of e-waste being formally collected and recycled. This information is directly provided in the snippet with ref_id=luccioni2025a, which references the UN report."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context mentions that the BLOOMz-7B model's training energy is 51,686 kWh, which includes pre-training. The table in the context lists 'Training energy (kWh)' for BLOOMz-7B as 51,686, confirming the pre-training energy consumption.","51686","MWh","[""luccioni2024""]","is_blank","is_blank","The context mentions that the BLOOMz-7B model's training energy is 51,686 kWh, which includes pre-training. The table in the context lists 'Training energy (kWh)' for BLOOMz-7B as 51,686, confirming the pre-training energy consumption."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context mentions the Switch Transformer model with 1.5 trillion parameters but does not specify the percentage of parameters activated per token. The provided snippets discuss carbon emissions, model architecture, and comparisons with other models but lack information on activation percentage.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context mentions the Switch Transformer model with 1.5 trillion parameters but does not specify the percentage of parameters activated per token. The provided snippets discuss carbon emissions, model architecture, and comparisons with other models but lack information on activation percentage."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context explicitly states in Table 1 of the hyperparameters section that JetMoE-8B has 8 experts (Nexperts=8) in each MoE layer.","8","experts","[""shen2024""]","is_blank","is_blank","The context explicitly states in Table 1 of the hyperparameters section that JetMoE-8B has 8 experts (Nexperts=8) in each MoE layer."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context mentions that for BlackMamba-D (dense) with a batch size of 30, the execution time breakdown is shown in Figure 4. The figure indicates that the total execution time for dense BlackMamba with batch size 30 is approximately 2.0 seconds.","2.0","second","[""xia2024""]","is_blank","is_blank","The context mentions that for BlackMamba-D (dense) with a batch size of 30, the execution time breakdown is shown in Figure 4. The figure indicates that the total execution time for dense BlackMamba with batch size 30 is approximately 2.0 seconds."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context explicitly states that the total cost of training FLM-101B is 52.76 zettaFLOPs, with 28.22 zettaFLOPs allocated to the English portion. This information is provided in the same sentence within the document referenced as li2025a.","28.22","zettaFLOPs","[""li2025a""]","is_blank","is_blank","The context explicitly states that the total cost of training FLM-101B is 52.76 zettaFLOPs, with 28.22 zettaFLOPs allocated to the English portion. This information is provided in the same sentence within the document referenced as li2025a."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context from samsi2024 describes the experimental setup using the MIT Supercloud HPC system with Intel Xeon Gold 6248 CPUs and NVIDIA V100 GPUs. The Financial Sentiment Classification task is mentioned in khan2025's dataset section, which used an 11th Gen Intel Core i7-1165G7 processor for local inference.","11th Gen Intel Core i7-1165G7","is_blank","[""samsi2024"", ""khan2025""]","is_blank","is_blank","The context from samsi2024 describes the experimental setup using the MIT Supercloud HPC system with Intel Xeon Gold 6248 CPUs and NVIDIA V100 GPUs. The Financial Sentiment Classification task is mentioned in khan2025's dataset section, which used an 11th Gen Intel Core i7-1165G7 processor for local inference."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context mentions that the study compared the inference costs of various categories of ML systems, including task-specific and general-purpose models. However, the exact number of different machine learning models sampled and analyzed is not explicitly stated in the provided snippets.","is_blank","models","[""is_blank""]","is_blank","is_blank","The context mentions that the study compared the inference costs of various categories of ML systems, including task-specific and general-purpose models. However, the exact number of different machine learning models sampled and analyzed is not explicitly stated in the provided snippets."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that increasing GPU utilization up to 80% results in the overall carbon footprint decreasing by 3×. This directly answers the question by providing the factor of reduction.","3","multiplier","[""wu2021a""]","is_blank","is_blank","The context states that increasing GPU utilization up to 80% results in the overall carbon footprint decreasing by 3×. This directly answers the question by providing the factor of reduction."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","The context from Cottier et al. (2024) states that the total compute for model development is estimated to be 1.2x to 4x larger than the final training run, with a median of 2.2x. This is based on a log-normal distribution with a 90% confidence interval.","1.2-4","multiplier","[""cottier2024""]","is_blank","is_blank","The context from Cottier et al. (2024) states that the total compute for model development is estimated to be 1.2x to 4x larger than the final training run, with a median of 2.2x. This is based on a log-normal distribution with a 90% confidence interval."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","","","MWh","[]","is_blank","is_blank",""
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The context explicitly states that the paper examines Jevons' Paradox in relation to AI, where efficiency gains lead to increased consumption. This economic principle is identified as the central reason why technical efficiency in AI may not yield net environmental benefits.","Jevons' Paradox","is_blank","[""luccioni2025a""]","is_blank","is_blank","The context explicitly states that the paper examines Jevons' Paradox in relation to AI, where efficiency gains lead to increased consumption. This economic principle is identified as the central reason why technical efficiency in AI may not yield net environmental benefits."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, as per multiple references to the same document (li2025b).","4.2 – 6.6","billion cubic meters","[""li2025b""]","is_blank","is_blank","The context states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, as per multiple references to the same document (li2025b)."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context explicitly states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).' This directly contradicts the assertion that Red AI is on the decline due to diminishing returns.","0","is_blank","[""schwartz2019""]","is_blank","is_blank","The context explicitly states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).' This directly contradicts the assertion that Red AI is on the decline due to diminishing returns."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context explicitly states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days.","21.54","days","[""li2025a""]","is_blank","is_blank","The context explicitly states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context explicitly states that a vast majority of model experimentation workflows at Facebook utilize GPUs at only 30-50% capacity, which directly contradicts the claim of over 80% utilization.","0","is_blank","[""wu2021a""]","is_blank","is_blank","The context explicitly states that a vast majority of model experimentation workflows at Facebook utilize GPUs at only 30-50% capacity, which directly contradicts the claim of over 80% utilization."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context snippets from ref_id=li2025b and ref_id=jegham2025 both explicitly state that the estimated training energy consumption for GPT-3 is 1287 MWh. This value is directly provided in the text and tables of these documents.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","is_blank","The context snippets from ref_id=li2025b and ref_id=jegham2025 both explicitly state that the estimated training energy consumption for GPT-3 is 1287 MWh. This value is directly provided in the text and tables of these documents."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions the 'AI Energy Score 21' project multiple times, which is described as aiming to establish a unified approach for comparing the inference efficiency of AI models.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context mentions the 'AI Energy Score 21' project multiple times, which is described as aiming to establish a unified approach for comparing the inference efficiency of AI models."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context mentions Figure 6 which shows the execution breakdown of the MoE layer for different kernels. For BlackMamba with a dense setup and batch size of 30, the longest kernel execution time is 2000 microseconds for the 'matmul(w2)' operation.","2000","microseconds","[""xia2024""]","is_blank","is_blank","The context mentions Figure 6 which shows the execution breakdown of the MoE layer for different kernels. For BlackMamba with a dense setup and batch size of 30, the longest kernel execution time is 2000 microseconds for the 'matmul(w2)' operation."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context explicitly states in Table 1 of the JetMoE-8B hyperparameters that the 'Top-k' value is 2. Additionally, the text mentions 'we set the same number of experts to 8 and top-k to 2 for every layer', confirming that 2 experts are selected per layer.","2","experts","[""shen2024""]","is_blank","is_blank","The context explicitly states in Table 1 of the JetMoE-8B hyperparameters that the 'Top-k' value is 2. Additionally, the text mentions 'we set the same number of experts to 8 and top-k to 2 for every layer', confirming that 2 experts are selected per layer."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context from Schwartz et al. (2019) states that the computations required for deep learning research have increased 300,000x from 2012 to 2018. The question specifies a 200,000x increase over six years (2012-2018), which is lower than the 300,000x reported in the context. Therefore, the statement is false.","0","is_blank","[""schwartz2019""]","is_blank","is_blank","The context from Schwartz et al. (2019) states that the computations required for deep learning research have increased 300,000x from 2012 to 2018. The question specifies a 200,000x increase over six years (2012-2018), which is lower than the 300,000x reported in the context. Therefore, the statement is false."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context snippet from ref_id=wu2021b explicitly states that global carbon emissions for 2020 dropped by 6.4% during the COVID-19 pandemic.","6.4","percent","[""wu2021b""]","is_blank","is_blank","The context snippet from ref_id=wu2021b explicitly states that global carbon emissions for 2020 dropped by 6.4% during the COVID-19 pandemic."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that in 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources, which is an increase from 19 regions in 2022.","22","data centers","[""amazon2023""]","is_blank","is_blank","The context states that in 2023, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources, which is an increase from 19 regions in 2022."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context states that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop. Since the question asks for the approximate age of the field in 2025, subtracting 1956 from 2025 gives 69 years.","69","years","[""stone2022""]","is_blank","is_blank","The context states that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop. Since the question asks for the approximate age of the field in 2025, subtracting 1956 from 2025 gives 69 years."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context states that the 'five cars' carbon footprint estimate from the 2019 study by Strubell et al. was based on neural architecture search (NAS), a process used to automate model development. The NAS approach was highlighted as a large-scale, infrequently performed procedure due to its resource intensity and intended reusability.","Neural architecture search (NAS)","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context states that the 'five cars' carbon footprint estimate from the 2019 study by Strubell et al. was based on neural architecture search (NAS), a process used to automate model development. The NAS approach was highlighted as a large-scale, infrequently performed procedure due to its resource intensity and intended reusability."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that targeting an average TPOT of 100 ms lands on the Pareto frontier at 77 ms, reducing energy consumption by 44% compared to minimizing latency.","44","percent","[""chung2025""]","is_blank","is_blank","The context states that targeting an average TPOT of 100 ms lands on the Pareto frontier at 77 ms, reducing energy consumption by 44% compared to minimizing latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context states that Table III shows the maximum batch size supported by LLM fine-tuning on an NVIDIA A40 GPU with 48GB memory. For BlackMamba-S (sparse setup) on the GS dataset (GSM8K), the maximum batch size is 20 samples.","20","samples","[""xia2024""]","is_blank","is_blank","The context states that Table III shows the maximum batch size supported by LLM fine-tuning on an NVIDIA A40 GPU with 48GB memory. For BlackMamba-S (sparse setup) on the GS dataset (GSM8K), the maximum batch size is 20 samples."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context explicitly states that as of 2023, Amazon's energy storage capacity was 1.3 GW, up from 445 MW in 2022. This information is directly provided in the snippet under the 'Carbon-Free Energy' section.","1.3","GW","[""amazon2023""]","is_blank","is_blank","The context explicitly states that as of 2023, Amazon's energy storage capacity was 1.3 GW, up from 445 MW in 2022. This information is directly provided in the snippet under the 'Carbon-Free Energy' section."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that GPT-4o's projected annual carbon emissions are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","is_blank","The context states that GPT-4o's projected annual carbon emissions are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The provided context does not mention the percentage of AI inference workloads in Asia powered by coal in 2023. While several references discuss energy consumption and projections for data centers, none specify coal usage percentages for Asian AI inference workloads.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context does not mention the percentage of AI inference workloads in Asia powered by coal in 2023. While several references discuss energy consumption and projections for data centers, none specify coal usage percentages for Asian AI inference workloads."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context states that the average data center PUE in 2023 was 1.58 globally, which applies to all data centers, including those dedicated to AI.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","The context states that the average data center PUE in 2023 was 1.58 globally, which applies to all data centers, including those dedicated to AI."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context explicitly states that JetMoE-8B has 8B parameters but only activates 2B for each input token during inference. This is mentioned multiple times across different sections of the document.","2B","parameters","[""shen2024""]","is_blank","is_blank","The context explicitly states that JetMoE-8B has 8B parameters but only activates 2B for each input token during inference. This is mentioned multiple times across different sections of the document."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context snippet from ref_id=luccioni2025c includes a table titled 'Range of Pre-Training Environmental Impacts (Representative Models Displayed)' which lists the Llama 7B model with GHG emissions of 14 tCO2e. This directly answers the question about the GHG emissions associated with pre-training the Llama 7B model.","14","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The context snippet from ref_id=luccioni2025c includes a table titled 'Range of Pre-Training Environmental Impacts (Representative Models Displayed)' which lists the Llama 7B model with GHG emissions of 14 tCO2e. This directly answers the question about the GHG emissions associated with pre-training the Llama 7B model."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context provides the number of electric delivery vans in the U.S. for 2022 (2,600) and 2023 (11,800). However, the question asks for the total added across both years. The increase from 2022 to 2023 in the U.S. is 11,800 - 2,600 = 9,200. However, the context does not provide data for other regions' additions in both years, only their 2023 numbers. Therefore, the total added across 2022 and 2023 cannot be determined from the provided context.","is_blank","electric delivery vans","[""is_blank""]","is_blank","is_blank","The context provides the number of electric delivery vans in the U.S. for 2022 (2,600) and 2023 (11,800). However, the question asks for the total added across both years. The increase from 2022 to 2023 in the U.S. is 11,800 - 2,600 = 9,200. However, the context does not provide data for other regions' additions in both years, only their 2023 numbers. Therefore, the total added across 2022 and 2023 cannot be determined from the provided context."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions from energy sources used for model training and deployment, which specifically mention servers and data center cooling. This directly contradicts the statement in the question that these emissions are not included.","0","is_blank","[""morrison2025""]","is_blank","is_blank","The context explicitly states that operational environmental impacts of LLMs include GHG emissions from energy sources used for model training and deployment, which specifically mention servers and data center cooling. This directly contradicts the statement in the question that these emissions are not included."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context includes Table 4 which lists energy consumption for the o3 model under the '10k input-1.5k output' (long prompt) scenario as 12.222 ± 1.082 Wh.","12.222","Wh","[""jegham2025""]","is_blank","is_blank","The context includes Table 4 which lists energy consumption for the o3 model under the '10k input-1.5k output' (long prompt) scenario as 12.222 ± 1.082 Wh."
"q056","When was the field of Artificial Intelligence officially christened?","The context explicitly states that the field of Artificial Intelligence was officially christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. Multiple snippets from the same source (ref_id=stone2022) confirm this date and event.","1956","year","[""stone2022""]","is_blank","is_blank","The context explicitly states that the field of Artificial Intelligence was officially christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. Multiple snippets from the same source (ref_id=stone2022) confirm this date and event."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context mentions that Microsoft's data centers have some of the lowest on-site WUE in the industry and provides specific WUE values for various locations in Table 1. However, the question specifically asks about Google's AI-dedicated data centers in 2024, which are not mentioned in the provided context. The context primarily discusses Microsoft's data centers and GPT-3, not Google's.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The context mentions that Microsoft's data centers have some of the lowest on-site WUE in the industry and provides specific WUE values for various locations in Table 1. However, the question specifically asks about Google's AI-dedicated data centers in 2024, which are not mentioned in the provided context. The context primarily discusses Microsoft's data centers and GPT-3, not Google's."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context explicitly states that approximately 770 million people do not have access to a stable supply of electricity, citing the International Energy Agency.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context explicitly states that approximately 770 million people do not have access to a stable supply of electricity, citing the International Energy Agency."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context explicitly states that for LLaMA-65B with a maximum generation length of 512 tokens, it takes about 3-4 Joules per output token. This is mentioned in the section discussing energy per token comparisons between different generation lengths.","3-4","joules per token","[""samsi2024""]","is_blank","is_blank","The context explicitly states that for LLaMA-65B with a maximum generation length of 512 tokens, it takes about 3-4 Joules per output token. This is mentioned in the section discussing energy per token comparisons between different generation lengths."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context snippet from ref_id=wu2021a explicitly states that converting the numerical representation from 32-bit to 16-bit reduced the overall RM2 model size by 15%.","15","percent","[""wu2021a""]","is_blank","is_blank","The context snippet from ref_id=wu2021a explicitly states that converting the numerical representation from 32-bit to 16-bit reduced the overall RM2 model size by 15%."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","","","is_blank","[]","is_blank","is_blank",""
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context from wu2021a states that training large, sparsely-activated neural networks achieves higher accuracy at lower operational energy footprint. Additionally, shen2024 mentions that JetMoE-8B, which uses sparse activation, reduces inference computation by about 70% compared to Llama2-7B. These points support the claim that sparsely activated DNNs consume significantly less energy without sacrificing accuracy.","1","is_blank","[""wu2021a"", ""shen2024""]","is_blank","is_blank","The context from wu2021a states that training large, sparsely-activated neural networks achieves higher accuracy at lower operational energy footprint. Additionally, shen2024 mentions that JetMoE-8B, which uses sparse activation, reduces inference computation by about 70% compared to Llama2-7B. These points support the claim that sparsely activated DNNs consume significantly less energy without sacrificing accuracy."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000.","25000","USD","[""schwartz2019""]","is_blank","is_blank","The context explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context explicitly states that the optimizer stage in BlackMamba sparse fine-tuning takes up to 53% of the running time when the batch size is 1.","53","percent","[""xia2024""]","is_blank","is_blank","The context explicitly states that the optimizer stage in BlackMamba sparse fine-tuning takes up to 53% of the running time when the batch size is 1."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","","","MWh","[]","is_blank","is_blank",""
"q067","What was the average global data center PUE in 2023?","The context snippet from ref_id=ebert2024 explicitly states that the average data center PUE in 2023 was 1.58 globally, which directly answers the question.","1.58","PUE","[""ebert2024""]","is_blank","is_blank","The context snippet from ref_id=ebert2024 explicitly states that the average data center PUE in 2023 was 1.58 globally, which directly answers the question."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The context snippets from Amazon's 2023 report and other documents discuss renewable energy projects and wind turbines, but there is no mention of Microsoft contracting wind turbines for Azure AI clusters in 2023. The question specifically asks about Microsoft's direct contracts, which are not addressed in the provided context.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The context snippets from Amazon's 2023 report and other documents discuss renewable energy projects and wind turbines, but there is no mention of Microsoft contracting wind turbines for Azure AI clusters in 2023. The question specifically asks about Microsoft's direct contracts, which are not addressed in the provided context."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context explicitly states that Gemini Ultra has the highest fraction of R&D staff cost at 49% when equity is included, which directly answers the question about the percentage attributed to R&D staff in the total development costs.","49","percent","[""cottier2024""]","is_blank","is_blank","The context explicitly states that Gemini Ultra has the highest fraction of R&D staff cost at 49% when equity is included, which directly answers the question about the percentage attributed to R&D staff in the total development costs."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context explicitly states that the inaugural 2015 Study Panel of the One Hundred Year Study on AI was composed of seventeen members. This information is found in multiple instances within the provided snippets from the document with ref_id=stone2022.","17","people","[""stone2022""]","is_blank","is_blank","The context explicitly states that the inaugural 2015 Study Panel of the One Hundred Year Study on AI was composed of seventeen members. This information is found in multiple instances within the provided snippets from the document with ref_id=stone2022."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context snippet from ref_id=wu2021a states that 'manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.' This directly answers the question by providing the percentage of the total carbon footprint attributed to manufacturing.","74","percent","[""wu2021a""]","is_blank","is_blank","The context snippet from ref_id=wu2021a states that 'manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.' This directly answers the question by providing the percentage of the total carbon footprint attributed to manufacturing."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context states that while models with more parameters generally consume more energy, this is not always the case. For example, the Phi-3 Small (7B) model can consume less energy than the Phi-3 Mini (3.8B) under certain batch sizes due to architectural differences affecting memory usage and batch size scalability.","0","is_blank","[""chung2025""]","is_blank","is_blank","The context states that while models with more parameters generally consume more energy, this is not always the case. For example, the Phi-3 Small (7B) model can consume less energy than the Phi-3 Mini (3.8B) under certain batch sizes due to architectural differences affecting memory usage and batch size scalability."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, directly contradicting the notion that they are concerned about it.","0","is_blank","[""stone2022""]","is_blank","is_blank","The context explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind, directly contradicting the notion that they are concerned about it."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The provided context does not mention any specific CO2 emissions from OpenAI's API requests in January 2024. The morrison2025 paper discusses emissions from OLMo model training and inference in general terms, while jegham2025 mentions GPT-4o's emissions at scale but without specific January 2024 figures. The dodge2022 paper focuses on measurement frameworks rather than OpenAI's emissions.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The provided context does not mention any specific CO2 emissions from OpenAI's API requests in January 2024. The morrison2025 paper discusses emissions from OLMo model training and inference in general terms, while jegham2025 mentions GPT-4o's emissions at scale but without specific January 2024 figures. The dodge2022 paper focuses on measurement frameworks rather than OpenAI's emissions."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context snippet from ref_id=luccioni2025c explicitly states that Meta's Llama 3 family of models emitted 11,390 tons CO2e during pre-training, which is over 40 times the 'five cars' estimate. This directly answers both parts of the question.","11390","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The context snippet from ref_id=luccioni2025c explicitly states that Meta's Llama 3 family of models emitted 11,390 tons CO2e during pre-training, which is over 40 times the 'five cars' estimate. This directly answers both parts of the question."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context explicitly states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over the 1.5-year period from Yr1-Q1 to Yr2-Q2 between 2019 and 2021. This is directly mentioned in multiple parts of the provided context snippets from ref_id=wu2021a.","2.9","multiplier","[""wu2021a""]","is_blank","is_blank","The context explicitly states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over the 1.5-year period from Yr1-Q1 to Yr2-Q2 between 2019 and 2021. This is directly mentioned in multiple parts of the provided context snippets from ref_id=wu2021a."
"q079","How many miles is the Earth from the Sun?","The provided context snippets do not mention the Earth's distance from the Sun. The context focuses on topics like energy consumption of AI models, emissions from car travel between LA and NYC, and data center electricity usage. There is no information about the Earth-Sun distance in miles.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The provided context snippets do not mention the Earth's distance from the Sun. The context focuses on topics like energy consumption of AI models, emissions from car travel between LA and NYC, and data center electricity usage. There is no information about the Earth-Sun distance in miles."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context explicitly states that AlphaGo, developed by Google Deepmind, defeated the human Go champion in a five-game match. Additionally, it mentions that the AlphaGo program used machine learning algorithms and reinforcement learning to achieve this victory.","1","is_blank","[""stone2022""]","is_blank","is_blank","The context explicitly states that AlphaGo, developed by Google Deepmind, defeated the human Go champion in a five-game match. Additionally, it mentions that the AlphaGo program used machine learning algorithms and reinforcement learning to achieve this victory."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context explicitly states that continuous batching mitigates GPU under-utilization by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time. This directly answers the question about the batching strategy's name.","Continuous batching","is_blank","[""fernandez2025""]","is_blank","is_blank","The context explicitly states that continuous batching mitigates GPU under-utilization by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time. This directly answers the question about the batching strategy's name."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours. This is directly mentioned in the section describing the dDPO fine-tuning process.","60","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The context states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours. This is directly mentioned in the section describing the dDPO fine-tuning process."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","Table VI in the context shows that for the 100 TPS offline workload, Max-Performance (g6e.xlarge) had a total price of $2.699 while InferSave-1st (g4dn.xlarge) cost $2.13. The percentage increase is calculated as ((2.699/2.13)-1)*100 ≈ 26.7%.","26.7","percent","[""kim2025""]","is_blank","is_blank","Table VI in the context shows that for the 100 TPS offline workload, Max-Performance (g6e.xlarge) had a total price of $2.699 while InferSave-1st (g4dn.xlarge) cost $2.13. The percentage increase is calculated as ((2.699/2.13)-1)*100 ≈ 26.7%."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context snippet from ref_id=luccioni2024 explicitly states that the stable-diffusion-xl-base-1.0 model generates 1,594 grams of CO2eq for 1,000 inferences.","1594","g CO2eq","[""luccioni2024""]","is_blank","is_blank","The context snippet from ref_id=luccioni2024 explicitly states that the stable-diffusion-xl-base-1.0 model generates 1,594 grams of CO2eq for 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context snippet from ref_id=luccioni2025c explicitly states that GPU energy usage for 1,000 queries ranges from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), as listed in the study's appendix tables.","0.06-3426","Wh","[""luccioni2025c""]","is_blank","is_blank","The context snippet from ref_id=luccioni2025c explicitly states that GPU energy usage for 1,000 queries ranges from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), as listed in the study's appendix tables."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context explicitly states, 'There is no one-size-ﬁts-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.' This directly contradicts the idea of a universal approach to AI ethics and sustainability.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context explicitly states, 'There is no one-size-ﬁts-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.' This directly contradicts the idea of a universal approach to AI ethics and sustainability."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context snippet from ref_id=luccioni2024 states that the average carbon intensity of the AWS us-west-2 region (Oregon) in 2021 was 297.6 grams of CO2eq per kWh. This directly provides the carbon intensity value for the U.S. average mix in the specified year.","297.6","kg of CO2e/KWh","[""luccioni2024""]","is_blank","is_blank","The context snippet from ref_id=luccioni2024 states that the average carbon intensity of the AWS us-west-2 region (Oregon) in 2021 was 297.6 grams of CO2eq per kWh. This directly provides the carbon intensity value for the U.S. average mix in the specified year."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context mentions Hivemind as a PyTorch-based framework designed for collaborative and decentralized deep learning training, specifically enabling distributed training across heterogeneous hardware and handling peer dropouts, which aligns with the requirements for distributed spot instance training across clouds and continents.","Hivemind","is_blank","[""erben2023""]","is_blank","is_blank","The context mentions Hivemind as a PyTorch-based framework designed for collaborative and decentralized deep learning training, specifically enabling distributed training across heterogeneous hardware and handling peer dropouts, which aligns with the requirements for distributed spot instance training across clouds and continents."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context mentions that the notion of transparency in AI can be expanded to encompass 'social transparency', which integrates socio-technical aspects and includes the societal and environmental footprint of AI systems. This is explicitly stated in the provided snippets from ref_id=luccioni2025b.","Social transparency","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context mentions that the notion of transparency in AI can be expanded to encompass 'social transparency', which integrates socio-technical aspects and includes the societal and environmental footprint of AI systems. This is explicitly stated in the provided snippets from ref_id=luccioni2025b."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context states that for emotion classification, the linear model with sentence embeddings is among the top-performing models. Additionally, it explicitly mentions that 'The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings' in the FKTG dataset experiments.","traditional linear model using pre-trained sentence embeddings","is_blank","[""zschache2025""]","is_blank","is_blank","The context states that for emotion classification, the linear model with sentence embeddings is among the top-performing models. Additionally, it explicitly mentions that 'The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings' in the FKTG dataset experiments."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context mentions that the LLM inference system developed in the Chen et al. paper is called Lamina, which incorporates model-attention disaggregation. The system is specifically named in the context snippets with ref_id=chen2024.","Lamina","is_blank","[""chen2024""]","is_blank","is_blank","The context mentions that the LLM inference system developed in the Chen et al. paper is called Lamina, which incorporates model-attention disaggregation. The system is specifically named in the context snippets with ref_id=chen2024."
"q093","How many parameters does the largest T5 model have?","The context from morrison2025 states that the OLMo series of transformer language models range in size from 20 million to 13 billion active parameters. The largest model mentioned is the 13 billion parameter model.","13B","parameters","[""morrison2025""]","is_blank","is_blank","The context from morrison2025 states that the OLMo series of transformer language models range in size from 20 million to 13 billion active parameters. The largest model mentioned is the 13 billion parameter model."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context explicitly states in Table 1 of the hyperparameters section that JetMoE-8B has a Ptotal of 8B parameters. Additionally, multiple sections mention that the model has 8B parameters while activating only 2B per token.","8B","parameters","[""shen2024""]","is_blank","is_blank","The context explicitly states in Table 1 of the hyperparameters section that JetMoE-8B has a Ptotal of 8B parameters. Additionally, multiple sections mention that the model has 8B parameters while activating only 2B per token."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context states that Google's data center water consumption increased by ∼20% from 2021 to 2022 according to ref_id=li2025b. Additionally, ref_id=luccioni2025a corroborates this with a 20% uptick reported by Google in the same period.","20","percent","[""li2025b"", ""luccioni2025a""]","is_blank","is_blank","The context states that Google's data center water consumption increased by ∼20% from 2021 to 2022 according to ref_id=li2025b. Additionally, ref_id=luccioni2025a corroborates this with a 20% uptick reported by Google in the same period."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context snippet from ref_id=khan2025's Table I explicitly defines 'Carbon Intensity' as 'CO₂ emissions per unit of electricity consumed' with the unit 'gCO2/kWh'. This matches the description in the question.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","is_blank","The context snippet from ref_id=khan2025's Table I explicitly defines 'Carbon Intensity' as 'CO₂ emissions per unit of electricity consumed' with the unit 'gCO2/kWh'. This matches the description in the question."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context includes Table 2 which lists the FLOPs utilization percentages for different growth stages of FLM-101B training. The final stage (101B parameters) shows a FLOPs utilization of 52.88%.","52.88","percent","[""li2025a""]","is_blank","is_blank","The context includes Table 2 which lists the FLOPs utilization percentages for different growth stages of FLM-101B training. The final stage (101B parameters) shows a FLOPs utilization of 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context from ref_id=cottier2024 explicitly states that the amortized hardware CapEx + energy cost for training GPT-4 is $40 million. Another context from ref_id=xia2024 mentions the training cost exceeding $100 million, but this is not specified as amortized. The question specifically asks for the estimated amortized training costs, which is $40 million according to Cottier et al.","40000000","USD","[""cottier2024""]","is_blank","is_blank","The context from ref_id=cottier2024 explicitly states that the amortized hardware CapEx + energy cost for training GPT-4 is $40 million. Another context from ref_id=xia2024 mentions the training cost exceeding $100 million, but this is not specified as amortized. The question specifically asks for the estimated amortized training costs, which is $40 million according to Cottier et al."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context states that optimization across algorithms, platforms, infrastructures, and hardware can reduce the operational carbon footprint for the Transformer-based universal translation model by 810×. Additionally, Figure 7 in the context explicitly shows an 810× reduction in operational power footprint through platform-level caching, GPU accelerators, and algorithmic optimization.","810","multiplier","[""wu2021a""]","is_blank","is_blank","The context states that optimization across algorithms, platforms, infrastructures, and hardware can reduce the operational carbon footprint for the Transformer-based universal translation model by 810×. Additionally, Figure 7 in the context explicitly shows an 810× reduction in operational power footprint through platform-level caching, GPU accelerators, and algorithmic optimization."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context mentions that when training was spread across four continents (C-8 experiment), the NLP task experienced a performance drop of 41% compared to the fully local experiment (A-8). This indicates that the throughput achieved was 59% (100% - 41%) of the local throughput.","0.59","multiplier","[""erben2023""]","is_blank","is_blank","The context mentions that when training was spread across four continents (C-8 experiment), the NLP task experienced a performance drop of 41% compared to the fully local experiment (A-8). This indicates that the throughput achieved was 59% (100% - 41%) of the local throughput."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context explicitly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023. This figure is mentioned multiple times across different sections of the document.","3500000000","liters","[""amazon2023""]","is_blank","is_blank","The context explicitly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023. This figure is mentioned multiple times across different sections of the document."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context states that using custom tags in prompts (including zero-shot, one-shot, and few-shots techniques) reduces energy consumption of LLMs during code completion tasks. Specifically, the study found energy consumption reductions across all three PETs when using custom tags.","1","is_blank","[""rubei2025""]","is_blank","is_blank","The context states that using custom tags in prompts (including zero-shot, one-shot, and few-shots techniques) reduces energy consumption of LLMs during code completion tasks. Specifically, the study found energy consumption reductions across all three PETs when using custom tags."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context snippet from ref_id=luccioni2025a explicitly states that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million units higher than the 2023 figure of 3.76 million. This directly answers the question about the number of data center GPUs shipped in 2024 as reported in a 2025 paper.","3700000","GPUs","[""luccioni2025a""]","is_blank","is_blank","The context snippet from ref_id=luccioni2025a explicitly states that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million units higher than the 2023 figure of 3.76 million. This directly answers the question about the number of data center GPUs shipped in 2024 as reported in a 2025 paper."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context states that on average, 44% of the total amortized hardware and energy cost is attributed to AI accelerator chips. This is explicitly mentioned in the section titled '3.4 Half of amortized hardware CapEx + energy cost is for AI accelerator chips'.","44","percent","[""cottier2024""]","is_blank","is_blank","The context states that on average, 44% of the total amortized hardware and energy cost is attributed to AI accelerator chips. This is explicitly mentioned in the section titled '3.4 Half of amortized hardware CapEx + energy cost is for AI accelerator chips'."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context explicitly states that Facebook's data centers have a Power Usage Effectiveness (PUE) of 1.10. This is mentioned in multiple references, including [wu2021a] and [wu2021b], which both cite the same PUE value for Facebook's data centers.","1.10","PUE","[""wu2021a"", ""wu2021b""]","is_blank","is_blank","The context explicitly states that Facebook's data centers have a Power Usage Effectiveness (PUE) of 1.10. This is mentioned in multiple references, including [wu2021a] and [wu2021b], which both cite the same PUE value for Facebook's data centers."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context mentions the Finnish ETAIROS project, which stands for Ethical AI for the Governance of the Society, and explicitly states that it proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context mentions the Finnish ETAIROS project, which stands for Ethical AI for the Governance of the Society, and explicitly states that it proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context states that the most expensive publicly-announced training runs include Google's Gemini Ultra at $30M. This figure is explicitly mentioned in the document as the estimated training cost using the amortized hardware CapEx + energy approach.","30000000","USD","[""cottier2024""]","is_blank","is_blank","The context states that the most expensive publicly-announced training runs include Google's Gemini Ultra at $30M. This figure is explicitly mentioned in the document as the estimated training cost using the amortized hardware CapEx + energy approach."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context states that the AI Act mandates risk assessments for providers of GPAI models with systemic risk but notes that these provisions lack sufficient emphasis on environmental factors. However, it also argues that environmental risks should be included in these assessments based on the Act's objectives. The Act requires risk assessments but does not explicitly mandate environmental risk inclusion, leading to ambiguity.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context states that the AI Act mandates risk assessments for providers of GPAI models with systemic risk but notes that these provisions lack sufficient emphasis on environmental factors. However, it also argues that environmental risks should be included in these assessments based on the Act's objectives. The Act requires risk assessments but does not explicitly mandate environmental risk inclusion, leading to ambiguity."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context explicitly states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, which is referenced multiple times in the provided snippets.","9","µg/m³","[""han2024""]","is_blank","is_blank","The context explicitly states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, which is referenced multiple times in the provided snippets."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context snippet [ref_id=luccioni2025a] explicitly states that a life cycle assessment found 115 books produce the same amount of CO2 as a single Amazon Kindle device.","115","books","[""luccioni2025a""]","is_blank","is_blank","The context snippet [ref_id=luccioni2025a] explicitly states that a life cycle assessment found 115 books produce the same amount of CO2 as a single Amazon Kindle device."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context states that in disadvantaged communities, the per-household health burden from air pollutants could be 200 times more than in less-impacted communities. This is explicitly mentioned in multiple sections of the provided document snippets from ref_id=han2024.","200","multiplier","[""han2024""]","is_blank","is_blank","The context states that in disadvantaged communities, the per-household health burden from air pollutants could be 200 times more than in less-impacted communities. This is explicitly mentioned in multiple sections of the provided document snippets from ref_id=han2024."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The context snippet from ref_id=zschache2025 in Table B1 lists the energy consumption for DS Llama 70B as 702.06 Wh for the inference task on the FKTG dataset.","702.06","Wh","[""zschache2025""]","is_blank","is_blank","The context snippet from ref_id=zschache2025 in Table B1 lists the energy consumption for DS Llama 70B as 702.06 Wh for the inference task on the FKTG dataset."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context mentions several models and their parameter counts, including BERT-large (350 million), openGPT2-XL (1.5 billion), Grover (1.5 billion), and OLMo models up to 13 billion parameters. However, none of these are explicitly linked to a 2022 paper by Dodge et al. The provided context does not reference a 2022 paper by Dodge et al. discussing a large language model's parameters.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context mentions several models and their parameter counts, including BERT-large (350 million), openGPT2-XL (1.5 billion), Grover (1.5 billion), and OLMo models up to 13 billion parameters. However, none of these are explicitly linked to a 2022 paper by Dodge et al. The provided context does not reference a 2022 paper by Dodge et al. discussing a large language model's parameters."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context mentions Jevons' Paradox multiple times, which describes how technological progress improving efficiency can lead to increased overall resource consumption. For example, the context states that 'these gains can result in higher overall consumption due to effects such as Jevons Paradox' and 'when a resource’s efficiency increases, overall consumption of that resource tends to increase, rather than decrease.'","Jevons' Paradox","is_blank","[""luccioni2025a"", ""morrison2025""]","is_blank","is_blank","The context mentions Jevons' Paradox multiple times, which describes how technological progress improving efficiency can lead to increased overall resource consumption. For example, the context states that 'these gains can result in higher overall consumption due to effects such as Jevons Paradox' and 'when a resource’s efficiency increases, overall consumption of that resource tends to increase, rather than decrease.'"
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context provides the energy consumption for GPT-3 (1287 MWh) and mentions that training Meena has a carbon footprint equivalent to 242,231 miles driven by a car. However, there is no direct comparison of energy consumption between Meena and GPT-3 training runs. The energy data for Meena is not provided, making it impossible to calculate the multiplier.","is_blank","multiplier","[""is_blank""]","is_blank","is_blank","The context provides the energy consumption for GPT-3 (1287 MWh) and mentions that training Meena has a carbon footprint equivalent to 242,231 miles driven by a car. However, there is no direct comparison of energy consumption between Meena and GPT-3 training runs. The energy data for Meena is not provided, making it impossible to calculate the multiplier."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","The context provides Table 2 from the 2024 study by Luccioni et al., which lists the mean energy consumption for image generation as 2.907 kWh per 1,000 inferences. This value is explicitly stated in the table and supporting text.","2.907","kWh","[""luccioni2024""]","is_blank","is_blank","The context provides Table 2 from the 2024 study by Luccioni et al., which lists the mean energy consumption for image generation as 2.907 kWh per 1,000 inferences. This value is explicitly stated in the table and supporting text."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context snippet from ref_id=strubell2019 explicitly states that the estimated CO2e for an average American life in one year is 36,156 lbs.","36156","lbs","[""strubell2019""]","is_blank","is_blank","The context snippet from ref_id=strubell2019 explicitly states that the estimated CO2e for an average American life in one year is 36,156 lbs."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context includes a table (Figure 6c) listing the top-10 counties by per-household health cost from 2019 to 2023, where Marion County, WV has the highest value at $1218.3. However, the question asks about 2030 projections, and the provided context only discusses projections up to 2028. The 2028 projections do not specify county-level data, making the 2030 answer unsupported.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context includes a table (Figure 6c) listing the top-10 counties by per-household health cost from 2019 to 2023, where Marion County, WV has the highest value at $1218.3. However, the question asks about 2030 projections, and the provided context only discusses projections up to 2028. The 2028 projections do not specify county-level data, making the 2030 answer unsupported."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context from ref_id=khan2025's Table III shows Mistral-small's CO2 emissions per inference task decreased from 0.020 kg before optimization to 0.015 kg after optimization. The multiplier is calculated as 0.015/0.020 = 0.75.","0.75","multiplier","[""khan2025""]","is_blank","is_blank","The context from ref_id=khan2025's Table III shows Mistral-small's CO2 emissions per inference task decreased from 0.020 kg before optimization to 0.015 kg after optimization. The multiplier is calculated as 0.015/0.020 = 0.75."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context from ref_id=ebert2024 and ref_id=luccioni2024 provides specific energy consumption figures for training and fine-tuning the BLOOMz-7B model. The training energy is 51,686 kWh and the fine-tuning energy is 7,571 kWh. Adding these together gives the combined total energy cost.","59257","kWh","[""ebert2024"", ""luccioni2024""]","is_blank","is_blank","The context from ref_id=ebert2024 and ref_id=luccioni2024 provides specific energy consumption figures for training and fine-tuning the BLOOMz-7B model. The training energy is 51,686 kWh and the fine-tuning energy is 7,571 kWh. Adding these together gives the combined total energy cost."
"q125","What is the total number of parameters in the final FLM-101B model?","The context explicitly states that the FLM-101B model is structured with a hidden state dimension of 10,240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100,256. However, none of the provided context snippets mention the total number of parameters directly. While the model's name suggests '101B', which could imply 101 billion parameters, the context does not explicitly confirm this number. The tables and discussions focus on computational costs, performance comparisons, and model configurations but do not provide the exact parameter count.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context explicitly states that the FLM-101B model is structured with a hidden state dimension of 10,240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100,256. However, none of the provided context snippets mention the total number of parameters directly. While the model's name suggests '101B', which could imply 101 billion parameters, the context does not explicitly confirm this number. The tables and discussions focus on computational costs, performance comparisons, and model configurations but do not provide the exact parameter count."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context provides the training energy for a 6.1B parameter model (103,500 kWh) from ref_id=dodge2022 and the inference energy (1.0 × 10⁻⁴ kWh) and cost parity (592,570,000 inferences) for BLOOMz-7B from ref_id=luccioni2024. Using the BLOOMz-7B's inference energy, the number of inferences needed to match the 6.1B model's training energy is calculated as 103,500 kWh / 1.0 × 10⁻⁴ kWh per inference = 1,035,000,000 inferences.","1035000000","inferences","[""dodge2022"", ""luccioni2024""]","is_blank","is_blank","The context provides the training energy for a 6.1B parameter model (103,500 kWh) from ref_id=dodge2022 and the inference energy (1.0 × 10⁻⁴ kWh) and cost parity (592,570,000 inferences) for BLOOMz-7B from ref_id=luccioni2024. Using the BLOOMz-7B's inference energy, the number of inferences needed to match the 6.1B model's training energy is calculated as 103,500 kWh / 1.0 × 10⁻⁴ kWh per inference = 1,035,000,000 inferences."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context explicitly states that the total energy consumed for all model experimentation and evaluation in the study was 754.66 kWh. This information is found in the 'ETHICAL CONSIDERATIONS STATEMENT' section of the provided document snippets.","754.66","kWh","[""luccioni2024""]","is_blank","is_blank","The context explicitly states that the total energy consumed for all model experimentation and evaluation in the study was 754.66 kWh. This information is found in the 'ETHICAL CONSIDERATIONS STATEMENT' section of the provided document snippets."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides a table (Table 5) from the study by Luccioni et al. (2024) that lists the 'Cost parity (# inferences)' for BLOOMz-7B as 592,570,000. This value represents the number of inferences required for the cumulative deployment energy to equal the initial training and fine-tuning costs.","592570000","inferences","[""luccioni2024""]","is_blank","is_blank","The context provides a table (Table 5) from the study by Luccioni et al. (2024) that lists the 'Cost parity (# inferences)' for BLOOMz-7B as 592,570,000. This value represents the number of inferences required for the cumulative deployment energy to equal the initial training and fine-tuning costs."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context mentions that the dataset used for classifying German nuclear waste site objections is called the FKTG-dataset, which was published after processing statements from the population during the repository site selection process.","FKTG-dataset","is_blank","[""zschache2025""]","is_blank","is_blank","The context mentions that the dataset used for classifying German nuclear waste site objections is called the FKTG-dataset, which was published after processing statements from the population during the repository site selection process."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context discusses water consumption related to AI models, including GPT-3 and Llama-3-70B. It mentions that Llama-3-70B's inference server energy consumption is approximately 0.010 kWh per request, but does not provide specific freshwater consumption figures for Meta's Llama 3 inference clusters in 2024.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context discusses water consumption related to AI models, including GPT-3 and Llama-3-70B. It mentions that Llama-3-70B's inference server energy consumption is approximately 0.010 kWh per request, but does not provide specific freshwater consumption figures for Meta's Llama 3 inference clusters in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context mentions that NVIDIA does not release information about the use of recycled rare earth metals in their GPUs. The provided snippets only discuss assumptions and estimates about the percentage of rare earth metals in H100 GPUs (0.1% by mass) and the environmental impact of mining them, but there is no information about the use of recycled materials in 2024 manufacturing.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context mentions that NVIDIA does not release information about the use of recycled rare earth metals in their GPUs. The provided snippets only discuss assumptions and estimates about the percentage of rare earth metals in H100 GPUs (0.1% by mass) and the environmental impact of mining them, but there is no information about the use of recycled materials in 2024 manufacturing."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context from strubell2019 states that training a model with neural architecture search emits 626,155 lbs of CO2e (equivalent to 3.2 tCO2e). It also provides that one passenger's round trip between NY and SF emits 1984 lbs CO2e. Converting 3.2 tCO2e to lbs (7054.8 lbs) and dividing by 1984 gives approximately 3.55 passengers.","3.5","passengers","[""strubell2019""]","is_blank","is_blank","The context from strubell2019 states that training a model with neural architecture search emits 626,155 lbs of CO2e (equivalent to 3.2 tCO2e). It also provides that one passenger's round trip between NY and SF emits 1984 lbs CO2e. Converting 3.2 tCO2e to lbs (7054.8 lbs) and dividing by 1984 gives approximately 3.55 passengers."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context from ref_id=luccioni2025c explicitly states that in May 2025 data from OpenRouter28, 84% of LLM token usage occurred through models with no disclosure of environmental impact. This directly answers the question about the percentage of token usage through non-disclosing models.","84","percent","[""luccioni2025c""]","is_blank","is_blank","The context from ref_id=luccioni2025c explicitly states that in May 2025 data from OpenRouter28, 84% of LLM token usage occurred through models with no disclosure of environmental impact. This directly answers the question about the percentage of token usage through non-disclosing models."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states in Table II that for the LLaMA 13B model, the bare minimum hardware required is 1 A100 80GB GPU. This table specifically lists the GPU count needed for different model sizes without compression or quantization.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","The context states in Table II that for the LLaMA 13B model, the bare minimum hardware required is 1 A100 80GB GPU. This table specifically lists the GPU count needed for different model sizes without compression or quantization."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","","","metric tons","[]","is_blank","is_blank",""
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context mentions that quantization can reduce energy consumption and carbon emissions by up to 45% post quantization. However, it does not provide a specific total amount of carbon emissions avoided in 2023, only a percentage reduction.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context mentions that quantization can reduce energy consumption and carbon emissions by up to 45% post quantization. However, it does not provide a specific total amount of carbon emissions avoided in 2023, only a percentage reduction."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context explicitly states that using a mix of 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy. This information is directly provided in the 'Mixing GPU Types' section of the document referenced as griggs2024.","24","percent","[""griggs2024""]","is_blank","is_blank","The context explicitly states that using a mix of 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy. This information is directly provided in the 'Mixing GPU Types' section of the document referenced as griggs2024."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context from Chen et al. (2024) includes a table listing the price per chip for NVIDIA H20 as $4.63/hr, with a note indicating it's an estimate based on system cost comparisons.","4.63","USD per hour","[""chen2024""]","is_blank","is_blank","The context from Chen et al. (2024) includes a table listing the price per chip for NVIDIA H20 as $4.63/hr, with a note indicating it's an estimate based on system cost comparisons."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context states that 'most carbon footprint analyses gather the information manually by writing to authors', which directly contradicts the claim that they gather information automatically without contacting authors. The example of Luccioni and Hernandez-Garcia reaching out to over 500 authors further supports this manual process.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context states that 'most carbon footprint analyses gather the information manually by writing to authors', which directly contradicts the claim that they gather information automatically without contacting authors. The example of Luccioni and Hernandez-Garcia reaching out to over 500 authors further supports this manual process."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context states that in 2023, the U.S. data centers' total public health cost was equivalent to approximately 44% of their total electricity cost using the average attribution method.","44","percent","[""han2024""]","is_blank","is_blank","The context states that in 2023, the U.S. data centers' total public health cost was equivalent to approximately 44% of their total electricity cost using the average attribution method."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states in Table II that for the A100 80GB GPU, the bare minimum hardware required for LLaMA-7B is 1 GPU. This is explicitly listed under the 'A100 80GB' column with 'Count' as 1 for the 7B model size.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","is_blank","The context states in Table II that for the A100 80GB GPU, the bare minimum hardware required for LLaMA-7B is 1 GPU. This is explicitly listed under the 'A100 80GB' column with 'Count' as 1 for the 7B model size."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context states that experimental results reveal that methods like strategic quantization and local inference techniques can reduce energy consumption and carbon emissions by up to 45% post quantization.","1","is_blank","[""khan2025""]","is_blank","is_blank","The context states that experimental results reveal that methods like strategic quantization and local inference techniques can reduce energy consumption and carbon emissions by up to 45% post quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context states that researchers reached out to over 500 authors and collected 95 answers, as mentioned in both ref_id=luccioni2025b and ref_id=luccioni2023. This directly answers the question about the number of responses received.","95","answers","[""luccioni2025b"", ""luccioni2023""]","is_blank","is_blank","The context states that researchers reached out to over 500 authors and collected 95 answers, as mentioned in both ref_id=luccioni2025b and ref_id=luccioni2023. This directly answers the question about the number of responses received."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context states that JetMoE-8B was trained with a $100k budget using 30,000 H100 GPU hours. To calculate the cost per GPU-hour, divide the total budget by the total GPU hours: $100,000 / 30,000 hours = approximately $3.33 per H100 GPU-hour.","3.33","USD per hour","[""shen2024""]","is_blank","is_blank","The context states that JetMoE-8B was trained with a $100k budget using 30,000 H100 GPU hours. To calculate the cost per GPU-hour, divide the total budget by the total GPU hours: $100,000 / 30,000 hours = approximately $3.33 per H100 GPU-hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context mentions that when training a Llama-3.1 scale model in Altoona, Iowa, the health cost is 122% of the electricity cost. This is explicitly stated in Table 2 under the Altoona, IA entry.","122","percent","[""han2024""]","is_blank","is_blank","The context mentions that when training a Llama-3.1 scale model in Altoona, Iowa, the health cost is 122% of the electricity cost. This is explicitly stated in Table 2 under the Altoona, IA entry."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context explicitly states in multiple sections that JetMoE-8B was trained on 1.25T tokens. Specifically, under '4.3 Training Data Mixture' and in the abstract, it mentions the model's training data as 1.25T tokens from open-source corpora.","1.25T","tokens","[""shen2024""]","is_blank","is_blank","The context explicitly states in multiple sections that JetMoE-8B was trained on 1.25T tokens. Specifically, under '4.3 Training Data Mixture' and in the abstract, it mentions the model's training data as 1.25T tokens from open-source corpora."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context includes a table titled 'Amazon Renewable Energy Projects* Projects announced as of January 2024' which lists the United Kingdom with 36 projects.","36","projects","[""amazon2023""]","is_blank","is_blank","The context includes a table titled 'Amazon Renewable Energy Projects* Projects announced as of January 2024' which lists the United Kingdom with 36 projects."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context includes a table under 'Amazon Workforce (All Levels)' for 2023 in the U.S., which shows Men at 31.1% and Women at 68.8%. This directly answers the question about the percentage of men in Amazon's U.S. workforce in 2023.","31.1","percent","[""amazon2023""]","is_blank","is_blank","The context includes a table under 'Amazon Workforce (All Levels)' for 2023 in the U.S., which shows Men at 31.1% and Women at 68.8%. This directly answers the question about the percentage of men in Amazon's U.S. workforce in 2023."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context snippet from ref_id=li2025b explicitly states that Apple reports its supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","is_blank","is_blank","The context snippet from ref_id=li2025b explicitly states that Apple reports its supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context mentions that for BlackMamba sparse fine-tuning with a batch size of 84, the execution time breakdown is shown in Figure 4. The figure indicates that the total execution time for sparse BlackMamba with batch size 84 is approximately 2.0 seconds.","2.0","seconds","[""xia2024""]","is_blank","is_blank","The context mentions that for BlackMamba sparse fine-tuning with a batch size of 84, the execution time breakdown is shown in Figure 4. The figure indicates that the total execution time for sparse BlackMamba with batch size 84 is approximately 2.0 seconds."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context introduces the 'granularity metric' as the ratio of calculation to communication time to assess model suitability for distributed training across different hardware setups, including geo-distributed scenarios.","granularity metric","is_blank","[""erben2023""]","is_blank","is_blank","The context introduces the 'granularity metric' as the ratio of calculation to communication time to assess model suitability for distributed training across different hardware setups, including geo-distributed scenarios."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context states that a Microsoft employee coalition estimated the Exxon Mobil deal could add up to 640% more carbon emissions compared to Microsoft's carbon removal targets. This percentage directly answers the question about the multiplier effect.","640","times","[""luccioni2025a"", ""luccioni2025b""]","is_blank","is_blank","The context states that a Microsoft employee coalition estimated the Exxon Mobil deal could add up to 640% more carbon emissions compared to Microsoft's carbon removal targets. This percentage directly answers the question about the multiplier effect."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context defines 'water withdrawal' as freshwater taken from ground or surface sources, either temporarily or permanently, for various uses such as agricultural, industrial, or municipal purposes. This directly matches the question's description.","Water withdrawal","is_blank","[""li2025b""]","is_blank","is_blank","The context defines 'water withdrawal' as freshwater taken from ground or surface sources, either temporarily or permanently, for various uses such as agricultural, industrial, or municipal purposes. This directly matches the question's description."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that the Standing Committee forms a Study Panel every five years as part of its core activity for the One Hundred Year Study on AI. This is explicitly mentioned in the preface of the 2016 report.","5","years","[""stone2022""]","is_blank","is_blank","The context states that the Standing Committee forms a Study Panel every five years as part of its core activity for the One Hundred Year Study on AI. This is explicitly mentioned in the preface of the 2016 report."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context states that in the US, the average household has an average of 25 connected devices, as reported by Deloitte in 2021. This information is repeated in multiple snippets from the same source (wu2021b).","25","devices","[""wu2021b""]","is_blank","is_blank","The context states that in the US, the average household has an average of 25 connected devices, as reported by Deloitte in 2021. This information is repeated in multiple snippets from the same source (wu2021b)."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context from ref_id=luccioni2025c explicitly states the range of energy consumption for pre-training LLMs as 0.8 MWh to 3,500 MWh, citing specific models (OLMo 20M and LLaMa 4 Scout). Additionally, ref_id=luccioni2025c's Appendix Table 1 confirms these values and includes other models within this range. The other references either discuss inference energy, specific model training estimates, or do not provide a clear range for pre-training LLMs.","0.8-3500","MWh","[""luccioni2025c""]","is_blank","is_blank","The context from ref_id=luccioni2025c explicitly states the range of energy consumption for pre-training LLMs as 0.8 MWh to 3,500 MWh, citing specific models (OLMo 20M and LLaMa 4 Scout). Additionally, ref_id=luccioni2025c's Appendix Table 1 confirms these values and includes other models within this range. The other references either discuss inference energy, specific model training estimates, or do not provide a clear range for pre-training LLMs."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context explicitly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. This directly contradicts the statement that Watson did NOT beat human contenders, making the correct answer False.","0","is_blank","[""stone2022""]","is_blank","is_blank","The context explicitly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. This directly contradicts the statement that Watson did NOT beat human contenders, making the correct answer False."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context snippet from ref_id=luccioni2025a states that 10–50 queries on GPT-3 consume around half a liter of water, directly answering the question.","10–50","queries","[""luccioni2025a""]","is_blank","is_blank","The context snippet from ref_id=luccioni2025a states that 10–50 queries on GPT-3 consume around half a liter of water, directly answering the question."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context explicitly states that JetMoE-8B-Chat achieved a higher MT-Bench score than Llama-2-13b-Chat after alignment. Table 4 in the context lists the MT-Bench scores, showing JetMoE-8B-chat with 6.681 and Llama-2-13b-chat with 6.650.","6.681","score","[""shen2024""]","is_blank","is_blank","The context explicitly states that JetMoE-8B-Chat achieved a higher MT-Bench score than Llama-2-13b-Chat after alignment. Table 4 in the context lists the MT-Bench scores, showing JetMoE-8B-chat with 6.681 and Llama-2-13b-chat with 6.650."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that GPT-3 needs to consume a 500ml bottle of water for roughly 10–50 medium-length responses, depending on deployment conditions. This directly answers the question about the number of responses per 500ml bottle.","10–50","responses","[""li2025b""]","is_blank","is_blank","The context states that GPT-3 needs to consume a 500ml bottle of water for roughly 10–50 medium-length responses, depending on deployment conditions. This directly answers the question about the number of responses per 500ml bottle."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The context snippets from ref_id=griggs2024 explicitly state that Mélange reduces deployment costs by up to 77% in conversational settings compared to using only a single GPU type. This percentage is directly mentioned in multiple sections of the document, including the abstract and evaluation results.","77","percent","[""griggs2024""]","is_blank","is_blank","The context snippets from ref_id=griggs2024 explicitly state that Mélange reduces deployment costs by up to 77% in conversational settings compared to using only a single GPU type. This percentage is directly mentioned in multiple sections of the document, including the abstract and evaluation results."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. This is mentioned in multiple sections of the provided context, including Table II and the discussion section.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","is_blank","The context explicitly states that 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. This is mentioned in multiple sections of the provided context, including Table II and the discussion section."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context from multiple references (han2024 and luccioni2025c) explicitly states that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.","10000","round trips","[""han2024"", ""luccioni2025c""]","is_blank","is_blank","The context from multiple references (han2024 and luccioni2025c) explicitly states that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context from ref_id=luccioni2024 states that according to AWS, inference is estimated to make up 80 to 90% of total ML cloud computing demand. Additionally, ref_id=fernandez2025 and ref_id=chung2025 corroborate this range by mentioning AWS cloud computing demand for ML inference as 80-90%.","80-90","percent","[""luccioni2024"", ""fernandez2025"", ""chung2025""]","is_blank","is_blank","The context from ref_id=luccioni2024 states that according to AWS, inference is estimated to make up 80 to 90% of total ML cloud computing demand. Additionally, ref_id=fernandez2025 and ref_id=chung2025 corroborate this range by mentioning AWS cloud computing demand for ML inference as 80-90%."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context states that for all model experimentation and evaluation, a total of 178.97 kg of CO2eq was emitted. This directly answers the question about the total CO2 equivalent emissions generated in the study.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","is_blank","The context states that for all model experimentation and evaluation, a total of 178.97 kg of CO2eq was emitted. This directly answers the question about the total CO2 equivalent emissions generated in the study."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context states that estimations using TDP are nearly always an overestimation because GPUs rarely draw maximum power continuously. It also provides examples where TDP-based estimations overestimated actual energy consumption by factors up to 4.1x.","0","is_blank","[""chung2025""]","is_blank","is_blank","The context states that estimations using TDP are nearly always an overestimation because GPUs rarely draw maximum power continuously. It also provides examples where TDP-based estimations overestimated actual energy consumption by factors up to 4.1x."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries and that its consumption is slightly higher (3.098 Wh vs. 2.875 Wh) due to older hardware. This directly contradicts the claim that GPT-4o mini consumes less energy per query.","0","is_blank","[""jegham2025""]","is_blank","is_blank","The context states that GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries and that its consumption is slightly higher (3.098 Wh vs. 2.875 Wh) due to older hardware. This directly contradicts the claim that GPT-4o mini consumes less energy per query."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context includes a figure (Fig. 8) showing the throughput of Mixtral-CS-A100-40GB with batch size 1 under dense configuration as 0.3 queries/sec. Additionally, another context snippet explicitly lists 'Dense(bsz=1)' with a value of 0.3 in the Mixtral-CS section.","0.3","queries/sec","[""xia2024""]","is_blank","is_blank","The context includes a figure (Fig. 8) showing the throughput of Mixtral-CS-A100-40GB with batch size 1 under dense configuration as 0.3 queries/sec. Additionally, another context snippet explicitly lists 'Dense(bsz=1)' with a value of 0.3 in the Mixtral-CS section."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context states that direct environmental disclosures peaked in 2022 with 10% of notable models that year releasing some degree of information. However, after 2022, the introduction of commercial and proprietary models led to a notable reversal, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of models fell under the 'no disclosure' category.","0","is_blank","[""luccioni2025c""]","is_blank","is_blank","The context states that direct environmental disclosures peaked in 2022 with 10% of notable models that year releasing some degree of information. However, after 2022, the introduction of commercial and proprietary models led to a notable reversal, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of models fell under the 'no disclosure' category."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context from Griggs et al. (2024) provides a table listing the normalized on-demand hourly price for H100 GPU as $7.5164 USD per hour under the 'On-demand Price ($/h)' column for H100 (SXM).","7.516","USD per hour","[""griggs2024""]","is_blank","is_blank","The context from Griggs et al. (2024) provides a table listing the normalized on-demand hourly price for H100 GPU as $7.5164 USD per hour under the 'On-demand Price ($/h)' column for H100 (SXM)."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context mentions that GPT-3's training required over 700 kiloliters (kL) of water for cooling, which converts to 700,000 liters. Additionally, another source estimates GPT-3's freshwater consumption during training as over 5 million liters. However, the question specifically asks about GPT-4, and while there are projections for GPT-4o's annual water consumption, no direct figure for GPT-4's training run is provided in the context.","is_blank","liters of water","[""is_blank""]","is_blank","is_blank","The context mentions that GPT-3's training required over 700 kiloliters (kL) of water for cooling, which converts to 700,000 liters. Additionally, another source estimates GPT-3's freshwater consumption during training as over 5 million liters. However, the question specifically asks about GPT-4, and while there are projections for GPT-4o's annual water consumption, no direct figure for GPT-4's training run is provided in the context."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context from griggs2024 states that serving Llama2-70b at BF16 precision with 2 NVIDIA A100-80GB GPUs costs over $5,200 per month. To calculate the hourly cost, divide the monthly cost by the number of hours in a month (30 days * 24 hours = 720 hours). $5,200 / 720 ≈ $7.22 per hour.","7.22","USD per hour","[""griggs2024""]","is_blank","is_blank","The context from griggs2024 states that serving Llama2-70b at BF16 precision with 2 NVIDIA A100-80GB GPUs costs over $5,200 per month. To calculate the hourly cost, divide the monthly cost by the number of hours in a month (30 days * 24 hours = 720 hours). $5,200 / 720 ≈ $7.22 per hour."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context explicitly states that increasing the BLEU score from 5 to 40 for GPT-3-based language translation requires a model 1,000× larger in size.","1000","multiplier","[""wu2021a""]","is_blank","is_blank","The context explicitly states that increasing the BLEU score from 5 to 40 for GPT-3-based language translation requires a model 1,000× larger in size."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","","","miles","[]","is_blank","is_blank",""
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The context states that the BLOOMz-7B model has an inference energy consumption of 1.0 × 10⁻⁴ kWh per inference. With 606,096 downloads and 1 million inferences per download, the total energy is calculated as 606,096,000,000 inferences × 1.0 × 10⁻⁴ kWh/inference = 60,609,600 kWh, which converts to 60,609.6 MWh.","60609.6","MWh","[""luccioni2024""]","is_blank","is_blank","The context states that the BLOOMz-7B model has an inference energy consumption of 1.0 × 10⁻⁴ kWh per inference. With 606,096 downloads and 1 million inferences per download, the total energy is calculated as 606,096,000,000 inferences × 1.0 × 10⁻⁴ kWh/inference = 60,609,600 kWh, which converts to 60,609.6 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours. This information is mentioned in multiple sections of the provided context snippets from the same reference (shen2024).","30000","H100 GPU hours","[""shen2024""]","is_blank","is_blank","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours. This information is mentioned in multiple sections of the provided context snippets from the same reference (shen2024)."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This is explicitly mentioned in multiple sections of the Cottier et al. (2024) document, including the abstract and section 3.2.","1000000000","USD","[""cottier2024""]","is_blank","is_blank","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This is explicitly mentioned in multiple sections of the Cottier et al. (2024) document, including the abstract and section 3.2."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context snippets from ref_id=li2025b mention that GPT-3's training energy was estimated at 1287 MWh. However, the question asks for the total number of floating point operations (FLOPS), which is not provided in any of the given contexts. The other references discuss costs, energy consumption, and water usage but do not mention FLOPS for GPT-3.","is_blank","FLOPS","[""is_blank""]","is_blank","is_blank","The context snippets from ref_id=li2025b mention that GPT-3's training energy was estimated at 1287 MWh. However, the question asks for the total number of floating point operations (FLOPS), which is not provided in any of the given contexts. The other references discuss costs, energy consumption, and water usage but do not mention FLOPS for GPT-3."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that 'at a minimum, 8 V100 GPUs each with 32 GB of RAM [...] are required for any meaningful inferences with the 65B LLaMA model' without compression or quantization.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","is_blank","The context explicitly states that 'at a minimum, 8 V100 GPUs each with 32 GB of RAM [...] are required for any meaningful inferences with the 65B LLaMA model' without compression or quantization."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The context provides the total computational work for FLM-101B's entire training process (28.22 zettaFLOPs) in Table 4 of li2025a, but does not break down the computational work specifically for the final 101B training stage. The throughput data in Table 2 shows per-GPU FLOP rates and GPU counts for the 101B stage, but lacks duration information specific to that stage to calculate total FLOPs.","is_blank","zettaFLOPs","[""is_blank""]","is_blank","is_blank","The context provides the total computational work for FLM-101B's entire training process (28.22 zettaFLOPs) in Table 4 of li2025a, but does not break down the computational work specifically for the final 101B training stage. The throughput data in Table 2 shows per-GPU FLOP rates and GPU counts for the 101B stage, but lacks duration information specific to that stage to calculate total FLOPs."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context mentions that AlexNet (2012) has more parameters than ResNet but much lower accuracy. However, the exact top-1 accuracy for AlexNet on ImageNet is not provided in the given snippets. The context discusses ResNet models and their accuracies but does not specify AlexNet's top-1 accuracy.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context mentions that AlexNet (2012) has more parameters than ResNet but much lower accuracy. However, the exact top-1 accuracy for AlexNet on ImageNet is not provided in the given snippets. The context discusses ResNet models and their accuracies but does not specify AlexNet's top-1 accuracy."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. Each server has 8 GPUs, so 24 servers × 8 GPUs per server equals 192 total A800 GPUs.","192","GPUs","[""li2025a""]","is_blank","is_blank","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. Each server has 8 GPUs, so 24 servers × 8 GPUs per server equals 192 total A800 GPUs."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context from ref_id=strubell2019 states that training a Transformer model with neural architecture search (NAS) produces 626,155 lbs of CO2 emissions. Additionally, ref_id=luccioni2023 converts this to 284,019 kg (626,155 lbs) of CO2 and compares it to the lifetime emissions of five US cars. The context also mentions that the average American lifetime CO2 emissions is 36,156 lbs per year, but the exact number of lifetimes equivalent is not directly provided. However, using the car comparison (five cars' lifetime emissions) and knowing that one car's lifetime is equivalent to 126,000 lbs CO2 (from strubell2019), the NAS emissions (626,155 lbs) divided by 126,000 lbs per car gives approximately five car lifetimes. Since the question asks for average American lifetimes, and the context does not provide a direct conversion to human lifetimes, the answer cannot be fully supported and should be 'is_blank'.","is_blank","lifetimes","[""is_blank""]","is_blank","is_blank","The context from ref_id=strubell2019 states that training a Transformer model with neural architecture search (NAS) produces 626,155 lbs of CO2 emissions. Additionally, ref_id=luccioni2023 converts this to 284,019 kg (626,155 lbs) of CO2 and compares it to the lifetime emissions of five US cars. The context also mentions that the average American lifetime CO2 emissions is 36,156 lbs per year, but the exact number of lifetimes equivalent is not directly provided. However, using the car comparison (five cars' lifetime emissions) and knowing that one car's lifetime is equivalent to 126,000 lbs CO2 (from strubell2019), the NAS emissions (626,155 lbs) divided by 126,000 lbs per car gives approximately five car lifetimes. Since the question asks for average American lifetimes, and the context does not provide a direct conversion to human lifetimes, the answer cannot be fully supported and should be 'is_blank'."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context explicitly states that FAIR's RoBERTa was trained on 160GB of text and required around 25,000 GPU hours to train.","25000","hours","[""schwartz2019""]","is_blank","is_blank","The context explicitly states that FAIR's RoBERTa was trained on 160GB of text and required around 25,000 GPU hours to train."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context explicitly states that Amazon's on-site solar energy systems avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","47400","metric tons","[""amazon2023""]","is_blank","is_blank","The context explicitly states that Amazon's on-site solar energy systems avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context mentions the use of Megatron-LM for efficient large-scale language model training on GPU clusters. Specifically, ref_id=shen2024 and ref_id=li2025a both reference the framework 'Megatron-LM' developed by Narayanan et al. (2021) for deploying large language models across multiple GPUs and nodes.","Megatron-LM","is_blank","[""shen2024"", ""li2025a""]","is_blank","is_blank","The context mentions the use of Megatron-LM for efficient large-scale language model training on GPU clusters. Specifically, ref_id=shen2024 and ref_id=li2025a both reference the framework 'Megatron-LM' developed by Narayanan et al. (2021) for deploying large language models across multiple GPUs and nodes."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context from zschache2025's Table B2 shows that when the Llama 3.1 70B model was deployed on two nodes instead of one, the energy consumption increased by a factor of 1.95. The table explicitly lists the single-node energy as 48.60 Wh and double-node as 94.88 Wh, with a ratio of 1.95.","1.95","multiplier","[""zschache2025""]","is_blank","is_blank","The context from zschache2025's Table B2 shows that when the Llama 3.1 70B model was deployed on two nodes instead of one, the energy consumption increased by a factor of 1.95. The table explicitly lists the single-node energy as 48.60 Wh and double-node as 94.88 Wh, with a ratio of 1.95."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context from ref_id=li2025b's Table 1 provides per-request water consumption estimates for GPT-3, with values ranging from 10.5 mL to 47.5 mL per request. Additionally, ref_id=luccioni2025a states that 10–50 queries on GPT-3 consume around half a liter (500 mL) of water, which aligns with the per-request values in Table 1. However, the question specifically asks about 2023 user sessions, and the context refers to GPT-3 (2020) and GPT-4o (2025 projections), not 2023 ChatGPT usage. There is no direct mention of 2023 user session data.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The context from ref_id=li2025b's Table 1 provides per-request water consumption estimates for GPT-3, with values ranging from 10.5 mL to 47.5 mL per request. Additionally, ref_id=luccioni2025a states that 10–50 queries on GPT-3 consume around half a liter (500 mL) of water, which aligns with the per-request values in Table 1. However, the question specifically asks about 2023 user sessions, and the context refers to GPT-3 (2020) and GPT-4o (2025 projections), not 2023 ChatGPT usage. There is no direct mention of 2023 user session data."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that scaling 700 million daily GPT-4o queries results in annual electricity consumption comparable to 35,000 U.S. residential households. This is explicitly mentioned in the document as a direct comparison.","35000","homes","[""jegham2025""]","is_blank","is_blank","The context states that scaling 700 million daily GPT-4o queries results in annual electricity consumption comparable to 35,000 U.S. residential households. This is explicitly mentioned in the document as a direct comparison."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context snippet from ref_id=luccioni2025a explicitly states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022.","34","percent","[""luccioni2025a""]","is_blank","is_blank","The context snippet from ref_id=luccioni2025a explicitly states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context explicitly states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This directly contradicts the statement that traditional models achieved comparable accuracy to LLMs.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The context explicitly states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This directly contradicts the statement that traditional models achieved comparable accuracy to LLMs."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context from morrison2025 states that the Augusta data center in Iowa had a trailing twelve-month average PUE of 1.12. The Evolved Transformer experiments were run in the Augusta cluster, which is located in Iowa.","1.12","PUE","[""morrison2025""]","is_blank","is_blank","The context from morrison2025 states that the Augusta data center in Iowa had a trailing twelve-month average PUE of 1.12. The Evolved Transformer experiments were run in the Augusta cluster, which is located in Iowa."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context explicitly states that a recent analysis projects approximately 772 billion GPT-4o queries in 2025, following a growth pattern from January to December 2025.","772000000000","queries","[""jegham2025""]","is_blank","is_blank","The context explicitly states that a recent analysis projects approximately 772 billion GPT-4o queries in 2025, following a growth pattern from January to December 2025."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context mentions that JetMoE-8B achieves an OpenLLM Leaderboard average score of 53.0, which is higher than other models listed in Table 3.","53.0","score","[""shen2024""]","is_blank","is_blank","The context mentions that JetMoE-8B achieves an OpenLLM Leaderboard average score of 53.0, which is higher than other models listed in Table 3."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The provided context does not mention the number of AI training runs conducted globally on renewable-only power in 2022. While there are references to renewable energy's impact on reducing carbon footprint and some specific training emissions, no global statistics for 2022 are provided.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The provided context does not mention the number of AI training runs conducted globally on renewable-only power in 2022. While there are references to renewable energy's impact on reducing carbon footprint and some specific training emissions, no global statistics for 2022 are provided."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context states that there is a proposal to eliminate the open-source exemption from reporting obligations under the AI Act, indicating that currently open-source models are exempt unless this proposal is adopted. The question asks if they are 'fully exempt... unless they pose systemic risk.' The context does not mention systemic risk as a condition for exemption removal, but rather proposes removing the exemption entirely. However, the existing AI Act's provisions are critiqued for not requiring energy consumption disclosure during inference phase unless modified, and the open-source exemption is noted as a current feature that should be eliminated. The answer is False because the exemption exists but the proposal seeks to remove it, implying that under current AI Act they are exempt regardless of systemic risk.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context states that there is a proposal to eliminate the open-source exemption from reporting obligations under the AI Act, indicating that currently open-source models are exempt unless this proposal is adopted. The question asks if they are 'fully exempt... unless they pose systemic risk.' The context does not mention systemic risk as a condition for exemption removal, but rather proposes removing the exemption entirely. However, the existing AI Act's provisions are critiqued for not requiring energy consumption disclosure during inference phase unless modified, and the open-source exemption is noted as a current feature that should be eliminated. The answer is False because the exemption exists but the proposal seeks to remove it, implying that under current AI Act they are exempt regardless of systemic risk."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context mentions that the average data center PUE in 2023 was 1.58 globally and 1.6 in the EU. However, the question specifically asks about the US national datacenter average PUE in 2020. The provided context does not include data for the US in 2020.","is_blank","PUE","[""is_blank""]","is_blank","is_blank","The context mentions that the average data center PUE in 2023 was 1.58 globally and 1.6 in the EU. However, the question specifically asks about the US national datacenter average PUE in 2020. The provided context does not include data for the US in 2020."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context explicitly states that for the OPT-2.7B model on a g4dn.xlarge instance with a batch size of 32, the KV Cache expands to 5.312GB.","5.312","GB","[""kim2025""]","is_blank","is_blank","The context explicitly states that for the OPT-2.7B model on a g4dn.xlarge instance with a batch size of 32, the KV Cache expands to 5.312GB."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context explicitly states that for the four notable models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs including equity accounted for between 29% and 49% of the total amortized cost.","29-49","percent","[""cottier2024""]","is_blank","is_blank","The context explicitly states that for the four notable models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs including equity accounted for between 29% and 49% of the total amortized cost."
"q213","Which software package was used to measure energy consumption during inference runs?","The context mentions that the energy consumption during inference was measured using the CodeCarbon package in multiple references (zschache2025, morrison2025, rubei2025). Specifically, zschache2025 states 'The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package'.","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025"", ""rubei2025""]","is_blank","is_blank","The context mentions that the energy consumption during inference was measured using the CodeCarbon package in multiple references (zschache2025, morrison2025, rubei2025). Specifically, zschache2025 states 'The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package'."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context from ref_id=luccioni2025c states that in an analysis of 100 news articles, 53% cited the 3 Wh per query or the '10 times more than a Google search' estimate.","53","percent","[""luccioni2025c""]","is_blank","is_blank","The context from ref_id=luccioni2025c states that in an analysis of 100 news articles, 53% cited the 3 Wh per query or the '10 times more than a Google search' estimate."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context explicitly mentions the Compute Time Calibration Function (CTCF) as the function proposed to adjust for discrepancies between theoretical and actual GPU performance, thereby improving instance selection accuracy.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","is_blank","The context explicitly mentions the Compute Time Calibration Function (CTCF) as the function proposed to adjust for discrepancies between theoretical and actual GPU performance, thereby improving instance selection accuracy."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that increasing the number of shards for LLaMA-65B leads to higher energy per second (Watts) and energy per decoded token. Specifically, it mentions that 'increasing the number of shards always increases the wattage' and 'energy per second increases with the number of shards even at the same batch size'. Additionally, Figure 4 and related sections show energy per response increases with shard count.","1","is_blank","[""samsi2024""]","is_blank","is_blank","The context states that increasing the number of shards for LLaMA-65B leads to higher energy per second (Watts) and energy per decoded token. Specifically, it mentions that 'increasing the number of shards always increases the wattage' and 'energy per second increases with the number of shards even at the same batch size'. Additionally, Figure 4 and related sections show energy per response increases with shard count."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context states that mining 1 kg of rare earth materials consumes 11 kL of water and that an H100 GPU is 0.1% rare earth metal by mass. However, the total mass of the GPU is not provided in the context, making it impossible to calculate the exact water consumption for the rare earth component. The context explicitly mentions that the estimated water consumption from mining rare earth metals adds '2.2 liters consumed' per GPU, but this value is not in kL and appears to include other factors like silicon wafer production.","is_blank","kL","[""is_blank""]","is_blank","is_blank","The context states that mining 1 kg of rare earth materials consumes 11 kL of water and that an H100 GPU is 0.1% rare earth metal by mass. However, the total mass of the GPU is not provided in the context, making it impossible to calculate the exact water consumption for the rare earth component. The context explicitly mentions that the estimated water consumption from mining rare earth metals adds '2.2 liters consumed' per GPU, but this value is not in kL and appears to include other factors like silicon wafer production."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context discusses a policy proposal to eliminate the open-source exemption from reporting obligations under the AI Act, indicating that currently open-source models are exempt. The proposal suggests removing this exemption to require open-source models to report energy consumption like proprietary models.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context discusses a policy proposal to eliminate the open-source exemption from reporting obligations under the AI Act, indicating that currently open-source models are exempt. The proposal suggests removing this exemption to require open-source models to report energy consumption like proprietary models."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context snippets from luccioni2025a and wu2021b explicitly state that Amazon, Microsoft, Meta, and Google accounted for almost 30% of all corporate PPAs purchased worldwide in 2020.","30","percent","[""luccioni2025a"", ""wu2021b""]","is_blank","is_blank","The context snippets from luccioni2025a and wu2021b explicitly state that Amazon, Microsoft, Meta, and Google accounted for almost 30% of all corporate PPAs purchased worldwide in 2020."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context explicitly states that in 2023, U.S. data centers resulted in a total public health cost of about $6.7 billion using the average attribution method, which is the standard methodology mentioned in the document.","6.7","USD","[""han2024""]","is_blank","is_blank","The context explicitly states that in 2023, U.S. data centers resulted in a total public health cost of about $6.7 billion using the average attribution method, which is the standard methodology mentioned in the document."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The context provides energy consumption data for various models in Table 4. For long prompts (10k input-1.5k output), the o3 model consumes 12.222 Wh while GPT-4.1 nano consumes 0.827 Wh. Dividing these values gives a factor of approximately 14.78.","14.8","multiplier","[""jegham2025""]","is_blank","is_blank","The context provides energy consumption data for various models in Table 4. For long prompts (10k input-1.5k output), the o3 model consumes 12.222 Wh while GPT-4.1 nano consumes 0.827 Wh. Dividing these values gives a factor of approximately 14.78."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context explicitly states that for the short-context dataset (Arena) with a 120ms SLO, Mélange achieved cost reductions in the range of 15-77% compared to single-GPU baselines. This information is directly provided in the 'Short-context Dataset (Arena)' section of the Cost Savings Analysis.","15-77","percent","[""griggs2024""]","is_blank","is_blank","The context explicitly states that for the short-context dataset (Arena) with a 120ms SLO, Mélange achieved cost reductions in the range of 15-77% compared to single-GPU baselines. This information is directly provided in the 'Short-context Dataset (Arena)' section of the Cost Savings Analysis."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context includes Table 3 which lists the net carbon emissions (net tCO2e) for FLM-101B as 26 metric tons of CO2 equivalent.","26","tCO2e","[""li2025a""]","is_blank","is_blank","The context includes Table 3 which lists the net carbon emissions (net tCO2e) for FLM-101B as 26 metric tons of CO2 equivalent."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context mentions that for a sparse Mixtral model on the MATH dataset with a batch size of 1, the throughput is 0.3 queries per second. To find the total execution time for 2M queries, we calculate 2,000,000 / 0.3 = 6,666,666.67 seconds. However, the provided context does not explicitly state the total execution time for this specific configuration, leading to uncertainty.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context mentions that for a sparse Mixtral model on the MATH dataset with a batch size of 1, the throughput is 0.3 queries per second. To find the total execution time for 2M queries, we calculate 2,000,000 / 0.3 = 6,666,666.67 seconds. However, the provided context does not explicitly state the total execution time for this specific configuration, leading to uncertainty."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context from han2024 explicitly states that the public health impact of AI is 'highly unevenly distributed across different counties and communities' and that disadvantaged communities bear a disproportionate share. For example, per-household health costs can be up to 200 times higher in some areas. This directly contradicts the notion of even distribution.","0","is_blank","[""han2024""]","is_blank","is_blank","The context from han2024 explicitly states that the public health impact of AI is 'highly unevenly distributed across different counties and communities' and that disadvantaged communities bear a disproportionate share. For example, per-household health costs can be up to 200 times higher in some areas. This directly contradicts the notion of even distribution."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context snippet from ref_id=wu2021b states that GPU theoretical performance per watt doubles every 3-4 years as a result of Moore’s law scaling and architectural optimization, citing Sun et al., 2019. This directly supports the statement in the question regarding 2019 product data.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context snippet from ref_id=wu2021b states that GPU theoretical performance per watt doubles every 3-4 years as a result of Moore’s law scaling and architectural optimization, citing Sun et al., 2019. This directly supports the statement in the question regarding 2019 product data."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions that in the financial sentiment case study, the framework applied 4-bit quantization through Ollama, an open-source platform known for supporting edge computing and privacy-centric deployments.","Ollama","is_blank","[""khan2025""]","is_blank","is_blank","The context mentions that in the financial sentiment case study, the framework applied 4-bit quantization through Ollama, an open-source platform known for supporting edge computing and privacy-centric deployments."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context mentions that when running experiments on spot instances which can terminate anytime, they used an independent S3 storage provider, Backblaze (B2), accessed via shards in tar format with the WebDataset library. This indicates that Backblaze B2 was the storage service used.","Backblaze B2","is_blank","[""erben2023""]","is_blank","is_blank","The context mentions that when running experiments on spot instances which can terminate anytime, they used an independent S3 storage provider, Backblaze (B2), accessed via shards in tar format with the WebDataset library. This indicates that Backblaze B2 was the storage service used."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context from ref_id=zschache2025 states there is a 'strong correlation between inference energy consumption and model runtime' and that 'execution time can serve as a practical proxy for energy usage'. A strong correlation implies a nearly linear relationship, supporting the answer.","1","is_blank","[""zschache2025""]","is_blank","is_blank","The context from ref_id=zschache2025 states there is a 'strong correlation between inference energy consumption and model runtime' and that 'execution time can serve as a practical proxy for energy usage'. A strong correlation implies a nearly linear relationship, supporting the answer."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context explicitly states that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024.","Edward J. Markey","is_blank","[""ebert2024""]","is_blank","is_blank","The context explicitly states that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context from Chen et al. (2024) includes a table listing the price per chip for an NVIDIA H100 as $11.06/hr. The note clarifies that the H20's price is estimated, but the H100's price is directly provided. Another context from Griggs et al. (2024) mentions a normalized H100 price of $7.516/hr, but this is adjusted from RunPod's pricing. Since the question specifically asks for Chen et al. (2025) and the Chen et al. (2024) context provides a direct price, the answer is based on that source.","11.06","USD per hour","[""chen2024""]","is_blank","is_blank","The context from Chen et al. (2024) includes a table listing the price per chip for an NVIDIA H100 as $11.06/hr. The note clarifies that the H20's price is estimated, but the H100's price is directly provided. Another context from Griggs et al. (2024) mentions a normalized H100 price of $7.516/hr, but this is adjusted from RunPod's pricing. Since the question specifically asks for Chen et al. (2025) and the Chen et al. (2024) context provides a direct price, the answer is based on that source."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context from ref_id=amazon2023 states that in February 2024, AWS announced the average expected server lifetime had improved from five to six years. However, the question specifically asks about GPU lifetime in AI data centers. The context mentions servers but does not explicitly state that this refers to GPUs. Another context snippet from ref_id=cottier2024 discusses hardware lifetime in terms of failures and mentions an expected hardware lifetime of 3.7 years under catastrophic failure scenarios, but this is not directly tied to retirement in AI data centers in 2024. Therefore, the provided context does not clearly support the answer.","is_blank","years","[""is_blank""]","is_blank","is_blank","The context from ref_id=amazon2023 states that in February 2024, AWS announced the average expected server lifetime had improved from five to six years. However, the question specifically asks about GPU lifetime in AI data centers. The context mentions servers but does not explicitly state that this refers to GPUs. Another context snippet from ref_id=cottier2024 discusses hardware lifetime in terms of failures and mentions an expected hardware lifetime of 3.7 years under catastrophic failure scenarios, but this is not directly tied to retirement in AI data centers in 2024. Therefore, the provided context does not clearly support the answer."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that for the 13B LLaMA model, the bare minimum hardware required is 2 V100 32GB GPUs according to Table II in the samsi2024 document. This information is provided under the 'Baseline configurations' section which lists the hardware requirements without compression or quantization.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","is_blank","The context states that for the 13B LLaMA model, the bare minimum hardware required is 2 V100 32GB GPUs according to Table II in the samsi2024 document. This information is provided under the 'Baseline configurations' section which lists the hardware requirements without compression or quantization."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context states that Google reported GHG emissions of 1247.61 tons CO2e for training the Gemma family, which is over 4 times the 'five cars' estimate of 284 metric tons CO2e.","1247.61 tCO2e, 4x","tCO2e","[""luccioni2025c""]","is_blank","is_blank","The context states that Google reported GHG emissions of 1247.61 tons CO2e for training the Gemma family, which is over 4 times the 'five cars' estimate of 284 metric tons CO2e."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context explicitly states that ELMo was trained on 3 NVIDIA GTX 1080 Ti GPUs, but the specific training duration for this hardware configuration is not provided. While the original paper by Peters et al. (2018) reports 336 hours on GTX 1080 GPUs, the question specifically asks about GTX 1080 Ti GPUs which the context does not directly associate with a training time.","is_blank","hours","[""is_blank""]","is_blank","is_blank","The context explicitly states that ELMo was trained on 3 NVIDIA GTX 1080 Ti GPUs, but the specific training duration for this hardware configuration is not provided. While the original paper by Peters et al. (2018) reports 336 hours on GTX 1080 GPUs, the question specifically asks about GTX 1080 Ti GPUs which the context does not directly associate with a training time."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context explicitly states that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh according to reference [8] in the document li2025b.","3.1","L/kWh","[""li2025b""]","is_blank","is_blank","The context explicitly states that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh according to reference [8] in the document li2025b."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context snippets from ref_id=wu2021b and ref_id=dodge2022 both explicitly state that Google's hyperscale data centers achieved a PUE of 1.10 in 2021. The first context mentions '1.10 (2021)' for Google, and the second confirms this with 'PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021'.","1.10","PUE","[""wu2021b"", ""dodge2022""]","is_blank","is_blank","The context snippets from ref_id=wu2021b and ref_id=dodge2022 both explicitly state that Google's hyperscale data centers achieved a PUE of 1.10 in 2021. The first context mentions '1.10 (2021)' for Google, and the second confirms this with 'PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021'."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that in North America, AWS can lower customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when using 100% renewable energy, which Amazon achieved in 2023.","96","percent","[""amazon2023""]","is_blank","is_blank","The context states that in North America, AWS can lower customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when using 100% renewable energy, which Amazon achieved in 2023."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context explicitly states that fine-tuning a sparse Mixtral model with 2 million queries using an NVIDIA H100 GPU has a net cost of $3460. This is mentioned in multiple sections of the document, including the example provided and the conclusion.","3460","USD","[""xia2024""]","is_blank","is_blank","The context explicitly states that fine-tuning a sparse Mixtral model with 2 million queries using an NVIDIA H100 GPU has a net cost of $3460. This is mentioned in multiple sections of the document, including the example provided and the conclusion."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context from ref_id=dodge2022 states that 'the GPU accounts for almost 3/4 of electricity consumption' and specifically mentions in Table 1 that the GPU alone accounts for 74% of the total energy consumption due to the components measured. This directly answers the question about the percentage of total provisioned power accounted for by GPUs in a typical datacenter.","74","percent","[""dodge2022""]","is_blank","is_blank","The context from ref_id=dodge2022 states that 'the GPU accounts for almost 3/4 of electricity consumption' and specifically mentions in Table 1 that the GPU alone accounts for 74% of the total energy consumption due to the components measured. This directly answers the question about the percentage of total provisioned power accounted for by GPUs in a typical datacenter."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context states that the training was conducted on a cluster containing 12 nodes and 96 H100 GPUs. This directly answers the question about the total number of H100 GPUs used.","96","H100 GPUs","[""shen2024""]","is_blank","is_blank","The context states that the training was conducted on a cluster containing 12 nodes and 96 H100 GPUs. This directly answers the question about the total number of H100 GPUs used."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context explicitly states that during active training, the average GPU power for a single node is over 600W, which directly answers the question about the first 300 logging steps of OLMo 2 7B training.","600","Watts","[""morrison2025""]","is_blank","is_blank","The context explicitly states that during active training, the average GPU power for a single node is over 600W, which directly answers the question about the first 300 logging steps of OLMo 2 7B training."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context snippet from ref_id=strubell2019 includes a table titled 'Estimated CO2 emissions from training common NLP models, compared to familiar consumption.' In this table, under 'Consumption CO2e (lbs)', the entry for 'Human life, avg, 1 year' is listed as 11,023 lbs.","11023","lbs","[""strubell2019""]","is_blank","is_blank","The context snippet from ref_id=strubell2019 includes a table titled 'Estimated CO2 emissions from training common NLP models, compared to familiar consumption.' In this table, under 'Consumption CO2e (lbs)', the entry for 'Human life, avg, 1 year' is listed as 11,023 lbs."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for the smaller LLaMA 7B and 13B models, there was a 1.25 times increase in inference throughput on A100 compared to V100 GPUs. This directly answers the question about the speedup for LLaMA-13B.","1.25","multiplier","[""samsi2024""]","is_blank","is_blank","The context states that for the smaller LLaMA 7B and 13B models, there was a 1.25 times increase in inference throughput on A100 compared to V100 GPUs. This directly answers the question about the speedup for LLaMA-13B."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context explicitly states multiple times that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). This value is directly mentioned in sections 5.3 Validation Against Public Disclosures and 6.1 Energy Cost of a Single GPT-4o User Session, with Figure 5 also supporting this data point.","0.42","Wh","[""jegham2025""]","is_blank","is_blank","The context explicitly states multiple times that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). This value is directly mentioned in sections 5.3 Validation Against Public Disclosures and 6.1 Energy Cost of a Single GPT-4o User Session, with Figure 5 also supporting this data point."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context explicitly states that the Max-Performance instance (g6e.xlarge) at $2.699 was 'about 280% more expensive than InferSave’s top choice' (g4dn.xlarge at $0.71) in the 400 TPS SLO experiment.","280","percent","[""kim2025""]","is_blank","is_blank","The context explicitly states that the Max-Performance instance (g6e.xlarge) at $2.699 was 'about 280% more expensive than InferSave’s top choice' (g4dn.xlarge at $0.71) in the 400 TPS SLO experiment."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context from zschache2025 explicitly states that for models generating a single token per inference, V100 or A30 GPUs are more efficient. This directly answers the question about the most energy-efficient GPU architecture for single classification token generation.","V100 or A30 GPU","is_blank","[""zschache2025""]","is_blank","is_blank","The context from zschache2025 explicitly states that for models generating a single token per inference, V100 or A30 GPUs are more efficient. This directly answers the question about the most energy-efficient GPU architecture for single classification token generation."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context snippet from ref_id=schwartz2019 explicitly states that 'Reporting the computational price tag of finding, training, and running models is a key Green AI practice'. This directly supports the statement in the question.","1","is_blank","[""schwartz2019""]","is_blank","is_blank","The context snippet from ref_id=schwartz2019 explicitly states that 'Reporting the computational price tag of finding, training, and running models is a key Green AI practice'. This directly supports the statement in the question."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context explicitly states that global electronic waste (e-waste) reached 62 million tonnes in 2022. This information is provided in the snippet from ref_id=luccioni2025a, which discusses AI's contribution to e-waste.","62","metric tons","[""luccioni2025a""]","is_blank","is_blank","The context explicitly states that global electronic waste (e-waste) reached 62 million tonnes in 2022. This information is provided in the snippet from ref_id=luccioni2025a, which discusses AI's contribution to e-waste."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context does not provide specific average system power per processor values for TPU v2 and V100 GPU. While some power ratings are mentioned (e.g., V100 at 300W in luccioni2023 and TPU v3 at 450W), there's no direct comparison between TPU v2 and V100 GPU power consumption.","is_blank","Watts","[""is_blank""]","is_blank","is_blank","The context does not provide specific average system power per processor values for TPU v2 and V100 GPU. While some power ratings are mentioned (e.g., V100 at 300W in luccioni2023 and TPU v3 at 450W), there's no direct comparison between TPU v2 and V100 GPU power consumption."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context states that training the GPT-3 model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This information is explicitly mentioned in the abstract and repeated in the introduction section of the provided document.","700000","liters","[""li2025b""]","is_blank","is_blank","The context states that training the GPT-3 model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This information is explicitly mentioned in the abstract and repeated in the introduction section of the provided document."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context explicitly states multiple times that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. For example, one snippet mentions 'Facebook’s recommendation model sizes have increased by 20× between 2019 and 2021' and another says 'Similarly, Figure 2(c) illustrates that between 2019 and 2021, the size of recommendation models at Facebook has increased by 20×'.","20","multiplier","[""wu2021a""]","is_blank","is_blank","The context explicitly states multiple times that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. For example, one snippet mentions 'Facebook’s recommendation model sizes have increased by 20× between 2019 and 2021' and another says 'Similarly, Figure 2(c) illustrates that between 2019 and 2021, the size of recommendation models at Facebook has increased by 20×'."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context from jegham2025 states that in the Cross-efficiency DEA Results, o3-mini achieved the highest cross-efficiency score of 0.884. This directly answers the question about which model ranked highest in the recent eco-efficiency analysis using DEA.","o3-mini","is_blank","[""jegham2025""]","is_blank","is_blank","The context from jegham2025 states that in the Cross-efficiency DEA Results, o3-mini achieved the highest cross-efficiency score of 0.884. This directly answers the question about which model ranked highest in the recent eco-efficiency analysis using DEA."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context from ref_id=wu2021b explicitly states that the current average lifetime of cell phones is less than 3 years, which directly supports the assertion in the question. This short lifespan contributes to e-waste concerns as mentioned in the sustainability section.","1","is_blank","[""wu2021b""]","is_blank","is_blank","The context from ref_id=wu2021b explicitly states that the current average lifetime of cell phones is less than 3 years, which directly supports the assertion in the question. This short lifespan contributes to e-waste concerns as mentioned in the sustainability section."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context states that for CV models in intra-zone experiments with T4 GPUs, the per-GPU speedup was almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2 to 8 GPUs. This indicates nearly linear scaling, supporting the statement as true.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context states that for CV models in intra-zone experiments with T4 GPUs, the per-GPU speedup was almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2 to 8 GPUs. This indicates nearly linear scaling, supporting the statement as true."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens.","2048","tokens","[""li2025a""]","is_blank","is_blank","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLM decoding is memory-intensive and does not fully utilize GPU compute resources, leading to lower power draw compared to diffusion models which are compute-intensive and reach near-maximum GPU power consumption. Additionally, it mentions that LLM decoding is bottlenecked by VRAM bandwidth, resulting in underutilized computation units and lower power draw.","1","is_blank","[""chung2025""]","is_blank","is_blank","The context states that LLM decoding is memory-intensive and does not fully utilize GPU compute resources, leading to lower power draw compared to diffusion models which are compute-intensive and reach near-maximum GPU power consumption. Additionally, it mentions that LLM decoding is bottlenecked by VRAM bandwidth, resulting in underutilized computation units and lower power draw."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context includes a table under 'People Managers' and 'Gender' which shows the percentage of women in 2023 as 31.6% globally. This is found in the Amazon Representation by the Numbers section.","31.6","percent","[""amazon2023""]","is_blank","is_blank","The context includes a table under 'People Managers' and 'Gender' which shows the percentage of women in 2023 as 31.6% globally. This is found in the Amazon Representation by the Numbers section."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context states that when excluding equity, the fraction of computing hardware costs for the four key models (GPT-3, OPT-175B, GPT-4, Gemini Ultra) analyzed by Cottier et al. (2025) rises to 61–76%.","61–76","percent","[""cottier2024""]","is_blank","is_blank","The context states that when excluding equity, the fraction of computing hardware costs for the four key models (GPT-3, OPT-175B, GPT-4, Gemini Ultra) analyzed by Cottier et al. (2025) rises to 61–76%."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context explicitly states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a trade-off between energy efficiency and predictive performance. Additionally, Table III shows specific models where accuracy and F1 scores decreased post-optimization.","0","is_blank","[""khan2025""]","is_blank","is_blank","The context explicitly states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a trade-off between energy efficiency and predictive performance. Additionally, Table III shows specific models where accuracy and F1 scores decreased post-optimization."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The context snippet from ref_id=strubell2019 explicitly states that the U.S. EPA provides an average CO2 production rate of 0.954 pounds per kilowatt-hour (lbs/kWh) for power consumed in the U.S. This value is used in the equation CO2e = 0.954pt to convert power consumption to CO2 emissions.","0.954","lbs/kWh","[""strubell2019""]","is_blank","is_blank","The context snippet from ref_id=strubell2019 explicitly states that the U.S. EPA provides an average CO2 production rate of 0.954 pounds per kilowatt-hour (lbs/kWh) for power consumed in the U.S. This value is used in the equation CO2e = 0.954pt to convert power consumption to CO2 emissions."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context mentions a recent study suggesting that global AI could consume 85–134 TWh of electricity in 2027, as cited in multiple sections of the document with ref_id=li2025b.","85–134","TWh","[""li2025b""]","is_blank","is_blank","The context mentions a recent study suggesting that global AI could consume 85–134 TWh of electricity in 2027, as cited in multiple sections of the document with ref_id=li2025b."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context explicitly states under the Europe section that Amazon delivered 150 million packages via EVs in 2023.","150","packages","[""amazon2023""]","is_blank","is_blank","The context explicitly states under the Europe section that Amazon delivered 150 million packages via EVs in 2023."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The context from kim2025 specifies that the online inference workload processed 3000 requests with 128 input tokens and 512 output tokens each. The total tokens per request are 128 + 512 = 640. Multiplying by 3000 requests gives 1,920,000 tokens.","1920000","tokens","[""kim2025""]","is_blank","is_blank","The context from kim2025 specifies that the online inference workload processed 3000 requests with 128 input tokens and 512 output tokens each. The total tokens per request are 128 + 512 = 640. Multiplying by 3000 requests gives 1,920,000 tokens."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context explicitly states that the AI Act fails to address greenhouse gas emissions from AI applications in sectors like oil and gas exploration. It also mentions that the Act does not mandate disclosure of GHG emissions, creating a reporting gap.","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context explicitly states that the AI Act fails to address greenhouse gas emissions from AI applications in sectors like oil and gas exploration. It also mentions that the Act does not mandate disclosure of GHG emissions, creating a reporting gap."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context explicitly states that for the DenseNet 201 experiment in the West US region using the Flexible Start optimization, the maximum CO2 emissions reduction is up to 80%. This is supported by Figure 3(a) and the accompanying text in the Dodge et al. paper.","80","percent","[""dodge2022""]","is_blank","is_blank","The context explicitly states that for the DenseNet 201 experiment in the West US region using the Flexible Start optimization, the maximum CO2 emissions reduction is up to 80%. This is supported by Figure 3(a) and the accompanying text in the Dodge et al. paper."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context states that the least energy-intensive task is text classification with a mean consumption of 0.002 kWh per 1,000 inferences, while the most energy-intensive task is image generation with a mean consumption of 2.9 kWh. The factor is calculated by dividing 2.9 by 0.002, resulting in 1450 times more energy required.","1450","times","[""luccioni2024""]","is_blank","is_blank","The context states that the least energy-intensive task is text classification with a mean consumption of 0.002 kWh per 1,000 inferences, while the most energy-intensive task is image generation with a mean consumption of 2.9 kWh. The factor is calculated by dividing 2.9 by 0.002, resulting in 1450 times more energy required."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context explicitly states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This directly contradicts the statement that traditional models achieved comparable accuracy to LLMs.","0","is_blank","[""zschache2025""]","is_blank","is_blank","The context explicitly states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs. This directly contradicts the statement that traditional models achieved comparable accuracy to LLMs."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context includes a table titled 'Amazon Renewable Energy Projects* Projects announced as of January 2024' which lists the United States with 244 projects under the 'Number of Projects' column.","244","projects","[""amazon2023""]","is_blank","is_blank","The context includes a table titled 'Amazon Renewable Energy Projects* Projects announced as of January 2024' which lists the United States with 244 projects under the 'Number of Projects' column."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context mentions that Amazon achieved matching 100% of the electricity consumed by AWS data centers with renewable energy in 2023. However, the question specifically asks about 2018. The provided documents do not contain any information about Amazon's renewable energy percentage for AWS in 2018.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context mentions that Amazon achieved matching 100% of the electricity consumed by AWS data centers with renewable energy in 2023. However, the question specifically asks about 2018. The provided documents do not contain any information about Amazon's renewable energy percentage for AWS in 2018."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The context from ebert2024 explicitly states that for energy consumption reporting, the authors recommend measurement at the cumulative server level to balance accuracy and feasibility. This is supported by the recommendation to report energy consumption at this level to capture total computation-related power usage and optimize energy efficiency.","Cumulative server level","is_blank","[""ebert2024""]","is_blank","is_blank","The context from ebert2024 explicitly states that for energy consumption reporting, the authors recommend measurement at the cumulative server level to balance accuracy and feasibility. This is supported by the recommendation to report energy consumption at this level to capture total computation-related power usage and optimize energy efficiency."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context states that in an experiment training a BERT-base model, the GPU accounted for 74% of the total electricity consumption. This is explicitly shown in Table 1, which provides the fraction of electricity consumption per hardware component.","74","percent","[""dodge2022""]","is_blank","is_blank","The context states that in an experiment training a BERT-base model, the GPU accounted for 74% of the total electricity consumption. This is explicitly shown in Table 1, which provides the fraction of electricity consumption per hardware component."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context from ref_id=griggs2024 explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. Another context from ref_id=samsi2024 discusses the 65B LLaMA model requiring 4 A100 GPUs, but the question specifically asks about Llama2-70b, which is addressed in griggs2024.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","is_blank","The context from ref_id=griggs2024 explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. Another context from ref_id=samsi2024 discusses the 65B LLaMA model requiring 4 A100 GPUs, but the question specifically asks about Llama2-70b, which is addressed in griggs2024."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context explicitly states that the iterative optimization process led to a 28.5% operational energy footprint reduction over a two-year period (2019 to 2021) at Facebook. This is directly mentioned in multiple sections and figures within the provided document snippets.","28.5","percent","[""wu2021a""]","is_blank","is_blank","The context explicitly states that the iterative optimization process led to a 28.5% operational energy footprint reduction over a two-year period (2019 to 2021) at Facebook. This is directly mentioned in multiple sections and figures within the provided document snippets."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The provided context does not mention any specific data about kilometers of fiber optic cable installed globally in 2023 to support AI workloads. The documents discuss growth in model sizes, data ingestion, infrastructure capacity, and water/carbon footprints, but no figures related to fiber optic cable installations are provided.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The provided context does not mention any specific data about kilometers of fiber optic cable installed globally in 2023 to support AI workloads. The documents discuss growth in model sizes, data ingestion, infrastructure capacity, and water/carbon footprints, but no figures related to fiber optic cable installations are provided."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context explicitly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800 million, as provided in the example comparing acquisition cost to amortized costs.","800000000","USD","[""cottier2024""]","is_blank","is_blank","The context explicitly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800 million, as provided in the example comparing acquisition cost to amortized costs."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context states that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. This directly contradicts the claim that it was proposed to only encompass climate-positive applications.","0","is_blank","[""luccioni2025b""]","is_blank","is_blank","The context states that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. This directly contradicts the claim that it was proposed to only encompass climate-positive applications."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context mentions that for NVIDIA A100-40GB GPU, the maximum batch size (MBS) for fine-tuning Mixtral is determined through their analytical model. Table IV in the context lists the MBS for A100-80GB and H100 but does not explicitly state the value for A100-40GB. However, Figure 13 in the context shows the 'Ground Truth Projection' for different GPUs, including A100-40GB, which correlates with the experimental data. The figure and related text indicate that the ground truth maximum batch size for A100-40GB is projected based on their model, though the exact number isn't directly stated in the text. The closest explicit mention in the context is for A100-80GB (17 samples) and future projections (28 and 35 for 100GB and 120GB), but not for 40GB. Therefore, based on the provided context, the answer cannot be determined with certainty and should be marked as 'is_blank'.","is_blank","samples","[""is_blank""]","is_blank","is_blank","The context mentions that for NVIDIA A100-40GB GPU, the maximum batch size (MBS) for fine-tuning Mixtral is determined through their analytical model. Table IV in the context lists the MBS for A100-80GB and H100 but does not explicitly state the value for A100-40GB. However, Figure 13 in the context shows the 'Ground Truth Projection' for different GPUs, including A100-40GB, which correlates with the experimental data. The figure and related text indicate that the ground truth maximum batch size for A100-40GB is projected based on their model, though the exact number isn't directly stated in the text. The closest explicit mention in the context is for A100-80GB (17 samples) and future projections (28 and 35 for 100GB and 120GB), but not for 40GB. Therefore, based on the provided context, the answer cannot be determined with certainty and should be marked as 'is_blank'."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context states that when the server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Swapping copies data without running extra computation, which is more energy-efficient than Recomputation's approach of recomputing from scratch.","Swapping","is_blank","[""chung2025""]","is_blank","is_blank","The context states that when the server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Swapping copies data without running extra computation, which is more energy-efficient than Recomputation's approach of recomputing from scratch."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context includes multiple references to Google's 2024 Environmental Report, but none of the provided snippets explicitly mention a percentage increase in GHG emissions since 2019. The documents cite the report's URL and discuss related topics like energy efficiency and water usage, but specific GHG emission figures are not present.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context includes multiple references to Google's 2024 Environmental Report, but none of the provided snippets explicitly mention a percentage increase in GHG emissions since 2019. The documents cite the report's URL and discuss related topics like energy efficiency and water usage, but specific GHG emission figures are not present."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context from Han et al. (2024) states that U.S. data center electricity consumption is projected to increase to 6.7–12.0% of the national total in 2028. However, the question asks about 2030 projections. Another context snippet from Fernandez et al. (2025) cites projections estimating 9.1–11.7% by 2030. These ranges from different sources provide the answer.","9.1–11.7","percent","[""han2024"", ""fernandez2025""]","is_blank","is_blank","The context from Han et al. (2024) states that U.S. data center electricity consumption is projected to increase to 6.7–12.0% of the national total in 2028. However, the question asks about 2030 projections. Another context snippet from Fernandez et al. (2025) cites projections estimating 9.1–11.7% by 2030. These ranges from different sources provide the answer."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context mentions that for the 6B parameter transformer training run, the Pause and Resume optimization can lead to significant savings up to about 25% when doubling the duration. Additionally, Table 9 shows a maximum emissions decrease of 11.4% for the 6B Transformer with a 100% duration increase. However, there is a discrepancy between the text and the table. The text mentions up to 25% savings, while the table shows 11.4%. Given the direct reference in Table 9, the answer is based on the table data.","11.4","percent","[""dodge2022""]","is_blank","is_blank","The context mentions that for the 6B parameter transformer training run, the Pause and Resume optimization can lead to significant savings up to about 25% when doubling the duration. Additionally, Table 9 shows a maximum emissions decrease of 11.4% for the 6B Transformer with a 100% duration increase. However, there is a discrepancy between the text and the table. The text mentions up to 25% savings, while the table shows 11.4%. Given the direct reference in Table 9, the answer is based on the table data."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. This is mentioned multiple times across different sections of the document.","70","percent","[""shen2024""]","is_blank","is_blank","The context explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. This is mentioned multiple times across different sections of the document."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context snippet from ref_id=luccioni2025b explicitly states that the 2019 study by Strubell et al. quantified the carbon footprint of training BERT as 626,155 pounds of CO2 emissions.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","is_blank","The context snippet from ref_id=luccioni2025b explicitly states that the 2019 study by Strubell et al. quantified the carbon footprint of training BERT as 626,155 pounds of CO2 emissions."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context snippet from ref_id=li2025b explicitly states that GPT-3's training energy was estimated at 1287 MWh, which directly answers the question.","1287","MWh","[""li2025b""]","is_blank","is_blank","The context snippet from ref_id=li2025b explicitly states that GPT-3's training energy was estimated at 1287 MWh, which directly answers the question."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context from xia2024 states that the MoE layer is the most time-consuming component in LLM fine-tuning, accounting for 85% of execution time, and is a prime target for optimization. Additionally, it mentions that optimizing the MoE layer is crucial for enhancing performance.","1","is_blank","[""xia2024""]","is_blank","is_blank","The context from xia2024 states that the MoE layer is the most time-consuming component in LLM fine-tuning, accounting for 85% of execution time, and is a prime target for optimization. Additionally, it mentions that optimizing the MoE layer is crucial for enhancing performance."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context mentions that for Mixtral fine-tuning on an NVIDIA A40 GPU with 48GB memory, the maximum batch size (MBS) is projected. Figure 13 shows the maximum batch size for Mixtral on A40 as 4 samples. Table IV also lists the MBS for A40 as 4 under the sparse setup, but the question specifies a dense setup. However, the context does not explicitly provide the maximum batch size for a dense setup on Hellaswag, leading to uncertainty.","is_blank","samples","[""is_blank""]","is_blank","is_blank","The context mentions that for Mixtral fine-tuning on an NVIDIA A40 GPU with 48GB memory, the maximum batch size (MBS) is projected. Figure 13 shows the maximum batch size for Mixtral on A40 as 4 samples. Table IV also lists the MBS for A40 as 4 under the sparse setup, but the question specifies a dense setup. However, the context does not explicitly provide the maximum batch size for a dense setup on Hellaswag, leading to uncertainty."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7% compared to the fully local experiment. This directly supports the statement in the question.","1","is_blank","[""erben2023""]","is_blank","is_blank","The context states that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7% compared to the fully local experiment. This directly supports the statement in the question."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","is_blank","is_blank","hectares","[""is_blank""]","is_blank","is_blank","is_blank"
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context explicitly states that the BERT-based model 'bert-base-multilingual-uncased-sentiment' emits 0.32g of CO2eq per 1,000 text classification queries, as part of a comparison with multi-purpose models.","0.32","g CO2eq","[""luccioni2024""]","is_blank","is_blank","The context explicitly states that the BERT-based model 'bert-base-multilingual-uncased-sentiment' emits 0.32g of CO2eq per 1,000 text classification queries, as part of a comparison with multi-purpose models."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context states that the most efficient regions emitted approximately 7k grams of CO2, while the least efficient regions emitted 26k grams. The range between these values is 26,000 - 7,000 = 19,000 grams, which is 19 thousand grams.","19000","grams","[""dodge2022""]","is_blank","is_blank","The context states that the most efficient regions emitted approximately 7k grams of CO2, while the least efficient regions emitted 26k grams. The range between these values is 26,000 - 7,000 = 19,000 grams, which is 19 thousand grams."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context explicitly states that 'The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.' This indicates that 2022 was the peak year before the decline.","2022","year","[""luccioni2025c""]","is_blank","is_blank","The context explicitly states that 'The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.' This indicates that 2022 was the peak year before the decline."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context from morrison2025's Table 2 lists the OLMo 60M† model with water consumption equivalent to 5 days for one person in the U.S. when trained on 1.7 to 5.6 trillion tokens.","5","days","[""morrison2025""]","is_blank","is_blank","The context from morrison2025's Table 2 lists the OLMo 60M† model with water consumption equivalent to 5 days for one person in the U.S. when trained on 1.7 to 5.6 trillion tokens."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context mentions that one technology company's self-owned data centers consumed more than 23 billion liters of freshwater for on-site cooling in 2023, with Google's sustainability report cited as a source. This directly answers the question about DeepMind AlphaFold servers' freshwater consumption under Google's infrastructure.","23000000000","liters of freshwater","[""li2025b""]","is_blank","is_blank","The context mentions that one technology company's self-owned data centers consumed more than 23 billion liters of freshwater for on-site cooling in 2023, with Google's sustainability report cited as a source. This directly answers the question about DeepMind AlphaFold servers' freshwater consumption under Google's infrastructure."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that adding compute resources to accelerate the MoE layers is a way to further reduce costs during fine-tuning. This implies that such an action does not increase costs but rather helps in lowering them.","0","is_blank","[""xia2024""]","is_blank","is_blank","The context states that adding compute resources to accelerate the MoE layers is a way to further reduce costs during fine-tuning. This implies that such an action does not increase costs but rather helps in lowering them."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The context mentions that the total cost of FLM-101B is computed as 52.76 zettaFLOPs, which includes energy consumption. Additionally, Table 3 lists the energy consumption for FLM-101B as 40 MkWh (40,000 kWh).","40000","kWh","[""li2025a""]","is_blank","is_blank","The context mentions that the total cost of FLM-101B is computed as 52.76 zettaFLOPs, which includes energy consumption. Additionally, Table 3 lists the energy consumption for FLM-101B as 40 MkWh (40,000 kWh)."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context states that under the high-growth scenario, the total public health impact of U.S. data centers is estimated to reach $20.9 billion in 2028. This directly answers the question about the projected amount for 2030, though the year in the context is 2028. Since the question mentions 'up to more than what amount' and the context provides $20.9 billion as the high estimate for 2028, it is reasonable to use this figure for the 2030 projection.","20.9","USD","[""han2024""]","is_blank","is_blank","The context states that under the high-growth scenario, the total public health impact of U.S. data centers is estimated to reach $20.9 billion in 2028. This directly answers the question about the projected amount for 2030, though the year in the context is 2028. Since the question mentions 'up to more than what amount' and the context provides $20.9 billion as the high estimate for 2028, it is reasonable to use this figure for the 2030 projection."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context includes Table IV which provides the estimated cost of fine-tuning Mixtral on the GSM8K dataset with sparse MoE using different GPUs. For the NVIDIA A40-48GB GPU, the table lists a total cost of $32.7.","32.7","USD","[""xia2024""]","is_blank","is_blank","The context includes Table IV which provides the estimated cost of fine-tuning Mixtral on the GSM8K dataset with sparse MoE using different GPUs. For the NVIDIA A40-48GB GPU, the table lists a total cost of $32.7."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context mentions Table III which lists the maximum batch sizes for Mixtral-S (sparse) on the MATH dataset as 3 samples when using an NVIDIA A40-48 GB GPU. The longest-running MoE layer's batch size would correspond to this maximum supported batch size.","3","samples","[""xia2024""]","is_blank","is_blank","The context mentions Table III which lists the maximum batch sizes for Mixtral-S (sparse) on the MATH dataset as 3 samples when using an NVIDIA A40-48 GB GPU. The longest-running MoE layer's batch size would correspond to this maximum supported batch size."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context provides Table IV which estimates the cost and throughput for fine-tuning Mixtral on different GPUs. For the NVIDIA A40-48GB GPU with a batch size of 4 (MBS), the throughput is 1.01 queries per second. To calculate the total execution time for a batch size of 10, the context does not provide direct data for batch size 10, but mentions that throughput does not scale linearly. However, since the maximum batch size supported for A40 is 4 (from Table III), using a batch size of 10 is not feasible, leading to an unanswerable question.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context provides Table IV which estimates the cost and throughput for fine-tuning Mixtral on different GPUs. For the NVIDIA A40-48GB GPU with a batch size of 4 (MBS), the throughput is 1.01 queries per second. To calculate the total execution time for a batch size of 10, the context does not provide direct data for batch size 10, but mentions that throughput does not scale linearly. However, since the maximum batch size supported for A40 is 4 (from Table III), using a batch size of 10 is not feasible, leading to an unanswerable question."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context explicitly states that GPU-level power consumption tracking is not recommended for overall energy measurements. Specifically, it mentions 'We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.'","0","is_blank","[""ebert2024""]","is_blank","is_blank","The context explicitly states that GPU-level power consumption tracking is not recommended for overall energy measurements. Specifically, it mentions 'We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.'"
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context states that in the 2023 article by Luccioni et al., training accounted for only half (50%) of BLOOM's overall emissions when using a Life Cycle Assessment approach.","50","percent","[""luccioni2025b""]","is_blank","is_blank","The context states that in the 2023 article by Luccioni et al., training accounted for only half (50%) of BLOOM's overall emissions when using a Life Cycle Assessment approach."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states in Table II that the bare minimum hardware required for LLaMA 7B is 1 V100 32GB GPU. This is specified under the 'V100 32GB' column with a count of 1 for the 7B model, indicating that a single GPU suffices for inference without compression or quantization.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","is_blank","The context states in Table II that the bare minimum hardware required for LLaMA 7B is 1 V100 32GB GPU. This is specified under the 'V100 32GB' column with a count of 1 for the 7B model, indicating that a single GPU suffices for inference without compression or quantization."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context snippet from ref_id=li2025b includes a table (Table 1) that specifically lists the number of requests needed to consume a 500ml bottle of water for GPT-3 in different data center locations. For Arizona, the table shows '16.7' under the '# of Requests for 500ml Water' column.","16.7","requests","[""li2025b""]","is_blank","is_blank","The context snippet from ref_id=li2025b includes a table (Table 1) that specifically lists the number of requests needed to consume a 500ml bottle of water for GPT-3 in different data center locations. For Arizona, the table shows '16.7' under the '# of Requests for 500ml Water' column."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context snippet from ref_id=dodge2022 explicitly states that the average US home energy use emits 8.3 metric tons of CO2 per year, which directly answers the question.","8.3","metric tons","[""dodge2022""]","is_blank","is_blank","The context snippet from ref_id=dodge2022 explicitly states that the average US home energy use emits 8.3 metric tons of CO2 per year, which directly answers the question."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context mentions that JetMoE-8B outperforms the Llama2-7B model and discusses its performance on various benchmarks, but it does not explicitly state the GSM8k benchmark score.","is_blank","score","[""is_blank""]","is_blank","is_blank","The context mentions that JetMoE-8B outperforms the Llama2-7B model and discusses its performance on various benchmarks, but it does not explicitly state the GSM8k benchmark score."
