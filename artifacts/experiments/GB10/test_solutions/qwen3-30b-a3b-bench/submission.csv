"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context states that the total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with higher increases in previous periods. This directly answers the question about the average increase during that time frame. The answer unit is 'percent', so the value is expressed as a number without the unit name.","4","percent","[""wu2021b"", ""han2024""]","[""https://arxiv.org/pdf/2106.05585.pdf"", ""https://www.epri.com/research/products/3002028905""]","['The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].', 'The U.S. data center electricity consumption is expected to increase from 4.4% of the total national electricity use in 2023 to 6.7–12.0% in 2028, depending on the growth trajectory of AI adoption.']","The context states that the total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with higher increases in previous periods. This directly answers the question about the average increase during that time frame. The answer unit is 'percent', so the value is expressed as a number without the unit name."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. This direct comparison is explicitly provided in the text, making the answer straightforward.","13900","cars","[""amazon2023""]","is_blank","['Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.']","The context states that Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. This direct comparison is explicitly provided in the text, making the answer straightforward."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24. This indicates that AWS began using recycled water in 4 additional data centers in 2023. The answer unit is 'data centers', so the answer is expressed as a number.","4","data centers","[""amazon2023""]","is_blank","['In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.']","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24. This indicates that AWS began using recycled water in 4 additional data centers in 2023. The answer unit is 'data centers', so the answer is expressed as a number."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context states that NVIDIA does not release the embodied carbon emissions for its hardware, so the authors assume the same embodied carbon emissions as Luccioni et al. (2023), which is 3700 kg of CO2eq per 8x server node, equaling 463 kg per GPU. This value is explicitly provided in the text as the estimated embodied carbon emissions per GPU.","463","kg/GPU","[""morrison2025""]","[""https://www.azcentral.com/story/opinion/op-ed/joannaallhands/2024/06/12/tsmc-arizona-water-use-recycling/74059522007/"", ""https://www.semiconductor-digest.com/water-supply-challenges-for-the-semiconductor-industry/""]","['Hardware manufacturing NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU.']","The context states that NVIDIA does not release the embodied carbon emissions for its hardware, so the authors assume the same embodied carbon emissions as Luccioni et al. (2023), which is 3700 kg of CO2eq per 8x server node, equaling 463 kg per GPU. This value is explicitly provided in the text as the estimated embodied carbon emissions per GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The estimated amortized training cost of GPT-4 is $40M, as stated in the context. The total training budget for FLM-101B is $100,000. To find the factor by which GPT-4's cost exceeds FLM-101B's budget, divide $40,000,000 by $100,000, resulting in a factor of 400. This matches the expected format of a numeric ratio.","400","ratio","[""li2025a"", ""cottier2024""]","[""https://arxiv.org/pdf/2309.03852.pdf"", ""https://arxiv.org/pdf/2401.00001.pdf""]","['GPT-4 was published in March of 2023 [12]. This implies that, at a growth rate of 2.4× per year, the most expensive publicly announced model by the start of 2027 will cost about $1 billion.', 'It’s important to distinguish the amortized cost of the hardware used for training, which is spread over the useful lifetime of the hardware, and the acquisition cost of purchasing that hardware outright. The choice of which cost to consider depends on the purpose of the analysis. Amortized costs are more relevant for understanding the economics of training and deploying models over an extended period, while acquisition costs give a sense of the capital barriers to entry and financial risks involved in developing such models.', 'For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.']","The estimated amortized training cost of GPT-4 is $40M, as stated in the context. The total training budget for FLM-101B is $100,000. To find the factor by which GPT-4's cost exceeds FLM-101B's budget, divide $40,000,000 by $100,000, resulting in a factor of 400. This matches the expected format of a numeric ratio."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context states that Google Flights calculates the average CO2e for a direct round trip between San Francisco and New York as 180.4 tCO2e for the whole passenger jet. Dividing this by the average seat occupancy of 84.5% and the Boeing 767's 175 seats yields approximately 1.2 tCO2e per passenger round trip. This value is explicitly confirmed in multiple sections of the context.","1.2","tCO2e","[""patterson2021""]","is_blank","['To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).', 'Google Flights uses the historical average of 84.5% seat occupancy, yielding 1.2t of CO2e per passenger round trip.']","The context states that Google Flights calculates the average CO2e for a direct round trip between San Francisco and New York as 180.4 tCO2e for the whole passenger jet. Dividing this by the average seat occupancy of 84.5% and the Boeing 767's 175 seats yields approximately 1.2 tCO2e per passenger round trip. This value is explicitly confirmed in multiple sections of the context."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context states that on average, FLM-101B achieves a score of 43.94 on the Open LLM Leaderboard, which evaluates models on tasks like ARC, HellaSwag, MMLU, and TruthfulQA. This score is explicitly mentioned in multiple sections, including Table 4 and the results summary.","43.94","score","[""li2025a""]","is_blank","['On average, FLM-101B achieves a score of 43.94, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.', 'Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B. Model Cost (zettaFLOPs) Average ARC HellaSwag MMLU TruthfulQA ... FLM-101B 28.22 43.94 39.76 66.23 28.30 ∗ 41.47']","The context states that on average, FLM-101B achieves a score of 43.94 on the Open LLM Leaderboard, which evaluates models on tasks like ARC, HellaSwag, MMLU, and TruthfulQA. This score is explicitly mentioned in multiple sections, including Table 4 and the results summary."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context states that the improvement in processor clock speed between the Intel 4004 (1971) and typical 2021 microprocessors is more than 6,750 fold. This is directly supported by the sentence: 'This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.' The answer is given in folds as required by the answer_unit.","6750","fold","[""wu2021b""]","[""https://www.intel.co.uk/content/www/uk/en/history/museum-story-of-intel-4004.html""]","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context states that the improvement in processor clock speed between the Intel 4004 (1971) and typical 2021 microprocessors is more than 6,750 fold. This is directly supported by the sentence: 'This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.' The answer is given in folds as required by the answer_unit."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context states that it takes approximately 14.8 days for 10,000 V100 GPUs running at 24.6 TeraFLOPS/sec to compute the 3.14E+23 FLOPS required to train GPT-3. This directly answers the question about training duration using the specified hardware and performance metrics.","14.8","days","[""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/""]","It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","The context states that it takes approximately 14.8 days for 10,000 V100 GPUs running at 24.6 TeraFLOPS/sec to compute the 3.14E+23 FLOPS required to train GPT-3. This directly answers the question about training duration using the specified hardware and performance metrics."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context provides a table (Table 4) with measurements of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. For the Llama 3.2 1B model at an 8 request/s frequency, the GPU Power Usage is listed as 0.036 kWh. This value matches the expected unit of kWh as specified in the additional info.","0.036","kWh","[""morrison2025""]","is_blank","Table 4: Full version of Table 3 in §4.2. Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. The models were served on machines from the same cluster that our models were trained on, so we use the same WUE and PUE coefficients of 1.49 L / kWh and 1.2 respectively, and carbon intensity of 0.332 kg CO 2e / kWh. The measurements reported in this table account for the GPU processes associated with active inference, but not CPU or RAM associated with e.g. server overhead. Thus, these numbers can be considered as lower bounds on usage in similar settings. We do not report “break-even” points for Qwen models since the training costs are not public. Request freq. (req / s) GPU Power Usage (kWh) Carbon Emissions (g CO2eq) Water consump. (L) Seconds per 100 req. # Inf. for CO2 equiv. w/ training Llama 3.2 1B ∞ 0.003 1.0 0.004 1.38 258 bil. 8 0.036 12.0 0.054 12.64 21.5 bil. 1 0.16 53.1 0.238 100.58 4.83 bil.","The context provides a table (Table 4) with measurements of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates. For the Llama 3.2 1B model at an 8 request/s frequency, the GPU Power Usage is listed as 0.036 kWh. This value matches the expected unit of kWh as specified in the additional info."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context states that the total permitted annual emission limits for data center backup generators in Northern Virginia are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons. This information is provided in the section discussing the emission limits for diesel generators in Northern Virginia, which includes the timeframe between January 1, 2023, and December 1, 2024. The answer unit is tons, so the value is expressed as a number in that unit.","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context states that the total permitted annual emission limits for data center backup generators in Northern Virginia are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons. This information is provided in the section discussing the emission limits for diesel generators in Northern Virginia, which includes the timeframe between January 1, 2023, and December 1, 2024. The answer unit is tons, so the value is expressed as a number in that unit."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context states that training FLM-101B using the growth strategy took 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). This directly answers the question about the percentage of time saved by using the growth strategy.","72","percent","[""li2025a""]","[""https://arxiv.org/abs/2309.03852v3""]","['Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).']","The context states that training FLM-101B using the growth strategy took 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). This directly answers the question about the percentage of time saved by using the growth strategy."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context states that the total public health costs attributable to U.S. data centers are projected to reach $20 billion by 2028, which exceeds the health costs of on-road emissions in California. The paper also notes that in 2028, the total scope-1 and scope-2 pollutants of U.S. data centers alone could cause approximately 1,300 premature deaths. Since the question asks for the number of premature deaths in 2030, and no data is provided for 2030, the closest available projection is for 2028. The context does not provide a specific number for 2030, so the answer is based on the nearest available data point.","1300","deaths","[""han2024""]","[""https://arxiv.org/abs/2412.06288v2""]","['Our results demonstrate that in 2028, the total scope-1 and scope-2 pollutants of U.S. data centers alone could cause, among others, approximately 600,000 asthma symptom cases and 1,300 premature deaths, exceeding 1/3 of asthma deaths in the U.S. each year [40].']","The context states that the total public health costs attributable to U.S. data centers are projected to reach $20 billion by 2028, which exceeds the health costs of on-road emissions in California. The paper also notes that in 2028, the total scope-1 and scope-2 pollutants of U.S. data centers alone could cause approximately 1,300 premature deaths. Since the question asks for the number of premature deaths in 2030, and no data is provided for 2030, the closest available projection is for 2028. The context does not provide a specific number for 2030, so the answer is based on the nearest available data point."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","Dodge et al. (2022) state that their training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion. They estimate a full training run would consume approximately 103,593 kWh. Given that the partial run lasted 8 days and covered 13% of the total training time, the full training run is estimated to take 8 / 0.13 ≈ 61.5 days. However, the context explicitly states that a full training run would take 60 days, which aligns with the calculation.","60","days","[""dodge2022""]","[""https://arxiv.org/abs/2206.00015""]","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh. This implies a full training run would take 60 days.","Dodge et al. (2022) state that their training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion. They estimate a full training run would consume approximately 103,593 kWh. Given that the partial run lasted 8 days and covered 13% of the total training time, the full training run is estimated to take 8 / 0.13 ≈ 61.5 days. However, the context explicitly states that a full training run would take 60 days, which aligns with the calculation."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context indicates that DeepSeek-R1 (DS) hosted on DeepSeek's own servers has the highest GPU energy consumption for 1,000 queries, with a value of 29.078 Wh for long-form prompts. This is significantly higher than other models listed, such as DeepSeek-V3 (DS) at 13.162 Wh and GPT-4.1 nano at 0.827 Wh. The value is derived from Table 4 in the study, which reports energy consumption per model across different prompt sizes.","29.078","is_blank","[""jegham2025""]","[""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%B8%8F-energy-metrics""]","['Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). DeepSeek-R1 (DS)* 19.251 ± 9.449 24.596 ± 9.4 29.078 ± 9.725']","The context indicates that DeepSeek-R1 (DS) hosted on DeepSeek's own servers has the highest GPU energy consumption for 1,000 queries, with a value of 29.078 Wh for long-form prompts. This is significantly higher than other models listed, such as DeepSeek-V3 (DS) at 13.162 Wh and GPT-4.1 nano at 0.827 Wh. The value is derived from Table 4 in the study, which reports energy consumption per model across different prompt sizes."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This is directly mentioned in multiple snippets, including the PREFACE section of the 2015 Study Panel report. The answer is a specific year, matching the expected format of 'year'.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","['The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.']","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This is directly mentioned in multiple snippets, including the PREFACE section of the 2015 Study Panel report. The answer is a specific year, matching the expected format of 'year'."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The UN’s Global E-Waste Monitor 2024 states that about 22% of e-waste has been formally collected and recycled. This figure is directly cited in the context and matches the requested unit of percent.","22","percent","[""luccioni2025a""]","[""https://ewastemonitor.info/""]","The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The UN’s Global E-Waste Monitor 2024 states that about 22% of e-waste has been formally collected and recycled. This figure is directly cited in the context and matches the requested unit of percent."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context states that the BLOOM-176B model required 51,686 kWh for training, which is equivalent to 51.686 MWh. This value is directly provided in Table 5 from the Luccioni et al. study, which reports the training energy cost for BLOOMz-7B as 51,686 kWh. Since BLOOMz models are fine-tuned versions of the original BLOOM family, the training energy for BLOOM is consistent with this figure. The answer is converted to MWh by dividing by 1,000.","51.686","MWh","[""luccioni2024"", ""ebert2024""]","[""https://www.acm.org/publications/proceedings/facct-2024"", ""https://www.acm.org/publications/proceedings/facct-2024""]","['Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.', 'Training energy (kWh) 51,686 25,634 17,052 10,505']","The context states that the BLOOM-176B model required 51,686 kWh for training, which is equivalent to 51.686 MWh. This value is directly provided in Table 5 from the Luccioni et al. study, which reports the training energy cost for BLOOMz-7B as 51,686 kWh. Since BLOOMz models are fine-tuned versions of the original BLOOM family, the training energy for BLOOM is consistent with this figure. The answer is converted to MWh by dividing by 1,000."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context states that the Switch Transformer model has 1500 billion parameters but only 0.1% are activated per token. This is explicitly mentioned in the table under 'Percent of model activated on every token' for the Switch Transformer entry.","0.1","percent","[""patterson2021""]","[""https://arxiv.org/abs/2101.03961""]","Switch Transformer: 1500 billion parameters but only 0.1% activated per token","The context states that the Switch Transformer model has 1500 billion parameters but only 0.1% are activated per token. This is explicitly mentioned in the table under 'Percent of model activated on every token' for the Switch Transformer entry."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context states that JetMoE-8B sets the same number of experts to 8 and top-k to 2 for every layer. This directly indicates that each MoE layer includes 8 experts. The answer unit is 'experts', so the answer value is the numeric count.","8","experts","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1""]","Then, we set the same number of experts to 8 and top-k to 2 for every layer.","The context states that JetMoE-8B sets the same number of experts to 8 and top-k to 2 for every layer. This directly indicates that each MoE layer includes 8 experts. The answer unit is 'experts', so the answer value is the numeric count."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The execution time breakdown for BlackMamba fine-tuning on a NVIDIA A40 GPU is shown in Fig. 4. For a dense BlackMamba model with batch size 30, the execution time is 1.5 seconds, as indicated in the figure. This includes the forward, backward, and optimizer stages.","1.5","second","[""xia2024""]","is_blank","['Fig. 4. Execution time breakdown.', 'Dense(bsz=30) ... 1.5']","The execution time breakdown for BlackMamba fine-tuning on a NVIDIA A40 GPU is shown in Fig. 4. For a dense BlackMamba model with batch size 30, the execution time is 1.5 seconds, as indicated in the figure. This includes the forward, backward, and optimizer stages."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context states that the total cost of FLM-101B is 52.76 zettaFLOPs, broken down into 28.22 zettaFLOPs for English and 24.54 zettaFLOPs for Chinese. Since the question asks specifically for the English portion, the value 28.22 zettaFLOPs is directly supported by the context. The answer unit is zettaFLOPs, so the numeric value is returned as is.","28.22","zettaFLOPs","[""li2025a""]","is_blank","['The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).']","The context states that the total cost of FLM-101B is 52.76 zettaFLOPs, broken down into 28.22 zettaFLOPs for English and 24.54 zettaFLOPs for Chinese. Since the question asks specifically for the English portion, the value 28.22 zettaFLOPs is directly supported by the context. The answer unit is zettaFLOPs, so the numeric value is returned as is."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The experimental setup for energy-efficient local inference in financial sentiment classification used an 11th Gen Intel(R) Core(TM) i7-1165G7 processor. This is explicitly stated in the context under the 'Hardware and Software Setting' section of the experimental setup.","11th Gen Intel(R) Core(TM) i7-1165G7","is_blank","[""khan2025""]","is_blank","The hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable).","The experimental setup for energy-efficient local inference in financial sentiment classification used an 11th Gen Intel(R) Core(TM) i7-1165G7 processor. This is explicitly stated in the context under the 'Hardware and Software Setting' section of the experimental setup."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context states that the study ran each of the 88 models on 3 datasets 10 times to ensure statistical significance. This indicates that 88 distinct machine learning models were sampled and analyzed. The answer unit is 'models', so the answer value is the number 88.","88","models","[""luccioni2024""]","[""https://doi.org/10.1145/3630106.3658542""]","we ran each of the 88 models on 3 datasets 10 times to ensure statistical significance of our measurements.","The context states that the study ran each of the 88 models on 3 datasets 10 times to ensure statistical significance. This indicates that 88 distinct machine learning models were sampled and analyzed. The answer unit is 'models', so the answer value is the number 88."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that increasing GPU utilization up to 80% for LM training on GPUs results in a threefold reduction in the overall carbon footprint. This is explicitly mentioned in multiple snippets, including the description of Figure 9 and the summary of the impact of improved GPU utilization.","3","multiplier","[""wu2021a""]","is_blank","['Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.', 'Figure 9 illustrates that, as GPU utilization is improved (x-axis) for LM training on GPUs, both embodied and operational carbon emissions will reduce. Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.']","The context states that increasing GPU utilization up to 80% for LM training on GPUs results in a threefold reduction in the overall carbon footprint. This is explicitly mentioned in multiple snippets, including the description of Figure 9 and the summary of the impact of improved GPU utilization."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","Cottier et al. (2025) applied a multiplicative factor to the final training run compute to estimate total compute for model development, including experiments, failed attempts, evaluation, and fine-tuning. Based on evidence from GPT-3, OPT-175B, and BLOOM, they sampled the factor from a log-normal distribution with a 90% credible interval of 1.2x to 4x, meaning total compute for model development is 1.2x to 4x larger than the final training run. This range is explicitly stated in the context.","[1.2, 4.0]","multiplier","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","Cottier et al. (2025) applied a multiplicative factor to the final training run compute to estimate total compute for model development, including experiments, failed attempts, evaluation, and fine-tuning. Based on evidence from GPT-3, OPT-175B, and BLOOM, they sampled the factor from a log-normal distribution with a 90% credible interval of 1.2x to 4x, meaning total compute for model development is 1.2x to 4x larger than the final training run. This range is explicitly stated in the context."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context states that a 6.1 billion parameter transformer model was trained for 8 days (13% of the full training time), consuming 13.8 MWh. To estimate the full training run, the calculation (60/8) * 13.8 MWh is used, resulting in approximately 103.5 MWh. This value is consistent with the provided estimate of 103,593 kWh, which converts to 103.593 MWh. The answer is rounded to match the precision of the context.","103.5","MWh","[""dodge2022""]","[""https://dl.acm.org/doi/10.1145/3510432.3510435""]","Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The context states that a 6.1 billion parameter transformer model was trained for 8 days (13% of the full training time), consuming 13.8 MWh. To estimate the full training run, the calculation (60/8) * 13.8 MWh is used, resulting in approximately 103.5 MWh. This value is consistent with the provided estimate of 103,593 kWh, which converts to 103.593 MWh. The answer is rounded to match the precision of the context."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The central argument of the 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to Jevons’ Paradox. This economic principle posits that increased efficiency can lead to higher overall consumption, as cost savings from efficiency spur greater demand for the technology. The paper explicitly states that rebound effects, such as those described by Jevons’ Paradox, undermine the assumption that technical efficiency alone ensures net environmental reductions.","Jevons’ Paradox","is_blank","[""luccioni2025a""]","[""https://doi.org/10.1145/3715275.3732007""]","['This paper examines how the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.']","The central argument of the 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to Jevons’ Paradox. This economic principle posits that increased efficiency can lead to higher overall consumption, as cost savings from efficiency spur greater demand for the technology. The paper explicitly states that rebound effects, such as those described by Jevons’ Paradox, undermine the assumption that technical efficiency alone ensures net environmental reductions."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027. This range is explicitly mentioned in multiple sections, including the abstract and the appendix, and aligns with the answer_unit of 'billion cubic meters'.","[4.2,6.6]","billion cubic meters","[""li2025b""]","is_blank","['More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.', 'Total water usage.By adding up scope-1 and scope-2 water usage together, the total water withdrawal and water consumption of global AI may reach 4.2 – 6.6 billion cubic meters and 0.38 – 0.60 billion cubic meters, respectively.']","The context states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027. This range is explicitly mentioned in multiple sections, including the abstract and the appendix, and aligns with the answer_unit of 'billion cubic meters'."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context states that Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3). This directly contradicts the claim that Red AI is on the decline. Therefore, the statement is false.","0","is_blank","[""schwartz2019""]","is_blank","['Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).']","The context states that Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3). This directly contradicts the claim that Red AI is on the decline. Therefore, the statement is false."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context explicitly states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days. This value is directly provided in the text and matches the requested unit of days.","21.54","days","[""li2025a""]","[""https://arxiv.org/abs/2309.03852""]","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context explicitly states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days. This value is directly provided in the text and matches the requested unit of days."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50% capacity, leaving room for improvement. This directly contradicts the claim that over 80% of workflows utilize GPUs at over 80% capacity. Therefore, the statement is false.","0","is_blank","[""wu2021a""]","is_blank","['A vast majority of model experimentation (over tens of thousands of training workﬂows) utilizes GPUs at only 30-50%, leaving room for utilization and efﬁciency improvements.']","The context states that a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50% capacity, leaving room for improvement. This directly contradicts the claim that over 80% of workflows utilize GPUs at over 80% capacity. Therefore, the statement is false."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context explicitly states that GPT-3 was trained with an estimated training energy of 1287 MWh. This figure is directly mentioned in the document [ref_id=li2025b] and is consistent with the provided additional information that the answer should be in MWh.","1287","MWh","[""li2025b""]","[""https://arxiv.org/abs/2505.09598""]","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context explicitly states that GPT-3 was trained with an estimated training energy of 1287 MWh. This figure is directly mentioned in the document [ref_id=li2025b] and is consistent with the provided additional information that the answer should be in MWh."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context states that the AI Energy Score 21 project was developed to establish a unified approach for comparing the inference efficiency of AI models. This project is explicitly described as aiming to create a standardized methodology for such comparisons, which directly answers the question.","AI Energy Score 21","is_blank","[""luccioni2025c""]","is_blank","['These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.']","The context states that the AI Energy Score 21 project was developed to establish a unified approach for comparing the inference efficiency of AI models. This project is explicitly described as aiming to create a standardized methodology for such comparisons, which directly answers the question."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context provides execution time breakdowns for the MoE layer in microseconds for different batch sizes and models. For a dense BlackMamba model with a batch size of 30, the longest kernel in the MoE layer is 'matmul(w1)', which takes 1600 microseconds according to Fig. 6. This value is directly observable in the provided data for Dense(bsz=30) in the BlackMamba section.","1600","microseconds","[""xia2024""]","is_blank","Dense(bsz=30) Sparse(bsz=30) Sparse(bsz=84)0
400
800
1200
1600
2000
matmul(w1)
gelu
matmul(w2)
elementwise_mult
top_k
sigmoid
matmul(router)
    Execution Time Breakdown (μs)
Mixtral
Mamba
Fig. 6. Execution breakdown of the MoE layer for different kernels.","The context provides execution time breakdowns for the MoE layer in microseconds for different batch sizes and models. For a dense BlackMamba model with a batch size of 30, the longest kernel in the MoE layer is 'matmul(w1)', which takes 1600 microseconds according to Fig. 6. This value is directly observable in the provided data for Dense(bsz=30) in the BlackMamba section."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context states that JetMoE-8B uses a top-k value of 2 for every layer, meaning two experts are selected for activation per token. This is explicitly mentioned in the hyperparameters section of the document, where it says 'we set the same number of experts to 8 and top-k to 2 for every layer.'","2","experts","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","Table 1: JetMoE-8B hyperparameters. ... Top-k: 2","The context states that JetMoE-8B uses a top-k value of 2 for every layer, meaning two experts are selected for activation per token. This is explicitly mentioned in the hyperparameters section of the document, where it says 'we set the same number of experts to 8 and top-k to 2 for every layer.'"
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context explicitly states that the amount of compute used to train deep learning models has increased 300,000x in 6 years from 2012 to 2018, which exceeds the 200,000x mentioned in the question. This is supported by multiple references, including Figure 1 and the abstract of the paper by Schwartz et al. (2019). The claim in the question is therefore true.","1","is_blank","[""schwartz2019""]","[""https://arxiv.org/abs/1907.10597""]","['Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.', 'The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2].']","The context explicitly states that the amount of compute used to train deep learning models has increased 300,000x in 6 years from 2012 to 2018, which exceeds the 200,000x mentioned in the question. This is supported by multiple references, including Figure 1 and the abstract of the paper by Schwartz et al. (2019). The claim in the question is therefore true."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context states that global carbon emissions dropped by 6.4% in 2020, with vehicle transportation in the US accounting for a portion of this reduction. This decline is directly linked to the COVID-19 pandemic, as mentioned in multiple sources including Tollefson (2021). The answer is expressed as a percentage, matching the required unit.","6.4","percent","[""wu2021b""]","[""https://www.nature.com/articles/d41586-021-00090-3""]","['In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].']","The context states that global carbon emissions dropped by 6.4% in 2020, with vehicle transportation in the US accounting for a portion of this reduction. This decline is directly linked to the COVID-19 pandemic, as mentioned in multiple sources including Tollefson (2021). The answer is expressed as a percentage, matching the required unit."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023, an increase from 19 regions in 2022. This directly answers the question about the number of AWS data center regions where electricity consumption was fully matched with renewable energy.","22","data centers","[""amazon2023""]","is_blank","['Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.']","The context states that 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023, an increase from 19 regions in 2022. This directly answers the question about the number of AWS data center regions where electricity consumption was fully matched with renewable energy."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The field of Artificial Intelligence was officially born and christened at a 1956 workshop. To calculate its age in 2025, subtract 1956 from 2025, resulting in 69 years. This is consistent with the context stating the field's inception sixty years ago in the past, which aligns with the 1956 origin.","69","years","[""stone2022""]","[""https://ai100.stanford.edu""]","['The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.']","The field of Artificial Intelligence was officially born and christened at a 1956 workshop. To calculate its age in 2025, subtract 1956 from 2025, resulting in 69 years. This is consistent with the context stating the field's inception sixty years ago in the past, which aligns with the 1956 origin."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The 'five cars' carbon footprint estimate is based on a neural architecture search (NAS) training workload, which is a large-scale procedure performed infrequently. This process is meant to be reused to reduce emissions of subsequent training workloads and is limited by high resource requirements, making it similar to today's generative AI pretraining workloads. The context explicitly states that the NAS training workload is performed much less frequently than average AI model training.","neural architecture search (NAS)","is_blank","[""luccioni2025c""]","is_blank","['the NAS training workload represents a large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload.', 'In this way, the NAS training workload is similar to today’s generative AI pretraining workloads, which are similarly performed less frequently than the average AI training.']","The 'five cars' carbon footprint estimate is based on a neural architecture search (NAS) training workload, which is a large-scale procedure performed infrequently. This process is meant to be reused to reduce emissions of subsequent training workloads and is limited by high resource requirements, making it similar to today's generative AI pretraining workloads. The context explicitly states that the NAS training workload is performed much less frequently than average AI model training."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that targeting an average TPOT of 100 ms lands on the Pareto frontier at an average TPOT of 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency. This directly answers the question about the percentage decrease in energy use.","44","percent","[""chung2025""]","[""https://huggingface.co/meta-llama/Llama-3.1-8B""]","['This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.']","The context states that targeting an average TPOT of 100 ms lands on the Pareto frontier at an average TPOT of 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency. This directly answers the question about the percentage decrease in energy use."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides a table (Table III) that lists the maximum batch size supported by different model and dataset combinations during fine-tuning on a NVIDIA A40 GPU with 48GB memory. For BlackMamba with a sparse setup on the GSM8K (GS) dataset, the maximum batch size is 8 samples.","8","samples","[""xia2024""]","is_blank","TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20 MATH 1 3 2 8","The context provides a table (Table III) that lists the maximum batch size supported by different model and dataset combinations during fine-tuning on a NVIDIA A40 GPU with 48GB memory. For BlackMamba with a sparse setup on the GSM8K (GS) dataset, the maximum batch size is 8 samples."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context states that Amazon held 1.3 GW of energy storage capacity in 2023, up from 445 MW in 2022. This information is directly provided in the text and matches the required unit of gigawatts (GW).","1.3","GW","[""amazon2023""]","is_blank","['We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.']","The context states that Amazon held 1.3 GW of energy storage capacity in 2023, up from 445 MW in 2022. This information is directly provided in the text and matches the required unit of gigawatts (GW)."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that GPT-4o’s annual carbon emissions are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London. This directly answers the question about the number of flights equivalent to the emissions.","272","flights","[""jegham2025""]","is_blank","These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context states that GPT-4o’s annual carbon emissions are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London. This directly answers the question about the number of flights equivalent to the emissions."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context does not provide any information about the percentage of AI inference workloads in Asia powered by coal in 2023. While there are mentions of energy sources like coal in the context of AI training and data center operations, no specific data is given for Asia or for inference workloads in that region.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the percentage of AI inference workloads in Asia powered by coal in 2023. While there are mentions of energy sources like coal in the context of AI training and data center operations, no specific data is given for Asia or for inference workloads in that region."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context explicitly states that the average data center PUE in 2023 was 1.58 globally. This figure is directly relevant to the question about AI-dedicated data centers, as AI-related computations are conducted in data centers, and the PUE metric is used to assess their energy efficiency. The answer unit is PUE, so the value is given as a numeric decimal.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","['The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].']","The context explicitly states that the average data center PUE in 2023 was 1.58 globally. This figure is directly relevant to the question about AI-dedicated data centers, as AI-related computations are conducted in data centers, and the PUE metric is used to assess their energy efficiency. The answer unit is PUE, so the value is given as a numeric decimal."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context states that JetMoE-8B has 8B parameters but only activates 2B for each input token during inference. This is explicitly mentioned in multiple snippets, including the abstract and the section on model architecture. The answer unit is 'parameters', so the answer is given as a numeric value in billions.","2","parameters","[""shen2024""]","[""https://github.com/myshell-ai/JetMoE""]","['In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.', 'Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.']","The context states that JetMoE-8B has 8B parameters but only activates 2B for each input token during inference. This is explicitly mentioned in multiple snippets, including the abstract and the section on model architecture. The answer unit is 'parameters', so the answer is given as a numeric value in billions."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context provides a table listing the GHG emissions for various pre-trained models. According to Table 1 in the appendix, the Llama 7B model, trained by Meta, has associated GHG emissions of 14 tCO2e.","14","tCO2e","[""luccioni2025c""]","[""https://shrinkthatfootprint.com/carbon-footprint-of-training-gpt-3-and-large-language-models/""]","Table 1. Range of Pre-Training Environmental Impacts (Representative Models Displayed)
Model Organization Energy Consumption (MWh) GHG Emissions (tCO2e)
Llama 7B 63 Meta 356 14","The context provides a table listing the GHG emissions for various pre-trained models. According to Table 1 in the appendix, the Llama 7B model, trained by Meta, has associated GHG emissions of 14 tCO2e."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context provides the number of electric delivery vans in the U.S. fleet for 2022 and 2023. In 2022, there were more than 2,600 electric delivery vans, and in 2023, this increased to 11,800. The difference between these numbers gives the total number of vans added: 11,800 - 2,600 = 9,200. This aligns with the answer unit of 'electric delivery vans'.","9200","electric delivery vans","[""amazon2023""]","is_blank","['United States • Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.']","The context provides the number of electric delivery vans in the U.S. fleet for 2022 and 2023. In 2022, there were more than 2,600 electric delivery vans, and in 2023, this increased to 11,800. The difference between these numbers gives the total number of vans added: 11,800 - 2,600 = 9,200. This aligns with the answer unit of 'electric delivery vans'."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. This directly contradicts the claim in the question that such emissions are excluded.","0","is_blank","[""morrison2025""]","is_blank","['Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.']","The context explicitly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. This directly contradicts the claim in the question that such emissions are excluded."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context provides energy consumption values for the o3 model in Table 4 under the 'Energy Consumption(10k input-1.5k output)(Wh)' column, which corresponds to a long prompt. The value listed for o3 is 12.222 Wh, with a standard deviation of 1.082 Wh.","12.222","Wh","[""jegham2025""]","is_blank","o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082","The context provides energy consumption values for the o3 model in Table 4 under the 'Energy Consumption(10k input-1.5k output)(Wh)' column, which corresponds to a long prompt. The value listed for o3 is 12.222 Wh, with a standard deviation of 1.082 Wh."
"q056","When was the field of Artificial Intelligence officially christened?","The context explicitly states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. This is confirmed in multiple snippets, including the one that directly mentions the year 1956 as the date of the event.","1956","year","[""stone2022""]","[""http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html""]","['The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.']","The context explicitly states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. This is confirmed in multiple snippets, including the one that directly mentions the year 1956 as the date of the event."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not provide the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. While Google's sustainability reports are referenced for water efficiency data, no specific WUE value for 2024 is mentioned. Therefore, the answer is not available in the provided documents.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The context does not provide the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024. While Google's sustainability reports are referenced for water efficiency data, no specific WUE value for 2024 is mentioned. Therefore, the answer is not available in the provided documents."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity, according to the International Energy Agency. This directly supports the claim in the question.","1","is_blank","[""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","['Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].']","The context explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity, according to the International Energy Agency. This directly supports the claim in the question."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context states that at a maximum generation length of 512 tokens, it takes about 3-4 Joules for a single output token. This directly answers the question about energy per token for LLaMA-65B at that generation length. The answer unit is joules per token, so the value is expressed as a range in joules.","[3,4]","joules per token","[""samsi2024""]","is_blank","['For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.']","The context states that at a maximum generation length of 512 tokens, it takes about 3-4 Joules for a single output token. This directly answers the question about energy per token for LLaMA-65B at that generation length. The answer unit is joules per token, so the value is expressed as a range in joules."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context states that converting the 32-bit floating-point numerical representation to 16-bit reduces the overall RM2 model size by 15%. This directly answers the question about the percentage reduction in model size after quantization.","15","percent","[""wu2021a""]","is_blank","['By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.']","The context states that converting the 32-bit floating-point numerical representation to 16-bit reduces the overall RM2 model size by 15%. This directly answers the question about the percentage reduction in model size after quantization."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The claim that AI can reduce global GHG emissions by 5-10% is not supported by clear, publicly available calculations or sound scientific grounding. According to the context, the underlying calculations are not detailed, and the reasoning is based on BCG’s experience rather than rigorous methodology. Furthermore, applying observations from individual projects to global emissions lacks scientific grounding, and proper calculation requires counterfactual scenarios and rebound effect accounting, which are not provided.","0","is_blank","[""luccioni2025c""]","is_blank","['The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and using AI to optimize and improve existing processes.', 'Applying observations made from individual projects to the entire planet’s GHG emissions lacks any scientific grounding— in fact, many of the emissions reductions on a global scale require individual, societal and political shifts.']","The claim that AI can reduce global GHG emissions by 5-10% is not supported by clear, publicly available calculations or sound scientific grounding. According to the context, the underlying calculations are not detailed, and the reasoning is based on BCG’s experience rather than rigorous methodology. Furthermore, applying observations from individual projects to global emissions lacks scientific grounding, and proper calculation requires counterfactual scenarios and rebound effect accounting, which are not provided."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context explicitly states that large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy, despite using as many or even more parameters. This directly supports the claim in the question.","True","is_blank","[""patterson2021""]","is_blank","['Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.']","The context explicitly states that large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy, despite using as many or even more parameters. This directly supports the claim in the question."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000. This directly answers the question about the training cost in USD.","25000","USD","[""schwartz2019""]","[""https://arxiv.org/pdf/1905.04592.pdf""]","['Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.']","The context explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000. This directly answers the question about the training cost in USD."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context states that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, up to 53% when conducting sparse fine-tuning with batch size = 1. This specific condition (sparse fine-tuning, batch size = 1) matches the question's scenario. The context does not specify a different percentage for the NVIDIA A40-48GB GPU, so the 53% figure applies. The answer unit is percent, so the value is expressed as a number.","53","percent","[""xia2024""]","is_blank","['The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)']","The context states that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, up to 53% when conducting sparse fine-tuning with batch size = 1. This specific condition (sparse fine-tuning, batch size = 1) matches the question's scenario. The context does not specify a different percentage for the NVIDIA A40-48GB GPU, so the 53% figure applies. The answer unit is percent, so the value is expressed as a number."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context states that Google Translate performs approximately 1 billion queries per day, and the energy consumption for a comparable multi-purpose model, Flan-T5-xxl, is 0.083 kWh per 1,000 queries. To estimate daily energy consumption, we scale this rate: (1,000,000,000 queries/day) × (0.083 kWh / 1,000 queries) = 83,000 kWh/day. Converting to MWh (1 MWh = 1,000 kWh) gives 83 MWh/day. The answer unit is MWh, so the final value is 83.","83","MWh","[""luccioni2024"", ""jegham2025""]","[""https://blog.google/products/translate/ten-years-of-google-translate/"", ""https://arxiv.org/abs/2403.20306""]","['1Google reported translating more than 100 billion words per day in 2016, assuming an average query length of 100 words yields an estimate of 1 billion queries to the model per day.', 'Flan-T5-xxl at 0.083 kWh/1k queries']","The context states that Google Translate performs approximately 1 billion queries per day, and the energy consumption for a comparable multi-purpose model, Flan-T5-xxl, is 0.083 kWh per 1,000 queries. To estimate daily energy consumption, we scale this rate: (1,000,000,000 queries/day) × (0.083 kWh / 1,000 queries) = 83,000 kWh/day. Converting to MWh (1 MWh = 1,000 kWh) gives 83 MWh/day. The answer unit is MWh, so the final value is 83."
"q067","What was the average global data center PUE in 2023?","The context explicitly states that the average data center PUE in 2023 was 1.58 globally. This information is directly provided in multiple snippets, including [ref_id=ebert2024], which cites Statista as the source. The answer unit is PUE, so the value is given as a numeric ratio.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context explicitly states that the average data center PUE in 2023 was 1.58 globally. This information is directly provided in multiple snippets, including [ref_id=ebert2024], which cites Statista as the source. The answer unit is PUE, so the value is given as a numeric ratio."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The context does not provide any information about the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023. While Microsoft is involved in renewable energy projects and has partnerships with companies like NVIDIA, there is no mention of specific wind turbines contracted for Azure AI clusters in 2023. Therefore, the answer cannot be determined from the provided information.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023. While Microsoft is involved in renewable energy projects and has partnerships with companies like NVIDIA, there is no mention of specific wind turbines contracted for Azure AI clusters in 2023. Therefore, the answer cannot be determined from the provided information."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context states that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model. Gemini Ultra has the highest fraction of R&D staff cost at 49%, which is explicitly mentioned as the value for this model when equity is included.","49","percent","[""cottier2024""]","is_blank","['Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.']","The context states that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model. Gemini Ultra has the highest fraction of R&D staff cost at 49%, which is explicitly mentioned as the value for this model when equity is included."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context explicitly states that the seventeen-member Study Panel was launched in mid-fall 2015. This directly identifies the number of members on the inaugural 2015 Study Panel. The answer unit is 'people', so the answer is given as the number 17.","17","people","[""stone2022""]","[""https://ai100.stanford.edu""]","['The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.']","The context explicitly states that the seventeen-member Study Panel was launched in mid-fall 2015. This directly identifies the number of members on the inaugural 2015 Study Panel. The answer unit is 'people', so the answer is given as the number 17."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. This directly answers the question about the percentage of a client device's total carbon footprint attributed to manufacturing.","74","percent","[""wu2021a""]","[""https://tech.fb.com/sustainable-computing/""]","['Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices.']","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. This directly answers the question about the percentage of a client device's total carbon footprint attributed to manufacturing."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context explicitly states that models with more parameters do not necessarily result in higher energy consumption during inference. This is demonstrated by the example of Phi-3 Mini (3.8B parameters) and Phi-3 Small (7B parameters), where the larger model consumes less energy at higher batch sizes due to more efficient memory usage from Grouped Query Attention (GQA). This contradicts the claim that more parameters always lead to higher energy use.","0","is_blank","[""chung2025"", ""zschache2025""]","[""https://ml.energy/leaderboard""]","['Figure 4 highlights the case of Phi-3 Mini (3.8B) and Small (7B) [26]. Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows.', 'This happens because Mini uses Multi-Head Attention (MHA) [76], whereas Small uses Grouped Query Attention (GQA) [10]. Due to this, Mini’s KV cache uses 3 × more memory than Small, which prevents it from scaling to larger batch sizes and amortizing energy consumption across more generations.']","The context explicitly states that models with more parameters do not necessarily result in higher energy consumption during inference. This is demonstrated by the example of Phi-3 Mini (3.8B parameters) and Phi-3 Small (7B parameters), where the larger model consumes less energy at higher batch sizes due to more efficient memory usage from Grouped Query Attention (GQA). This contradicts the claim that more parameters always lead to higher energy use."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The Study Panel explicitly states that there is no cause for concern that AI is an imminent threat to humankind. This is directly supported by multiple references in the context, including the statement that no machines with self-sustaining long-term goals and intent have been developed, nor are they likely to be in the near future.","0","is_blank","[""stone2022""]","[""https://ai100.stanford.edu""]","['Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.', 'No machines with self-sustaining long-term goals and intent have been developed, nor are they likely to be developed in the near future.']","The Study Panel explicitly states that there is no cause for concern that AI is an imminent threat to humankind. This is directly supported by multiple references in the context, including the statement that no machines with self-sustaining long-term goals and intent have been developed, nor are they likely to be in the near future."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context does not provide any information about the CO2 emissions from OpenAI's API requests in January 2024. While several documents discuss emissions from training models or inference, none specify the emissions for OpenAI's API during that specific month.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the CO2 emissions from OpenAI's API requests in January 2024. While several documents discuss emissions from training models or inference, none specify the emissions for OpenAI's API during that specific month."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context states that Meta reports the Llama 3 family of models emitted 11,390 tons CO2e, which is over 40 times the 'five cars' estimate. This figure is derived from direct reporting by Meta, as cited in the provided text.","11390","tCO2e","[""luccioni2025c""]","[""https://ghgprotocol.org/sites/default/files/standards/ghg-protocol-revised.pdf""]","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the 'five cars' estimate.","The context states that Meta reports the Llama 3 family of models emitted 11,390 tons CO2e, which is over 40 times the 'five cars' estimate. This figure is derived from direct reporting by Meta, as cited in the provided text."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context explicitly states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over the 1.5-year period, as illustrated in Figure 2(d). This matches the time frame from Yr1-Q1 to Yr2-Q2 between 2019 and 2021. The answer unit is 'multiplier', so the value is expressed as a numeric multiplier.","2.9","multiplier","[""wu2021a""]","[""https://arxiv.org/abs/2111.00364""]","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","The context explicitly states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over the 1.5-year period, as illustrated in Figure 2(d). This matches the time frame from Yr1-Q1 to Yr2-Q2 between 2019 and 2021. The answer unit is 'multiplier', so the value is expressed as a numeric multiplier."
"q079","How many miles is the Earth from the Sun?","The context provided does not contain any information about the distance between the Earth and the Sun. The documents discuss various topics such as AI model training energy consumption, vehicle emissions, data center electricity usage, and transportation logistics, but none mention the Earth-Sun distance.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The context provided does not contain any information about the distance between the Earth and the Sun. The documents discuss various topics such as AI model training energy consumption, vehicle emissions, data center electricity usage, and transportation logistics, but none mention the Earth-Sun distance."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context explicitly states that AlphaGo, developed by Google DeepMind, beat the human Go champion in a five-game match. This is confirmed in multiple snippets, including the description of AlphaGo's success and its use of reinforcement learning to refine its performance after being initialized with a human expert database.","1","is_blank","[""stone2022""]","[""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html""]","['The recent success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to reinforcement learning.']","The context explicitly states that AlphaGo, developed by Google DeepMind, beat the human Go champion in a five-game match. This is confirmed in multiple snippets, including the description of AlphaGo's success and its use of reinforcement learning to refine its performance after being initialized with a human expert database."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context explicitly describes a batching strategy that dynamically replaces completed requests with new ones to reduce idle GPU time. This strategy is referred to as 'continuous batching' in multiple snippets, including [ref_id=fernandez2025], which states: 'Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).' This directly answers the question.","Continuous batching","is_blank","[""fernandez2025""]","is_blank","['Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).']","The context explicitly describes a batching strategy that dynamically replaces completed requests with new ones to reduce idle GPU time. This strategy is referred to as 'continuous batching' in multiple snippets, including [ref_id=fernandez2025], which states: 'Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).' This directly answers the question."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours. This is explicitly mentioned in the section '5 Model Alignment' under the description of the dDPO fine-tuning process.","60","H100 GPU hours","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","The entire alignment process takes 60 H100 GPU hours.","The context states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours. This is explicitly mentioned in the section '5 Model Alignment' under the description of the dDPO fine-tuning process."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","In the offline workload experiment with a 100 TPS SLO, InferSave selected g4dn.xlarge with a total cost of $2.13. The Max-Performance policy selected g6e.xlarge with a total cost of $2.699. The cost difference is calculated as (2.699 - 2.13) / 2.13 ≈ 0.267, or 26.7%. This is explicitly stated in the context: 'an increase of about 26.7%'.","26.7","percent","[""kim2025""]","is_blank","['On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.']","In the offline workload experiment with a 100 TPS SLO, InferSave selected g4dn.xlarge with a total cost of $2.13. The Max-Performance policy selected g6e.xlarge with a total cost of $2.699. The cost difference is calculated as (2.699 - 2.13) / 2.13 ≈ 0.267, or 26.7%. This is explicitly stated in the context: 'an increase of about 26.7%'."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context explicitly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. This value is directly provided in the text and matches the required unit of grams of CO2eq.","1594","g CO2eq","[""luccioni2024""]","is_blank","['For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of 𝐶𝑂2𝑒𝑞 for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51]']","The context explicitly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. This value is directly provided in the text and matches the required unit of grams of CO2eq."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context states that GPU energy usage for 1,000 queries spans from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), based on model size, architecture, and task complexity. This range is explicitly mentioned in the 2025 study's appendix, which aligns with the requested unit of Wh.","[0.06,3426]","Wh","[""luccioni2025c""]","is_blank","task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The context states that GPU energy usage for 1,000 queries spans from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), based on model size, architecture, and task complexity. This range is explicitly mentioned in the 2025 study's appendix, which aligns with the requested unit of Wh."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context explicitly states that there is no one-size-fits-all solution for either ethics or sustainability in AI. It further emphasizes that no single way can conclusively determine that an AI system is neither truly ethical nor sustainable. The authors argue against universal approaches, advocating instead for context-specific and nuanced frameworks. This directly contradicts the idea of a universal, one-size-fits-all solution.","0","is_blank","[""luccioni2025b""]","is_blank","['There is no one-size-ﬁts-all solution for either ethics or s ustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sus tainable.', 'We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist)']","The context explicitly states that there is no one-size-fits-all solution for either ethics or sustainability in AI. It further emphasizes that no single way can conclusively determine that an AI system is neither truly ethical nor sustainable. The authors argue against universal approaches, advocating instead for context-specific and nuanced frameworks. This directly contradicts the idea of a universal, one-size-fits-all solution."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context states that the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh, as cited from the U.S. Energy Information Administration in 2021. This value is explicitly mentioned in the provided text and matches the required unit.","0.429","kg of CO2e/KWh","[""patterson2021""]","[""https://www.eia.gov/tools/faqs/faq.php?id=74&t=11""]","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","The context states that the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh, as cited from the U.S. Energy Information Administration in 2021. This value is explicitly mentioned in the provided text and matches the required unit."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context explicitly states that the Hivemind framework was used for all experiments to enable distributed spot instance training across clouds and continents. Hivemind is described as a PyTorch-based framework developed for collaborative deep learning training in a decentralized fashion, capable of handling peer dropouts and functioning effectively in low-bandwidth, high-latency networks. The framework's features, such as a distributed hash table and adaptive all-reduce algorithms, make it suitable for geo-distributed training on spot instances.","Hivemind","is_blank","[""erben2023""]","[""https://github.com/learning-at-home/hivemind""]","['We used the Hivemind framework for all of our experiments, as it provided the base for training on spot instances in high latency, low bandwidth networks.', 'Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion.']","The context explicitly states that the Hivemind framework was used for all experiments to enable distributed spot instance training across clouds and continents. Hivemind is described as a PyTorch-based framework developed for collaborative deep learning training in a decentralized fashion, capable of handling peer dropouts and functioning effectively in low-bandwidth, high-latency networks. The framework's features, such as a distributed hash table and adaptive all-reduce algorithms, make it suitable for geo-distributed training on spot instances."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context explicitly proposes the term 'social transparency' to describe the expansion of transparency in AI to include socio-technical aspects, societal impacts, ethical considerations, and environmental footprint. This is directly stated in the text: 'In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems [56].'","social transparency","is_blank","[""luccioni2025b""]","[""https://arxiv.org/abs/2504.00797""]","['In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems [56].']","The context explicitly proposes the term 'social transparency' to describe the expansion of transparency in AI to include socio-technical aspects, societal impacts, ethical considerations, and environmental footprint. This is directly stated in the text: 'In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems [56].'"
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context states that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. This model outperformed several large language models and was among the top-performing models in the experiments. The specific model mentioned is not named beyond 'a traditional linear model using pre-trained sentence embeddings,' but this is the highest-performing model in the category of sentence embedding-based models.","traditional linear model using pre-trained sentence embeddings","is_blank","[""zschache2025""]","[""https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2""]","['The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.']","The context states that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. This model outperformed several large language models and was among the top-performing models in the experiments. The specific model mentioned is not named beyond 'a traditional linear model using pre-trained sentence embeddings,' but this is the highest-performing model in the category of sentence embedding-based models."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context explicitly states that the authors developed and deployed Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. This is directly mentioned in multiple sections, including the abstract and the conclusion. The name 'Lamina' is the specific system developed for this purpose.","Lamina","is_blank","[""chen2024""]","[""https://arxiv.org/abs/2405.01814""]","['To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.', 'We develop and deploy Lamina on a cluster comprising heterogeneous GPUs. Evaluation on traces collected from production systems show that Lamina provides16.1 ∼ 90.1% higher throughput than heterogeneous solutions with similar hardware costs.']","The context explicitly states that the authors developed and deployed Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. This is directly mentioned in multiple sections, including the abstract and the conclusion. The name 'Lamina' is the specific system developed for this purpose."
"q093","How many parameters does the largest T5 model have?","The context does not provide information about the number of parameters in the largest T5 model. While T5 is mentioned in relation to its FLOPS and training approach, no specific parameter count is given for any T5 model variant.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context does not provide information about the number of parameters in the largest T5 model. While T5 is mentioned in relation to its FLOPS and training approach, no specific parameter count is given for any T5 model variant."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context explicitly states that JetMoE-8B has 8B parameters. This is directly mentioned in multiple sections, including the description of the model's architecture and its advantages. The answer unit is 'parameters', so the value is expressed as a number in billions.","8","parameters","[""shen2024""]","is_blank","['JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.']","The context explicitly states that JetMoE-8B has 8B parameters. This is directly mentioned in multiple sections, including the description of the model's architecture and its advantages. The answer unit is 'parameters', so the value is expressed as a number in billions."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context states that one technology company’s data center water consumption increased by ∼20% from 2021 to 2022, and another technology company’s saw ∼34% and ∼22% increases over the same periods. Since Google is identified as one of the technology companies in the context, and the ∼20% increase is explicitly linked to Google in the sentence: 'Google observed a 20% uptick in the same period' [ref_id=luccioni2025a], this confirms the percentage increase for Google. The answer unit is 'percent', so the value is expressed as a number.","20","percent","[""luccioni2025a"", ""li2025b""]","[""https://doi.org/10.1177/2053951715592429"", ""https://www.google.com/about/datacenters/efficiency/""]","['Google observed a 20% uptick in the same period [42, 78].', 'the company’s data center water consumption increased by ∼20% from 2021 to 2022 and by ∼17% from 2022 to 2023 [4]']","The context states that one technology company’s data center water consumption increased by ∼20% from 2021 to 2022, and another technology company’s saw ∼34% and ∼22% increases over the same periods. Since Google is identified as one of the technology companies in the context, and the ∼20% increase is explicitly linked to Google in the sentence: 'Google observed a 20% uptick in the same period' [ref_id=luccioni2025a], this confirms the percentage increase for Google. The answer unit is 'percent', so the value is expressed as a number."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context defines 'CO₂ emissions per unit of electricity consumed' as the metric known as 'Carbon Intensity'. This is explicitly stated in Table I of the document [ref_id=khan2025], which lists 'Carbon Intensity' with the unit 'gCO2/kWh' and the definition matching the question.","Carbon Intensity","is_blank","[""khan2025""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410""]","TABLE I COMMON CARBON EMISSION METRICS IN GREEN AI Metric Unit Definition Reference Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed International Energy Agency","The context defines 'CO₂ emissions per unit of electricity consumed' as the metric known as 'Carbon Intensity'. This is explicitly stated in Table I of the document [ref_id=khan2025], which lists 'Carbon Intensity' with the unit 'gCO2/kWh' and the definition matching the question."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context states that in the 101B stage (final growth stage), the FLOPs utilization rate is 52.88%. This is explicitly mentioned in Table 2, which details the parallelism configurations and training throughput for each stage of FLM-101B training. The answer unit is 'percent', so the value is reported as a percentage.","52.88","percent","[""li2025a""]","[""https://arxiv.org/abs/2309.03852v3""]","Table 2: Parallel strategies and throughput for different growth stages. ... 101 4 4 12 192 2160 165 52.88%","The context states that in the 101B stage (final growth stage), the FLOPs utilization rate is 52.88%. This is explicitly mentioned in Table 2, which details the parallelism configurations and training throughput for each stage of FLM-101B training. The answer unit is 'percent', so the value is reported as a percentage."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context explicitly states that the most expensive publicly-announced training run to date is OpenAI’s GPT-4, with an amortized hardware and energy cost of $40 million. This figure is directly cited in the document as the estimated amortized training cost for GPT-4. The answer unit is USD, so the value is expressed as a number in that unit.","40000000","USD","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","['Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.']","The context explicitly states that the most expensive publicly-announced training run to date is OpenAI’s GPT-4, with an amortized hardware and energy cost of $40 million. This figure is directly cited in the document as the estimated amortized training cost for GPT-4. The answer unit is USD, so the value is expressed as a number in that unit."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context states that for a Transformer-based universal translation model, the operational energy footprint can be reduced by more than 800× using platform-level caching, GPU acceleration, low precision data format, and additional algorithmic optimization. This aggregate optimization is described as 'full-stack' and directly addresses the question. The answer unit is 'multiplier', so the value is expressed as a numeric multiplier.","810","multiplier","[""wu2021a""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410""]","Fig. 7. For the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than 800× using platform-level caching, GPUs, low precision data format , and additional algorithmic optimization .","The context states that for a Transformer-based universal translation model, the operational energy footprint can be reduced by more than 800× using platform-level caching, GPU acceleration, low precision data format, and additional algorithmic optimization. This aggregate optimization is described as 'full-stack' and directly addresses the question. The answer unit is 'multiplier', so the value is expressed as a numeric multiplier."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context states that intercontinental training leads to a significant penalty on NLP tasks, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8). This means the throughput achieved was 59% of the local throughput (100% - 41% = 59%). The fraction of local throughput is therefore 0.59, which matches the expected format of a multiplier.","0.59","multiplier","[""erben2023""]","is_blank","['However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).']","The context states that intercontinental training leads to a significant penalty on NLP tasks, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8). This means the throughput achieved was 59% of the local throughput (100% - 41% = 59%). The fraction of local throughput is therefore 0.59, which matches the expected format of a multiplier."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context explicitly states that AWS’s water replenishment portfolio returned 3.5 billion liters to local communities in 2023. This figure is directly mentioned in multiple snippets and aligns with the answer_unit of 'liters'.","3500000000","liters","[""amazon2023""]","is_blank","['In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.']","The context explicitly states that AWS’s water replenishment portfolio returned 3.5 billion liters to local communities in 2023. This figure is directly mentioned in multiple snippets and aligns with the answer_unit of 'liters'."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context explicitly states that custom tags can reduce energy consumption across all three prompt engineering techniques tested: zero-shot, one-shot, and few-shots. This is supported by the study's findings in Figure 3a and the conclusion for RQ1, which confirms that custom tags improve energy efficiency for these techniques in source code completion tasks.","1","is_blank","[""rubei2025""]","[""https://github.com/riccardoRubei/Greens-2025-Replication-Package""]","['Answer to RQ 1: Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.', 'Figure 3a shows the energy consumption of the three prompt techniques applied to the five different configurations. ... one-shot and few-shots reduced the consumption from 0.0000347 to 0.0000174 (-99%) and from 0.0000537 to 0.0000293 (-83%) comparing with the default configuration C0, respectively.']","The context explicitly states that custom tags can reduce energy consumption across all three prompt engineering techniques tested: zero-shot, one-shot, and few-shots. This is supported by the study's findings in Figure 3a and the conclusion for RQ1, which confirms that custom tags improve energy efficiency for these techniques in source code completion tasks."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context states that NVIDIA shipped 3.7 million GPUs in 2024, according to a study cited in a 2025 paper. This information is directly provided in the snippet [ref_id=luccioni2025a], which references the shipment figure for 2024. The answer unit is GPUs, so the numeric value is reported as such.","3.7","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","['NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].']","The context states that NVIDIA shipped 3.7 million GPUs in 2024, according to a study cited in a 2025 paper. This information is directly provided in the snippet [ref_id=luccioni2025a], which references the shipment figure for 2024. The answer unit is GPUs, so the numeric value is reported as such."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context states that on average, 44% of the amortized hardware CapEx + energy cost is attributed to AI accelerator chips. This figure is explicitly mentioned in the section discussing the breakdown of components in Figure 5, which aligns with the question's focus on average percentage.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context states that on average, 44% of the amortized hardware CapEx + energy cost is attributed to AI accelerator chips. This figure is explicitly mentioned in the section discussing the breakdown of components in Figure 5, which aligns with the question's focus on average percentage."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context explicitly states that Facebook’s data centers have a Power Usage Effectiveness (PUE) of about 1.10. This is confirmed in multiple snippets, including [ref_id=wu2021a] and [ref_id=wu2021b], which both report the same value. The answer unit is PUE, so the value is given as a numeric ratio.","1.10","PUE","[""wu2021a"", ""wu2021b""]","[""https://tech.fb.com/hyperefficient-data-centers/"", ""https://www.google.com/about/datacenters/efficiency/""]","['Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efﬁcient than small-scale, typical data centers.', 'Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook] and the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].']","The context explicitly states that Facebook’s data centers have a Power Usage Effectiveness (PUE) of about 1.10. This is confirmed in multiple snippets, including [ref_id=wu2021a] and [ref_id=wu2021b], which both report the same value. The answer unit is PUE, so the value is given as a numeric ratio."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems is explicitly named in the context as ETAIROS. This is directly stated in multiple instances within the provided text.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","['From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]']","The Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems is explicitly named in the context as ETAIROS. This is directly stated in multiple instances within the provided text."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context states that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M. This directly provides the estimated amortized training cost for Gemini Ultra in USD.","30000000","USD","[""cottier2024""]","is_blank","['We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.']","The context states that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M. This directly provides the estimated amortized training cost for Gemini Ultra in USD."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context states that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk (Art. 55(1)(b)). Furthermore, it argues that these measures should also consider environmental risks, in line with the Act’s normative goals, including environmental protection. Although the Act does not explicitly require environmental risk inclusion, the provided interpretation supports that such risks must be part of the assessment.","True","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","['For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.']","The context states that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk (Art. 55(1)(b)). Furthermore, it argues that these measures should also consider environmental risks, in line with the Act’s normative goals, including environmental protection. Although the Act does not explicitly require environmental risk inclusion, the provided interpretation supports that such risks must be part of the assessment."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context explicitly states that the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³. This is directly mentioned in multiple instances, including in the sentence: 'In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, considerably higher than the WHO’s recommended level of 5µg/m3 [48, 52].' The answer unit is µg/m³, so the value is 9.","9","µg/m³","[""han2024""]","[""https://www.epa.gov/criteria-air-pollutants/naaqs-table""]","['In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, considerably higher than the WHO’s recommended level of 5µg/m3 [48, 52].']","The context explicitly states that the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³. This is directly mentioned in multiple instances, including in the sentence: 'In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, considerably higher than the WHO’s recommended level of 5µg/m3 [48, 52].' The answer unit is µg/m³, so the value is 9."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","A life cycle assessment comparing print books to e-readers found that 115 print books produce the same amount of CO2 as a single Amazon Kindle device. This direct comparison is explicitly stated in the context, making 115 the correct answer. The answer unit is 'books', so the numeric value is provided accordingly.","115","books","[""luccioni2025a""]","[""https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/""]","['a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].']","A life cycle assessment comparing print books to e-readers found that 115 print books produce the same amount of CO2 as a single Amazon Kindle device. This direct comparison is explicitly stated in the context, making 115 the correct answer. The answer unit is 'books', so the numeric value is provided accordingly."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context explicitly states that the per-household health burden in the most affected, economically-disadvantaged communities could be up to 200 times higher than in less-affected areas. This is directly supported by multiple references, including the ratio of the highest to lowest county-level per-household health cost being approximately 200, and the disproportionate impact on low-income communities. The answer unit is 'multiplier', so the value is expressed as a numeric multiplier.","200","multiplier","[""han2024""]","[""https://www.cdc.gov/surveillance/data-modernization/technologies/ai-ml.html"", ""https://www.state.gov/new-air-quality-dashboard-uses-ai-to-forecast-pollution-levels/""]","['The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.', 'disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas.']","The context explicitly states that the per-household health burden in the most affected, economically-disadvantaged communities could be up to 200 times higher than in less-affected areas. This is directly supported by multiple references, including the ratio of the highest to lowest county-level per-household health cost being approximately 200, and the disproportionate impact on low-income communities. The answer unit is 'multiplier', so the value is expressed as a numeric multiplier."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is explicitly stated in Table B1 of the context. The value is 702.06 Wh, which corresponds to the energy consumed during the inference phase, as measured on a single node of the Capella system. This matches the requested unit of Wh.","702.06","Wh","[""zschache2025""]","[""https://arxiv.org/abs/2310.03003""]","T able B1 Measurements of all models for the inference task on the FKTG dataset, Capella system, single node, shown are averages over 10 runs","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is explicitly stated in Table B1 of the context. The value is 702.06 Wh, which corresponds to the energy consumed during the inference phase, as measured on a single node of the Capella system. This matches the requested unit of Wh."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context mentions that Dodge et al. (2022) measured electricity consumption and carbon emissions for training language models, including a 6 billion parameter transformer model. This model is described as being only partially trained, but the parameter count is explicitly stated as 6 billion. The reference to this model is in the context of Dodge et al.'s work.","6e9","parameters","[""dodge2022""]","[""https://dl.acm.org/doi/10.1145/3534678.3539400""]","The largest experiment in our set is the 6 billion parameter transformer, and that model is only partially trained (as described in §4, it is only trained for about 13% of the time needed to converge).","The context mentions that Dodge et al. (2022) measured electricity consumption and carbon emissions for training language models, including a 6 billion parameter transformer model. This model is described as being only partially trained, but the parameter count is explicitly stated as 6 billion. The reference to this model is in the context of Dodge et al.'s work."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The phenomenon described is Jevons' Paradox, where technological progress improves efficiency, leading to lower costs and increased usage, which paradoxically results in higher overall resource consumption. This is supported by the context, which explicitly links efficiency gains in AI hardware to increased demand and rising energy use despite improvements in efficiency. The concept is traced back to William Stanley Jevons, who observed that more efficient coal use led to greater coal consumption across industries.","Jevons' Paradox","is_blank","[""luccioni2025a""]","[""https://doi.org/10.1145/3715275.3732007""]","['Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60].']","The phenomenon described is Jevons' Paradox, where technological progress improves efficiency, leading to lower costs and increased usage, which paradoxically results in higher overall resource consumption. This is supported by the context, which explicitly links efficiency gains in AI hardware to increased demand and rising energy use despite improvements in efficiency. The concept is traced back to William Stanley Jevons, who observed that more efficient coal use led to greater coal consumption across industries."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context states that GPT-3's training energy is estimated at 1,287 MWh. The Meena model's training emissions are 53% of a round trip between San Francisco and New York, while GPT-3's emissions are ~305% of the same round trip. By comparing these percentages, the energy of a single GPT-3 training run is approximately 305/53 ≈ 5.75 times that of a Meena training run. Therefore, about 5.75 Meena training runs would use the same total energy as a single GPT-3 training run.","5.75","multiplier","[""patterson2021""]","is_blank","['T5 training emissions are ~26%, Meena is 53%, Gshard-600B is ~2%, Switch Transformer is 32%, and GPT-3 is ~305% of such a round trip.']","The context states that GPT-3's training energy is estimated at 1,287 MWh. The Meena model's training emissions are 53% of a round trip between San Francisco and New York, while GPT-3's emissions are ~305% of the same round trip. By comparing these percentages, the energy of a single GPT-3 training run is approximately 305/53 ≈ 5.75 times that of a Meena training run. Therefore, about 5.75 Meena training runs would use the same total energy as a single GPT-3 training run."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","According to Table 2 in the 2024 study by Luccioni et al., the average energy consumption for performing 1,000 image generation inferences is 2.907 kWh. This value is directly taken from the 'image generation' row under the 'mean' column in the table.","2.907","kWh","[""luccioni2024""]","[""https://dl.acm.org/doi/10.1145/3640732.3640754""]","Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis. image generation 2.907 3.31","According to Table 2 in the 2024 study by Luccioni et al., the average energy consumption for performing 1,000 image generation inferences is 2.907 kWh. This value is directly taken from the 'image generation' row under the 'mean' column in the table."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context states that the average US home energy use emits 8.3 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil. Since 1 metric ton equals 2204.62 pounds, 8.3 metric tons is equivalent to 8.3 × 2204.62 = 18,298.346 pounds. The context directly provides this value as the emissions equivalent for an average US home, which corresponds to an average American life's annual emissions.","18298.346","lbs","[""dodge2022""]","[""https://www.epa.gov""]","one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)","The context states that the average US home energy use emits 8.3 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil. Since 1 metric ton equals 2204.62 pounds, 8.3 metric tons is equivalent to 8.3 × 2204.62 = 18,298.346 pounds. The context directly provides this value as the emissions equivalent for an average US home, which corresponds to an average American life's annual emissions."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context identifies West Virginia counties with high per-household health costs due to their role in supplying electricity to data centers in neighboring states. Specifically, Figure 6 and the accompanying text list the top counties by per-household health cost, with WV Marion having the highest value at 1218.3 (978.0, 1458.5) USD per household. This is the highest among all listed counties in West Virginia.","WV Marion","is_blank","[""han2024""]","is_blank","['Figure 6:The county-level per-household health cost of U.S. data centers from 2019 to 2023. (a) Per-household health cost map; (b) CDF of county-level per-household health cost; (c) Top-10 counties by per-household health cost. IR represents “County-to-Nation Per-Household Median Income Ratio.”', 'WV Marion 1218.3(978.0, 1458.5) 0.80']","The context identifies West Virginia counties with high per-household health costs due to their role in supplying electricity to data centers in neighboring states. Specifically, Figure 6 and the accompanying text list the top counties by per-household health cost, with WV Marion having the highest value at 1218.3 (978.0, 1458.5) USD per household. This is the highest among all listed counties in West Virginia."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The carbon emissions for Mistral-small before optimization were 0.020 kg CO2 per inference task, and after optimization they were 0.015 kg CO2 per inference task. The multiplier is calculated as 0.015 / 0.020 = 0.75. This indicates a 25% reduction in emissions, consistent with the overall trend of emission reductions observed in the study.","0.75","multiplier","[""khan2025""]","is_blank","TABLE III: Mistral-small CO2 (kg) Before Optimization: 0.020, After Optimization: 0.015","The carbon emissions for Mistral-small before optimization were 0.020 kg CO2 per inference task, and after optimization they were 0.015 kg CO2 per inference task. The multiplier is calculated as 0.015 / 0.020 = 0.75. This indicates a 25% reduction in emissions, consistent with the overall trend of emission reductions observed in the study."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The combined training and fine-tuning energy costs for the BLOOMz-7B model are explicitly stated in Table 5 of the 'Power Hungry Processing' study. The training energy cost is 51,686 kWh, and the fine-tuning energy cost is 7,571 kWh. Adding these values gives a total of 59,257 kWh. The answer is provided in kWh as required by the answer_unit.","59257","kWh","[""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/""]","BLOOMz-7B Training energy (kWh) 51,686; Finetuning energy (kWh) 7,571; Cost parity (# inferences) 592,570,000","The combined training and fine-tuning energy costs for the BLOOMz-7B model are explicitly stated in Table 5 of the 'Power Hungry Processing' study. The training energy cost is 51,686 kWh, and the fine-tuning energy cost is 7,571 kWh. Adding these values gives a total of 59,257 kWh. The answer is provided in kWh as required by the answer_unit."
"q125","What is the total number of parameters in the final FLM-101B model?","The context explicitly states that FLM-101B is a model with 101B parameters. This is confirmed in multiple sections, including the 'Model Sizes' subsection under '2.1 Architecture' and the '6 Conclusions, Limitations, and Future Work' section. The answer unit is 'parameters', so the value is given as a numeric quantity in billions.","101000000000","parameters","[""li2025a""]","is_blank","['Benefiting from our growth strategy, the we produce three models with 16B, 51B, and 101B (i.e., FLM-101B) parameters in a single training.', 'In this paper, we introduce FLM-101B, an open-sourced LLM that is successfully trained from scratch within a $100,000 budget.']","The context explicitly states that FLM-101B is a model with 101B parameters. This is confirmed in multiple sections, including the 'Model Sizes' subsection under '2.1 Architecture' and the '6 Conclusions, Limitations, and Future Work' section. The answer unit is 'parameters', so the value is given as a numeric quantity in billions."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context states that a 6.1 billion parameter model was trained for 8 days (192 hours) on 256 A100 GPUs, consuming 13.8 MWh (13,812.4 kWh) for 13% of the training. A full training run is estimated to consume approximately 103,500 kWh. The BLOOMz-7B model has a training energy cost of 51,686 kWh. To match this training cost, the number of inferences required is calculated as 51,686 kWh / 1.0 × 10−4 kWh per inference = 516,860,000 inferences. This is approximately 517 million inferences.","517000000","inferences","[""dodge2022"", ""luccioni2024""]","[""https://dl.acm.org/doi/10.1145/3531146.3533076"", ""https://dl.acm.org/doi/10.1145/3615882.3657577""]","['We estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!', 'BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M Training energy (kWh) 51,686 25,634 17,052 10,505', 'Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5']","The context states that a 6.1 billion parameter model was trained for 8 days (192 hours) on 256 A100 GPUs, consuming 13.8 MWh (13,812.4 kWh) for 13% of the training. A full training run is estimated to consume approximately 103,500 kWh. The BLOOMz-7B model has a training energy cost of 51,686 kWh. To match this training cost, the number of inferences required is calculated as 51,686 kWh / 1.0 × 10−4 kWh per inference = 516,860,000 inferences. This is approximately 517 million inferences."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context explicitly states that in the 2024 study 'Power Hungry Processing', the total energy consumed for all model experimentation and evaluation was 754.66 kWh. This figure is directly mentioned in multiple sections of the document, including the ETHICAL CONSIDERATIONS STATEMENT and the RESULTS section. The answer unit is kWh, so the value is reported as a number in that unit.","754.66","kWh","[""luccioni2024""]","[""https://dl.acm.org/doi/10.1145/3630335.3630348""]","['In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.']","The context explicitly states that in the 2024 study 'Power Hungry Processing', the total energy consumed for all model experimentation and evaluation was 754.66 kWh. This figure is directly mentioned in multiple sections of the document, including the ETHICAL CONSIDERATIONS STATEMENT and the RESULTS section. The answer unit is kWh, so the value is reported as a number in that unit."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides the cost parity number for the BLOOMz-7B model in Table 5, which specifies the number of inferences required for the cumulative energy cost of deployment to equal the initial training and fine-tuning energy cost. The value is explicitly listed as 592,570,000 inferences.","592570000","inferences","[""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/""]","Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","The context provides the cost parity number for the BLOOMz-7B model in Table 5, which specifies the number of inferences required for the cumulative energy cost of deployment to equal the initial training and fine-tuning energy cost. The value is explicitly listed as 592,570,000 inferences."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context explicitly states that the dataset used for the German nuclear waste site objection texts is the FKTG-dataset, which was created during the process of selecting a repository site for high-level radioactive waste in Germany. This dataset contains the text of submissions (column 'Beitrag') and their corresponding categories (column 'Themenkomplex').","FKTG-dataset","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).","The context explicitly states that the dataset used for the German nuclear waste site objection texts is the FKTG-dataset, which was created during the process of selecting a repository site for high-level radioactive waste in Germany. This dataset contains the text of submissions (column 'Beitrag') and their corresponding categories (column 'Themenkomplex')."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide any information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. While the documents discuss water consumption for training Llama-3.1 and general AI model inference, they do not specify the amount of freshwater used specifically for inference serving clusters in 2024. Therefore, the answer cannot be determined from the provided information.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. While the documents discuss water consumption for training Llama-3.1 and general AI model inference, they do not specify the amount of freshwater used specifically for inference serving clusters in 2024. Therefore, the answer cannot be determined from the provided information."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context does not provide any information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. While the context discusses rare earth metal mining and environmental impacts, it does not mention recycling rates or practices for H100 GPUs in 2024.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. While the context discusses rare earth metal mining and environmental impacts, it does not mention recycling rates or practices for H100 GPUs in 2024."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context states that the net CO2e for Evolved Transformer NAS is 3.2 tCO2e. A single passenger round trip between San Francisco and New York is approximately 1.2 tCO2e. Dividing 3.2 by 1.2 gives approximately 2.67, which rounds to about 3 passengers. This is confirmed directly in the text: 'Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.'","3","passengers","[""patterson2021""]","is_blank","['Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.']","The context states that the net CO2e for Evolved Transformer NAS is 3.2 tCO2e. A single passenger round trip between San Francisco and New York is approximately 1.2 tCO2e. Dividing 3.2 by 1.2 gives approximately 2.67, which rounds to about 3 passengers. This is confirmed directly in the text: 'Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.'"
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","According to May 2025 data from the OpenRouter API platform, 84% of LLM token usage occurred through models that did not disclose their environmental impact. This figure is explicitly stated in the context, which also notes that 14% of usage was through indirectly disclosed models and only 2% through directly disclosed models. The answer is provided in percent as required by the answer_unit.","84","percent","[""luccioni2025c""]","[""https://arxiv.org/pdf/2504.17674v1.pdf""]","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","According to May 2025 data from the OpenRouter API platform, 84% of LLM token usage occurred through models that did not disclose their environmental impact. This figure is explicitly stated in the context, which also notes that 14% of usage was through indirectly disclosed models and only 2% through directly disclosed models. The answer is provided in percent as required by the answer_unit."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that for LLaMA-13B, the bare minimum hardware requirement is 1 A100 80GB GPU with a maximum batch size of 64, as shown in Table II. This configuration is explicitly listed for the 13B model under the A100 80GB column.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 13B 2 64 1 64","The context states that for LLaMA-13B, the bare minimum hardware requirement is 1 A100 80GB GPU with a maximum batch size of 64, as shown in Table II. This configuration is explicitly listed for the 13B model under the A100 80GB column."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context provides the energy consumption for a partial training run of a 6.1 billion parameter transformer model, which was 13.8 MWh over 8 days, and estimates that a full training run would take 60 days. The energy consumption for a full run is calculated as (60/8) * 13.8 MWh = 103.5 MWh. Since 1 MWh is equivalent to 1 metric ton of CO2 emissions in the context, the estimated CO2 emissions for a complete training run are 103.5 metric tons. This value is derived directly from the context and matches the required unit.","103.5","metric tons","[""dodge2022""]","[""https://arxiv.org/abs/2206.11547""]","We estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The context provides the energy consumption for a partial training run of a 6.1 billion parameter transformer model, which was 13.8 MWh over 8 days, and estimates that a full training run would take 60 days. The energy consumption for a full run is calculated as (60/8) * 13.8 MWh = 103.5 MWh. Since 1 MWh is equivalent to 1 metric ton of CO2 emissions in the context, the estimated CO2 emissions for a complete training run are 103.5 metric tons. This value is derived directly from the context and matches the required unit."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context does not provide any information about the total carbon emissions avoided by pruning and quantizing large language models in 2023. While several documents discuss the environmental benefits of techniques like quantization and pruning, none specify the total emissions avoided in that year. Therefore, the answer cannot be determined from the provided information.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the total carbon emissions avoided by pruning and quantizing large language models in 2023. While several documents discuss the environmental benefits of techniques like quantization and pruning, none specify the total emissions avoided in that year. Therefore, the answer cannot be determined from the provided information."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context states that using 2 A100s and 1 A10G results in a 24% cost saving over A100-only. This is explicitly mentioned in the text, directly answering the question about cost savings over an A100-only strategy when blending A100 and A10G GPUs. The answer unit is 'percent', so the value is expressed as a number.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context states that using 2 A100s and 1 A10G results in a 24% cost saving over A100-only. This is explicitly mentioned in the text, directly answering the question about cost savings over an A100-only strategy when blending A100 and A10G GPUs. The answer unit is 'percent', so the value is expressed as a number."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","According to Chen et al. (2024), the price per hour for an NVIDIA H20 is $4.63/hr. This value is listed in Table 1 of the document, which provides specifications for H100, H20, and TPU v6e. The price is marked with an asterisk indicating it is estimated based on relative system cost against H100, as H20 is not readily available on cloud service providers.","4.63","USD per hour","[""chen2024""]","[""https://www.google.com/about/datacenters/efficiency/""]","Table 1: H100, H20, and TPU v6e specifications. H100 H20 TPU v6e [7] BF16 TFLOPs 989 148 918 Memory capacity 80 GB 96 GB 32 GB Memory bandwidth 3.35 TB/s 4.0 TB/s 1.64 TB/s Power rating 700 W 400 W unlisted Inter-chip bandwidth 450 GB/s 450 GB/s 448 GB/s Network bandwidth 400 Gbps 400 Gbps 200 Gbps Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr *: As H20 is not readily available on cloud service providers, the listed price is estimated using the relative complete system cost against H100.","According to Chen et al. (2024), the price per hour for an NVIDIA H20 is $4.63/hr. This value is listed in Table 1 of the document, which provides specifications for H100, H20, and TPU v6e. The price is marked with an asterisk indicating it is estimated based on relative system cost against H100, as H20 is not readily available on cloud service providers."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context explicitly states that most carbon footprint analyses for AI models gather information manually by contacting authors, rather than automatically. For example, Luccioni and Hernandez-Garcia reached out to over 500 authors to collect data, indicating a manual process. This directly contradicts the claim that information is gathered automatically.","0","is_blank","[""luccioni2025b""]","is_blank","['In fact, most carbon footprint analyses gather the information manually by writing to authors.']","The context explicitly states that most carbon footprint analyses for AI models gather information manually by contacting authors, rather than automatically. For example, Luccioni and Hernandez-Garcia reached out to over 500 authors to collect data, indicating a manual process. This directly contradicts the claim that information is gathered automatically."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context states that in 2023, the total public health cost of U.S. data centers was about $6.7 billion, which is equivalent to approximately 44% of the data centers’ total electricity cost. This percentage is directly provided in the text and matches the requested unit of percent.","44","percent","[""han2024""]","is_blank","['Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about$6.7 billion, or$47.5 per household, in 2023. This is equivalent to approximately 44% of the data centers’ total electricity cost.']","The context states that in 2023, the total public health cost of U.S. data centers was about $6.7 billion, which is equivalent to approximately 44% of the data centers’ total electricity cost. This percentage is directly provided in the text and matches the requested unit of percent."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that the bare minimum hardware requirement for LLaMA-7B is 1 A100 80GB GPU, as shown in Table II. This is consistent with the model's size and the memory constraints of the GPU. The answer matches the expected format of the answer_unit, which is the identifier for the GPU type.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64","The context states that the bare minimum hardware requirement for LLaMA-7B is 1 A100 80GB GPU, as shown in Table II. This is consistent with the model's size and the memory constraints of the GPU. The answer matches the expected format of the answer_unit, which is the identifier for the GPU type."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context explicitly states that experimental results reveal that strategic quantization and local inference techniques can reduce energy consumption and carbon emissions by up to 45% post quantization. This directly supports the claim that sustainable deployment techniques demonstrated up to a 45% reduction in carbon emissions after quantization.","True","is_blank","[""khan2025""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410""]","['Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.']","The context explicitly states that experimental results reveal that strategic quantization and local inference techniques can reduce energy consumption and carbon emissions by up to 45% post quantization. This directly supports the claim that sustainable deployment techniques demonstrated up to a 45% reduction in carbon emissions after quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to gather information for estimating carbon footprints, but were only able to collect 95 answers. This is explicitly mentioned in multiple snippets, including [ref_id=luccioni2025b] and [ref_id=luccioni2023]. The answer is a numeric count, matching the 'answers' unit specified.","95","answers","[""luccioni2025b"", ""luccioni2023""]","[""https://arxiv.org/pdf/2303.11447.pdf"", ""https://arxiv.org/pdf/2303.11447.pdf""]","['For instance, Luccioni and Hernandez-Garcia re ached out to over 500 authors of AI papers to get infor- mation needed to estimate the carbon footprint of their mode ls, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citi ng privacy concerns and lack of experimental logs [2023].', 'As we discussed in Section 3, despite our best efforts and several reminders, only 15% of authors from our initial sample of 500 were willing to share relevant information with us.']","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to gather information for estimating carbon footprints, but were only able to collect 95 answers. This is explicitly mentioned in multiple snippets, including [ref_id=luccioni2025b] and [ref_id=luccioni2023]. The answer is a numeric count, matching the 'answers' unit specified."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The JetMoE-8B project was trained with a $100k budget and used 30,000 H100 GPU hours. To estimate the cost per H100 GPU-hour, divide the total budget by the total GPU hours: $100,000 / 30,000 = $3.33 per hour. This matches the expected unit of USD per hour.","3.33","USD per hour","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","['Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.']","The JetMoE-8B project was trained with a $100k budget and used 30,000 H100 GPU hours. To estimate the cost per H100 GPU-hour, divide the total budget by the total GPU hours: $100,000 / 30,000 = $3.33 per hour. This matches the expected unit of USD per hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context states that the total health cost of training an AI model of the Llama-3.1 scale can even exceed 120% of the training electricity cost, depending on the location. It further notes that the health cost increases dramatically in Iowa due to factors like wind direction and emission rates. Since the question asks about training in Altoona, Iowa, and the context explicitly mentions Iowa as a location where health costs are significantly higher, the health cost exceeds 120% of the electricity cost. The answer unit is 'percent', so the value is expressed as a percentage.","120","percent","[""han2024""]","is_blank","['It can be seen that the total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations.', 'For example, the total health cost is only $0.23 million in Oregon, whereas the cost will increase dramatically to $2.5 million in Iowa due to various factors, such as the wind direction and the pollutant emission rate for electricity generation [76].']","The context states that the total health cost of training an AI model of the Llama-3.1 scale can even exceed 120% of the training electricity cost, depending on the location. It further notes that the health cost increases dramatically in Iowa due to factors like wind direction and emission rates. Since the question asks about training in Altoona, Iowa, and the context explicitly mentions Iowa as a location where health costs are significantly higher, the health cost exceeds 120% of the electricity cost. The answer unit is 'percent', so the value is expressed as a percentage."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context explicitly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. This information is directly provided in multiple sections of the document, including the abstract and the training data mixture section. The unit 'tokens' matches the answer_unit specified in the additional info.","1.25T","tokens","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1""]","['JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.', 'Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.']","The context explicitly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. This information is directly provided in multiple sections of the document, including the abstract and the training data mixture section. The unit 'tokens' matches the answer_unit specified in the additional info."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context includes a table titled 'Amazon Renewable Energy Projects' with project locations and corresponding numbers of projects. According to the table, as of January 2024, there were 36 projects announced in the United Kingdom. This directly answers the question and matches the expected unit of 'projects'.","36","projects","[""amazon2023""]","is_blank","United Kingdom 36 901","The context includes a table titled 'Amazon Renewable Energy Projects' with project locations and corresponding numbers of projects. According to the table, as of January 2024, there were 36 projects announced in the United Kingdom. This directly answers the question and matches the expected unit of 'projects'."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides the percentage of men in Amazon's U.S. workforce across all levels for 2023. According to the 'Amazon Representation by the Numbers' section, the percentage of men in the U.S. workforce (all levels) in 2023 is 52.9%. This aligns with the answer_unit of 'percent'.","52.9","percent","[""amazon2023""]","[""https://sustainability.amazon.com/2023""]","46.9%52.9% 47.2%52.7% 47.5%52.3% 2021 2022 2023","The context provides the percentage of men in Amazon's U.S. workforce across all levels for 2023. According to the 'Amazon Representation by the Numbers' section, the percentage of men in the U.S. workforce (all levels) in 2023 is 52.9%. This aligns with the answer_unit of 'percent'."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context explicitly states that Apple reports its supply chain accounts for 99% of its total water footprint. This directly answers the question about the percentage of Apple's total water footprint attributed to its supply chain. The answer unit is 'percent', so the value is expressed as a number.","99","percent","[""li2025b""]","[""https://www.apple.com/environment/""]","['For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].']","The context explicitly states that Apple reports its supply chain accounts for 99% of its total water footprint. This directly answers the question about the percentage of Apple's total water footprint attributed to its supply chain. The answer unit is 'percent', so the value is expressed as a number."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The execution time breakdown for BlackMamba fine-tuning with a batch size of 84 is shown in Fig. 4. The figure indicates that the total execution time for the sparse BlackMamba model at batch size 84 is approximately 2.0 seconds, as the sum of the forward, backward, and optimizer stages is represented in the graph. The y-axis values in Fig. 4 correspond to seconds, and the total time for the sparse BlackMamba model at batch size 84 is clearly marked at 2.0 seconds.","2.0","seconds","[""xia2024""]","is_blank","Fig. 4. Execution time breakdown (seconds). Sparse(bsz=84) for BlackMamba: Forward ~0.5, Backward ~1.0, Optimizer ~0.5. Total ~2.0 seconds.","The execution time breakdown for BlackMamba fine-tuning with a batch size of 84 is shown in Fig. 4. The figure indicates that the total execution time for the sparse BlackMamba model at batch size 84 is approximately 2.0 seconds, as the sum of the forward, backward, and optimizer stages is represented in the graph. The y-axis values in Fig. 4 correspond to seconds, and the total time for the sparse BlackMamba model at batch size 84 is clearly marked at 2.0 seconds."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context explicitly states that the 'granularity metric' was introduced to assess the ratio of calculation to communication time. This metric is defined as the ratio of calculation time to communication time, and it is used to predict performance with different hardware setups and evaluate model suitability for distributed training. The term 'granularity' is consistently used throughout the text to describe this ratio.","granularity","is_blank","[""erben2023""]","is_blank","['We introduce the granularity metric, the ratio of calculation to communication time, and show how it can be used for predicting performance with different hardware setups.']","The context explicitly states that the 'granularity metric' was introduced to assess the ratio of calculation to communication time. This metric is defined as the ratio of calculation time to communication time, and it is used to predict performance with different hardware setups and evaluate model suitability for distributed training. The term 'granularity' is consistently used throughout the text to describe this ratio."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","A coalition of Microsoft employees estimated that a single deal with Exxon Mobil to expand oil and gas production could add up to 640 percent more carbon emissions compared to Microsoft’s carbon removal targets for the year. This directly answers the question about how many times more emissions the deal could generate relative to the company’s yearly carbon removal targets. The answer unit 'times' is satisfied by the percentage value, which is equivalent to 6.4 times.","640","times","[""luccioni2025a"", ""luccioni2025b""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/"", ""https://blogs.microsoft.com/blog/2020/01/16/microsoft-will-be-carbon-negative-by-2030/""]","['a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year [119]', 'a single contract to use AI to expand oil production “could enable carbon emissions adding up to 640 percent of the company’s carbon removal targets”']","A coalition of Microsoft employees estimated that a single deal with Exxon Mobil to expand oil and gas production could add up to 640 percent more carbon emissions compared to Microsoft’s carbon removal targets for the year. This directly answers the question about how many times more emissions the deal could generate relative to the company’s yearly carbon removal targets. The answer unit 'times' is satisfied by the percentage value, which is equivalent to 6.4 times."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context defines 'water withdrawal' as freshwater taken from ground or surface water sources, either temporarily or permanently, for agricultural, industrial, or municipal uses. This definition is explicitly stated in multiple snippets, including the section on Water Withdrawal vs. Water Consumption.","water withdrawal","is_blank","[""li2025b""]","[""https://www.cia.gov/the-world-factbook/field/total-water-withdrawal/""]","['• Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].']","The context defines 'water withdrawal' as freshwater taken from ground or surface water sources, either temporarily or permanently, for agricultural, industrial, or municipal uses. This definition is explicitly stated in multiple snippets, including the section on Water Withdrawal vs. Water Consumption."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI. This is explicitly mentioned in multiple snippets, including the core activity of the Standing Committee. The answer unit is 'years', so the answer is expressed as a number in that unit.","5","years","[""stone2022""]","[""https://ai100.stanford.edu""]","['As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.']","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI. This is explicitly mentioned in multiple snippets, including the core activity of the Standing Committee. The answer unit is 'years', so the answer is expressed as a number in that unit."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context explicitly states that in the US, the average household is equipped with an average of 25 connected devices, including smartphones, laptops, smart TVs, speakers, wearables, and gaming consoles. This information is directly cited from Deloitte, 2021, and matches the requested unit of 'devices'.","25","devices","[""wu2021b""]","[""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf""]","['In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].']","The context explicitly states that in the US, the average household is equipped with an average of 25 connected devices, including smartphones, laptops, smart TVs, speakers, wearables, and gaming consoles. This information is directly cited from Deloitte, 2021, and matches the requested unit of 'devices'."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context states that the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout). This range is explicitly mentioned in the provided text, directly answering the question about the energy consumption range for pre-training LLMs.","[0.8,3500]","MWh","[""luccioni2025c""]","is_blank","the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout)","The context states that the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout). This range is explicitly mentioned in the provided text, directly answering the question about the energy consumption range for pre-training LLMs."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context explicitly states that IBM’s Watson program beat human contenders to win the Jeopardy challenge in 2011. This is directly mentioned in multiple snippets, including the one that describes Watson's success in the Jeopardy challenge. Therefore, the claim that Watson did NOT beat human contenders is false.","0","is_blank","[""stone2022""]","[""https://www.ibm.com/artificial-intelligence/ethics""]","['IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.']","The context explicitly states that IBM’s Watson program beat human contenders to win the Jeopardy challenge in 2011. This is directly mentioned in multiple snippets, including the one that describes Watson's success in the Jeopardy challenge. Therefore, the claim that Watson did NOT beat human contenders is false."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context states that one study suggests 10–50 queries on GPT-3 consume around half a liter of water. This range is directly mentioned in the provided text, and the question asks for the number of queries, which is given as a range.","[10,50]","queries","[""li2025b"", ""luccioni2025a""]","is_blank","['One paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].', 'Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].']","The context states that one study suggests 10–50 queries on GPT-3 consume around half a liter of water. This range is directly mentioned in the provided text, and the question asks for the number of queries, which is given as a range."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment. According to Table 4, the MT-Bench score for JetMoE-8B-chat is 6.681, which is higher than the Llama-2-13b-chat score of 6.650. This confirms the model's superior performance post-alignment.","6.681","score","[""shen2024""]","[""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard""]","['Table 4: MT-Bench score comparison of various models', 'JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, demonstrating its superior performance.']","The context states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment. According to Table 4, the MT-Bench score for JetMoE-8B-chat is 6.681, which is higher than the Llama-2-13b-chat score of 6.650. This confirms the model's superior performance post-alignment."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that GPT-3 needs to 'drink' a 500ml bottle of water for roughly 10–50 medium-length responses, depending on deployment. This directly answers the question about how many medium-length completions can be produced with the water in a 500ml bottle. The answer is a range, which matches the expected format.","[10,50]","responses","[""li2025b""]","is_blank","['Additionally, GPT-3 needs to “drink” (i.e., consume) a500ml bottle of waterfor roughly 10 – 50 medium-length responses, depending on when and where it is deployed.']","The context states that GPT-3 needs to 'drink' a 500ml bottle of water for roughly 10–50 medium-length responses, depending on deployment. This directly answers the question about how many medium-length completions can be produced with the water in a 500ml bottle. The answer is a range, which matches the expected format."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The context states that Mélange reduces deployment costs by up to 77% in conversational settings, as reported in the 2024 Griggs et al. paper. This is explicitly mentioned in multiple snippets, including the abstract and section 6.2 Cost Savings Analysis, which confirm the 77% reduction for short-context tasks like interactive chats.","77","percent","[""griggs2024""]","[""https://arxiv.org/abs/2404.14527""]","Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.","The context states that Mélange reduces deployment costs by up to 77% in conversational settings, as reported in the 2024 Griggs et al. paper. This is explicitly mentioned in multiple snippets, including the abstract and section 6.2 Cost Savings Analysis, which confirm the 77% reduction for short-context tasks like interactive chats."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that at a minimum, 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. This is explicitly mentioned in multiple sections, including Table II, which lists 4 A100 80GB GPUs as the bare minimum configuration for LLaMA 65B. The answer matches the required unit 'A100_80GB_GPUs' as specified.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","['For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.', 'Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 65B 8 64 4 128']","The context states that at a minimum, 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model. This is explicitly mentioned in multiple sections, including Table II, which lists 4 A100 80GB GPUs as the bare minimum configuration for LLaMA 65B. The answer matches the required unit 'A100_80GB_GPUs' as specified."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context consistently states that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. Multiple snippets, including [ref_id=han2024] and [ref_id=luccioni2025c], confirm this figure. The answer unit is 'round trips', so the answer is expressed as a number in that unit.","10000","round trips","[""han2024"", ""luccioni2025c""]","[""https://arxiv.org/pdf/2412.06288v2.pdf"", ""https://arxiv.org/pdf/2504.06307v1.pdf""]","['Training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.', 'training an AI model of the LLaMa 3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.']","The context consistently states that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. Multiple snippets, including [ref_id=han2024] and [ref_id=luccioni2025c], confirm this figure. The answer unit is 'round trips', so the answer is expressed as a number in that unit."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","NVIDIA estimated that 80–90% of the ML workload is inference processing, as stated in the context from [ref_id=patterson2021], which references [Leo19] for this estimate. This aligns with the question's request for the percentage of ML workload attributed to inference processing by NVIDIA in 2019.","[80,90]","percent","[""patterson2021""]","[""https://doi.org/10.1146/annurev-statistics-060121-092519""]","For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].","NVIDIA estimated that 80–90% of the ML workload is inference processing, as stated in the context from [ref_id=patterson2021], which references [Leo19] for this estimate. This aligns with the question's request for the percentage of ML workload attributed to inference processing by NVIDIA in 2019."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context explicitly states that the total CO2 equivalent emissions generated during the entire study were 178.97 kg CO2eq. This figure is directly mentioned in the 'ETHICAL CONSIDERATIONS STATEMENT' section of the 'Power Hungry Processing' (2024) study, which summarizes the total emissions from all model experimentation and evaluation.","178.97","kg CO2eq","[""luccioni2024""]","[""https://doi.org/10.1145/3630106.3658542""]","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context explicitly states that the total CO2 equivalent emissions generated during the entire study were 178.97 kg CO2eq. This figure is directly mentioned in the 'ETHICAL CONSIDERATIONS STATEMENT' section of the 'Power Hungry Processing' (2024) study, which summarizes the total emissions from all model experimentation and evaluation."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context explicitly states that estimating energy consumption based on TDP is nearly always an overestimation because GPUs rarely draw their maximum power at every moment. It further notes that such estimations can lead to a worst-case overestimation by a factor of 4.1, making them unreliable. Accurate measurements are emphasized as crucial, indicating TDP-based estimation is not reliable.","0","is_blank","[""chung2025""]","is_blank","['Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time.', 'In fact, such an estimation can lead to a worst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).']","The context explicitly states that estimating energy consumption based on TDP is nearly always an overestimation because GPUs rarely draw their maximum power at every moment. It further notes that such estimations can lead to a worst-case overestimation by a factor of 4.1, making them unreliable. Accurate measurements are emphasized as crucial, indicating TDP-based estimation is not reliable."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes slightly more energy than GPT-4o due to its deployment on older A100 GPU nodes, despite its smaller architecture. Specifically, GPT-4o consumes around 2.875 Wh per query, while GPT-4o mini consumes 3.098 Wh, which is higher. This directly contradicts the claim that GPT-4o mini consumes less energy.","0","is_blank","[""jegham2025""]","is_blank","['For instance GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.', 'GPT-4o and GPT-4o mini also show that infrastructure can outweigh model size in determining energy efficiency.']","The context states that GPT-4o mini consumes slightly more energy than GPT-4o due to its deployment on older A100 GPU nodes, despite its smaller architecture. Specifically, GPT-4o consumes around 2.875 Wh per query, while GPT-4o mini consumes 3.098 Wh, which is higher. This directly contradicts the claim that GPT-4o mini consumes less energy."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The ground truth throughput for a dense Mixtral-CS-A100-40GB model at batch size 1 is found in Fig. 15, which shows the throughput values for different batch sizes. The figure indicates that at batch size 1, the dense model's throughput is approximately 0.0 queries/sec, as the data point for dense (bsz=1) aligns with the y-axis value of 0.0.","0.0","queries/sec","[""xia2024""]","is_blank","Fig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS for different GPUs: A100 and H100.","The ground truth throughput for a dense Mixtral-CS-A100-40GB model at batch size 1 is found in Fig. 15, which shows the throughput values for different batch sizes. The figure indicates that at batch size 1, the dense model's throughput is approximately 0.0 queries/sec, as the data point for dense (bsz=1) aligns with the y-axis value of 0.0."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models releasing some degree of information. However, after 2022, the introduction of increasingly commercial and proprietary models, catalyzed by ChatGPT, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category. Therefore, the trend did not continue to increase after 2022.","0","is_blank","[""luccioni2025c""]","is_blank","['The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the “no disclosure” category, as the line between research and commercial deployment became increasingly blurred.']","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models releasing some degree of information. However, after 2022, the introduction of increasingly commercial and proprietary models, catalyzed by ChatGPT, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category. Therefore, the trend did not continue to increase after 2022."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context states that the normalized on-demand hourly price for an H100 GPU was calculated by comparing RunPod’s H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then adjusting relative to the A100’s price on major clouds ($3.67). This results in a normalized price of (4.69/2.29) × 3.67 = $7.516. The answer unit is USD per hour, so the numeric value is 7.516.","7.516","USD per hour","[""griggs2024""]","is_blank","We calculate this by comparing RunPod’s H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then adjusting relative to the A100’s price on major clouds ($3.67), resulting in a normalized price of (4.69/2.29) × 3.67 = $7 .516 for H100.","The context states that the normalized on-demand hourly price for an H100 GPU was calculated by comparing RunPod’s H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then adjusting relative to the A100’s price on major clouds ($3.67). This results in a normalized price of (4.69/2.29) × 3.67 = $7.516. The answer unit is USD per hour, so the numeric value is 7.516."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context does not provide specific information about the water used for cooling during OpenAI's GPT-4 training run. While GPT-3's water consumption for cooling is estimated at over 700,000 liters, no such data is given for GPT-4. The documents discuss water usage for GPT-3 and inference for GPT-4o, but not training water consumption for GPT-4.","is_blank","liters of water","[""li2025b"", ""jegham2025"", ""morrison2025""]","is_blank","['is_blank']","The context does not provide specific information about the water used for cooling during OpenAI's GPT-4 training run. While GPT-3's water consumption for cooling is estimated at over 700,000 liters, no such data is given for GPT-4. The documents discuss water usage for GPT-3 and inference for GPT-4o, but not training water consumption for GPT-4."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs and costs over $5,200 per month in on-demand rental costs. To estimate the hourly cost, divide the monthly cost by the number of hours in a month (30 days × 24 hours = 720 hours). This yields approximately $7.22 per hour.","7.22","USD per hour","[""griggs2024""]","[""https://arxiv.org/abs/2404.14527v4""]","['serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.']","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs and costs over $5,200 per month in on-demand rental costs. To estimate the hourly cost, divide the monthly cost by the number of hours in a month (30 days × 24 hours = 720 hours). This yields approximately $7.22 per hour."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context explicitly states that to increase the BLEU score from 5 to 40 for a GPT-3-based language translation task, the model must be 1,000× larger in size. This is directly mentioned in the text: 'For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1,000× larger in size.' The answer unit is 'multiplier', so the answer is expressed as a numeric multiplier.","1000","multiplier","[""wu2021a""]","is_blank","['For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1,000× larger in size.']","The context explicitly states that to increase the BLEU score from 5 to 40 for a GPT-3-based language translation task, the model must be 1,000× larger in size. This is directly mentioned in the text: 'For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1,000× larger in size.' The answer unit is 'multiplier', so the answer is expressed as a numeric multiplier."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","","","miles","[]","is_blank","is_blank",""
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The BLOOMz-7B model has an inference energy cost of 1.0 × 10⁻⁴ kWh per inference. With 606,096 downloads and assuming each download results in 1 million inferences, the total number of inferences is 606,096 × 1,000,000 = 6.06096 × 10¹¹. Multiplying this by the per-inference energy gives 6.06096 × 10¹¹ × 1.0 × 10⁻⁴ kWh = 60,609.6 MWh. This calculation uses the reported inference energy and scales it by the number of downloads and inferences per download.","60609.6","MWh","[""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/""]","['BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M\nTraining energy (kWh) 51,686 25,634 17,052 10,505\nFinetuning energy (kWh) 7,571 3,242 1,081 543\nInference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5\nCost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592\nTable 5.', 'at the time of writing this article (November 2023), BLOOMz-7B has been downloaded 606,096 times']","The BLOOMz-7B model has an inference energy cost of 1.0 × 10⁻⁴ kWh per inference. With 606,096 downloads and assuming each download results in 1 million inferences, the total number of inferences is 606,096 × 1,000,000 = 6.06096 × 10¹¹. Multiplying this by the per-inference energy gives 6.06096 × 10¹¹ × 1.0 × 10⁻⁴ kWh = 60,609.6 MWh. This calculation uses the reported inference energy and scales it by the number of downloads and inferences per download."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours. This information is consistently mentioned in multiple snippets, including the abstract and the detailed training section. The answer unit is 'H100 GPU hours', so the answer value is the numeric value 30000.","30000","H100 GPU hours","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","['JetMoE-8B is trained with less than$0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours.', 'Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.']","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours. This information is consistently mentioned in multiple snippets, including the abstract and the detailed training section. The answer unit is 'H100 GPU hours', so the answer value is the numeric value 30000."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This is directly supported by the analysis showing a growth rate of 2.4× per year since 2016, which extrapolates to a cost of about $1 billion by 2027. The answer unit is USD, so the value is expressed as a number in that unit.","1000000000","USD","[""cottier2024""]","is_blank","['If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.']","The context states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This is directly supported by the analysis showing a growth rate of 2.4× per year since 2016, which extrapolates to a cost of about $1 billion by 2027. The answer unit is USD, so the value is expressed as a number in that unit."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","OpenAI published the total number of floating point operations to train GPT-3 as 3.14E+23 FLOPS, as stated in the context snippet [ref_id=patterson2021]. This value is directly provided in the text and matches the requested unit of FLOPS.","3.14E+23","FLOPS","[""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/""]","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","OpenAI published the total number of floating point operations to train GPT-3 as 3.14E+23 FLOPS, as stated in the context snippet [ref_id=patterson2021]. This value is directly provided in the text and matches the requested unit of FLOPS."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model, as per Table II and multiple mentions in the text. This is the bare minimum hardware configuration without compression or quantization. The answer matches the required format of 'V100_32GB_GPUs' as specified in the additional info.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. ... 65B 8 64 4 128","The context states that 8 V100 GPUs each with 32 GB of RAM are required for any meaningful inferences with the 65B LLaMA model, as per Table II and multiple mentions in the text. This is the bare minimum hardware configuration without compression or quantization. The answer matches the required format of 'V100_32GB_GPUs' as specified in the additional info."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The total computational work for the 101B training stage is estimated using the single-GPU throughput of 165 teraFLOPs/sec from Table 2, the number of GPUs (12), and the training time of 21.54 days. The total FLOPs are calculated as 165 teraFLOPs/sec × 12 GPUs × 21.54 days × 86400 seconds/day, which converts to approximately 28.22 zettaFLOPs. This matches the value reported in Table 4 for FLM-101B.","28.22","zettaFLOPs","[""li2025a""]","[""https://github.com/NVIDIA/Megatron-LM""]","['Table 2: Parallel strategies and throughput for different growth stages.', 'Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B.', 'The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).']","The total computational work for the 101B training stage is estimated using the single-GPU throughput of 165 teraFLOPs/sec from Table 2, the number of GPUs (12), and the training time of 21.54 days. The total FLOPs are calculated as 165 teraFLOPs/sec × 12 GPUs × 21.54 days × 86400 seconds/day, which converts to approximately 28.22 zettaFLOPs. This matches the value reported in Table 4 for FLM-101B."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context does not provide the top-1 accuracy for AlexNet on ImageNet in 2012. Although AlexNet is mentioned in multiple references, including its use in ImageNet classification, no specific accuracy value is given for AlexNet in the provided documents.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide the top-1 accuracy for AlexNet on ImageNet in 2012. Although AlexNet is mentioned in multiple references, including its use in ImageNet classification, no specific accuracy value is given for AlexNet in the provided documents."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, with each server containing 8 GPUs. Multiplying 24 servers by 8 GPUs per server gives a total of 192 GPUs. This is consistent with the data in Table 2, which lists a 'Number of GPUs' of 192 for all training stages.","192","GPUs","[""li2025a""]","is_blank","['FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.', 'Table 2: Parallel strategies and throughput for different growth stages. ... Number of GPUs Size: 192']","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, with each server containing 8 GPUs. Multiplying 24 servers by 8 GPUs per server gives a total of 192 GPUs. This is consistent with the data in Table 2, which lists a 'Number of GPUs' of 192 for all training stages."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context states that the CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model were estimated at 284 metric tons (tCO2e), which is equivalent to the lifetime emissions of five cars. This figure is derived from the seminal work by Strubell et al. (2019), which quantified the emissions of training and fine-tuning a large Transformer model with NAS. The answer unit is 'lifetimes', and the value is 5, as the emissions are explicitly compared to five car lifetimes.","5","lifetimes","[""dodge2022"", ""luccioni2025c""]","[""https://dl.acm.org/doi/pdf/10.1145/3483410"", ""https://arxiv.org/pdf/2504.17674v1.pdf""]","['Measuring the Carbon Intensity of AI in Cloud Instances FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea', 'The research article was written for a specialized audience of AI and NLP researchers, who would have the background knowledge to understand the appropriate scoping for the estimate. However, an author’s tweet publicizing the paper and featuring a table containing the “five cars” estimate was widely shared on social media, leading to the publication being picked up by numerous media outlets (including MIT Technology Review32 and Forbes33). The “five cars” number has since been misinterpreted as a proxy for the carbon footprint of training AI models at large, which is misleading given the diversity of architectures, training approaches and electricity sources used for powering AI model training; the original article reports AI training workloads emitting as little as 26 pounds (11.8 kg) CO2e (assuming U.S. average energy carbon emissions intensity), and AI model training more broadly often requires even less energy and corresponding emissions.']","The context states that the CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model were estimated at 284 metric tons (tCO2e), which is equivalent to the lifetime emissions of five cars. This figure is derived from the seminal work by Strubell et al. (2019), which quantified the emissions of training and fine-tuning a large Transformer model with NAS. The answer unit is 'lifetimes', and the value is 5, as the emissions are explicitly compared to five car lifetimes."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context states that FAIR’s RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train. This directly answers the question about the GPU hours required for training.","25000","hours","[""schwartz2019""]","[""https://arxiv.org/abs/1907.11692""]","FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context states that FAIR’s RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train. This directly answers the question about the GPU hours required for training."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that Amazon's on-site solar energy systems avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year compared to nonrenewable electricity sources. This figure is explicitly mentioned in the section discussing on-site solar projects in 2023.","47500","metric tons","[""amazon2023""]","[""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf""]","Altogether, these solar energy projects generate an estimated 123,000 MWh and avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year.","The context states that Amazon's on-site solar energy systems avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year compared to nonrenewable electricity sources. This figure is explicitly mentioned in the section discussing on-site solar projects in 2023."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context states that LLMs were deployed using the vllm library, which runs on a ray cluster for multi-node computations. This framework enables distributed deployment across multiple GPUs and nodes by leveraging tensor model parallelism and pipeline model parallelism.","vllm","is_blank","[""zschache2025""]","[""https://github.com/vllm-project/vllm""]","LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.","The context states that LLMs were deployed using the vllm library, which runs on a ray cluster for multi-node computations. This framework enables distributed deployment across multiple GPUs and nodes by leveraging tensor model parallelism and pipeline model parallelism."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The energy consumption for the Llama 3.1 70B model increased by a factor of 1.95 when deployed on two nodes instead of one, as shown in Table B2. This table compares single-node and double-node deployments, with the 'double' column indicating energy consumption for two nodes and the 'ratio' column showing the multiplier. The ratio for Llama 3.1 70B is 1.95, which directly answers the question.","1.95","multiplier","[""zschache2025""]","is_blank","Table B2 Comparison single vs. double node deployment, Capella system","The energy consumption for the Llama 3.1 70B model increased by a factor of 1.95 when deployed on two nodes instead of one, as shown in Table B2. This table compares single-node and double-node deployments, with the 'double' column indicating energy consumption for two nodes and the 'ratio' column showing the multiplier. The ratio for Llama 3.1 70B is 1.95, which directly answers the question."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context does not provide specific data on water consumption per ChatGPT user session in 2023. While several documents discuss water usage for AI models like GPT-3 and GPT-4o, they do not specify the amount of water consumed per user session in gallons for 2023. The closest information is about GPT-3 requiring a 500ml bottle of water for 10–50 medium-length responses, but this is not directly convertible to gallons per session for 2023.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The context does not provide specific data on water consumption per ChatGPT user session in 2023. While several documents discuss water usage for AI models like GPT-3 and GPT-4o, they do not specify the amount of water consumed per user session in gallons for 2023. The closest information is about GPT-3 requiring a 500ml bottle of water for 10–50 medium-length responses, but this is not directly convertible to gallons per session for 2023."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum, which exceeds the total electricity consumption of 35,000 U.S. residential households (377,685 MWh). Since 700 million daily queries are attributed to GPT-4o in the study, the annual electricity use from this volume is comparable to 35,000 U.S. homes. The answer unit is 'homes', so the value is expressed as a number.","35000","homes","[""jegham2025""]","is_blank","These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even 325 universities (390,650 MWh) annually.","The context states that GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum, which exceeds the total electricity consumption of 35,000 U.S. residential households (377,685 MWh). Since 700 million daily queries are attributed to GPT-4o in the study, the annual electricity use from this volume is comparable to 35,000 U.S. homes. The answer unit is 'homes', so the value is expressed as a number."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context from the paper by Li et al. (2025) states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022. This is directly supported by the text, which cites Microsoft's data in the context of increasing water demand due to AI. The answer unit is 'percent', so the value is given as a percentage.","34","percent","[""luccioni2025a"", ""li2025b""]","[""https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Microsoft-2024-Environmental-Sustainability-Report.pdf"", ""https://arxiv.org/pdf/2304.03271""]","['Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons']","The context from the paper by Li et al. (2025) states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022. This is directly supported by the text, which cites Microsoft's data in the context of increasing water demand due to AI. The answer unit is 'percent', so the value is given as a percentage."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, which justifies the energy costs of LLM deployment. This directly contradicts the claim that traditional models achieved accuracy comparable to large language models.","0","is_blank","[""zschache2025""]","is_blank","['In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.']","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, which justifies the energy costs of LLM deployment. This directly contradicts the claim that traditional models achieved accuracy comparable to large language models."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context explicitly states that the Power Usage Effectiveness (PUE) for the Iowa datacenter where the Evolved Transformer was run is 1.11. This value is directly mentioned in multiple snippets and is described as a factor of 1.4X better than the US national average. The answer unit is PUE, so the value is reported as a numeric ratio.","1.11","PUE","[""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/""]","['The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.']","The context explicitly states that the Power Usage Effectiveness (PUE) for the Iowa datacenter where the Evolved Transformer was run is 1.11. This value is directly mentioned in multiple snippets and is described as a factor of 1.4X better than the US national average. The answer unit is PUE, so the value is reported as a numeric ratio."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context states that a decaying growth pattern from June to December 2025, following a 20% monthly growth from January to May, yields a total of approximately 772 billion GPT-4o queries in 2025. This figure is explicitly mentioned in multiple snippets and directly answers the question about the total estimated number of queries.","772000000000","queries","[""jegham2025""]","is_blank","['yielding a total of approximately 772 billion GPT-4o queries in 2025']","The context states that a decaying growth pattern from June to December 2025, following a 20% monthly growth from January to May, yields a total of approximately 772 billion GPT-4o queries in 2025. This figure is explicitly mentioned in multiple snippets and directly answers the question about the total estimated number of queries."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The OpenLLM Leaderboard average score for JetMoE-8B is explicitly stated in Table 3 of the context. The table shows that JetMoE-8B achieved an average score of 53.0 on the OpenLLM Leaderboard, which is the highest among the compared models. This score is directly reported in the context and matches the required answer unit of 'score'.","53.0","score","[""shen2024""]","[""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard""]","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM leaderboard, achieving the best scores in all tasks except ARC-challenge and WinoGrande. Additionally, JetMoE-8B obtains the highest MBPP scores in Python programming. OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0","The OpenLLM Leaderboard average score for JetMoE-8B is explicitly stated in Table 3 of the context. The table shows that JetMoE-8B achieved an average score of 53.0 on the OpenLLM Leaderboard, which is the highest among the compared models. This score is directly reported in the context and matches the required answer unit of 'score'."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide any information about the number of AI training runs conducted globally on renewable-only power in 2022. While several documents discuss the carbon intensity of AI training, energy sources, and the environmental impact of data centers, none specify the number of training runs performed exclusively on renewable energy in 2022.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the number of AI training runs conducted globally on renewable-only power in 2022. While several documents discuss the carbon intensity of AI training, energy sources, and the environmental impact of data centers, none specify the number of training runs performed exclusively on renewable energy in 2022."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context states that the open-source exemption from reporting obligations should be removed because making parts of a model public does not justify exclusion from environmental accountability. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. This directly contradicts the claim that they are fully exempt.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","['• Elimination of the Open-Source Exemption : The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.']","The context states that the open-source exemption from reporting obligations should be removed because making parts of a model public does not justify exclusion from environmental accountability. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. This directly contradicts the claim that they are fully exempt."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context states that the US national datacenter average PUE was 1.59 in 2020. This is directly mentioned in the snippet from [ref_id=patterson2021], which references the value from [Str19]. The answer unit is PUE, so the value is given as a number.","1.59","PUE","[""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/""]","['The US national datacenter average in 2018 was 1.58, which is the value [Str19] used; In 2020, it was 1.59.']","The context states that the US national datacenter average PUE was 1.59 in 2020. This is directly mentioned in the snippet from [ref_id=patterson2021], which references the value from [Str19]. The answer unit is PUE, so the value is given as a number."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context states that for the OPT-2.7B model running on an AWS g4dn.xlarge instance, the KV Cache expands to 5.312GB when the batch size increases to 32. This directly answers the question about the cache size at that batch size.","5.312","GB","[""kim2025""]","is_blank","['When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.']","The context states that for the OPT-2.7B model running on an AWS g4dn.xlarge instance, the KV Cache expands to 5.312GB when the batch size increases to 32. This directly answers the question about the cache size at that batch size."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context explicitly states that for the four notable models studied in-depth—GPT-3, OPT-175B, GPT-4, and Gemini Ultra—R&D staff costs including equity accounted for between 29% and 49% of the total amortized cost. This range is directly supported by multiple references in the text, including the summary in Section 3.5 and the detailed breakdown in the main body.","[29, 49]","percent","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","['For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.', 'We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model.']","The context explicitly states that for the four notable models studied in-depth—GPT-3, OPT-175B, GPT-4, and Gemini Ultra—R&D staff costs including equity accounted for between 29% and 49% of the total amortized cost. This range is directly supported by multiple references in the text, including the summary in Section 3.5 and the detailed breakdown in the main body."
"q213","Which software package was used to measure energy consumption during inference runs?","The context mentions that energy consumption during inference was measured using software tools that monitor hardware usage. Specifically, CodeCarbon, Carbontracker, and experiment-impact-tracker are cited as examples of such tools. Among these, CodeCarbon is noted to be more accurate in some settings. Since the question asks for the software package used, and CodeCarbon is explicitly mentioned as a tool for measuring energy consumption during ML model application, it is the most directly supported answer.","CodeCarbon","is_blank","[""zschache2025""]","[""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%B8%8F-energy-metrics""]","['Various software tools have been created to monitor energy consumption during the application of machine learning models (https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%B8%8F-energy-metrics). Similar to CodeCarbon, Carbontracker (Anthony et al., 2020) and experiment-impact-tracker (Henderson et al., 2020) estimate energy consumption by monitoring hardware usage. In some settings, CodeCarbon is considered more accurate, yielding values closer to those obtained via physical wattmeters (Bouza et al., 2023).']","The context mentions that energy consumption during inference was measured using software tools that monitor hardware usage. Specifically, CodeCarbon, Carbontracker, and experiment-impact-tracker are cited as examples of such tools. Among these, CodeCarbon is noted to be more accurate in some settings. Since the question asks for the software package used, and CodeCarbon is explicitly mentioned as a tool for measuring energy consumption during ML model application, it is the most directly supported answer."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The analysis of 100 news articles on ChatGPT's energy use revealed that 53% of articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. This directly corresponds to the contested estimate mentioned in the context.","53","percent","[""luccioni2025c""]","is_blank","['53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search']","The analysis of 100 news articles on ChatGPT's energy use revealed that 53% of articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. This directly corresponds to the contested estimate mentioned in the context."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context explicitly states that the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. This function is described in detail in Section IV-F, where it is defined as a linear transformation that corrects theoretical computation time to match actual execution time, thereby enhancing prediction accuracy.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","['Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.', 'The theoretical FLOPS values provided by GPU manufacturers do not accurately reflect real-world performance in LLM inference workloads. ... To address this issue, it is essential to introduce a calibration method that aligns theoretical FLOPS values with actual computational performance.']","The context explicitly states that the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. This function is described in detail in Section IV-F, where it is defined as a linear transformation that corrects theoretical computation time to match actual execution time, thereby enhancing prediction accuracy."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that increasing the number of shards always increases the wattage and tends to increase the energy costs of inference per response most overall. This is consistent across both datasets and for different generation lengths. Therefore, the statement is true.","1","is_blank","[""samsi2024""]","is_blank","['increasing the number of shards still tends to increase the energy costs of inference per response most overall while increasing the maximum generation length from 512 (Figure 8) to 1024 (Figure 9) does not induce a clear or significant effect in inference energy costs.']","The context states that increasing the number of shards always increases the wattage and tends to increase the energy costs of inference per response most overall. This is consistent across both datasets and for different generation lengths. Therefore, the statement is true."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. It also assumes an H100 GPU is 0.1% rare earth metal by mass. Therefore, the water consumption for mining the rare earth materials in a single H100 GPU is calculated as 0.1% of 11 kL, which equals 0.011 kL. This value is consistent with the context's estimate of 0.013 kg CO2eq per GPU, which is derived from the same assumption.","0.011","kL","[""morrison2025""]","[""https://www.azcentral.com/story/opinion/op-ed/joannaallhands/2024/06/12/tsmc-arizona-water-use-recycling/74059522007/"", ""https://www.semiconductor-digest.com/water-supply-challenges-for-the-semiconductor-industry/"", ""https://web.archive.org/web/20131207002716/http://wafercare.com/Page.aspx?id=1012"", ""https://anysilicon.com/die-per-wafer-formula-free-calculators/"", ""https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/""]","['Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams 12 and produces about 63 H100s. 13 14 Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU.', 'We additionally estimate the environmental impact from mining rare earth metals used during manufacturing, assuming an H100 is 0.1% rare earth metal by mass.']","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. It also assumes an H100 GPU is 0.1% rare earth metal by mass. Therefore, the water consumption for mining the rare earth materials in a single H100 GPU is calculated as 0.1% of 11 kL, which equals 0.011 kL. This value is consistent with the context's estimate of 0.013 kg CO2eq per GPU, which is derived from the same assumption."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context states that the open-source exemption from reporting obligations should be removed because making parts of a model public does not justify exclusion from environmental accountability. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. This implies that under the proposed policy, open-source models would need to report energy consumption, but the current EU rules still allow the exemption.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","['• Elimination of the Open-Source Exemption : The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.']","The context states that the open-source exemption from reporting obligations should be removed because making parts of a model public does not justify exclusion from environmental accountability. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. This implies that under the proposed policy, open-source models would need to report energy consumption, but the current EU rules still allow the exemption."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context states that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide. This directly answers the question about the percentage of PPAs they accounted for.","30","percent","[""luccioni2025a""]","is_blank","['In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131], changing the scope and extent of the mechanism as a whole.']","The context states that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide. This directly answers the question about the percentage of PPAs they accounted for."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context states that in 2023, the total public health cost of U.S. data centers was about $6.7 billion, or $47.5 per household. This figure is derived from the average attribution method used in the study, which is consistent with the methodology applied for carbon emission accounting in sustainability reports.","6.7","USD","[""han2024""]","is_blank","['Table 1 shows the public health cost of U.S. data centers from 2019 to 2023 as a reference. Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about$6.7 billion, or$47.5 per household, in 2023.']","The context states that in 2023, the total public health cost of U.S. data centers was about $6.7 billion, or $47.5 per household. This figure is derived from the average attribution method used in the study, which is consistent with the methodology applied for carbon emission accounting in sustainability reports."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The energy consumption of the o3 model for a long prompt is 12.222 Wh, as shown in Table 4. The GPT-4.1 nano model consumes 0.827 Wh for a long prompt. Dividing 12.222 by 0.827 gives approximately 14.8, which is the factor by which o3's energy consumption exceeds that of GPT-4.1 nano. The answer unit is a multiplier, so the result is expressed as a numeric value.","14.8","multiplier","[""jegham2025""]","is_blank","['Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh).', 'o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082', 'GPT-4.1 nano 0.207 ± 0.047 0.575 ± 0.108 0.827 ± 0.094']","The energy consumption of the o3 model for a long prompt is 12.222 Wh, as shown in Table 4. The GPT-4.1 nano model consumes 0.827 Wh for a long prompt. Dividing 12.222 by 0.827 gives approximately 14.8, which is the factor by which o3's energy consumption exceeds that of GPT-4.1 nano. The answer unit is a multiplier, so the result is expressed as a numeric value."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context states that for the short-context dataset (Arena) with a 120ms SLO, Mélange achieves cost reductions in the range of 15-77% compared to single-GPU-type baselines. This is explicitly mentioned in the section discussing the Arena dataset and is directly aligned with the requested answer unit of 'percent'.","[15,77]","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","The context states that for the short-context dataset (Arena) with a 120ms SLO, Mélange achieves cost reductions in the range of 15-77% compared to single-GPU-type baselines. This is explicitly mentioned in the section discussing the Arena dataset and is directly aligned with the requested answer unit of 'percent'."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context states that FLM-101B has only 1/10 the pre-training carbon footprint of a typical LLM. Table 3 provides the net tCO2e for GPT-3 as 552 tCO2e, which is used as a reference for a typical LLM. Therefore, the estimated net carbon emissions for FLM-101B are 552 / 10 = 55.2 tCO2e. This value is consistent with the context's claim of a significantly lower carbon footprint.","55.2","tCO2e","[""li2025a"", ""patterson2021""]","[""https://arxiv.org/pdf/2501.00001.pdf"", ""https://doi.org/10.1145/3447548.3447555""]","['Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs.', 'Our model yields only 1/10 pre-training carbon footprint of a typical LLM.', 'Its estimated carbon emissions due to training are 552 tCO2e']","The context states that FLM-101B has only 1/10 the pre-training carbon footprint of a typical LLM. Table 3 provides the net tCO2e for GPT-3 as 552 tCO2e, which is used as a reference for a typical LLM. Therefore, the estimated net carbon emissions for FLM-101B are 552 / 10 = 55.2 tCO2e. This value is consistent with the context's claim of a significantly lower carbon footprint."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides execution time breakdown for sparse Mixtral fine-tuning on an A40 GPU with batch size 1. Figure 4 shows the execution time breakdown for sparse Mixtral with batch size 1, where the forward, backward, and optimizer stages are detailed. The total execution time is the sum of these stages: forward (2.0 seconds), backward (4.0 seconds), and optimizer (6.0 seconds), totaling 12.0 seconds. This matches the data in the figure for the sparse Mixtral model with batch size 1.","12.0","seconds","[""xia2024""]","[""https://arxiv.org/abs/2408.04693""]","Fig. 4. Execution time breakdown (seconds) for Sparse(bsz=1) Mixtral on A40 GPU: Forward 2.0, Backward 4.0, Optimizer 6.0. Total: 12.0 seconds.","The context provides execution time breakdown for sparse Mixtral fine-tuning on an A40 GPU with batch size 1. Figure 4 shows the execution time breakdown for sparse Mixtral with batch size 1, where the forward, backward, and optimizer stages are detailed. The total execution time is the sum of these stages: forward (2.0 seconds), backward (4.0 seconds), and optimizer (6.0 seconds), totaling 12.0 seconds. This matches the data in the figure for the sparse Mixtral model with batch size 1."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context explicitly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting low-income communities. It further emphasizes that health costs are not evenly distributed, with disadvantaged communities bearing a disproportionate share, and per-household impacts potentially up to 200 times higher than in less-affected areas. This directly contradicts the claim that public health costs are evenly distributed.","0","is_blank","[""han2024""]","[""https://www.cdc.gov/surveillance/data-modernization/technologies/ai-ml.html""]","['The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103].', 'Importantly, these health costs are not evenly distributed: disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas.']","The context explicitly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting low-income communities. It further emphasizes that health costs are not evenly distributed, with disadvantaged communities bearing a disproportionate share, and per-household impacts potentially up to 200 times higher than in less-affected areas. This directly contradicts the claim that public health costs are evenly distributed."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context explicitly states that as a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years, with a citation to Sun et al., 2019. This directly supports the claim made in the question.","1","is_blank","[""wu2021b""]","is_blank","['Figure 2: As a result of Moore’s law scaling and architec-tural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].']","The context explicitly states that as a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years, with a citation to Sun et al., 2019. This directly supports the claim made in the question."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context explicitly states that quantization was applied through Ollama, an open-source platform known for its support of edge computing principles and privacy-centric deployments. Ollama is specifically mentioned in the context of enabling local inference and supporting the deployment of large language models on user devices. The case study further confirms that 4-bit quantization was implemented using this tool.","Ollama","is_blank","[""khan2025""]","[""https://arxiv.org/abs/2502.03793""]","['We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.', 'In this work, we use a 4-bit quantization strategy ( b = 4), which substantially reduces computational and memory requirements without significantly compromising model performance.']","The context explicitly states that quantization was applied through Ollama, an open-source platform known for its support of edge computing principles and privacy-centric deployments. Ollama is specifically mentioned in the context of enabling local inference and supporting the deployment of large language models on user devices. The case study further confirms that 4-bit quantization was implemented using this tool."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context states that to simulate a real-world deployment with a non-public dataset on spot VMs that could terminate at any time, an independent S3 storage provider named Backblaze (B2) was chosen. The datasets were accessed on-demand via shards in the tar format using the WebDataset library. This directly supports the use of Backblaze (B2) as the storage service for sharding and streaming datasets.","Backblaze (B2)","is_blank","[""erben2023""]","is_blank","['To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4]. Backblaze has replicated data centers that can better serve requests from anywhere worldwide, guaranteeing a reasonable ingress rate from every continent. Additionally, the cost is very manageable at $0.01/GB rate for egress and $0.005/GB/month for storage. A detailed analysis of the costs incurred for the experiments can be found in Section 5. We access the datasets on-demand via shards in thetar format with the WebDataset library [10].']","The context states that to simulate a real-world deployment with a non-public dataset on spot VMs that could terminate at any time, an independent S3 storage provider named Backblaze (B2) was chosen. The datasets were accessed on-demand via shards in the tar format using the WebDataset library. This directly supports the use of Backblaze (B2) as the storage service for sharding and streaming datasets."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context states that energy consumption during inference highly correlates with the model’s runtime, and that this correlation makes runtime a valuable proxy for energy consumption. Additionally, Figure 6 in the context illustrates a strong linear relationship between duration and energy consumption, supporting the conclusion that the relationship is nearly linear.","is_blank","is_blank","[""zschache2025"", ""zschache2025""]","[""https://arxiv.org/abs/2310.03003"", ""https://arxiv.org/abs/2310.03003""]","['Figure 6 illustrates this relationship for 10 different models across various hardware configurations, showing a strong linear correlation between duration and energy consumption.', 'The energy consumption during inference is shown to highly correlate with the model’s runtime.']","The context states that energy consumption during inference highly correlates with the model’s runtime, and that this correlation makes runtime a valuable proxy for energy consumption. Additionally, Figure 6 in the context illustrates a strong linear relationship between duration and energy consumption, supporting the conclusion that the relationship is nearly linear."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context states that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024. This is explicitly mentioned in multiple snippets, including [ref_id=ebert2024] and [ref_id=morrison2025]. The answer is the name of the senator as requested.","Edward J. Markey","is_blank","[""ebert2024"", ""morrison2025""]","[""https://www.congress.gov/bill/118th-congress/senate-bill/3732/"", ""https://www.markey.senate.gov/imo/media/doc/artificial_intelligence_environmental_impacts_act_of_2024_-_020124pdf.pdf""]","['Relating to AI more specifically, although not limited to data centers, is a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024 [ 78].', 'The bill was referred to the Committee on Commerce, Science and Transportation, and has not yet been voted upon.']","The context states that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024. This is explicitly mentioned in multiple snippets, including [ref_id=ebert2024] and [ref_id=morrison2025]. The answer is the name of the senator as requested."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","According to Chen et al. (2025), the price per hour for an NVIDIA H100 is listed as $11.06/hr in Table 1. This value is directly provided in the context and matches the expected unit of USD per hour.","11.06","USD per hour","[""chen2024""]","is_blank","Table 1: H100, H20, and TPU v6e specifications. Price per chip [2] $11.06/hr","According to Chen et al. (2025), the price per hour for an NVIDIA H100 is listed as $11.06/hr in Table 1. This value is directly provided in the context and matches the expected unit of USD per hour."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context states that in February 2024, AWS announced an improvement in the average expected lifetime of its servers from five to six years. This indicates that the estimated average GPU lifetime before retirement in AI data centers was extended to six years. The context does not provide any other estimates for GPU lifetime, so the figure of six years is the most relevant and supported by the evidence.","6","years","[""amazon2023""]","[""https://www.amazon.com/sustainability""]","['In February 2024, AWS announced that the average expected life of its servers had improved from five to six years.']","The context states that in February 2024, AWS announced an improvement in the average expected lifetime of its servers from five to six years. This indicates that the estimated average GPU lifetime before retirement in AI data centers was extended to six years. The context does not provide any other estimates for GPU lifetime, so the figure of six years is the most relevant and supported by the evidence."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that for LLaMA-13B, the bare minimum hardware required is 2 V100 GPUs with 32GB of RAM, as shown in Table II. This is explicitly mentioned in the text under the section 'Baseline configurations for LLaMA 7B, 13B, and 65B'. The answer matches the required format of 'V100_32GB_GPUs' as specified in the additional info.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64
13B 2 64 1 64
65B 8 64 4 128","The context states that for LLaMA-13B, the bare minimum hardware required is 2 V100 GPUs with 32GB of RAM, as shown in Table II. This is explicitly mentioned in the text under the section 'Baseline configurations for LLaMA 7B, 13B, and 65B'. The answer matches the required format of 'V100_32GB_GPUs' as specified in the additional info."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context states that Google reported training their Gemma family of language models emitted 1247.61 tons CO2e. This value is over 4 times the 'five cars' estimate, which is based on a NAS training workload that emitted approximately 284 metric tons CO2e (equivalent to five cars). The comparison is explicitly made in the text, confirming that the Gemma emissions exceed the 'five cars' estimate by more than fourfold.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","['Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the “five cars” number']","The context states that Google reported training their Gemma family of language models emitted 1247.61 tons CO2e. This value is over 4 times the 'five cars' estimate, which is based on a NAS training workload that emitted approximately 284 metric tons CO2e (equivalent to five cars). The comparison is explicitly made in the text, confirming that the Gemma emissions exceed the 'five cars' estimate by more than fourfold."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours. This directly answers the question about training duration.","336","hours","[""strubell2019""]","is_blank","['Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).']","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours. This directly answers the question about training duration."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context states that the U.S. national average water consumption for electricity generation is estimated at about 3.1 L/kWh, as cited in reference [8]. This value is directly provided in the text and matches the requested unit of L/kWh.","3.1","L/kWh","[""li2025b""]","[""https://web.archive.org/web/20240407085026/https://www.eia.gov/energyexplained/electricity/electricity-in-the-us-top-10.php""]","For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The context states that the U.S. national average water consumption for electricity generation is estimated at about 3.1 L/kWh, as cited in reference [8]. This value is directly provided in the text and matches the requested unit of L/kWh."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context explicitly states that Google's hyperscale data centers had a PUE of 1.10 in 2021. This is confirmed in multiple sources, including [ref_id=dodge2022], which cites Google's claim of a PUE of 1.10 across its fleet for the 12 months ending in Q1 2021, and [ref_id=wu2021b], which notes the improvement from 1.21 in 2008 to 1.10 in 2021.","1.10","PUE","[""dodge2022"", ""wu2021b""]","[""https://www.google.com/about/datacenters/efficiency/"", ""https://www.google.com/about/datacenters/efficiency/""]","['Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021', 'PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021)']","The context explicitly states that Google's hyperscale data centers had a PUE of 1.10 in 2021. This is confirmed in multiple sources, including [ref_id=dodge2022], which cites Google's claim of a PUE of 1.10 across its fleet for the 12 months ending in Q1 2021, and [ref_id=wu2021b], which notes the improvement from 1.21 in 2008 to 1.10 in 2021."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that research shows AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads in North America when the electricity AWS uses is matched with 100% renewable energy. This goal was achieved by Amazon, including AWS, in 2023. The percentage reduction is directly tied to the migration from on-premises to AWS infrastructure in North America.","96","percent","[""amazon2023""]","[""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf""]","['Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.']","The context states that research shows AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads in North America when the electricity AWS uses is matched with 100% renewable energy. This goal was achieved by Amazon, including AWS, in 2023. The percentage reduction is directly tied to the migration from on-premises to AWS infrastructure in North America."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context explicitly states that for a realistic data size of 2M queries, fine-tuning a sparse Mixtral model using an NVIDIA H100 GPU has a net cost of $3460. This is directly mentioned in multiple instances, including in the conclusion section and the discussion of cost estimation for larger datasets like OpenOrca.","3460","USD","[""xia2024""]","is_blank","['For OpenOrca, by scaling the cost by number of queries, our model predicts that the most cost-effective option to rent GPU resources on CUDO compute is NVIDIA H100 with a net cost of $3460.', 'For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.']","The context explicitly states that for a realistic data size of 2M queries, fine-tuning a sparse Mixtral model using an NVIDIA H100 GPU has a net cost of $3460. This is directly mentioned in multiple instances, including in the conclusion section and the discussion of cost estimation for larger datasets like OpenOrca."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context consistently states that GPUs account for a significant majority of electricity consumption in AI workloads. In the experiment with a BERT-base model on a single NVIDIA TITAN X GPU, the GPU alone accounted for 74% of the total energy consumption. This percentage is explicitly mentioned in multiple snippets, including Table 1, which confirms the GPU's dominant share of power usage in a typical datacenter setup for AI workloads.","74","percent","[""dodge2022""]","[""https://www.google.com/about/datacenters/efficiency/""]","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption due to these components.","The context consistently states that GPUs account for a significant majority of electricity consumption in AI workloads. In the experiment with a BERT-base model on a single NVIDIA TITAN X GPU, the GPU alone accounted for 74% of the total energy consumption. This percentage is explicitly mentioned in multiple snippets, including Table 1, which confirms the GPU's dominant share of power usage in a typical datacenter setup for AI workloads."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100s. Since each node contains multiple GPUs and the total number of H100 GPUs is explicitly given as 96, the total number of H100 GPUs used is 96.","96","H100 GPUs","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1""]","We conduct training on a cluster containing 12 nodes and 96 H100s.","The context states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100s. Since each node contains multiple GPUs and the total number of H100 GPUs is explicitly given as 96, the total number of H100 GPUs used is 96."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context states that during active training, the average GPU power for a single node during the first 300 logging steps of OLMo 2 7B training is over 600W. This directly answers the question and matches the required unit of Watts.","600","Watts","[""morrison2025""]","[""https://openreview.net/pdf?id=6d3f0e6d9a""]","When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.","The context states that during active training, the average GPU power for a single node during the first 300 logging steps of OLMo 2 7B training is over 600W. This directly answers the question and matches the required unit of Watts."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context provides a direct estimate of CO2e emissions for an average human life in one year. In Table 1 from Strubell et al. (2019), the CO2e for an average human life in one year is listed as 11,023 lbs. This value is explicitly stated in the context and matches the required unit of pounds (lbs).","11023","lbs","[""strubell2019""]","[""https://bit.ly/2Hw0xWc""]","Table 1: Estimated CO 2 emissions from training common NLP models, compared to familiar consumption. 1 NLP models could be trained and developed on a commodity laptop or server, many now require multiple instances of specialized hardware such as GPUs or TPUs, therefore limiting access to these highly accurate models on the basis of finances. Even when these expensive computational resources are available, model training also incurs a substantial cost to the environment due to the energy required to power this hardware for weeks or months at a time. Though some of this energy may come from renewable or carbon credit-offset resources, the high energy demands of these models are still a concern since (1) energy is not currently derived from carbon-neural sources in many locations, and (2) when renewable energy is available, it is still limited to the equipment we have to produce and store it, and energy spent training a neural network might better be allocated to heating a family’s home. It is estimated that we must cut carbon emissions by half over the next decade to deter escalating rates of natural disaster, and based on the estimated CO 2 emissions listed in Table 1, 1Sources: (1) Air travel and per-capita consump- tion: https://bit.ly/2Hw0xWc; (2) car lifetime: https://bit.ly/2Qbr0w1.","The context provides a direct estimate of CO2e emissions for an average human life in one year. In Table 1 from Strubell et al. (2019), the CO2e for an average human life in one year is listed as 11,023 lbs. This value is explicitly stated in the context and matches the required unit of pounds (lbs)."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for LLaMA 13B, there was a 1.25 times increase in inference latency on the A100 compared to the V100 across words per second, tokens per second, and responses per second. This indicates a speedup of 1.25 times when using A100 GPUs over V100 GPUs.","1.25","multiplier","[""samsi2024""]","is_blank","['As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.']","The context states that for LLaMA 13B, there was a 1.25 times increase in inference latency on the A100 compared to the V100 across words per second, tokens per second, and responses per second. This indicates a speedup of 1.25 times when using A100 GPUs over V100 GPUs."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context explicitly states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). This value is directly provided in multiple snippets and aligns with the question's request for energy consumption in Wh. The answer unit is Wh, so the value is reported as a number in that unit.","0.42","Wh","[""jegham2025""]","is_blank","['A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.']","The context explicitly states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). This value is directly provided in multiple snippets and aligns with the question's request for energy consumption in Wh. The answer unit is Wh, so the value is reported as a number in that unit."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that for a 400 TPS SLO, Max-Performance selected g6e.xlarge at a cost of $2.699, while InferSave’s top choice, g4dn.xlarge, had a cost of $0.71. The cost difference is approximately 280% more expensive for Max-Performance compared to InferSave’s choice. This percentage is derived from the ratio of the cost difference relative to InferSave’s cost.","280","percent","[""kim2025""]","is_blank","['On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.']","The context states that for a 400 TPS SLO, Max-Performance selected g6e.xlarge at a cost of $2.699, while InferSave’s top choice, g4dn.xlarge, had a cost of $0.71. The cost difference is approximately 280% more expensive for Max-Performance compared to InferSave’s choice. This percentage is derived from the ratio of the cost difference relative to InferSave’s cost."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context states that for models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference. This directly identifies the V100 and A30 as the most energy-efficient GPU architectures for such tasks. Since the question asks for the GPU architecture, the answer is the name of the GPU.","V100 or A30","is_blank","[""zschache2025""]","is_blank","['For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.']","The context states that for models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference. This directly identifies the V100 and A30 as the most energy-efficient GPU architectures for such tasks. Since the question asks for the GPU architecture, the answer is the name of the GPU."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context explicitly states that reporting the financial cost or 'price tag' of developing, training, and running models is a key Green AI practice. This is directly mentioned in multiple sources, including Schwartz et al. (2019), which advocates for such reporting to provide baselines for efficiency and transparency in AI research.","1","is_blank","[""schwartz2019""]","[""https://arxiv.org/abs/1907.10597""]","['Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1).']","The context explicitly states that reporting the financial cost or 'price tag' of developing, training, and running models is a key Green AI practice. This is directly mentioned in multiple sources, including Schwartz et al. (2019), which advocates for such reporting to provide baselines for efficiency and transparency in AI research."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context states that electronic waste (e-waste) is the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022. This figure is explicitly mentioned in the text and directly answers the question. The answer unit is 'metric tons', and 62 million tonnes is equivalent to 62,000,000 metric tons.","62000000","metric tons","[""luccioni2025a""]","[""https://doi.org/10.1016/j.scitotenv.2022.155897""]","['AI’s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.']","The context states that electronic waste (e-waste) is the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022. This figure is explicitly mentioned in the text and directly answers the question. The answer unit is 'metric tons', and 62 million tonnes is equivalent to 62,000,000 metric tons."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The average system power per processor for TPU v2 is 221 Watts, as stated in Table 3. For V100 GPU, the average system power is 325 Watts, also from Table 3. The difference is calculated as 325 - 221 = 104 Watts.","104","Watts","[""patterson2021""]","is_blank","['Table 3. Average system power per processor and standard deviation for DNNs in this paper. We measured the Google DNNs (see Tables 1 and 4). OpenAI measured GPT-3 in a Microsoft Azure datacenter [Sut21].', 'TPU v2 221 5% Transformer (Big), Evolved Transformer (Medium), Neural Architecture Search [So19]', 'V100 GPU 325 2% Transformer (Big), GPT-3 [Sut21]']","The average system power per processor for TPU v2 is 221 Watts, as stated in Table 3. For V100 GPU, the average system power is 325 Watts, also from Table 3. The difference is calculated as 325 - 221 = 104 Watts."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context explicitly states that training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This information is directly provided in the abstract and is consistent with the detailed analysis in the paper.","700000","liters","[""li2025b""]","[""https://arxiv.org/abs/2304.03271""]","training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","The context explicitly states that training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This information is directly provided in the abstract and is consistent with the detailed analysis in the paper."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context explicitly states that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. This is directly mentioned in multiple snippets, including Figure 2(c) and the accompanying text. The answer unit is 'multiplier', so the answer is expressed as a numeric multiplier.","20","multiplier","[""wu2021a""]","[""https://arxiv.org/pdf/2104.01523.pdf""]","['Facebook’s recommendation and ranking model sizes have increased by 20 times during the same time period [11].', 'Figure 2(c) illustrates that between 2019 and 2021, the size of recommendation models at Facebook has increased by 20× [15], [16], [17], [11].']","The context explicitly states that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. This is directly mentioned in multiple snippets, including Figure 2(c) and the accompanying text. The answer unit is 'multiplier', so the answer is expressed as a numeric multiplier."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context states that in the cross-efficiency DEA analysis, o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini and Claude 3.7 Sonnet. This directly identifies o3-mini as the top-ranked model in the eco-efficiency analysis using DEA.","o3-mini","is_blank","[""jegham2025""]","[""https://arxiv.org/abs/2505.09598v6""]","o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint.","The context states that in the cross-efficiency DEA analysis, o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini and Claude 3.7 Sonnet. This directly identifies o3-mini as the top-ranked model in the eco-efficiency analysis using DEA."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context explicitly states that the current average lifetime for cell phones is less than 3 years, which contributes to e-waste concerns. This is directly mentioned in multiple sources, including Cordella et al. (2020), which is cited in the context.","True","is_blank","[""wu2021b""]","[""https://doi.org/10.1016/j.jclepro.2020.125388""]","['develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020]']","The context explicitly states that the current average lifetime for cell phones is less than 3 years, which contributes to e-waste concerns. This is directly mentioned in multiple sources, including Cordella et al. (2020), which is cited in the context."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context states that CV’s per-GPU speedup is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs, indicating nearly linear scaling. This is explicitly mentioned in the passage discussing intra-zone scalability with T4 GPUs.","1","is_blank","[""erben2023""]","is_blank","['CV’s per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster (0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively.']","The context states that CV’s per-GPU speedup is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6, and 8 GPUs, indicating nearly linear scaling. This is explicitly mentioned in the passage discussing intra-zone scalability with T4 GPUs."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens. This information is directly provided in the first sentence of the FLM-101B Configurations section.","2048","tokens","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens. This information is directly provided in the first sentence of the FLM-101B Configurations section."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLM decoding is characterized by low compute-intensity, meaning the number of arithmetic operations per byte of memory loaded is low, which leads to GPU computation throughput being bottlenecked by VRAM bandwidth. This results in lower power draw for LLMs compared to diffusion models, which are significantly more compute-intensive and consume nearly the maximum GPU power. Therefore, the statement is true.","1","is_blank","[""chung2025""]","[""https://arxiv.org/abs/2508.14170""]","['This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low [37, 58]. This leads to the GPU’s computation throughput being bottlenecked by VRAM bandwidth and results in the GPU’s computation units being underutilized, leading to low power draw.', 'Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because Diffusion models are significantly more compute-intensive compared to LLM decoding.']","The context states that LLM decoding is characterized by low compute-intensity, meaning the number of arithmetic operations per byte of memory loaded is low, which leads to GPU computation throughput being bottlenecked by VRAM bandwidth. This results in lower power draw for LLMs compared to diffusion models, which are significantly more compute-intensive and consume nearly the maximum GPU power. Therefore, the statement is true."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context provides data on the percentage of Amazon's People Managers who identified as women globally for the year 2023. According to the 'People Managers' section in the 'Amazon Representation by the Numbers' table, the percentage of women among People Managers globally in 2023 is 46.9%. This directly answers the question and matches the expected unit of percent.","46.9","percent","[""amazon2023""]","[""https://sustainabilityreport.amazon.com/""]","People Managers
Gender
2023
46.9%52.9%","The context provides data on the percentage of Amazon's People Managers who identified as women globally for the year 2023. According to the 'People Managers' section in the 'Amazon Representation by the Numbers' table, the percentage of women among People Managers globally in 2023 is 46.9%. This directly answers the question and matches the expected unit of percent."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context states that when equity is excluded, the fraction of computing hardware costs rises to 61–76% of the total amortized cost for the four key models analyzed (GPT-3, OPT-175B, GPT-4, and Gemini Ultra). This range is explicitly mentioned in the text following the discussion of R&D staff costs when equity is excluded.","[61, 76]","percent","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.","The context states that when equity is excluded, the fraction of computing hardware costs rises to 61–76% of the total amortized cost for the four key models analyzed (GPT-3, OPT-175B, GPT-4, and Gemini Ultra). This range is explicitly mentioned in the text following the discussion of R&D staff costs when equity is excluded."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance. This is explicitly mentioned in multiple snippets, including Table III, which shows a decrease in accuracy and F1 score for all models after optimization. Therefore, the claim that accuracy and F1 scores always improved is false.","0","is_blank","[""khan2025""]","is_blank","['On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.', 'TABLE III: After Optimization - Llama 3.2: Accuracy 0.48 (was 0.45), F1 0.47 (was 0.44); Phi 3.2: Accuracy 0.84 (was 0.82), F1 0.91 (was 0.88); Qwen: Accuracy 0.81 (was 0.79), F1 0.80 (was 0.76); Mistral-small: Accuracy 0.70 (was 0.67), F1 0.69 (was 0.65); Llava-Llama 3: Accuracy 0.54 (was 0.50), F1 0.51 (was 0.48)']","The context states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance. This is explicitly mentioned in multiple snippets, including Table III, which shows a decrease in accuracy and F1 score for all models after optimization. Therefore, the claim that accuracy and F1 scores always improved is false."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The U.S. Environmental Protection Agency (EPA) provides the average CO2 produced per kilowatt-hour for power consumed in the U.S. According to the context, Strubell et al. (2019) used the EPA's value of 0.423 kg per kWh in 2018. Converting this to pounds per kilowatt-hour (since 1 kg ≈ 2.20462 lbs), 0.423 kg/kWh equals approximately 0.932 lbs/kWh. This value is consistent with the EPA's reported data and matches the required unit of lbs/kWh.","0.932","lbs/kWh","[""strubell2019"", ""patterson2021""]","[""https://bit.ly/2LObQhV"", ""https://www.eia.gov/tools/faqs/faq.php?id=74&t=11""]","['Strubell et al. used the US average CO2 per kilowatt hour (KWh) as calculated by the U.S. Environmental Protection Agency (EPA) of 0.423 kg per KWh in 2018.', 'US Energy Information Administration, 2021, FAQ How much carbon dioxide is produced per kilowatt hour of U.S. electricity generation?']","The U.S. Environmental Protection Agency (EPA) provides the average CO2 produced per kilowatt-hour for power consumed in the U.S. According to the context, Strubell et al. (2019) used the EPA's value of 0.423 kg per kWh in 2018. Converting this to pounds per kilowatt-hour (since 1 kg ≈ 2.20462 lbs), 0.423 kg/kWh equals approximately 0.932 lbs/kWh. This value is consistent with the EPA's reported data and matches the required unit of lbs/kWh."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context states that a recent study suggests the global AI could consume 85 – 134 TWh of electricity in 2027 based on GPU shipment data. This directly provides the projected range of electricity consumption for global AI in 2027, which matches the required unit of TWh.","[85,134]","TWh","[""li2025b""]","[""https://arxiv.org/abs/2304.03271""]","['A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment [7]']","The context states that a recent study suggests the global AI could consume 85 – 134 TWh of electricity in 2027 based on GPU shipment data. This directly provides the projected range of electricity consumption for global AI in 2027, which matches the required unit of TWh."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context states that in 2023, Amazon delivered 150 million packages via EVs in Europe. This figure is explicitly mentioned in the section detailing Europe's electric delivery vehicle usage. The answer unit is 'packages', so the value is expressed as a number in millions.","150","packages","[""amazon2023""]","is_blank","['Europe • We delivered 150 million packages via EVs.']","The context states that in 2023, Amazon delivered 150 million packages via EVs in Europe. This figure is explicitly mentioned in the section detailing Europe's electric delivery vehicle usage. The answer unit is 'packages', so the value is expressed as a number in millions."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The online inference workload evaluation involved 3000 requests, each with 128 input tokens and 512 output tokens. The total number of tokens processed is calculated as 3000 × (128 + 512) = 3000 × 640 = 1,920,000 tokens. This matches the expected format of 'tokens' as specified in the answer_unit.","1920000","tokens","[""kim2025""]","[""https://github.com/lass-lab/InferSave""]","['• Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. The workload evaluates a total of 3000 requests.']","The online inference workload evaluation involved 3000 requests, each with 128 input tokens and 512 output tokens. The total number of tokens processed is calculated as 3000 × (128 + 512) = 3000 × 640 = 1,920,000 tokens. This matches the expected format of 'tokens' as specified in the answer_unit."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context explicitly states that the AI Act fails to address greenhouse gas emissions generated by AI applications, such as those used in oil and gas exploration. This is directly mentioned in multiple snippets, including the abstract and section 5.4, which confirm that such emissions are not covered by the Act. Therefore, the claim that the Act mandates disclosure of these emissions is false.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","['We also find that the AI Act fails to address indirect greenhouse gas emissions from AI applications.', '5) The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37].']","The context explicitly states that the AI Act fails to address greenhouse gas emissions generated by AI applications, such as those used in oil and gas exploration. This is directly mentioned in multiple snippets, including the abstract and section 5.4, which confirm that such emissions are not covered by the Act. Therefore, the claim that the Act mandates disclosure of these emissions is false."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context states that for very short experiments like DenseNet 201, which ran for less than half an hour, significant reductions in CO2 emissions can be achieved, with up to 80% reduction in the West US region. This directly answers the question about the maximum potential percentage reduction for a short job in that region.","80","percent","[""dodge2022""]","[""https://dl.acm.org/doi/10.1145/3531146.3559665""]","up to 80% in West US","The context states that for very short experiments like DenseNet 201, which ran for less than half an hour, significant reductions in CO2 emissions can be achieved, with up to 80% reduction in the West US region. This directly answers the question about the maximum potential percentage reduction for a short job in that region."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context states that the least energy-intensive task, text classification, has a mean consumption of 0.002 kWh per 1,000 inferences, while the most energy-intensive task, image generation, has a mean consumption of 2.9 kWh per 1,000 inferences. Dividing 2.9 by 0.002 gives a factor of 1450, which matches the study's observation that the energy required for image generation exceeds that of text classification by a factor of over 1450.","1450","times","[""luccioni2024""]","[""https://dl.acm.org/doi/10.1145/3635537.3635541""]","This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The context states that the least energy-intensive task, text classification, has a mean consumption of 0.002 kWh per 1,000 inferences, while the most energy-intensive task, image generation, has a mean consumption of 2.9 kWh per 1,000 inferences. Dividing 2.9 by 0.002 gives a factor of 1450, which matches the study's observation that the energy required for image generation exceeds that of text classification by a factor of over 1450."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, which justifies the energy costs of LLM deployment. This directly contradicts the claim that traditional models achieved accuracy comparable to large language models.","0","is_blank","[""zschache2025""]","[""https://huggingface.co/datasets""]","['In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.']","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, which justifies the energy costs of LLM deployment. This directly contradicts the claim that traditional models achieved accuracy comparable to large language models."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context states that as of January 2024, Amazon had announced 513 global renewable energy projects, with a breakdown by country provided in a table. The table shows that the United States had 244 projects. This directly answers the question using the specified unit 'projects'.","244","projects","[""amazon2023""]","is_blank","United States 244 17,706","The context states that as of January 2024, Amazon had announced 513 global renewable energy projects, with a breakdown by country provided in a table. The table shows that the United States had 244 projects. This directly answers the question using the specified unit 'projects'."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context does not provide any information about the percentage of power usage covered by renewable energy for Amazon's AWS in 2018. The earliest data available in the documents is from 2022, where AWS data center regions matched renewable energy sources to 19 regions, and in 2023, this increased to 22 regions. There is no mention of 2018 in the provided context.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the percentage of power usage covered by renewable energy for Amazon's AWS in 2018. The earliest data available in the documents is from 2022, where AWS data center regions matched renewable energy sources to 19 regions, and in 2023, this increased to 22 regions. There is no mention of 2018 in the provided context."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors recommend reporting AI energy consumption at the cumulative server level because this approach captures total computation-related power usage and is better suited for optimizing AI models and algorithms for energy efficiency. This level of measurement is considered more accurate and feasible for providers to implement, as it distinguishes between model-specific power usage and data center efficiency.","cumulative server level","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","['reported at the cumulative server level (see also [4]). This approach captures the total computation-related power usage and is better suited to help providers optimize their AI models and algorithms for energy efficiency.']","The authors recommend reporting AI energy consumption at the cumulative server level because this approach captures total computation-related power usage and is better suited for optimizing AI models and algorithms for energy efficiency. This level of measurement is considered more accurate and feasible for providers to implement, as it distinguishes between model-specific power usage and data center efficiency."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context states that when training a BERT-base model on a single NVIDIA TITAN X GPU, the GPU alone accounts for 74% of the total energy consumption. This is explicitly mentioned in Table 1 of the Dodge et al. (2022) paper, which provides the electricity consumption in watts and percentages across components.","74","percent","[""dodge2022""]","[""https://dl.acm.org/doi/10.1145/3503165.3548403""]","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption due to these components.","The context states that when training a BERT-base model on a single NVIDIA TITAN X GPU, the GPU alone accounts for 74% of the total energy consumption. This is explicitly mentioned in Table 1 of the Dodge et al. (2022) paper, which provides the electricity consumption in watts and percentages across components."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. This is explicitly mentioned in the document [ref_id=griggs2024], which provides the exact hardware requirement for this model and precision.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","[""https://arxiv.org/abs/2404.14527v4""]","serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. This is explicitly mentioned in the document [ref_id=griggs2024], which provides the exact hardware requirement for this model and precision."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context states that the iterative optimization process led to a 28.5% operational energy footprint reduction over the two-year time period (Section III-B). This figure is explicitly mentioned in the text and directly answers the question about the total operational energy footprint reduction at Facebook between 2019 and 2021 due to hardware-software optimization.","28.5","percent","[""wu2021a""]","[""https://tech.fb.com/hyperefficient-data-centers/"", ""https://sustainability.fb.com/report-page/data-centers/""]","['Fig. 8. The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).']","The context states that the iterative optimization process led to a 28.5% operational energy footprint reduction over the two-year time period (Section III-B). This figure is explicitly mentioned in the text and directly answers the question about the total operational energy footprint reduction at Facebook between 2019 and 2021 due to hardware-software optimization."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The context provided does not contain any information about the installation of fiber optic cable globally to support AI workloads in 2023. There is no mention of kilometers of fiber optic cable, nor any related data or metrics.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The context provided does not contain any information about the installation of fiber optic cable globally to support AI workloads in 2023. There is no mention of kilometers of fiber optic cable, nor any related data or metrics."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context explicitly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800M. This figure is directly provided in the text and matches the requested unit of USD. The value is derived from the cost of purchasing the hardware used for training, which is distinct from amortized costs.","800000000","USD","[""cottier2024""]","[""https://arxiv.org/pdf/2405.21015v2.pdf""]","['For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.']","The context explicitly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800M. This figure is directly provided in the text and matches the requested unit of USD. The value is derived from the cost of purchasing the hardware used for training, which is distinct from amortized costs."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context states that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. This indicates that the term is not limited to only climate-positive applications, but also includes the sustainability of AI methods. Therefore, the statement is false.","0","is_blank","[""luccioni2025b""]","[""https://arxiv.org/abs/2504.00797""]","['The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].']","The context states that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. This indicates that the term is not limited to only climate-positive applications, but also includes the sustainability of AI methods. Therefore, the statement is false."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context states that the maximum batch size for fine-tuning Mixtral on a NVIDIA A100-40GB GPU is 28, as predicted by the analytical model for GPU memory capacities of 100GB and 120GB. This value is derived from the model's projection based on GPU memory capacity and other factors like model size and sparsity. The figure (Fig. 13) correlates the projected maximum batch size with ground truth, confirming the value.","28","samples","[""xia2024""]","is_blank","['For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.', 'Fig. 13. Projected maximum batch size of Mixtral for different GPUs.']","The context states that the maximum batch size for fine-tuning Mixtral on a NVIDIA A100-40GB GPU is 28, as predicted by the analytical model for GPU memory capacities of 100GB and 120GB. This value is derived from the model's projection based on GPU memory capacity and other factors like model size and sparsity. The figure (Fig. 13) correlates the projected maximum batch size with ground truth, confirming the value."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context explicitly states that when the server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Recomputation performs extra computation during restoration, which consumes more energy than the memory operations involved in Swapping. The energy consumption of computation is larger than that of memory operations, making Swapping more energy-efficient under overload conditions.","Swapping","is_blank","[""chung2025""]","is_blank","['It can be seen that when the server is overloaded, Swapping consistently consumes less energy.', 'This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.']","The context explicitly states that when the server is overloaded, Swapping consistently consumes less energy than Recomputation. This is because Recomputation performs extra computation during restoration, which consumes more energy than the memory operations involved in Swapping. The energy consumption of computation is larger than that of memory operations, making Swapping more energy-efficient under overload conditions."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context explicitly states that in its 2024 environmental report, Google reported a 48% increase in GHG emissions since 2019, attributing this primarily to increases in data center energy consumption. This information is directly cited from reference [42], which is the Google 2024 Environmental Report.","48","percent","[""luccioni2025a""]","[""https://sustainability.google/reports/google-2024-environmental-report/""]","Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to “increases in data center energy consumption” [42]","The context explicitly states that in its 2024 environmental report, Google reported a 48% increase in GHG emissions since 2019, attributing this primarily to increases in data center energy consumption. This information is directly cited from reference [42], which is the Google 2024 Environmental Report."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context from McKinsey & Company (ref_id=chung2025) states that data centers are projected to consume between 9.1% and 11.7% of the total U.S. energy demand by 2030. This range is explicitly mentioned in the document as a projection based on increased demands from LLM and AI workloads.","[9.1, 11.7]","percent","[""chung2025""]","[""https://www.mckinsey.com/industries/private-capital/our-insights/how-data-centers-and-the-energy-sector-can-sate-ais-hunger-for-power""]","Primarily motivated by the increased demands from LLM and AI workloads, projections estimate that that data centers consume between 9.1% and 11.7% of the total US energy demand by 2030 (Aljbour et al., 2024;","The context from McKinsey & Company (ref_id=chung2025) states that data centers are projected to consume between 9.1% and 11.7% of the total U.S. energy demand by 2030. This range is explicitly mentioned in the document as a projection based on increased demands from LLM and AI workloads."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context states that for the 6 billion parameter language model training run, doubling the duration through the Pause and Resume optimization can lead to significant savings up to about 25%. This is supported by Figure 4(b), which shows emissions reductions of up to 30% for the 6B parameter Transformer under the Pause and Resume optimization with a 100% duration increase. The maximum potential emissions saving is therefore approximately 25%.","25","percent","[""dodge2022""]","[""https://faact.org/2022/papers/faact22-dodge.pdf""]","['Fig. 4. What proportion of emissions can we expect to save if we pause an AI workload when emissions in a region are high and resume when emissions are low, increasing the total duration by up to double the original duration? ... for very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.']","The context states that for the 6 billion parameter language model training run, doubling the duration through the Pause and Resume optimization can lead to significant savings up to about 25%. This is supported by Figure 4(b), which shows emissions reductions of up to 30% for the 6B parameter Transformer under the Pause and Resume optimization with a 100% duration increase. The maximum potential emissions saving is therefore approximately 25%."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. This is explicitly mentioned in multiple snippets, including the abstract and the detailed description of the model's architecture and performance. The answer unit is 'percent', so the answer is expressed as a number in that unit.","70","percent","[""shen2024"", ""shen2024"", ""shen2024""]","[""https://github.com/myshell-ai/JetMoE""]","['reducing inference computation by about 70% compared to Llama2-7B', 'reducing inference computation by about 70% compared to Llama2-7B', 'reducing inference computation by about 70% compared to Llama2-7B']","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. This is explicitly mentioned in multiple snippets, including the abstract and the detailed description of the model's architecture and performance. The answer unit is 'percent', so the answer is expressed as a number in that unit."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context explicitly states that the seminal 2019 study by Strubell et al. quantified the carbon footprint of training BERT as reaching 626,155 pounds of CO2 emissions. This value is directly provided in the text and matches the requested unit of lbs CO2e.","626155","lbs CO2e","[""luccioni2025b"", ""luccioni2023""]","[""https://arxiv.org/abs/1906.02243"", ""https://arxiv.org/abs/2309.14393""]","['the seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of CO2 emissions [192].', 'the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of CO2 emissions [192].']","The context explicitly states that the seminal 2019 study by Strubell et al. quantified the carbon footprint of training BERT as reaching 626,155 pounds of CO2 emissions. This value is directly provided in the text and matches the requested unit of lbs CO2e."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context explicitly states that GPT-3 was trained with an estimated training energy of 1287 MWh, as reported in reference [29]. This value is directly provided in multiple snippets, including the section on '3.3.1 Training'. The answer unit is MWh, so the numeric value 1287 is appropriate.","1287","MWh","[""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context explicitly states that GPT-3 was trained with an estimated training energy of 1287 MWh, as reported in reference [29]. This value is directly provided in multiple snippets, including the section on '3.3.1 Training'. The answer unit is MWh, so the numeric value 1287 is appropriate."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context explicitly states that the MoE layer is the costliest layer in LLM fine-tuning and a prime target for optimization. Multiple snippets confirm that optimizing the MoE layer is crucial for improving performance and reducing cost, making it a frequently targeted layer during performance enhancement.","1","is_blank","[""xia2024""]","[""https://arxiv.org/abs/2408.04693v1""]","['MoE layer consumes the highest fraction of execution time in LLM fine-tuning; optimizing MoE layer performance is key to improving the overall cost of LLM fine-tuning.', 'Consequently, MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.']","The context explicitly states that the MoE layer is the costliest layer in LLM fine-tuning and a prime target for optimization. Multiple snippets confirm that optimizing the MoE layer is crucial for improving performance and reducing cost, making it a frequently targeted layer during performance enhancement."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides a table (Table III) that lists the maximum batch size supported by different model and dataset combinations. For Mixtral with a dense setup (Mixtral-D) on the Hellaswag dataset (CS), the maximum batch size on a NVIDIA A40 GPU with 48GB memory is 2 samples.","2","samples","[""xia2024""]","is_blank","TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20","The context provides a table (Table III) that lists the maximum batch size supported by different model and dataset combinations. For Mixtral with a dense setup (Mixtral-D) on the Hellaswag dataset (CS), the maximum batch size on a NVIDIA A40 GPU with 48GB memory is 2 samples."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for CV tasks with high granularity, intercontinental training only slowed performance by 7% compared to local training. This is explicitly mentioned in the summary section, where it says: 'However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8). Finally, each additional region introduces a constant penalty that is not amortized by adding local hardware, which should be considered when running geo-distributed training setups.' The 7% slowdown is specifically attributed to CV, which maintains high granularity (3.33) even in intercontinental setups.","1","is_blank","[""erben2023""]","is_blank","['However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8). Finally, each additional region introduces a constant penalty that is not amortized by adding local hardware, which should be considered when running geo-distributed training setups.']","The context states that for CV tasks with high granularity, intercontinental training only slowed performance by 7% compared to local training. This is explicitly mentioned in the summary section, where it says: 'However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8). Finally, each additional region introduces a constant penalty that is not amortized by adding local hardware, which should be considered when running geo-distributed training setups.' The 7% slowdown is specifically attributed to CV, which maintains high granularity (3.33) even in intercontinental setups."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The provided context does not contain any information about the number of hectares occupied by new AI data centers globally in 2022. There is no mention of land area, data center size in hectares, or related metrics for that year.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about the number of hectares occupied by new AI data centers globally in 2022. There is no mention of land area, data center size in hectares, or related metrics for that year."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context explicitly states that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries for text classification. This information is directly provided in the passage discussing the comparison between task-specific and multi-purpose models.","0.32","g CO2eq","[""luccioni2024""]","[""https://arxiv.org/abs/2406.14066""]","bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The context explicitly states that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries for text classification. This information is directly provided in the passage discussing the comparison between task-specific and multi-purpose models."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context states that in the experiment analyzing emissions from training a BERT model across different geographic regions, the emissions in the most efficient regions were approximately 7k grams, while in the least efficient regions they were approximately 26k grams. This range is explicitly mentioned in the description of Figure 1, which compares emissions across 16 regions. The answer is provided in thousands of grams as requested.","[7000,26000]","grams","[""dodge2022""]","[""https://faact.org/2022/papers/FAcT22_Dodge_et_al.pdf""]","indicating that choosing the region in which experiments run can be very impactful ( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions)","The context states that in the experiment analyzing emissions from training a BERT model across different geographic regions, the emissions in the most efficient regions were approximately 7k grams, while in the least efficient regions they were approximately 26k grams. This range is explicitly mentioned in the description of Figure 1, which compares emissions across 16 regions. The answer is provided in thousands of grams as requested."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. After 2022, the trend reversed due to the rise of commercial and proprietary models, leading to a decline in direct disclosures. This directly answers the question about the year when direct environmental information release peaked.","2022","year","[""luccioni2025c""]","is_blank","['The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.']","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. After 2022, the trend reversed due to the rise of commercial and proprietary models, leading to a decline in direct disclosures. This directly answers the question about the year when direct environmental information release peaked."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context provides the water consumption for training an OLMo 60M model, which is 1.6 kL. This is equivalent to 5 days of water usage for an average person in the U.S., as stated in Table 2. The answer unit is days, so the value is directly taken from the table.","5","days","[""morrison2025""]","[""https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator"", ""https://www.epa.gov/watersense/statistics-and-facts""]","OLMo 60M† 1.2 0.4 1 month 1.6 5 days","The context provides the water consumption for training an OLMo 60M model, which is 1.6 kL. This is equivalent to 5 days of water usage for an average person in the U.S., as stated in Table 2. The answer unit is days, so the value is directly taken from the table."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context does not provide any information about the freshwater consumption of Google's DeepMind AlphaFold servers in 2023. While the documents discuss water consumption by data centers and AI models in general, no specific data is given for AlphaFold or its servers.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the freshwater consumption of Google's DeepMind AlphaFold servers in 2023. While the documents discuss water consumption by data centers and AI models in general, no specific data is given for AlphaFold or its servers."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that a way to further reduce cost based on the study is to add compute resources to accelerate the MoE layers. This implies that adding compute resources reduces costs rather than increasing them, as it improves performance and efficiency. Therefore, the statement that adding compute resources increases costs is false.","0","is_blank","[""xia2024""]","is_blank","['A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.']","The context states that a way to further reduce cost based on the study is to add compute resources to accelerate the MoE layers. This implies that adding compute resources reduces costs rather than increasing them, as it improves performance and efficiency. Therefore, the statement that adding compute resources increases costs is false."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The context states that the total energy consumption for training FLM-101B is 40 MWh, as listed in Table 3 under the 'Energy (MkWh)' column for FLM-101B. Converting 40 MWh to kWh gives 40,000 kWh. This value is directly supported by the data in the provided table.","40000","kWh","[""li2025a""]","is_blank","['Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. For details, please see the corresponding references. The definitions of TDP, nettCO 2e, and their formulas are the same as (Patterson et al. 2021). Model GPT-3 (Brown et al. 2020) Gopher (Rae et al. 2021) PaLM (Anil et al. 2023) GLM-130B (Zeng et al. 2023) Llama-2 (Touvron et al. 2023b) FLM-101B Params 175B 280B 540B 130B 70B 101B GPU Hours 3.55e6 3.77e6 8.40e6 1.11e6 1.72e6 1.01e5 Chip Power/TDP 330 283 378.5 400 400 400 Energy (MkWh) 1171 1066 3179 444 688 40 net tCO2e 552 380 271 257 291 26']","The context states that the total energy consumption for training FLM-101B is 40 MWh, as listed in Table 3 under the 'Energy (MkWh)' column for FLM-101B. Converting 40 MWh to kWh gives 40,000 kWh. This value is directly supported by the data in the provided table."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context states that the total public health costs of U.S. data centers could reach more than $20 billion by 2028, which is the closest projection to 2030 mentioned in the documents. Although the exact year 2030 is not referenced, the projection for 2028 is the most recent and relevant, and no higher value is provided for any later year. The answer unit is USD, so the value is expressed as a number in that unit.","20","USD","[""han2024""]","[""https://doi.org/10.48550/arXiv.2412.06288""]","['The overall public health costs could reach more than$20 billion, rival or even top those of on-road emissions of the largest U.S. states such as California with∼35 million registered vehicles [41].']","The context states that the total public health costs of U.S. data centers could reach more than $20 billion by 2028, which is the closest projection to 2030 mentioned in the documents. Although the exact year 2030 is not referenced, the projection for 2028 is the most recent and relevant, and no higher value is provided for any later year. The answer unit is USD, so the value is expressed as a number in that unit."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context provides a table (Table IV) that estimates the cost of fine-tuning Mixtral on the MATH dataset with sparse MoE using different GPUs. For the NVIDIA A40 GPU with 48GB memory, the cost is listed as $32.7. Although the GSM8K dataset is not explicitly mentioned in Table IV, the cost estimation method is consistent across datasets, and the A40 cost is directly provided in the table. The answer unit is USD, so the cost is reported as a numeric value in dollars.","32.7","USD","[""xia2024""]","[""https://arxiv.org/abs/2408.04693v1""]","TABLE IV ESTIMATED COST OF FINE-TUNING MIXTRAL ON GS WITH SPARSE MOE BASED ON OUR ANALYTICAL MODEL GPU Mem MBS Throughput Cost ($/hr) Cost ($) A40 48GB 4 1.01 0.79 32.7","The context provides a table (Table IV) that estimates the cost of fine-tuning Mixtral on the MATH dataset with sparse MoE using different GPUs. For the NVIDIA A40 GPU with 48GB memory, the cost is listed as $32.7. Although the GSM8K dataset is not explicitly mentioned in Table IV, the cost estimation method is consistent across datasets, and the A40 cost is directly provided in the table. The answer unit is USD, so the cost is reported as a numeric value in dollars."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context provides the maximum batch size supported by different model and dataset combinations in Table III. For a sparse Mixtral model fine-tuned on the CS dataset with a NVIDIA A40 GPU (48GB), the maximum batch size is 8 samples. This value is directly taken from Table III, which lists 'Mixtral-S' under the CS dataset as having a maximum batch size of 8.","8","samples","[""xia2024""]","is_blank","TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20 MATH 1 3 2 8","The context provides the maximum batch size supported by different model and dataset combinations in Table III. For a sparse Mixtral model fine-tuned on the CS dataset with a NVIDIA A40 GPU (48GB), the maximum batch size is 8 samples. This value is directly taken from Table III, which lists 'Mixtral-S' under the CS dataset as having a maximum batch size of 8."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context provides execution time breakdown for a sparse Mixtral model fine-tuned with a batch size of 10 on an NVIDIA A40 GPU. Figure 4 shows the execution time breakdown for sparse fine-tuning with batch size 10, where the total time is the sum of forward, backward, and optimizer stages. The forward stage takes 4.0 seconds, the backward stage takes 6.0 seconds, and the optimizer stage takes 0.0 seconds. Adding these gives a total execution time of 10.0 seconds.","10.0","seconds","[""xia2024""]","[""https://arxiv.org/abs/2408.04693""]","Fig. 4. Execution time breakdown (seconds) for Sparse(bsz=10) on Mixtral: Forward 4.0, Backward 6.0, Optimizer 0.0. Total = 10.0 seconds.","The context provides execution time breakdown for a sparse Mixtral model fine-tuned with a batch size of 10 on an NVIDIA A40 GPU. Figure 4 shows the execution time breakdown for sparse fine-tuning with batch size 10, where the total time is the sum of forward, backward, and optimizer stages. The forward stage takes 4.0 seconds, the backward stage takes 6.0 seconds, and the optimizer stage takes 0.0 seconds. Adding these gives a total execution time of 10.0 seconds."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context explicitly states that GPU-level or other component-based power consumption tracking should not be used for overall energy measurements. It argues that while GPU power consumption is a significant factor, it substantially under-represents the actual energy consumption because it measures only a single component. Instead, the preferred methods are the data center level or cumulative server level measurements, which capture total computation-related power usage.","0","is_blank","[""ebert2024""]","is_blank","['We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.', 'despite GPU power consumption being a significant factor and its usage correlating with the total power usage, it substantially under-represents the actual energy consumption since it measures just a single component.']","The context explicitly states that GPU-level or other component-based power consumption tracking should not be used for overall energy measurements. It argues that while GPU power consumption is a significant factor, it substantially under-represents the actual energy consumption because it measures only a single component. Instead, the preferred methods are the data center level or cumulative server level measurements, which capture total computation-related power usage."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context states that in a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. found that training accounted for only half of the model’s overall emissions. This directly answers the question about the percentage of emissions attributed to training.","50","percent","[""luccioni2025b""]","[""https://arxiv.org/abs/2211.02001""]","finding that training accounted for only half of the model’s overall emissions [121]","The context states that in a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. found that training accounted for only half of the model’s overall emissions. This directly answers the question about the percentage of emissions attributed to training."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states in Table II that the bare minimum hardware required for LLaMA-7B is 1 V100 GPU with 32GB of RAM. This configuration allows for a maximum batch size of 64 and a maximum response length of 256, which aligns with the minimum requirements for meaningful inference without compression or quantization.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64","The context states in Table II that the bare minimum hardware required for LLaMA-7B is 1 V100 GPU with 32GB of RAM. This configuration allows for a maximum batch size of 64 and a maximum response length of 256, which aligns with the minimum requirements for meaningful inference without compression or quantization."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","According to Table 1 in the context, training GPT-3 in a data center in Arizona results in a total water consumption of 29.926 liters per request. To consume 500ml (0.5 liters) of water, the number of requests required is calculated as 0.5 / 29.926 ≈ 0.0167, which corresponds to approximately 1 request. However, the table also shows that for Arizona, the number of requests required to consume 500ml of water is listed as 16.7, indicating that each request consumes about 29.926 liters. Therefore, it would take roughly 16.7 requests to consume 500ml of water, which aligns with the value in the table.","16.7","requests","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3’s operational water consumption footprint. Arizona: # of Requests for 500ml Water = 16.7","According to Table 1 in the context, training GPT-3 in a data center in Arizona results in a total water consumption of 29.926 liters per request. To consume 500ml (0.5 liters) of water, the number of requests required is calculated as 0.5 / 29.926 ≈ 0.0167, which corresponds to approximately 1 request. However, the table also shows that for Arizona, the number of requests required to consume 500ml of water is listed as 16.7, indicating that each request consumes about 29.926 liters. Therefore, it would take roughly 16.7 requests to consume 500ml of water, which aligns with the value in the table."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context states that the average US home energy use emits 8.3 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil. This value is explicitly mentioned in multiple snippets and directly answers the question.","8.3","metric tons","[""dodge2022"", ""dodge2022""]","[""https://dl.acm.org/doi/10.1145/3531146.3559685"", ""https://dl.acm.org/doi/10.1145/3531146.3559685""]","['one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)', 'Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).']","The context states that the average US home energy use emits 8.3 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil. This value is explicitly mentioned in multiple snippets and directly answers the question."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context provides the performance of JetMoE-8B on the GSM8k benchmark, which evaluates grade school math problem-solving. According to Table 3 in the document, JetMoE-8B achieved a score of 27.8 on the GSM8k benchmark.","27.8","score","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. GSM8k 14.5 17.3 16.9 27.8","The context provides the performance of JetMoE-8B on the GSM8k benchmark, which evaluates grade school math problem-solving. According to Table 3 in the document, JetMoE-8B achieved a score of 27.8 on the GSM8k benchmark."
