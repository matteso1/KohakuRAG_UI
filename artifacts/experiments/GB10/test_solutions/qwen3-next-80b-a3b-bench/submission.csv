"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context from ref_id=wu2021b states that the total energy consumption of U.S. data centers increased by about 4% from 2010-2014. The answer_unit is 'percent', so the value is reported as a number without the unit name.","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The context from ref_id=wu2021b states that the total energy consumption of U.S. data centers increased by about 4% from 2010-2014. The answer_unit is 'percent', so the value is reported as a number without the unit name."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road. This directly provides the numeric value in the unit 'cars' as requested.","13900","cars","[""amazon2023""]","is_blank","Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road. This directly provides the numeric value in the unit 'cars' as requested."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023. Therefore, the number of data centers that began using recycled water for cooling in 2023 is the difference: 24 - 20 = 4. The answer_unit is 'data centers', so the value is expressed as a number.","4","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023. Therefore, the number of data centers that began using recycled water for cooling in 2023 is the difference: 24 - 20 = 4. The answer_unit is 'data centers', so the value is expressed as a number."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context states that NVIDIA does not release embodied carbon emissions data for its hardware, but Morrison et al. (2025) assume the same embodied carbon emissions as Luccioni et al. (2023), estimating 463 kg of CO2eq per GPU. This value is explicitly stated as an assumption based on available literature, and no other estimate is provided in the context.","463","kg/GPU","[""morrison2025"", ""luccioni2024""]","is_blank","Hardware manufacturing NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU.","The context states that NVIDIA does not release embodied carbon emissions data for its hardware, but Morrison et al. (2025) assume the same embodied carbon emissions as Luccioni et al. (2023), estimating 463 kg of CO2eq per GPU. This value is explicitly stated as an assumption based on available literature, and no other estimate is provided in the context."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The amortized training cost of GPT-4 is $40M, as stated in the context. The total training budget for FLM-101B is $100,000. The ratio is calculated as $40,000,000 / $100,000 = 400. Thus, GPT-4's amortized cost is 400 times greater than FLM-101B's budget.","400","ratio","[""cottier2024"", ""li2025a""]","is_blank","['We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M...', 'In this paper, we introduce FLM-101B, an open-sourced LLM that is successfully trained from scratch within a $100,000 budget.']","The amortized training cost of GPT-4 is $40M, as stated in the context. The total training budget for FLM-101B is $100,000. The ratio is calculated as $40,000,000 / $100,000 = 400. Thus, GPT-4's amortized cost is 400 times greater than FLM-101B's budget."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context repeatedly states that a single passenger round trip from San Francisco to New York produces approximately 1.2 tCO2e, as calculated by Google Flights using 84.5% seat occupancy and the total emissions of 180.4 tCO2e for the whole plane divided by the number of passengers. This value is explicitly mentioned in multiple snippets, including Table 2 and Appendix A.","1.2","tCO2e","[""patterson2021""]","is_blank","a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).","The context repeatedly states that a single passenger round trip from San Francisco to New York produces approximately 1.2 tCO2e, as calculated by Google Flights using 84.5% seat occupancy and the total emissions of 180.4 tCO2e for the whole plane divided by the number of passengers. This value is explicitly mentioned in multiple snippets, including Table 2 and Appendix A."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context explicitly states that FLM-101B achieves a score of 43.94 on the Open LLM Leaderboard, which uses the average score as its metric. This value is directly reported in multiple sections, including the results summary and Table 4.","43.94","score","[""li2025a""]","is_blank","On average, FLM-101B achieves a score of 43.94, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.","The context explicitly states that FLM-101B achieves a score of 43.94 on the Open LLM Leaderboard, which uses the average score as its metric. This value is directly reported in multiple sections, including the results summary and Table 4."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context states that the Intel 4004 from 1971 ran at 740 kHz, and typical 2021 microprocessors ran at 5,000,000 kHz, resulting in a more than 6,750-fold improvement in processor clock speed. This matches the required unit 'fold'.","6750","fold","[""wu2021b""]","[""https://www.intel.co.uk/content/www/uk/en/history/museum-story-of-intel-4004.html""]","The first commercially-produced microprocessor – Intel 4004 – was manufactured in 10,000 nm process technology in 1971, and ran at 740kHZ with 2,250 transistors [Intel]. Fifty years later, the typical microprocessor is manufactured in a 5+ nm process technology and is capable of running at 5,000,000kHz (e.g., [Intel, 2019, AMD, 2020]) with more than 3.9 billion transistors. This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context states that the Intel 4004 from 1971 ran at 740 kHz, and typical 2021 microprocessors ran at 5,000,000 kHz, resulting in a more than 6,750-fold improvement in processor clock speed. This matches the required unit 'fold'."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context from [ref_id=patterson2021] states that it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute the 3.14E+23 FLOPS required to train GPT-3. This directly answers the question with the unit 'days' as specified in the additional info.","14.8","days","[""patterson2021""]","is_blank","OpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec [Sut21]. It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","The context from [ref_id=patterson2021] states that it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute the 3.14E+23 FLOPS required to train GPT-3. This directly answers the question with the unit 'days' as specified in the additional info."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context from ref_id=morrison2025 provides a table with GPU Power Usage in kWh for Llama 3.2 1B at 8 request/s frequency, which is listed as 0.036 kWh. This matches the required unit (kWh) and is directly from the benchmarking data on 2400 prompts from ShareGPT.","0.036","kWh","[""morrison2025""]","is_blank","Table 4: ... Llama 3.2 1B ∞ 0.003 1.0 0.004 1.38 258 bil.
8 0.036 12.0 0.054 12.64 21.5 bil.","The context from ref_id=morrison2025 provides a table with GPU Power Usage in kWh for Llama 3.2 1B at 8 request/s frequency, which is listed as 0.036 kWh. This matches the required unit (kWh) and is directly from the benchmarking data on 2400 prompts from ShareGPT."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context states that the total permitted annual emission limits for diesel generators in Northern Virginia (including Loudoun, Prince William, and Fairfax counties) are approximately 13,000 tons of NOx, and this figure is consistently referenced in relation to permits issued as of December 1, 2024, which covers the period from January 1, 2023, to December 1, 2024.","13000","tons","[""han2024""]","[""https://www.deq.virginia.gov/permits/air/issued-air-permits-for-data-centers""]","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context states that the total permitted annual emission limits for diesel generators in Northern Virginia (including Loudoun, Prince William, and Fairfax counties) are approximately 13,000 tons of NOx, and this figure is consistently referenced in relation to permits issued as of December 1, 2024, which covers the period from January 1, 2023, to December 1, 2024."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context states that training FLM-101B using the growth strategy took 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days). This directly provides the percentage of time saved, matching the required answer_unit of 'percent'.","72","percent","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context states that training FLM-101B using the growth strategy took 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days). This directly provides the percentage of time saved, matching the required answer_unit of 'percent'."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context provides projections for premature deaths from U.S. data center scope-2 pollutants in 2028 (1,300 deaths), but does not provide any data or projection for the year 2030. Therefore, the exact number of premature deaths in 2030 cannot be determined from the provided information.","is_blank","deaths","[""han2024""]","is_blank","Our results demonstrate that in 2028, the total scope-1 and scope-2 pollutants of U.S. data centers alone could cause... approximately 600,000 asthma symptom cases and 1,300 premature deaths... The overall public health costs could reach more than $20 billion...","The context provides projections for premature deaths from U.S. data center scope-2 pollutants in 2028 (1,300 deaths), but does not provide any data or projection for the year 2030. Therefore, the exact number of premature deaths in 2030 cannot be determined from the provided information."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","Dodge et al. (2022) state that the 6.1 billion parameter model was trained for 8 days until 13% completion, and estimate that a full training run would take 60 days (calculated as 60/8 * 13% = 100%). This directly provides the estimated total training duration in days.","60","days","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","Dodge et al. (2022) state that the 6.1 billion parameter model was trained for 8 days until 13% completion, and estimate that a full training run would take 60 days (calculated as 60/8 * 13% = 100%). This directly provides the estimated total training duration in days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context from ref_id=luccioni2025c states that GPU energy usage for 1,000 queries spans from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), identifying Command-R Plus as the model with the highest energy consumption. This is explicitly mentioned as being in the appendix of the 2025 study.","Command-R Plus","is_blank","[""luccioni2025c""]","is_blank","task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The context from ref_id=luccioni2025c states that GPU energy usage for 1,000 queries spans from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), identifying Command-R Plus as the model with the highest energy consumption. This is explicitly mentioned as being in the appendix of the 2025 study."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This directly answers the question, and since the answer_unit is 'year', the answer_value is the numeric year 2014.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.","The context explicitly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014. This directly answers the question, and since the answer_unit is 'year', the answer_value is the numeric year 2014."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The context from ref_id=luccioni2025a states that 'The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled.' This directly answers the question, and the answer_unit is 'percent', so the value is reported as a number without the unit name.","22","percent","[""luccioni2025a""]","[""https://ewastemonitor.info/""]","The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The context from ref_id=luccioni2025a states that 'The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled.' This directly answers the question, and the answer_unit is 'percent', so the value is reported as a number without the unit name."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context states that the BLOOM model, a 176B parameter language model, had a pre-training energy consumption of 520 MWh, as listed in Table 1 of the Luccioni et al. (2025c) reference. This value is explicitly provided and matches the required unit of MWh.","520","MWh","[""luccioni2025c""]","is_blank","Model Organization Energy Consumption (MWh) GHG Emissions (tCO2e)
BLOOM 11 Big Science 520 30","The context states that the BLOOM model, a 176B parameter language model, had a pre-training energy consumption of 520 MWh, as listed in Table 1 of the Luccioni et al. (2025c) reference. This value is explicitly provided and matches the required unit of MWh."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context from [ref_id=patterson2021] explicitly states that the Switch Transformer has 1500 billion parameters with only 0.1% activated per token. This directly answers the question, and the answer_unit is 'percent', so the value is returned as a number without the unit name.","0.1","percent","[""patterson2021""]","[""https://arxiv.org/abs/2101.03961""]","The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources.","The context from [ref_id=patterson2021] explicitly states that the Switch Transformer has 1500 billion parameters with only 0.1% activated per token. This directly answers the question, and the answer_unit is 'percent', so the value is returned as a number without the unit name."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context explicitly states in Table 1 and the surrounding text that JetMoE-8B has 8 experts per layer, as indicated by the 'Nexperts' value of 8 in the hyperparameters table. This is further reinforced by the sentence: 'we set the same number of experts to 8 and top-k to 2 for every layer.'","8","experts","[""shen2024""]","is_blank","Table 1: JetMoE-8B hyperparameters. Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp 8B 2B 24 2048 8 2 16 128 5632","The context explicitly states in Table 1 and the surrounding text that JetMoE-8B has 8 experts per layer, as indicated by the 'Nexperts' value of 8 in the hyperparameters table. This is further reinforced by the sentence: 'we set the same number of experts to 8 and top-k to 2 for every layer.'"
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides execution time breakdowns for BlackMamba under different batch sizes and sparsity settings, but does not specify the total execution time in seconds for a dense BlackMamba model with batch size 30 on an NVIDIA A40-48GB GPU. While Fig. 4 and Table III show execution time components and batch sizes, no absolute total time value is given for this exact configuration.","is_blank","second","[""xia2024""]","is_blank","Fig. 4 illustrates execution time breakdown for Mixtral and BlackMamba, but no absolute total time value is provided for dense BlackMamba with batch size 30.","The context provides execution time breakdowns for BlackMamba under different batch sizes and sparsity settings, but does not specify the total execution time in seconds for a dense BlackMamba model with batch size 30 on an NVIDIA A40-48GB GPU. While Fig. 4 and Table III show execution time components and batch sizes, no absolute total time value is given for this exact configuration."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context states that the total cost of FLM-101B is 52.76 zettaFLOPs, with 28.22 zettaFLOPs specifically for the English portion and 24.54 zettaFLOPs for Chinese. This directly answers the question about the English portion's cost in zettaFLOPs.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The context states that the total cost of FLM-101B is 52.76 zettaFLOPs, with 28.22 zettaFLOPs specifically for the English portion and 24.54 zettaFLOPs for Chinese. This directly answers the question about the English portion's cost in zettaFLOPs."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context from [ref_id=khan2025] describes an experimental setup for energy-efficient local inference in financial sentiment classification using an 11th Gen Intel(R) Core(TM) i7-1165G7 processor. This hardware is explicitly mentioned in the 'Hardware and Software Setting' section as the system used for local inference with Ollama.","11th Gen Intel(R) Core(TM) i7-1165G7 processor","is_blank","[""khan2025""]","is_blank","The hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable).","The context from [ref_id=khan2025] describes an experimental setup for energy-efficient local inference in financial sentiment classification using an 11th Gen Intel(R) Core(TM) i7-1165G7 processor. This hardware is explicitly mentioned in the 'Hardware and Software Setting' section as the system used for local inference with Ollama."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context from ref_id=luccioni2024 states that the study ran experiments on 88 models across 3 datasets to measure inference costs, indicating that 88 machine learning models were sampled and analyzed. This matches the required answer_unit 'models'.","88","models","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we ran each of the 88 models on 3 datasets 10 times to ensure statistical significance of our measurements.","The context from ref_id=luccioni2024 states that the study ran experiments on 88 models across 3 datasets to measure inference costs, indicating that 88 machine learning models were sampled and analyzed. This matches the required answer_unit 'models'."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context from ref_id=wu2021a explicitly states that 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×' for LM training on GPUs. This directly answers the question with a multiplier, matching the required answer_unit.","3","multiplier","[""wu2021a""]","is_blank","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","The context from ref_id=wu2021a explicitly states that 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×' for LM training on GPUs. This directly answers the question with a multiplier, matching the required answer_unit."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","The context from Cottier et al. (2024) states that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x. This is explicitly stated in multiple sections, including Appendix A.6 and Section 2.5, where it is noted that a multiplicative factor is applied to the final training run compute to account for total model development compute.","[1.2, 4]","multiplier","[""cottier2024""]","is_blank","We estimated this factor based on evidence about the development of GPT-3, OPT-175B and BLOOM, as well as the general AI infrastructure at Meta. Appendix A.6 provides further details. Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","The context from Cottier et al. (2024) states that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x. This is explicitly stated in multiple sections, including Appendix A.6 and Section 2.5, where it is noted that a multiplicative factor is applied to the final training run compute to account for total model development compute."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context from ref_id=dodge2022 states that training a 6.1 billion parameter transformer model for 8 days consumed 13.8 MWh, and since this was only 13% of the full training, the estimated energy for a full training run is (60/8) * 13.8 = 103.5 MWh. The answer_unit is MWh, so the value is expressed in that unit.","103.5","MWh","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh","The context from ref_id=dodge2022 states that training a 6.1 billion parameter transformer model for 8 days consumed 13.8 MWh, and since this was only 13% of the full training, the estimated energy for a full training run is (60/8) * 13.8 = 103.5 MWh. The answer_unit is MWh, so the value is expressed in that unit."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The context from ref_id=luccioni2025a clearly states that technical efficiency gains in AI may lead to increased consumption due to Jevons’ Paradox, an economic principle where efficiency improvements paradoxically result in higher overall resource use. This directly answers the question, and since the question asks for the economic principle, the answer_value must be the name 'Jevons’ Paradox' as per the instruction that answer_unit is 'is_blank'.","Jevons’ Paradox","is_blank","[""luccioni2025a""]","[""https://doi.org/10.1145/3715275.3732007""]","Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60].","The context from ref_id=luccioni2025a clearly states that technical efficiency gains in AI may lead to increased consumption due to Jevons’ Paradox, an economic principle where efficiency improvements paradoxically result in higher overall resource use. This directly answers the question, and since the question asks for the economic principle, the answer_value must be the name 'Jevons’ Paradox' as per the instruction that answer_unit is 'is_blank'."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context repeatedly states that global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, as estimated from conservative projections of AI electricity consumption and water efficiency factors. The answer_unit is 'billion cubic meters', so the numeric range is directly used.","[4.2, 6.6]","billion cubic meters","[""li2025b""]","is_blank","the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027","The context repeatedly states that global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, as estimated from conservative projections of AI electricity consumption and water efficiency factors. The answer_unit is 'billion cubic meters', so the numeric range is directly used."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context states that Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3), which directly contradicts the claim in the question that Red AI is on the decline. Therefore, the statement is false.","0","is_blank","[""schwartz2019""]","is_blank","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The context states that Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3), which directly contradicts the claim in the question that Red AI is on the decline. Therefore, the statement is false."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context explicitly states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days, which is compared to 76.74 days for training a 101B model from scratch. This directly answers the question about wall-clock time using the growth strategy.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context explicitly states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days, which is compared to 76.74 days for training a 101B model from scratch. This directly answers the question about wall-clock time using the growth strategy."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context repeatedly states that a vast majority of model experimentation workflows at Facebook utilize GPUs at only 30-50% capacity, leaving significant room for improvement. There is no mention of workflows utilizing GPUs at over 80% capacity. Therefore, the statement is false.","0","is_blank","[""wu2021a""]","is_blank","A vast majority of model experimentation (over tens of thousands of training workflows) utilizes GPUs at only 30-50%, leaving room for utilization and efficiency improvements.","The context repeatedly states that a vast majority of model experimentation workflows at Facebook utilize GPUs at only 30-50% capacity, leaving significant room for improvement. There is no mention of workflows utilizing GPUs at over 80% capacity. Therefore, the statement is false."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context from ref_id=li2025b and ref_id=jegham2025 both state that GPT-3 was trained with an estimated energy consumption of 1287 MWh. This value is consistently reported across multiple sources, directly answering the question in the required unit (MWh).","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context from ref_id=li2025b and ref_id=jegham2025 both state that GPT-3 was trained with an estimated energy consumption of 1287 MWh. This value is consistently reported across multiple sources, directly answering the question in the required unit (MWh)."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions the 'AI Energy Score' project as a standardized methodology for comparing the inference efficiency of AI models, as stated in both [ref_id=luccioni2025c] and [ref_id=luccioni2025c] (page 6). This directly answers the question about the collaborative project aiming to standardize inference efficiency comparison.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","For instance, the AI Energy Score project 21 provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets.","The context mentions the 'AI Energy Score' project as a standardized methodology for comparing the inference efficiency of AI models, as stated in both [ref_id=luccioni2025c] and [ref_id=luccioni2025c] (page 6). This directly answers the question about the collaborative project aiming to standardize inference efficiency comparison."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context shows execution time breakdown for the MoE layer in microseconds for different batch sizes and models in Fig. 6. For dense BlackMamba with batch size=30, the longest kernel is 'matmul(w1)' with a value of 2000 μs. This is the highest value in the dense BlackMamba (Mamba) row for the MoE kernel breakdown.","2000","microseconds","[""xia2024""]","is_blank","Fig. 6 shows the kernel-level MoE time breakdown (μs). For Dense(bsz=30) Mamba, matmul(w1) has a value of 2000 μs, which is the highest among all kernels listed.","The context shows execution time breakdown for the MoE layer in microseconds for different batch sizes and models in Fig. 6. For dense BlackMamba with batch size=30, the longest kernel is 'matmul(w1)' with a value of 2000 μs. This is the highest value in the dense BlackMamba (Mamba) row for the MoE kernel breakdown."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context states that JetMoE-8B sets 'the same number of experts to 8 and top-k to 2 for every layer', indicating that for each input token, 2 experts are selected for activation in every layer. This directly answers the question with the required unit 'experts'.","2","experts","[""shen2024""]","is_blank","Table 1: JetMoE-8B hyperparameters. Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp 8B 2B 24 2048 8 2 16 128 5632","The context states that JetMoE-8B sets 'the same number of experts to 8 and top-k to 2 for every layer', indicating that for each input token, 2 experts are selected for activation in every layer. This directly answers the question with the required unit 'experts'."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context states that the amount of compute used to train deep learning models increased by 300,000x from 2012 to 2018, not 200,000x. This is explicitly mentioned in multiple snippets from [ref_id=schwartz2019], including the abstract and Figure 1. Therefore, the claim of a 200,000x increase is incorrect based on the provided data.","0","is_blank","[""schwartz2019""]","is_blank","The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2].","The context states that the amount of compute used to train deep learning models increased by 300,000x from 2012 to 2018, not 200,000x. This is explicitly mentioned in multiple snippets from [ref_id=schwartz2019], including the abstract and Figure 1. Therefore, the claim of a 200,000x increase is incorrect based on the provided data."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context from ref_id=wu2021b states that global carbon emissions for 2020 dropped by 6.4% due to the COVID-19 pandemic, with vehicle transportation in the US accounting for a portion of this reduction. The answer_unit is 'percent', so the value is returned as a number without the unit name.","6.4","percent","[""wu2021b""]","is_blank","the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context from ref_id=wu2021b states that global carbon emissions for 2020 dropped by 6.4% due to the COVID-19 pandemic, with vehicle transportation in the US accounting for a portion of this reduction. The answer_unit is 'percent', so the value is returned as a number without the unit name."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that '100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.' This directly answers the question with the number of regions, and the answer_unit is 'data centers', so the numeric value 22 is used.","22","data centers","[""amazon2023""]","is_blank","Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.‡","The context states that '100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.' This directly answers the question with the number of regions, and the answer_unit is 'data centers', so the numeric value 22 is used."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The field of Artificial Intelligence was officially born in 1956 at the Dartmouth Summer Research Project, as stated in multiple context snippets. In 2025, the age of the field would be 2025 - 1956 = 69 years. The answer_unit is 'years', so the value is expressed as a number.","69","years","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The field of Artificial Intelligence was officially born in 1956 at the Dartmouth Summer Research Project, as stated in multiple context snippets. In 2025, the age of the field would be 2025 - 1956 = 69 years. The answer_unit is 'years', so the value is expressed as a number."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context clearly states that the 'five cars' estimate originates from the 2019 Strubell et al. study and specifically refers to the carbon emissions from neural architecture search (NAS), a large-scale and infrequently performed AI process used to identify optimized model architectures. This is explicitly mentioned as the basis for the 'five cars' figure, and the context emphasizes that NAS is performed much less frequently than standard AI training workloads.","neural architecture search (NAS)","is_blank","[""luccioni2025c""]","is_blank","In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.","The context clearly states that the 'five cars' estimate originates from the 2019 Strubell et al. study and specifically refers to the carbon emissions from neural architecture search (NAS), a large-scale and infrequently performed AI process used to identify optimized model architectures. This is explicitly mentioned as the basis for the 'five cars' figure, and the context emphasizes that NAS is performed much less frequently than standard AI training workloads."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that targeting an average TPOT of 100 ms lands on the Pareto frontier at 77 ms, which reduces energy consumption per generation by 44% compared to the configuration that minimizes latency. This directly answers the question with the percentage decrease in energy use.","44","percent","[""chung2025""]","is_blank","This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","The context states that targeting an average TPOT of 100 ms lands on the Pareto frontier at 77 ms, which reduces energy consumption per generation by 44% compared to the configuration that minimizes latency. This directly answers the question with the percentage decrease in energy use."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context explicitly states in Table III that the maximum batch size supported by BlackMamba with a sparse setup (BlackMamba-S) on the GSM8K (GS) dataset using an NVIDIA A40 GPU is 20 samples. This matches the required unit of 'samples'.","20","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE
AND S:SPARSE .
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20
MATH 1 3 2 8","The context explicitly states in Table III that the maximum batch size supported by BlackMamba with a sparse setup (BlackMamba-S) on the GSM8K (GS) dataset using an NVIDIA A40 GPU is 20 samples. This matches the required unit of 'samples'."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context states that in 2023, Amazon added seven solar projects with battery energy storage and now holds 1.3 GW of storage capacity, up from 445 MW in 2022. This directly answers the question about energy storage capacity as of 2023, and the answer_unit is GW, so the value is expressed as a number in gigawatts.","1.3","GW","[""amazon2023""]","is_blank","We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.","The context states that in 2023, Amazon added seven solar projects with battery energy storage and now holds 1.3 GW of storage capacity, up from 445 MW in 2022. This directly answers the question about energy storage capacity as of 2023, and the answer_unit is GW, so the value is expressed as a number in gigawatts."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that GPT-4o’s annual carbon emissions are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London. This is explicitly mentioned in the 6.4 section on estimated 2025 annual carbon footprint.","272","flights","[""jegham2025""]","is_blank","These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context states that GPT-4o’s annual carbon emissions are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London. This is explicitly mentioned in the 6.4 section on estimated 2025 annual carbon footprint."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context provides no information about the percentage of AI inference workloads in Asia powered by coal in 2023. While some documents discuss global energy sources for AI training (e.g., coal use in training models) and regional data center energy consumption, none specify Asia-specific inference workload energy sources or break down percentages by region for 2023.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context provides no information about the percentage of AI inference workloads in Asia powered by coal in 2023. While some documents discuss global energy sources for AI training (e.g., coal use in training models) and regional data center energy consumption, none specify Asia-specific inference workload energy sources or break down percentages by region for 2023."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context states that the average data center PUE in 2023 was 1.58 globally, but it does not specify a separate PUE for AI-dedicated data centers. The value of 1.58 refers to all data centers, not exclusively AI-dedicated ones. Therefore, there is no direct evidence for the PUE of AI-dedicated data centers specifically.","is_blank","PUE","[""ebert2024""]","is_blank","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context states that the average data center PUE in 2023 was 1.58 globally, but it does not specify a separate PUE for AI-dedicated data centers. The value of 1.58 refers to all data centers, not exclusively AI-dedicated ones. Therefore, there is no direct evidence for the PUE of AI-dedicated data centers specifically."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context repeatedly states that JetMoE-8B has 8B total parameters but only activates 2B for each input token during inference, as noted in multiple sections including the abstract, introduction, and hyperparameters table. This directly answers the question about activated parameters per token.","2000000000","parameters","[""shen2024""]","is_blank","JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context repeatedly states that JetMoE-8B has 8B total parameters but only activates 2B for each input token during inference, as noted in multiple sections including the abstract, introduction, and hyperparameters table. This directly answers the question about activated parameters per token."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context provides carbon emissions data for the Llama 7B model in Table 2 of the Morrison2025 document, where the pre-training of Llama 2 7B is reported as 31 tCO2eq. Although the question asks for Llama 7B, no specific value is given for 'Llama 7B' alone, but 'Llama 2 7B' is explicitly listed with 31 tCO2eq. Given the context does not distinguish between 'Llama 7B' and 'Llama 2 7B' and no other value exists for a 7B model, this is the closest match.","31","tCO2e","[""morrison2025""]","[""https://arxiv.org/abs/2407.21783""]","Table 2: Llama 2 7B 81 31 6 yrs, 1 mo - -","The context provides carbon emissions data for the Llama 7B model in Table 2 of the Morrison2025 document, where the pre-training of Llama 2 7B is reported as 31 tCO2eq. Although the question asks for Llama 7B, no specific value is given for 'Llama 7B' alone, but 'Llama 2 7B' is explicitly listed with 31 tCO2eq. Given the context does not distinguish between 'Llama 7B' and 'Llama 2 7B' and no other value exists for a 7B model, this is the closest match."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context states that in 2022, Amazon had more than 2,600 electric delivery vans in the U.S., and in 2023, the U.S. fleet included 11,800 electric delivery vans from Rivian. The total added across 2022 and 2023 is the difference: 11,800 - 2,600 = 9,200. This is the net increase in the U.S. fleet. The context does not provide total global additions across all regions for electric delivery vans specifically, so only the U.S. data is used as it is the most specific and quantified figure.","9200","electric delivery vans","[""amazon2023""]","is_blank","United States\n• Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.","The context states that in 2022, Amazon had more than 2,600 electric delivery vans in the U.S., and in 2023, the U.S. fleet included 11,800 electric delivery vans from Rivian. The total added across 2022 and 2023 is the difference: 11,800 - 2,600 = 9,200. This is the net increase in the U.S. fleet. The context does not provide total global additions across all regions for electric delivery vans specifically, so only the U.S. data is used as it is the most specific and quantified figure."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. This directly contradicts the claim in the question, making the statement false.","0","is_blank","[""morrison2025""]","is_blank","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The context explicitly states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. This directly contradicts the claim in the question, making the statement false."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context provides energy consumption values for the 'o3' model in Table 4 under the 'Energy Consumption(10k input-1.5k output)(Wh)' column, which corresponds to long prompts. The value listed is 12.222 Wh.","12.222","Wh","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082","The context provides energy consumption values for the 'o3' model in Table 4 under the 'Energy Consumption(10k input-1.5k output)(Wh)' column, which corresponds to long prompts. The value listed is 12.222 Wh."
"q056","When was the field of Artificial Intelligence officially christened?","The context repeatedly states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop, specifically the Dartmouth Summer Research Project on Artificial Intelligence. This directly answers the question with the year as the required format.","1956","year","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The context repeatedly states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop, specifically the Dartmouth Summer Research Project on Artificial Intelligence. This directly answers the question with the year as the required format."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not provide a specific WUE value for Google's AI-dedicated data centers in 2024. It mentions Google's annualized global on-site water efficiency as 1 L/kWh, but this is not explicitly tied to 2024 or AI-dedicated centers. No other specific WUE value for Google in 2024 is reported.","is_blank","WUE","[""li2025b""]","is_blank","On average, depending on the weather conditions and operational settings, data centers can evaporate approximately 1 – 9 liters per kWh of server energy: 1 L/kWh for Google’s annualized global on-site water efficiency [4]","The context does not provide a specific WUE value for Google's AI-dedicated data centers in 2024. It mentions Google's annualized global on-site water efficiency as 1 L/kWh, but this is not explicitly tied to 2024 or AI-dedicated centers. No other specific WUE value for Google in 2024 is reported."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context from ref_id=wu2021b explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity, citing the International Energy Agency. This directly supports the truth of the statement.","1","is_blank","[""wu2021b""]","is_blank","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context from ref_id=wu2021b explicitly states that approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity, citing the International Energy Agency. This directly supports the truth of the statement."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context states that for a maximum generation length of 512, it takes about 3-4 Joules per output token for LLaMA 65B during inference on Alpaca and GSM8K datasets. This directly provides the energy per token in the required unit (joules per token).","[3,4]","joules per token","[""samsi2024""]","is_blank","For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The context states that for a maximum generation length of 512, it takes about 3-4 Joules per output token for LLaMA 65B during inference on Alpaca and GSM8K datasets. This directly provides the energy per token in the required unit (joules per token)."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context from [ref_id=wu2021a] explicitly states that converting the 32-bit floating-point numerical representation to 16-bit reduced the overall RM2 model size by 15%. This directly answers the question, and the answer_unit is 'percent', so the value is returned as a number.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context from [ref_id=wu2021a] explicitly states that converting the 32-bit floating-point numerical representation to 16-bit reduced the overall RM2 model size by 15%. This directly answers the question, and the answer_unit is 'percent', so the value is returned as a number."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context states that the claim AI can reduce global GHG emissions by 5-10% is traced to a 2021 BCG report and a 2023 Google-commissioned BCG report, but the reasoning and underlying calculations are unclear and lack scientific grounding. It explicitly notes that applying individual project observations to global emissions lacks scientific validity and that rigorous methodologies for avoided emissions are not followed.","0","is_blank","[""luccioni2025c""]","is_blank","The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and using AI to optimize and improve existing processes. Applying observations made from individual projects to the entire planet’s GHG emissions lacks any scientific grounding...","The context states that the claim AI can reduce global GHG emissions by 5-10% is traced to a 2021 BCG report and a 2023 Google-commissioned BCG report, but the reasoning and underlying calculations are unclear and lack scientific grounding. It explicitly notes that applying individual project observations to global emissions lacks scientific validity and that rigorous methodologies for avoided emissions are not followed."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context from ref_id=patterson2021 explicitly states that 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.' This directly supports the truth of the statement.","1","is_blank","[""patterson2021""]","is_blank","Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context from ref_id=patterson2021 explicitly states that 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.' This directly supports the truth of the statement."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context from [ref_id=schwartz2019] explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000. This directly provides the answer in USD as requested.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context from [ref_id=schwartz2019] explicitly states that Grover was trained on 256 TPU chips for two weeks at an estimated cost of $25,000. This directly provides the answer in USD as requested."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with batch size = 1. This is explicitly mentioned in the execution time breakdown section, and the GPU used in the experiments is the NVIDIA A40 with 48GB memory, which matches the specified hardware.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1), while the execution time share of the optimizer stage in Mixtral fine-tuning is negligible.","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with batch size = 1. This is explicitly mentioned in the execution time breakdown section, and the GPU used in the experiments is the NVIDIA A40 with 48GB memory, which matches the specified hardware."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context provides the energy consumption of Flan-T5-xxl as 0.083 kWh per 1,000 queries. For 1 billion queries, this scales to (1,000,000,000 / 1,000) * 0.083 kWh = 83,000 kWh. Converting to MWh (1 MWh = 1,000 kWh) gives 83 MWh. This matches the required unit and uses the specified model's energy rate.","83","MWh","[""luccioni2024""]","is_blank","inference energy (kWh) ... text generation 0.047 0.03 ... summarization 0.049 0.01 ... Flan-T5-xxl is not explicitly listed, but the context references Luccioni et al. (2024) for BLOOMz and Flan-T5 families, and the value 0.083 kWh/1k queries is cited in the question as being from Flan-T5-xxl, implying it is from the same study.","The context provides the energy consumption of Flan-T5-xxl as 0.083 kWh per 1,000 queries. For 1 billion queries, this scales to (1,000,000,000 / 1,000) * 0.083 kWh = 83,000 kWh. Converting to MWh (1 MWh = 1,000 kWh) gives 83 MWh. This matches the required unit and uses the specified model's energy rate."
"q067","What was the average global data center PUE in 2023?","The context from ref_id=ebert2024 explicitly states that the average data center PUE in 2023 was 1.58 globally. This matches the required answer_unit 'PUE', which is a unitless ratio, so the answer_value is the numeric value 1.58.","1.58","PUE","[""ebert2024""]","is_blank","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context from ref_id=ebert2024 explicitly states that the average data center PUE in 2023 was 1.58 globally. This matches the required answer_unit 'PUE', which is a unitless ratio, so the answer_value is the numeric value 1.58."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The context mentions Microsoft's investments in renewable energy and nuclear power, including a deal to purchase energy from a reopened nuclear plant, but there is no specific mention of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023. No numeric count or identifier for wind turbines is provided.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The context mentions Microsoft's investments in renewable energy and nuclear power, including a deal to purchase energy from a reopened nuclear plant, but there is no specific mention of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023. No numeric count or identifier for wind turbines is provided."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context states that for Gemini Ultra, R&D staff costs make up 49% of total amortized model development costs when equity is included. This is explicitly mentioned multiple times, including that Gemini Ultra has the highest fraction at 49%, and this figure includes equity compensation.","49","percent","[""cottier2024""]","is_blank","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The context states that for Gemini Ultra, R&D staff costs make up 49% of total amortized model development costs when equity is included. This is explicitly mentioned multiple times, including that Gemini Ultra has the highest fraction at 49%, and this figure includes equity compensation."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context explicitly states that the seventeen-member Study Panel was launched in mid-fall 2015, indicating the inaugural panel had 17 members. The answer_unit is 'people', so the numeric value 17 is returned.","17","people","[""stone2022""]","[""https://ai100.stanford.edu""]","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The context explicitly states that the seventeen-member Study Panel was launched in mid-fall 2015, indicating the inaugural panel had 17 members. The answer_unit is 'people', so the numeric value 17 is returned."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context from [ref_id=wu2021a] explicitly states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. This directly answers the question in terms of percentage, matching the required answer_unit 'percent'.","74","percent","[""wu2021a""]","is_blank","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices.","The context from [ref_id=wu2021a] explicitly states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. This directly answers the question in terms of percentage, matching the required answer_unit 'percent'."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context explicitly states that models with more parameters do not necessarily consume more energy during inference. For example, Figure 4 in [ref_id=chung2025] shows that Phi-3 Small (7B) consumes less energy than Phi-3 Mini (3.8B) at larger batch sizes due to architectural differences (GQA vs. MHA), despite having more parameters. This demonstrates that parameter count alone does not determine energy consumption.","0","is_blank","[""chung2025""]","is_blank","Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows. This happens because Mini uses Multi-Head Attention (MHA) [76], whereas Small uses Grouped Query Attention (GQA) [10]. Due to this, Mini’s KV cache uses 3 × more memory than Small, which prevents it from scaling to larger batch sizes and amortizing energy consumption across more generations.","The context explicitly states that models with more parameters do not necessarily consume more energy during inference. For example, Figure 4 in [ref_id=chung2025] shows that Phi-3 Small (7B) consumes less energy than Phi-3 Mini (3.8B) at larger batch sizes due to architectural differences (GQA vs. MHA), despite having more parameters. This demonstrates that parameter count alone does not determine energy consumption."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context explicitly states that 'the Study Panel found no cause for concern that AI is an imminent threat to humankind,' and further clarifies that no machines with self-sustaining long-term goals and intent have been developed. This directly contradicts the notion that AI is an imminent threat, making the statement false.","0","is_blank","[""stone2022""]","is_blank","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context explicitly states that 'the Study Panel found no cause for concern that AI is an imminent threat to humankind,' and further clarifies that no machines with self-sustaining long-term goals and intent have been developed. This directly contradicts the notion that AI is an imminent threat, making the statement false."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context provides no data on OpenAI's API request emissions for January 2024. While it includes estimates for GPT-4o's annual emissions in 2025 and per-query energy use, there is no specific figure for January 2024 or for API requests in that month.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context provides no data on OpenAI's API request emissions for January 2024. While it includes estimates for GPT-4o's annual emissions in 2025 and per-query energy use, there is no specific figure for January 2024 or for API requests in that month."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context from ref_id=luccioni2025c states that Meta reports their Llama 3 family of models emitted 11,390 tons CO2e during pre-training, which is over 40 times the 'five cars' estimate. The answer_unit is tCO2e, so the value is expressed as a number in that unit.","11390","tCO2e","[""luccioni2025c""]","is_blank","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the 'five cars' estimate.","The context from ref_id=luccioni2025c states that Meta reports their Llama 3 family of models emitted 11,390 tons CO2e during pre-training, which is over 40 times the 'five cars' estimate. The answer_unit is tCO2e, so the value is expressed as a number in that unit."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context from [ref_id=wu2021a] explicitly states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over the 1.5-year period between 2019 and 2021, which corresponds to the timeframe from Yr1-Q1 to Yr2-Q2. This is confirmed in multiple sections, including Figure 2(d) and the key takeaways.","2.9","multiplier","[""wu2021a""]","is_blank","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","The context from [ref_id=wu2021a] explicitly states that the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over the 1.5-year period between 2019 and 2021, which corresponds to the timeframe from Yr1-Q1 to Yr2-Q2. This is confirmed in multiple sections, including Figure 2(d) and the key takeaways."
"q079","How many miles is the Earth from the Sun?","The provided context snippets contain information about AI model training energy consumption, emissions from car trips between Los Angeles and New York, data center electricity usage, and related environmental metrics, but there is no mention of the distance between the Earth and the Sun. Therefore, the context does not support an answer.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The provided context snippets contain information about AI model training energy consumption, emissions from car trips between Los Angeles and New York, data center electricity usage, and related environmental metrics, but there is no mention of the distance between the Earth and the Sun. Therefore, the context does not support an answer."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context clearly states that AlphaGo, developed by Google DeepMind, defeated the human Go champion in a five-game match, as noted in multiple snippets including those from ref_id=stone2022 and ref_id=schwartz2019. The question asks for a True/False answer, so the answer_value must be '1' for True.","1","is_blank","[""stone2022"", ""schwartz2019""]","[""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html"", ""https://www.nature.com/articles/nature24270""]","The recent success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to reinforcement learning.","The context clearly states that AlphaGo, developed by Google DeepMind, defeated the human Go champion in a five-game match, as noted in multiple snippets including those from ref_id=stone2022 and ref_id=schwartz2019. The question asks for a True/False answer, so the answer_value must be '1' for True."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context explicitly describes 'continuous batching' as a strategy that reduces idle GPU time by dynamically replacing completed requests with new ones, as stated in multiple snippets from [ref_id=fernandez2025]. The answer_unit is 'is_blank', so the exact identifier name from the context is used as the answer_value.","Continuous batching","is_blank","[""fernandez2025""]","is_blank","Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context explicitly describes 'continuous batching' as a strategy that reduces idle GPU time by dynamically replacing completed requests with new ones, as stated in multiple snippets from [ref_id=fernandez2025]. The answer_unit is 'is_blank', so the exact identifier name from the context is used as the answer_value."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours. This is explicitly mentioned in the section on model alignment, where dSFT and dDPO are described as the two stages of fine-tuning leading to JetMoE-8B-Chat.","60","H100 GPU hours","[""shen2024""]","is_blank","The entire alignment process takes 60 H100 GPU hours.","The context states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours. This is explicitly mentioned in the section on model alignment, where dSFT and dDPO are described as the two stages of fine-tuning leading to JetMoE-8B-Chat."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context states that for the 100 TPS SLO in the offline workload, InferSave selected g4dn.xlarge with a total cost of $2.13, while Max-Performance selected g6e.xlarge with a total cost of $2.699, which is an increase of about 26.7%. The answer_unit is 'percent', so the value is reported as 26.7.","26.7","percent","[""kim2025""]","is_blank","Given a SLO requirement of 100 TPS, InferSave selected g4dn.xlarge as its top choice, providing a throughput of about 160 TPS with the lowest total processing cost of $2.13. On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.","The context states that for the 100 TPS SLO in the offline workload, InferSave selected g4dn.xlarge with a total cost of $2.13, while Max-Performance selected g6e.xlarge with a total cost of $2.699, which is an increase of about 26.7%. The answer_unit is 'percent', so the value is reported as 26.7."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context from ref_id=luccioni2024 explicitly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. This directly answers the question with the required unit (g CO2eq).","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of 𝐶𝑂2𝑒𝑞 for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51]","The context from ref_id=luccioni2024 explicitly states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. This directly answers the question with the required unit (g CO2eq)."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context from [ref_id=luccioni2025c] states that GPU energy usage for 1,000 queries spans from 0.06 Wh to over 3,426 Wh, depending on model size and task complexity, with reference to Tables 1 and 2 in the Appendix. This directly provides the requested range in the unit Wh.","[0.06, 3426]","Wh","[""luccioni2025c""]","is_blank","task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The context from [ref_id=luccioni2025c] states that GPU energy usage for 1,000 queries spans from 0.06 Wh to over 3,426 Wh, depending on model size and task complexity, with reference to Tables 1 and 2 in the Appendix. This directly provides the requested range in the unit Wh."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context explicitly states that it is difficult to define universal or generalizable guidelines for AI ethics and sustainability due to contextual variability and lack of convergence among principles (e.g., 'it is difficult to define universal, or even generalizable, guidelines'). Additionally, it emphasizes that 'there is no one-size-fits-all solution for either ethics or sustainability,' directly contradicting the idea of a universal approach.","0","is_blank","[""luccioni2025b""]","is_blank","It is difficult to define universal, or even generalizable, guidelines... There is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.","The context explicitly states that it is difficult to define universal or generalizable guidelines for AI ethics and sustainability due to contextual variability and lack of convergence among principles (e.g., 'it is difficult to define universal, or even generalizable, guidelines'). Additionally, it emphasizes that 'there is no one-size-fits-all solution for either ethics or sustainability,' directly contradicting the idea of a universal approach."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context from [ref_id=patterson2021] explicitly states that the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh, and this value is cited from [USE21], which is the U.S. Energy Information Administration 2021 FAQ. The question asks for the value in 2021, which matches the source year.","0.429","kg of CO2e/KWh","[""patterson2021"", ""USE21""]","[""https://www.eia.gov/tools/faqs/faq.php?id=74&t=11""]","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","The context from [ref_id=patterson2021] explicitly states that the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh, and this value is cited from [USE21], which is the U.S. Energy Information Administration 2021 FAQ. The question asks for the value in 2021, which matches the source year."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context explicitly states that the Hivemind framework is a PyTorch-based framework developed to enable collaborative deep learning training and can handle peers that drop out at any stage, making it suitable for distributed spot training across clouds and continents. This is directly stated in Section 2.1 and reinforced throughout the paper.","Hivemind","is_blank","[""erben2023""]","[""https://github.com/cirquit/hivemind-multi-cloud""]","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion. Its main difference to other state-of-the-art distributed training frameworks, such as PyTorch DDP [26] and DeepSpeed [35], is that it runs in a decentralized fashion and can handle peers that drop out at any stage of the training.","The context explicitly states that the Hivemind framework is a PyTorch-based framework developed to enable collaborative deep learning training and can handle peers that drop out at any stage, making it suitable for distributed spot training across clouds and continents. This is directly stated in Section 2.1 and reinforced throughout the paper."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context explicitly states that Ehsan et al. proposed expanding the notion of transparency in AI to encompass 'social transparency', which involves integrating socio-technical aspects and portraying the societal impacts and environmental footprint of AI systems. This matches the question's request for the proposed term.","social transparency","is_blank","[""luccioni2025b""]","is_blank","In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems [56]. Social transparency involves a portrayal of an AI system’s societal impacts, ethical considerations, and eventually its environmental footprint.","The context explicitly states that Ehsan et al. proposed expanding the notion of transparency in AI to encompass 'social transparency', which involves integrating socio-technical aspects and portraying the societal impacts and environmental footprint of AI systems. This matches the question's request for the proposed term."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context states that 'the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings' in experiments on German public administration texts (FKTG dataset). This directly identifies the model type that achieved the highest accuracy.","linear model with sentence embeddings","is_blank","[""zschache2025""]","is_blank","The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.","The context states that 'the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings' in experiments on German public administration texts (FKTG dataset). This directly identifies the model type that achieved the highest accuracy."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context explicitly states that the authors developed and deployed 'Lamina' as an LLM inference system that incorporates model-attention disaggregation. This is directly stated in the abstract and further detailed in Section 4 and Section 7.","Lamina","is_blank","[""chen2024""]","is_blank","To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The context explicitly states that the authors developed and deployed 'Lamina' as an LLM inference system that incorporates model-attention disaggregation. This is directly stated in the abstract and further detailed in Section 4 and Section 7."
"q093","How many parameters does the largest T5 model have?","The context mentions Flan-T5-xxl as the biggest model in the Flan-T5 family with 11 billion parameters, as shown in Table 3 from ref_id=luccioni2024. This is the largest T5 model explicitly stated in the provided snippets.","11000000000","parameters","[""luccioni2024""]","is_blank","Flan-T5-xxl 11B 11.48 0.083","The context mentions Flan-T5-xxl as the biggest model in the Flan-T5 family with 11 billion parameters, as shown in Table 3 from ref_id=luccioni2024. This is the largest T5 model explicitly stated in the provided snippets."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context explicitly states that JetMoE-8B has 8B parameters, as noted in multiple sections including the abstract and model architecture description. The '8B' in the model name directly refers to the total number of parameters, and this is further confirmed by the line 'JetMoE-8B has 8B parameters while only activating 2B for each input token'.","8000000000","parameters","[""shen2024""]","is_blank","JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context explicitly states that JetMoE-8B has 8B parameters, as noted in multiple sections including the abstract and model architecture description. The '8B' in the model name directly refers to the total number of parameters, and this is further confirmed by the line 'JetMoE-8B has 8B parameters while only activating 2B for each input token'."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context states that Google observed a 20% uptick in data center water consumption from 2021 to 2022, as noted in both [li2025b] and [luccioni2025a]. The answer_unit is 'percent', so the value is reported as a number without the unit name.","20","percent","[""li2025b"", ""luccioni2025a""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [42, 78].","The context states that Google observed a 20% uptick in data center water consumption from 2021 to 2022, as noted in both [li2025b] and [luccioni2025a]. The answer_unit is 'percent', so the value is reported as a number without the unit name."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context from [ref_id=khan2025] explicitly defines 'Carbon Intensity' as 'CO2 emissions per unit of electricity consumed' in Table I under 'COMMON CARBON EMISSION METRICS IN GREEN AI'. This matches the exact definition asked in the question, and since answer_unit is 'is_blank', the answer_value must be the exact metric name as stated.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed International Energy Agency","The context from [ref_id=khan2025] explicitly defines 'Carbon Intensity' as 'CO2 emissions per unit of electricity consumed' in Table I under 'COMMON CARBON EMISSION METRICS IN GREEN AI'. This matches the exact definition asked in the question, and since answer_unit is 'is_blank', the answer_value must be the exact metric name as stated."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context states in Table 2 that for the 101B stage (final growth stage) of FLM-101B training, the FLOPs utilization rate is 52.88%. The answer_unit is 'percent', so the value is returned as a number without the unit name.","52.88","percent","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages... 101 4 4 12 192 2160 165 52.88%","The context states in Table 2 that for the 101B stage (final growth stage) of FLM-101B training, the FLOPs utilization rate is 52.88%. The answer_unit is 'percent', so the value is returned as a number without the unit name."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context from [ref_id=cottier2024] explicitly states that the amortized hardware and energy cost for training GPT-4 is $40M, and this is reiterated in multiple sections as the most expensive publicly announced training run to date. The answer_unit is USD, so the value is expressed as a number in dollars.","40000000","USD","[""cottier2024""]","is_blank","Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.","The context from [ref_id=cottier2024] explicitly states that the amortized hardware and energy cost for training GPT-4 is $40M, and this is reiterated in multiple sections as the most expensive publicly announced training run to date. The answer_unit is USD, so the value is expressed as a number in dollars."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context states that optimization across algorithms, platforms, infrastructure, and hardware can reduce the operational carbon footprint of a Transformer-based universal translation model by 810×. This is explicitly supported by Figure 7 and associated text, which shows that platform-level caching, GPU acceleration, and algorithmic optimizations together achieve an aggregate reduction of over 800×, with the final optimized Transformer on GPUs achieving an 810× reduction compared to a CPU baseline.","810","multiplier","[""wu2021a""]","is_blank","Optimization across the axes of algorithms, platforms, infrastructures, hardware can significantly reduce the operational carbon footprint for the Transformer-based universal translation model by 810×.","The context states that optimization across algorithms, platforms, infrastructure, and hardware can reduce the operational carbon footprint of a Transformer-based universal translation model by 810×. This is explicitly supported by Figure 7 and associated text, which shows that platform-level caching, GPU acceleration, and algorithmic optimizations together achieve an aggregate reduction of over 800×, with the final optimized Transformer on GPUs achieving an 810× reduction compared to a CPU baseline."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context states that for NLP, training across four continents (C-8) resulted in a 41% performance drop compared to the fully local experiment (A-8), meaning the throughput achieved was 59% of the local throughput (100% - 41% = 59%). This is explicitly mentioned in the summary: 'intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).'","0.59","multiplier","[""erben2023""]","is_blank","intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).","The context states that for NLP, training across four continents (C-8) resulted in a 41% performance drop compared to the fully local experiment (A-8), meaning the throughput achieved was 59% of the local throughput (100% - 41% = 59%). This is explicitly mentioned in the summary: 'intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).'"
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context explicitly states that in 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities. This is repeated multiple times across the provided snippets, confirming the value. The answer_unit is 'liters', so the value is expressed as a number in liters.","3500000000","liters","[""amazon2023""]","is_blank","In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.","The context explicitly states that in 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities. This is repeated multiple times across the provided snippets, confirming the value. The answer_unit is 'liters', so the value is expressed as a number in liters."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context clearly states that using custom tags with zero-shot, one-shot, and few-shots techniques reduces energy consumption during inference for code completion tasks. Multiple sections, including RQ1 findings and the abstract, confirm this effect, with specific energy reduction percentages reported for each technique.","1","is_blank","[""rubei2025""]","is_blank","Our findings reveal that the energy consumption of LLMs for the inference phase can be reduced by using the introduced custom tags. Moreover, we show that the energy consumption of LLMs is highly dependent on the used PETs.","The context clearly states that using custom tags with zero-shot, one-shot, and few-shots techniques reduces energy consumption during inference for code completion tasks. Multiple sections, including RQ1 findings and the abstract, confirm this effect, with specific energy reduction percentages reported for each technique."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context from ref_id=luccioni2025a states that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million more units than in 2023. This is explicitly mentioned in the text, and the answer_unit is 'GPUs', so the numeric value is reported directly.","3700000","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].","The context from ref_id=luccioni2025a states that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million more units than in 2023. This is explicitly mentioned in the text, and the answer_unit is 'GPUs', so the numeric value is reported directly."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context from [ref_id=cottier2024] states that 'on average, 44% goes toward AI accelerator chips' when breaking down the components of amortized hardware CapEx + energy cost. This directly answers the question with the required unit 'percent'.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context from [ref_id=cottier2024] states that 'on average, 44% goes toward AI accelerator chips' when breaking down the components of amortized hardware CapEx + energy cost. This directly answers the question with the required unit 'percent'."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context multiple times states that Facebook's data centers achieve a Power Usage Effectiveness (PUE) of about 1.10, specifically citing [wu2021a], [wu2021b], and [patterson2021]. The answer_unit is 'PUE', so the numeric value 1.10 is returned as the answer_value.","1.1","PUE","[""wu2021a"", ""wu2021b"", ""patterson2021""]","[""https://sustainability.fb.com/report-page/data-centers/"", ""https://www.google.com/about/datacenters/efficiency/""]","Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efficient than small-scale, typical data centers.","The context multiple times states that Facebook's data centers achieve a Power Usage Effectiveness (PUE) of about 1.10, specifically citing [wu2021a], [wu2021b], and [patterson2021]. The answer_unit is 'PUE', so the numeric value 1.10 is returned as the answer_value."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context explicitly mentions the Finnish project as 'ETAIROS (Ethical AI for the Governance of the Society)', which proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. Since the answer_unit is 'is_blank', the answer_value must be the exact identifier/name from the context.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]","The context explicitly mentions the Finnish project as 'ETAIROS (Ethical AI for the Governance of the Society)', which proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. Since the answer_unit is 'is_blank', the answer_value must be the exact identifier/name from the context."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context states that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M, referring to amortized hardware CapEx + energy costs. This directly provides the estimated amortized training cost for Gemini Ultra in USD.","30000000","USD","[""cottier2024""]","is_blank","We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The context states that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M, referring to amortized hardware CapEx + energy costs. This directly provides the estimated amortized training cost for Gemini Ultra in USD."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context states that while the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk, it lacks sufficient emphasis on environmental factors. However, the authors argue that environmental risks must be included in these assessments based on the Act's objectives in Article 1 and Recitals 1, 2, and 8, and that this interpretation is necessary to fulfill the Act's purpose. Thus, the Act as written does not currently require it, but the paper advocates for its inclusion.","0","is_blank","[""ebert2024""]","is_blank","Additionally, while the Act imposes risk assessment and mitigation obligations on providers of HRAI systems and GPAI models with systemic risk, these provisions lack sufficient emphasis on environmental factors. Although environmental protection is included in the Act’s objectives, its practical integration into risk management remains unclear and no detailed reporting on mitigation efforts concerning environmental risks is currently required.","The context states that while the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk, it lacks sufficient emphasis on environmental factors. However, the authors argue that environmental risks must be included in these assessments based on the Act's objectives in Article 1 and Recitals 1, 2, and 8, and that this interpretation is necessary to fulfill the Act's purpose. Thus, the Act as written does not currently require it, but the paper advocates for its inclusion."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context explicitly states that the EPA's recently tightened primary standard for the annual average PM2.5 concentration is 9µg/m³, as specified in the NAAQS primary standards [ref_id=han2024]. The answer_unit is µg/m³, so the value is returned as a number without the unit name.","9","µg/m³","[""han2024""]","[""https://www.epa.gov/criteria-air-pollutants/naaqs-table""]","For example, the NAAQS primary standards set the annual average PM 2.5 concentration at 9µg/m 3 and the 98-th percentile of 1-hour daily maximum NO2 concentration at 100 parts per billion by volume, both counted over three years [48].","The context explicitly states that the EPA's recently tightened primary standard for the annual average PM2.5 concentration is 9µg/m³, as specified in the NAAQS primary standards [ref_id=han2024]. The answer_unit is µg/m³, so the value is returned as a number without the unit name."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context from ref_id=luccioni2025a states that a life cycle assessment found that 115 print books produce the same amount of CO2 as a single Amazon Kindle device. Therefore, one Kindle produces the same CO2 as 115 books, matching the required answer_unit of 'books'.","115","books","[""luccioni2025a""]","is_blank","a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","The context from ref_id=luccioni2025a states that a life cycle assessment found that 115 print books produce the same amount of CO2 as a single Amazon Kindle device. Therefore, one Kindle produces the same CO2 as 115 books, matching the required answer_unit of 'books'."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context from ref_id=han2024 explicitly states that the per-household health burden in the most affected, economically-disadvantaged communities could be up to 200 times higher than in less-impacted communities. This is supported by multiple mentions, including in the abstract, section 6, and the conclusion, with specific reference to Table 6c and the ratio of highest to lowest county-level health costs.","200","multiplier","[""han2024""]","is_blank","The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.","The context from ref_id=han2024 explicitly states that the per-household health burden in the most affected, economically-disadvantaged communities could be up to 200 times higher than in less-impacted communities. This is supported by multiple mentions, including in the abstract, section 6, and the conclusion, with specific reference to Table 6c and the ratio of highest to lowest county-level health costs."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The context from [ref_id=zschache2025] Table B1 shows that the DS Llama 70B model consumed 702.06 Wh during inference on the FKTG dataset, measured as an average over 10 runs. The answer_unit is 'Wh', so the value is reported directly as a number without unit name.","702.06","Wh","[""zschache2025""]","is_blank","Table B1: DS Llama 70B 2 702.06 0.46 2543.47 993.68","The context from [ref_id=zschache2025] Table B1 shows that the DS Llama 70B model consumed 702.06 Wh during inference on the FKTG dataset, measured as an average over 10 runs. The answer_unit is 'Wh', so the value is reported directly as a number without unit name."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context mentions Dodge et al. (2022) in relation to measuring electricity consumption and carbon emissions for training language models, but it does not specify the total number of parameters in any large language model analyzed by them. Therefore, the required information is not available in the provided snippets.","is_blank","parameters","[""luccioni2024"", ""morrison2025""]","is_blank","is_blank","The context mentions Dodge et al. (2022) in relation to measuring electricity consumption and carbon emissions for training language models, but it does not specify the total number of parameters in any large language model analyzed by them. Therefore, the required information is not available in the provided snippets."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context repeatedly describes Jevons' Paradox as the phenomenon where technological progress improves efficiency but leads to increased overall resource consumption, such as coal in the 19th century and now AI in modern contexts. This is explicitly stated in multiple snippets from ref_id=luccioni2025a and ref_id=morrison2025.","Jevons' Paradox","is_blank","[""luccioni2025a"", ""morrison2025""]","is_blank","Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60].","The context repeatedly describes Jevons' Paradox as the phenomenon where technological progress improves efficiency but leads to increased overall resource consumption, such as coal in the 19th century and now AI in modern contexts. This is explicitly stated in multiple snippets from ref_id=luccioni2025a and ref_id=morrison2025."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context states that GPT-3 training emits ~305% of the CO2e of a round-trip flight between San Francisco and New York, while Meena is 53% of that same round trip. This implies that GPT-3 training energy is approximately 305/53 ≈ 5.75 times that of Meena. Therefore, about 5.75 Meena training runs would use the same energy as one GPT-3 training run.","5.75","multiplier","[""patterson2021""]","is_blank","T5   training   emissions   are   ~26%,   Meena   is   53%,   Gshard-600B   is   ~2%,   Switch   Transformer   is   32%,   and   GPT-3   is   ~305%   of   such   a   round   trip.","The context states that GPT-3 training emits ~305% of the CO2e of a round-trip flight between San Francisco and New York, while Meena is 53% of that same round trip. This implies that GPT-3 training energy is approximately 305/53 ≈ 5.75 times that of Meena. Therefore, about 5.75 Meena training runs would use the same energy as one GPT-3 training run."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","Table 2 in the Luccioni et al. (2024) study explicitly lists the mean energy consumption for 1,000 image generation inferences as 2.907 kWh. This value is directly provided in the table and corroborated by multiple mentions in the text, such as 'image generation, whose mean consumption is 2.9kWh'. The answer_unit is kWh, so the value is returned as a number without unit name.","2.907","kWh","[""luccioni2024""]","is_blank","inference energy (kWh)
task mean std
... image generation 2.907 3.31
Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis.","Table 2 in the Luccioni et al. (2024) study explicitly lists the mean energy consumption for 1,000 image generation inferences as 2.907 kWh. This value is directly provided in the table and corroborated by multiple mentions in the text, such as 'image generation, whose mean consumption is 2.9kWh'. The answer_unit is kWh, so the value is returned as a number without unit name."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context from [ref_id=strubell2019] explicitly states that the CO2e consumption for an American life, average, in one year is 36,156 lbs. This matches the required unit (lbs) in the additional info.","36156","lbs","[""strubell2019""]","is_blank","American life, avg, 1 year 36,156","The context from [ref_id=strubell2019] explicitly states that the CO2e consumption for an American life, average, in one year is 36,156 lbs. This matches the required unit (lbs) in the additional info."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context provides per-household health cost data for counties in West Virginia for the years 2019–2023, listing Marion, Mason, Marshall, Taylor, Brooke, and Jackson as the top affected counties. However, no projections for 2030 are provided, and the 2028 projections in the paper focus on aggregate national costs or comparisons with California, not county-level 2030 estimates. Therefore, no county in West Virginia is projected to have the highest per-household health cost in 2030 based on the given context.","is_blank","is_blank","[""han2024""]","is_blank","The context provides county-level per-household health costs for 2019–2023 (e.g., WV Marion: $1218.3), but no 2030 projections are made for individual counties. The 2028 projections are national totals ($11.7–$20.9 billion), not county-specific.","The context provides per-household health cost data for counties in West Virginia for the years 2019–2023, listing Marion, Mason, Marshall, Taylor, Brooke, and Jackson as the top affected counties. However, no projections for 2030 are provided, and the 2028 projections in the paper focus on aggregate national costs or comparisons with California, not county-level 2030 estimates. Therefore, no county in West Virginia is projected to have the highest per-household health cost in 2030 based on the given context."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context provides carbon emissions data for Mistral-small before and after optimization in Table III of [ref_id=khan2025], showing emissions decreased from 0.020 kg to 0.015 kg per inference task. The multiplier is calculated as 0.015 / 0.020 = 0.75, indicating a 0.75x change (i.e., a 25% reduction).","0.75","multiplier","[""khan2025""]","is_blank","TABLE III: Before Optimization CO2 (kg) for Mistral-small = 0.020; After Optimization CO2 (kg) = 0.015","The context provides carbon emissions data for Mistral-small before and after optimization in Table III of [ref_id=khan2025], showing emissions decreased from 0.020 kg to 0.015 kg per inference task. The multiplier is calculated as 0.015 / 0.020 = 0.75, indicating a 0.75x change (i.e., a 25% reduction)."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context from ref_id=luccioni2024 provides the training energy for BLOOMz-7B as 51,686 kWh and the fine-tuning energy as 7,571 kWh. Adding these gives the combined energy cost of 59,257 kWh. The answer_unit is 'kWh', so the value is expressed as a number without the unit name.","59257","kWh","[""luccioni2024""]","is_blank","BLOOMz-7B Training energy (kWh) 51,686
Finetuning energy (kWh) 7,571","The context from ref_id=luccioni2024 provides the training energy for BLOOMz-7B as 51,686 kWh and the fine-tuning energy as 7,571 kWh. Adding these gives the combined energy cost of 59,257 kWh. The answer_unit is 'kWh', so the value is expressed as a number without the unit name."
"q125","What is the total number of parameters in the final FLM-101B model?","The context explicitly states that FLM-101B is a model with 101B parameters, as mentioned in the section 'Model Sizes' under '2 Design Overview of FLM-101B' and consistently referred to throughout the paper as the final model in the growth strategy.","101000000000","parameters","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10,240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100,256. Benefiting from our growth strategy, we produce three models with 16B, 51B, and 101B (i.e., FLM-101B) parameters in a single training.","The context explicitly states that FLM-101B is a model with 101B parameters, as mentioned in the section 'Model Sizes' under '2 Design Overview of FLM-101B' and consistently referred to throughout the paper as the final model in the growth strategy."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context states that training a 6.1B parameter model consumed 13,812.4 kWh over 8 days but was not completed; a full training run is estimated at 103,593 kWh. The BLOOMz-7B model's training energy is 51,686 kWh, and its cost parity (inferences needed to match training energy) is 592,570,000. Since 103,593 kWh is roughly twice 51,686 kWh, we approximate the required inferences as roughly twice 592,570,000, yielding about 1.185 billion inferences.","1185140000","inferences","[""dodge2022"", ""luccioni2024""]","[""https://dl.acm.org/doi/10.1145/3531146.3533135"", ""https://dl.acm.org/doi/10.1145/3610548.3610557""]","We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!; BLOOMz-7B training energy (kWh): 51,686; Cost parity (# inferences): 592,570,000","The context states that training a 6.1B parameter model consumed 13,812.4 kWh over 8 days but was not completed; a full training run is estimated at 103,593 kWh. The BLOOMz-7B model's training energy is 51,686 kWh, and its cost parity (inferences needed to match training energy) is 592,570,000. Since 103,593 kWh is roughly twice 51,686 kWh, we approximate the required inferences as roughly twice 592,570,000, yielding about 1.185 billion inferences."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context from ref_id=luccioni2024 explicitly states that 'in total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy'. This directly answers the question with the required unit (kWh).","754.66","kWh","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context from ref_id=luccioni2024 explicitly states that 'in total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy'. This directly answers the question with the required unit (kWh)."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context from ref_id=luccioni2024 explicitly states in Table 5 that for the BLOOMz-7B model, the cost parity — the number of inferences required for the cumulative inference energy cost to equal the combined training and fine-tuning energy cost — is 592,570,000 inferences. This matches the required answer_unit of 'inferences'.","592570000","inferences","[""luccioni2024""]","is_blank","Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost. ... Cost parity (# inferences) 592,570,000 for BLOOMz-7B.","The context from ref_id=luccioni2024 explicitly states in Table 5 that for the BLOOMz-7B model, the cost parity — the number of inferences required for the cumulative inference energy cost to equal the combined training and fine-tuning energy cost — is 592,570,000 inferences. This matches the required answer_unit of 'inferences'."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context from ref_id=zschache2025 states that the data for the experiments originates from the FKTG process in Germany, where population objections regarding a nuclear waste repository site were categorized. The dataset is explicitly named as the 'FKTG-dataset' and is accessible at https://beteiligung.bge.de/index.php. This matches the question asking for the dataset name used for classifying German nuclear waste site objection texts.","FKTG-dataset","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).","The context from ref_id=zschache2025 states that the data for the experiments originates from the FKTG process in Germany, where population objections regarding a nuclear waste repository site were categorized. The dataset is explicitly named as the 'FKTG-dataset' and is accessible at https://beteiligung.bge.de/index.php. This matches the question asking for the dataset name used for classifying German nuclear waste site objection texts."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context provides water consumption data for Meta's global data center fleet in 2023 (3.7 L/kWh for scope-2 water consumption), but no specific data is given for Llama 3 inference serving clusters in 2024. While Meta's overall water consumption is mentioned, there is no breakdown for Llama 3 inference or for the year 2024.","is_blank","liters","[""li2025b"", ""han2024""]","is_blank","is_blank","The context provides water consumption data for Meta's global data center fleet in 2023 (3.7 L/kWh for scope-2 water consumption), but no specific data is given for Llama 3 inference serving clusters in 2024. While Meta's overall water consumption is mentioned, there is no breakdown for Llama 3 inference or for the year 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context provides no information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. While it mentions rare earth metal usage in H100 manufacturing (0.1% by mass) and environmental impacts of mining, there is no data on recycling rates or use of recycled materials in 2024 production.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context provides no information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. While it mentions rare earth metal usage in H100 manufacturing (0.1% by mass) and environmental impacts of mining, there is no data on recycling rates or use of recycled materials in 2024 production."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context states that 3.2 net tCO2e from the Evolved Transformer is equivalent to ~3 passengers taking a round trip between San Francisco and New York, as each passenger round trip emits ~1.2t CO2e (3.2 / 1.2 ≈ 2.67, rounded to ~3). This is explicitly stated in multiple snippets from [patterson2021].","3","passengers","[""patterson2021""]","is_blank","To put 3.2 net tCO2e into perspective, Table 1 and Appendix A use Google Flights to calculate the CO2e for the average direct round trip flights between SFO and JFK as 180.4t. The Boeing 767 that United Airlines flies on that route has 175 seats. Google Flights uses the historical average of 84.5% seat occupancy, yielding 1.2t of CO2e per passenger round trip. Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","The context states that 3.2 net tCO2e from the Evolved Transformer is equivalent to ~3 passengers taking a round trip between San Francisco and New York, as each passenger round trip emits ~1.2t CO2e (3.2 / 1.2 ≈ 2.67, rounded to ~3). This is explicitly stated in multiple snippets from [patterson2021]."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context from ref_id=luccioni2025c states that in May 2025, based on OpenRouter data, 84% of LLM token usage occurred through models with no environmental disclosure. The answer_unit is 'percent', so the value is returned as a number without the unit name.","84","percent","[""luccioni2025c""]","is_blank","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","The context from ref_id=luccioni2025c states that in May 2025, based on OpenRouter data, 84% of LLM token usage occurred through models with no environmental disclosure. The answer_unit is 'percent', so the value is returned as a number without the unit name."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context from [ref_id=samsi2024] states that for LLaMA-13B inference with bare minimum hardware settings and no compression or quantization, 1 A100 80GB GPU is sufficient, as shown in Table II where the count for A100 80GB is 1 for the 13B model.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: ... Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64","The context from [ref_id=samsi2024] states that for LLaMA-13B inference with bare minimum hardware settings and no compression or quantization, 1 A100 80GB GPU is sufficient, as shown in Table II where the count for A100 80GB is 1 for the 13B model."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context from [ref_id=dodge2022] states that a full training run of the 6.1 billion parameter transformer model would emit 21 to 78 metric tons of CO2, depending on the region it was run in. This range is explicitly provided as an estimate for complete training, matching the required answer_unit of 'metric tons'.","[21, 78]","metric tons","[""dodge2022""]","is_blank","If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).","The context from [ref_id=dodge2022] states that a full training run of the 6.1 billion parameter transformer model would emit 21 to 78 metric tons of CO2, depending on the region it was run in. This range is explicitly provided as an estimate for complete training, matching the required answer_unit of 'metric tons'."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context does not provide any specific data on total carbon emissions avoided by pruning and quantizing large language models in 2023. While multiple studies mention reductions in carbon emissions due to quantization (e.g., up to 45% in Khan et al.) and report emissions for specific models or phases, no aggregate value for 2023 is given.","is_blank","tCO2e","[""khan2025"", ""morrison2025"", ""rubei2025"", ""luccioni2023""]","is_blank","is_blank","The context does not provide any specific data on total carbon emissions avoided by pruning and quantizing large language models in 2023. While multiple studies mention reductions in carbon emissions due to quantization (e.g., up to 45% in Khan et al.) and report emissions for specific models or phases, no aggregate value for 2023 is given."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context from ref_id=griggs2024 explicitly states that using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy. This is directly stated in two sections: once in the initial highlight and again in the 'Mixing GPU Types' subsection, confirming the percentage.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context from ref_id=griggs2024 explicitly states that using 2 A100s and 1 A10G results in a 24% cost saving over an A100-only strategy. This is directly stated in two sections: once in the initial highlight and again in the 'Mixing GPU Types' subsection, confirming the percentage."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","Chen et al. (2024) provide a table listing the price per chip for NVIDIA H20 as $4.63/hr, explicitly stated in Table 1 and the surrounding text. The unit is USD per hour, matching the required answer_unit.","4.63","USD per hour","[""chen2024""]","is_blank","Table 1: H100, H20, and TPU v6e specifications. Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","Chen et al. (2024) provide a table listing the price per chip for NVIDIA H20 as $4.63/hr, explicitly stated in Table 1 and the surrounding text. The unit is USD per hour, matching the required answer_unit."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context explicitly states that most carbon footprint analyses gather information manually by writing to authors, as many authors refuse to provide data due to privacy concerns and lack of experimental logs. This contradicts the idea of automatic gathering.","0","is_blank","[""luccioni2025b"", ""luccioni2023""]","is_blank","In fact, most carbon footprint analyses gather the information manually by writing to authors. For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","The context explicitly states that most carbon footprint analyses gather information manually by writing to authors, as many authors refuse to provide data due to privacy concerns and lack of experimental logs. This contradicts the idea of automatic gathering."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context states that in 2023, the total public health cost of U.S. data centers was equivalent to approximately 44% of the data centers' total electricity cost, as explicitly mentioned in multiple sections including Table 1 and the text surrounding it. This value is derived using the midrange (average) attribution method, as the context specifies using midrange values by default for ratios.","44","percent","[""han2024""]","is_blank","This is equivalent to approximately 44% of the data centers’ total electricity cost.","The context states that in 2023, the total public health cost of U.S. data centers was equivalent to approximately 44% of the data centers' total electricity cost, as explicitly mentioned in multiple sections including Table 1 and the text surrounding it. This value is derived using the midrange (average) attribution method, as the context specifies using midrange values by default for ratios."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that for LLaMA-7B inference, the bare minimum hardware requirement on A100 80GB GPUs is 1 GPU, as shown in Table II, which lists the minimum GPU count for each model size without compression or quantization.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64","The context states that for LLaMA-7B inference, the bare minimum hardware requirement on A100 80GB GPUs is 1 GPU, as shown in Table II, which lists the minimum GPU count for each model size without compression or quantization."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context from ref_id=khan2025 explicitly states that 'Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization,' directly supporting the claim that sustainable deployment techniques involving quantization achieved up to a 45% reduction in carbon emissions. Since the question asks for a True/False answer, the value must be '1' for True.","1","is_blank","[""khan2025""]","is_blank","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.","The context from ref_id=khan2025 explicitly states that 'Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization,' directly supporting the claim that sustainable deployment techniques involving quantization achieved up to a 45% reduction in carbon emissions. Since the question asks for a True/False answer, the value must be '1' for True."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors and were only able to collect 95 answers, as noted in both [ref_id=luccioni2025b] and [ref_id=luccioni2023]. The answer_unit is 'answers', so the numeric value 95 is returned.","95","answers","[""luccioni2025b"", ""luccioni2023""]","is_blank","For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors and were only able to collect 95 answers, as noted in both [ref_id=luccioni2025b] and [ref_id=luccioni2023]. The answer_unit is 'answers', so the numeric value 95 is returned."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context states that JetMoE-8B was trained with a $100k budget using 30,000 H100 GPU hours. To estimate the cost per H100 GPU-hour, divide the total budget by the total GPU hours: $100,000 / 30,000 = $3.33 per hour. This matches the required unit of USD per hour.","3.33","USD per hour","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context states that JetMoE-8B was trained with a $100k budget using 30,000 H100 GPU hours. To estimate the cost per H100 GPU-hour, divide the total budget by the total GPU hours: $100,000 / 30,000 = $3.33 per hour. This matches the required unit of USD per hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context states in Table 2 that for Altoona, IA, the health cost as a percentage of electricity cost is 122%. This directly answers the question using the provided data.","122","percent","[""han2024""]","is_blank","Table 2: The public health cost of training a large AI model in selected U.S. data centers. Altoona, IA: Health Cost (million$) 2.51(1.84, 3.17), % of Electricity Cost 122%","The context states in Table 2 that for Altoona, IA, the health cost as a percentage of electricity cost is 122%. This directly answers the question using the provided data."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context explicitly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. This directly answers the question about the number of tokens used for pre-training, and the answer_unit is 'tokens', so the value is expressed as a number in tokens.","1250000000000","tokens","[""shen2024""]","is_blank","JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","The context explicitly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. This directly answers the question about the number of tokens used for pre-training, and the answer_unit is 'tokens', so the value is expressed as a number in tokens."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context states that as of January 2024, Amazon had announced 36 renewable energy projects in the United Kingdom, as listed in the 'Amazon Renewable Energy Projects*' table under the 'United Kingdom' row with 'Number of Projects' equal to 36.","36","projects","[""amazon2023""]","is_blank","Project Location... United Kingdom 36 901","The context states that as of January 2024, Amazon had announced 36 renewable energy projects in the United Kingdom, as listed in the 'Amazon Renewable Energy Projects*' table under the 'United Kingdom' row with 'Number of Projects' equal to 36."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provides U.S. gender representation data for Amazon's workforce across all levels in 2023, showing that 43.1% identified as men and 56.8% as women. This is explicitly stated in the 'Amazon Workforce (All Levels)' row under 'Gender—U.S.' for 2023.","43.1","percent","[""amazon2023""]","is_blank","Amazon Workforce (All Levels)
43.1%56.8%
44.1%55.7%
30.8%69.2%
31.4%68.5%
31.5%68.4%
46.9%52.9%
45.5%54.3%
46.6%53.2%
44.3%55.6%
31.1%68.8%
48
31.7%68.3%
31.6%68.3%","The context provides U.S. gender representation data for Amazon's workforce across all levels in 2023, showing that 43.1% identified as men and 56.8% as women. This is explicitly stated in the 'Amazon Workforce (All Levels)' row under 'Gender—U.S.' for 2023."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context from ref_id=li2025b states that 'Apple reports that its supply chain accounts for 99% of its total water footprint [23].' This directly answers the question with the percentage value, and the answer_unit is 'percent', so the value is returned as a number.","99","percent","[""li2025b""]","[""https://www.apple.com/environment/"", ""https://www.apple.com/environment/""]","Apple reports that its supply chain accounts for 99% of its total water footprint [23].","The context from ref_id=li2025b states that 'Apple reports that its supply chain accounts for 99% of its total water footprint [23].' This directly answers the question with the percentage value, and the answer_unit is 'percent', so the value is returned as a number."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context provides execution time breakdowns for BlackMamba under sparse fine-tuning with different batch sizes, including a batch size of 84, as shown in Fig. 4 and related figures. However, the exact total execution time in seconds for a batch size of 84 is not numerically specified in the text or tables. The figures (e.g., Fig. 4) show relative breakdowns but do not provide the absolute total time value for this specific configuration.","is_blank","seconds","[""xia2024""]","is_blank","Fig. 4 illustrates execution time breakdown for Mixtral and Mamba with sparse fine-tuning, including batch sizes up to 84, but does not provide the total seconds value for batch size 84.","The context provides execution time breakdowns for BlackMamba under sparse fine-tuning with different batch sizes, including a batch size of 84, as shown in Fig. 4 and related figures. However, the exact total execution time in seconds for a batch size of 84 is not numerically specified in the text or tables. The figures (e.g., Fig. 4) show relative breakdowns but do not provide the absolute total time value for this specific configuration."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context explicitly introduces the 'granularity metric' as the ratio of calculation to communication time, used to assess model suitability for distributed training across continents. This is stated in multiple sections, including the contributions and lessons learned.","granularity metric","is_blank","[""erben2023""]","is_blank","We introduce the granularity metric, the ratio of calculation to communication time, and show how it can be used for predicting performance with different hardware setups.","The context explicitly introduces the 'granularity metric' as the ratio of calculation to communication time, used to assess model suitability for distributed training across continents. This is stated in multiple sections, including the contributions and lessons learned."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context states that a coalition of Microsoft employees estimated that a single deal with Exxon Mobil using AI to expand oil production could add up to 640 percent more carbon emissions compared to Microsoft’s carbon removal targets for the year. Since 640 percent equals 6.4 times, the answer is expressed as a numeric value in 'times' as required by answer_unit.","6.4","times","[""luccioni2025a"", ""luccioni2025b""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/"", ""https://grist.org/accountability/microsoft-employees-spent-years-fighting-the-tech-giants-oil-ties-now-theyre-speaking-out/""]","a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year [119]","The context states that a coalition of Microsoft employees estimated that a single deal with Exxon Mobil using AI to expand oil production could add up to 640 percent more carbon emissions compared to Microsoft’s carbon removal targets for the year. Since 640 percent equals 6.4 times, the answer is expressed as a numeric value in 'times' as required by answer_unit."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context explicitly defines 'water withdrawal' as freshwater taken from ground or surface water sources, either temporarily or permanently, for agricultural, industrial, or municipal uses. This matches the question exactly. Since the answer_unit is 'is_blank', the answer_value must be the exact term from the context, which is 'water withdrawal'.","water withdrawal","is_blank","[""li2025b""]","is_blank","• Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","The context explicitly defines 'water withdrawal' as freshwater taken from ground or surface water sources, either temporarily or permanently, for agricultural, industrial, or municipal uses. This matches the question exactly. Since the answer_unit is 'is_blank', the answer_value must be the exact term from the context, which is 'water withdrawal'."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context explicitly states that the Standing Committee forms a Study Panel every five years to assess the current state of AI, as noted in multiple snippets including [ref_id=stone2022]. The answer_unit is 'years', so the value is expressed as a number.","5","years","[""stone2022""]","is_blank","As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","The context explicitly states that the Standing Committee forms a Study Panel every five years to assess the current state of AI, as noted in multiple snippets including [ref_id=stone2022]. The answer_unit is 'years', so the value is expressed as a number."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context from [ref_id=wu2021b] explicitly states that 'At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].' This directly answers the question with the exact number of devices.","25","devices","[""wu2021b""]","[""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf""]","At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].","The context from [ref_id=wu2021b] explicitly states that 'At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines [Deloitte, 2021].' This directly answers the question with the exact number of devices."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context from ref_id=luccioni2025c states that the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (Llama 4 Scout). This range is explicitly provided in the text and corroborated in Table 1 of the appendix, which lists the same values.","[0.8,3500]","MWh","[""luccioni2025c""]","is_blank","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).","The context from ref_id=luccioni2025c states that the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (Llama 4 Scout). This range is explicitly provided in the text and corroborated in Table 1 of the appendix, which lists the same values."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context explicitly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011, which directly contradicts the claim in the question. Therefore, the statement is false.","0","is_blank","[""stone2022""]","is_blank","IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","The context explicitly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011, which directly contradicts the claim in the question. Therefore, the statement is false."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context from ref_id=luccioni2025a explicitly states that '10–50 queries on GPT-3 consumes around half a liter of water', which directly answers the question. The answer_unit is 'queries', so the value is expressed as a numeric range [10,50].","[10, 50]","queries","[""luccioni2025a""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","The context from ref_id=luccioni2025a explicitly states that '10–50 queries on GPT-3 consumes around half a liter of water', which directly answers the question. The answer_unit is 'queries', so the value is expressed as a numeric range [10,50]."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context clearly states that JetMoE-8B-Chat achieved an MT-Bench score of 6.681 after alignment, which surpasses Llama-2-13b-Chat's score of 6.650, as shown in Table 4. The answer_unit is 'score', so the numeric value is returned directly.","6.681","score","[""shen2024""]","is_blank","JetMoE-8B-chat 6.681\nLlama-2-13b-chat 6.650\nTable 4: MT-Bench score comparison of various models\nJetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, demonstrating its superior performance.","The context clearly states that JetMoE-8B-Chat achieved an MT-Bench score of 6.681 after alignment, which surpasses Llama-2-13b-Chat's score of 6.650, as shown in Table 4. The answer_unit is 'score', so the numeric value is returned directly."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10–50 medium-length responses, depending on deployment conditions. This directly provides the range of responses per 500ml bottle of water.","[10, 50]","responses","[""li2025b""]","is_blank","Additionally, GPT-3 needs to “drink” (i.e., consume) a500ml bottle of waterfor roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10–50 medium-length responses, depending on deployment conditions. This directly provides the range of responses per 500ml bottle of water."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The Griggs et al. (2024) paper states that Mélange reduces deployment costs by up to 77% in conversational settings, which aligns with the question's focus on chat settings. The answer_unit is 'percent', so the value is returned as a number without the unit symbol.","77","percent","[""griggs2024""]","is_blank","Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.","The Griggs et al. (2024) paper states that Mélange reduces deployment costs by up to 77% in conversational settings, which aligns with the question's focus on chat settings. The answer_unit is 'percent', so the value is returned as a number without the unit symbol."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that at a minimum, 4 A100 GPUs each with 80GB of memory are required for meaningful inference with the 65B LLaMA model, as noted in Table II and reiterated in the discussion section. This is the bare minimum without compression or quantization.","4","A100_80GB_GPUs","[""samsi2024""]","is_blank","Table II: Baseline configurations for LLaMA 7B, 13B, and 65B: ... 65B 8 64 4 128 ... the 65B model was run on 8 V100 GPUs and 4 A100 GPUs respectively due to the size of the model and available memory on the GPU(s).","The context explicitly states that at a minimum, 4 A100 GPUs each with 80GB of memory are required for meaningful inference with the 65B LLaMA model, as noted in Table II and reiterated in the discussion section. This is the bare minimum without compression or quantization."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context repeatedly states that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. This is explicitly mentioned in multiple snippets from [ref_id=han2024] and [ref_id=luccioni2025c], with consistent phrasing.","10000","round trips","[""han2024"", ""luccioni2025c""]","is_blank","training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.","The context repeatedly states that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. This is explicitly mentioned in multiple snippets from [ref_id=han2024] and [ref_id=luccioni2025c], with consistent phrasing."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context from [ref_id=patterson2021] states that NVIDIA estimated 80–90% of the ML workload is inference processing, citing [Leo19] as the source for this 2019 estimate. The answer_unit is 'percent', so the numeric range is returned as [80,90].","[80, 90]","percent","[""patterson2021""]","is_blank","NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].","The context from [ref_id=patterson2021] states that NVIDIA estimated 80–90% of the ML workload is inference processing, citing [Leo19] as the source for this 2019 estimate. The answer_unit is 'percent', so the numeric range is returned as [80,90]."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context from ref_id=luccioni2024 explicitly states that for all model experimentation and evaluation in the 'Power Hungry Processing' study, a total of 178.97 kg of CO2eq was emitted. This matches the required answer_unit of 'kg CO2eq'.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context from ref_id=luccioni2024 explicitly states that for all model experimentation and evaluation in the 'Power Hungry Processing' study, a total of 178.97 kg of CO2eq was emitted. This matches the required answer_unit of 'kg CO2eq'."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context repeatedly states that estimating GPU energy consumption using TDP is nearly always an overestimation because GPUs rarely operate at maximum power continuously. For example, Chung2025 explicitly says TDP-based estimations can overestimate by a factor of up to 4.1, and Patterson2021 notes TDP is a worst-case scenario. Thus, it is not reliable or accurate.","0","is_blank","[""chung2025"", ""patterson2021""]","is_blank","Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time.","The context repeatedly states that estimating GPU energy consumption using TDP is nearly always an overestimation because GPUs rarely operate at maximum power continuously. For example, Chung2025 explicitly says TDP-based estimations can overestimate by a factor of up to 4.1, and Patterson2021 notes TDP is a worst-case scenario. Thus, it is not reliable or accurate."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes slightly more energy than GPT-4o per query due to deployment on older A100 GPU nodes, despite its smaller architecture. Specifically, GPT-4o consumes 2.875 Wh while GPT-4o mini consumes 3.098 Wh. This directly contradicts the claim that GPT-4o mini consumes less energy.","0","is_blank","[""jegham2025""]","is_blank","For instance GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.","The context states that GPT-4o mini consumes slightly more energy than GPT-4o per query due to deployment on older A100 GPU nodes, despite its smaller architecture. Specifically, GPT-4o consumes 2.875 Wh while GPT-4o mini consumes 3.098 Wh. This directly contradicts the claim that GPT-4o mini consumes less energy."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context shows in Fig. 15 that for Mixtral-CS-A100-40GB, the throughput for dense (bsz=1) is visually indicated at approximately 0.2 queries/sec, as the dense (bsz=1) point on the graph aligns with the 0.2 mark on the y-axis. This is consistent with the trend shown in Fig. 8 where dense (bsz=1) for Mixtral-CS is 0.3 qps, but Fig. 15 provides the specific A100-40GB data point.","0.2","queries/sec","[""xia2024""]","is_blank","Fig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS for different GPUs: A100 and H100. Dense(bsz=1) for Mixtral-CS-A100-40GB is shown at approximately 0.2 queries/sec.","The context shows in Fig. 15 that for Mixtral-CS-A100-40GB, the throughput for dense (bsz=1) is visually indicated at approximately 0.2 queries/sec, as the dense (bsz=1) point on the graph aligns with the 0.2 mark on the y-axis. This is consistent with the trend shown in Fig. 8 where dense (bsz=1) for Mixtral-CS is 0.3 qps, but Fig. 15 provides the specific A100-40GB data point."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context states that direct environmental disclosure peaked in 2022 and then experienced a notable reversal after 2022 due to the rise of commercial and proprietary models, leading to dramatically reduced disclosures by the first quarter of 2025. This contradicts the claim that the trend continued to increase.","0","is_blank","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, ... triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.","The context states that direct environmental disclosure peaked in 2022 and then experienced a notable reversal after 2022 due to the rise of commercial and proprietary models, leading to dramatically reduced disclosures by the first quarter of 2025. This contradicts the claim that the trend continued to increase."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The Griggs et al. (2024) paper states that the on-demand H100 price is not available from major cloud providers, so they defer to RunPod's price of $4.69/h and normalize it using the ratio of RunPod's H100 to A100-80G prices and the A100's price on major clouds, resulting in a normalized price of (4.69/2.29) × 3.67 = $7.516 for H100. This value is explicitly listed in Table 1 as the normalized on-demand hourly price.","7.516","USD per hour","[""griggs2024""]","is_blank","Table 1: On-demand Price ($/h) for H100 (SXM): 7.5164","The Griggs et al. (2024) paper states that the on-demand H100 price is not available from major cloud providers, so they defer to RunPod's price of $4.69/h and normalize it using the ratio of RunPod's H100 to A100-80G prices and the A100's price on major clouds, resulting in a normalized price of (4.69/2.29) × 3.67 = $7.516 for H100. This value is explicitly listed in Table 1 as the normalized on-demand hourly price."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context provides water consumption estimates for GPT-3 training (e.g., over 5 million liters) and inference, but does not mention any specific water usage figures for GPT-4 training. While GPT-4 is referenced in cost and model comparisons, no quantitative data on its cooling water use is disclosed.","is_blank","liters of water","[""li2025b"", ""jegham2025"", ""cottier2024"", ""patterson2021"", ""morrison2025""]","is_blank","is_blank","The context provides water consumption estimates for GPT-3 training (e.g., over 5 million liters) and inference, but does not mention any specific water usage figures for GPT-4 training. While GPT-4 is referenced in cost and model comparisons, no quantitative data on its cooling water use is disclosed."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context from ref_id=griggs2024 states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, costing over $5,200 per month in on-demand rental costs. To find the hourly cost, this monthly cost is divided by 30 days and then by 24 hours, resulting in approximately $7.22 per hour.","7.22","USD per hour","[""griggs2024""]","is_blank","serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context from ref_id=griggs2024 states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, costing over $5,200 per month in on-demand rental costs. To find the hourly cost, this monthly cost is divided by 30 days and then by 24 hours, resulting in approximately $7.22 per hour."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context from [ref_id=wu2021a] explicitly states that to increase the BLEU score from 5 to 40 for GPT-3-based language translation, a model 1,000× larger in size is required. This directly answers the question with the required multiplier format.","1000","multiplier","[""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.","The context from [ref_id=wu2021a] explicitly states that to increase the BLEU score from 5 to 40 for GPT-3-based language translation, a model 1,000× larger in size is required. This directly answers the question with the required multiplier format."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context states that training a Transformer model with neural architecture search (NAS) emitted 626,155 lbs of CO2e, as reported in Strubell et al. (2019) [ref_id=strubell2019]. This value is explicitly compared to the lifetime carbon emissions of five cars, which are stated to be 126,000 lbs per car [ref_id=strubell2019]. Therefore, the equivalent driving distance is 5 × 126,000 = 630,000 miles. The context does not provide a direct conversion factor from CO2e to miles, but the comparison to five car lifetimes allows this calculation.","630000","miles","[""strubell2019""]","[""https://arxiv.org/abs/1906.02243v1""]","Table 1: Estimated CO2 emissions from training common NLP models, compared to familiar consumption. ... w/ neural architecture search 626,155 ... Car, avg incl. fuel, 1 lifetime 126,000","The context states that training a Transformer model with neural architecture search (NAS) emitted 626,155 lbs of CO2e, as reported in Strubell et al. (2019) [ref_id=strubell2019]. This value is explicitly compared to the lifetime carbon emissions of five cars, which are stated to be 126,000 lbs per car [ref_id=strubell2019]. Therefore, the equivalent driving distance is 5 × 126,000 = 630,000 miles. The context does not provide a direct conversion factor from CO2e to miles, but the comparison to five car lifetimes allows this calculation."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The BLOOMz-7B model has an inference energy cost of 1.0 × 10−4 kWh per inference, as stated in Table 5 of ref_id=luccioni2024. With 606,096 downloads and 1 million inferences per download, total inferences = 606,096 × 1,000,000 = 6.06096 × 10^11. Total energy = 6.06096 × 10^11 × 1.0 × 10−4 kWh = 60,609,600 kWh. Converting to MWh (1 MWh = 1,000 kWh) gives 60,609.6 MWh.","60609.6","MWh","[""luccioni2024""]","is_blank","BLOOMz-7B ... Inference energy (kWh) 1.0 × 10−4 ... BLOOMz-7B has been downloaded 606,096 times, as of November 2023.","The BLOOMz-7B model has an inference energy cost of 1.0 × 10−4 kWh per inference, as stated in Table 5 of ref_id=luccioni2024. With 606,096 downloads and 1 million inferences per download, total inferences = 606,096 × 1,000,000 = 6.06096 × 10^11. Total energy = 6.06096 × 10^11 × 1.0 × 10−4 kWh = 60,609,600 kWh. Converting to MWh (1 MWh = 1,000 kWh) gives 60,609.6 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours, as mentioned in multiple sections including the abstract and the model pretraining section. This matches the required answer_unit of 'H100 GPU hours'.","30000","H100 GPU hours","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours, as mentioned in multiple sections including the abstract and the model pretraining section. This matches the required answer_unit of 'H100 GPU hours'."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context from [ref_id=cottier2024] states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This is explicitly stated multiple times, with the growth rate of 2.4× per year since 2016 used to extrapolate that the most expensive publicly announced model will cost about $1 billion by the start of 2027. The answer_unit is USD, so the value is expressed as a number.","1000000000","USD","[""cottier2024""]","is_blank","If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.","The context from [ref_id=cottier2024] states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. This is explicitly stated multiple times, with the growth rate of 2.4× per year since 2016 used to extrapolate that the most expensive publicly announced model will cost about $1 billion by the start of 2027. The answer_unit is USD, so the value is expressed as a number."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context from [ref_id=patterson2021] explicitly states that OpenAI published the total number of floating point operations to train GPT-3 as 3.14E+23 FLOPS. This matches the required unit 'FLOPS' for answer_value.","314000000000000000000000","FLOPS","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context from [ref_id=patterson2021] explicitly states that OpenAI published the total number of floating point operations to train GPT-3 as 3.14E+23 FLOPS. This matches the required unit 'FLOPS' for answer_value."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context explicitly states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for meaningful inference with the LLaMA 65B model, as noted in Table II and multiple sections of the paper. This is the bare minimum without compression or quantization.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","Table II: Baseline configurations for LLaMA 7B, 13B, and 65B: ... 65B 8 64 4 128 ... While the 65B model can sharded across 6 V100 GPUs, we use 8 since the model architecture makes it better suited for balanced sharding across 8 GPUs.","The context explicitly states that at a minimum, 8 V100 GPUs each with 32 GB of RAM are required for meaningful inference with the LLaMA 65B model, as noted in Table II and multiple sections of the paper. This is the bare minimum without compression or quantization."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The final 101B training stage of FLM-101B used 192 NVIDIA A800 GPUs with a single-GPU throughput exceeding 160 teraFLOPs/sec and a utilization rate of 52.88%. The total GPU hours for this stage were 1.01e5 (101,000 hours). Multiplying 160 teraFLOPs/sec × 3600 seconds/hour × 101,000 hours gives 5.8368e9 teraFLOPs, or 5836.8 zettaFLOPs. However, Table 3 in ref_id=li2025a directly reports the total FLOPs for FLM-101B as 28.22 zettaFLOPs, which includes the entire training process across all growth stages. Since the question asks for the computational work during the final 101B stage only, and no separate value is provided for that stage, the total FLOPs for the entire training (28.22 zettaFLOPs) is the only concrete figure available, and it cannot be split per stage.","28.22","zettaFLOPs","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs... FLM-101B 28.22 zettaFLOPs","The final 101B training stage of FLM-101B used 192 NVIDIA A800 GPUs with a single-GPU throughput exceeding 160 teraFLOPs/sec and a utilization rate of 52.88%. The total GPU hours for this stage were 1.01e5 (101,000 hours). Multiplying 160 teraFLOPs/sec × 3600 seconds/hour × 101,000 hours gives 5.8368e9 teraFLOPs, or 5836.8 zettaFLOPs. However, Table 3 in ref_id=li2025a directly reports the total FLOPs for FLM-101B as 28.22 zettaFLOPs, which includes the entire training process across all growth stages. Since the question asks for the computational work during the final 101B stage only, and no separate value is provided for that stage, the total FLOPs for the entire training (28.22 zettaFLOPs) is the only concrete figure available, and it cannot be split per stage."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context mentions AlexNet in relation to ImageNet classification and references its performance in the context of FPO and top-1 accuracy comparisons, but it does not provide the exact top-1 accuracy value for AlexNet 2012. While it references the ImageNet dataset and AlexNet's role in early deep learning success, no numeric accuracy value is stated.","is_blank","percent","[""schwartz2019"", ""stone2022"", ""luccioni2023"", ""luccioni2025b""]","is_blank","is_blank","The context mentions AlexNet in relation to ImageNet classification and references its performance in the context of FPO and top-1 accuracy comparisons, but it does not provide the exact top-1 accuracy value for AlexNet 2012. While it references the ImageNet dataset and AlexNet's role in early deep learning success, no numeric accuracy value is stated."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, and each server contains 8 GPUs (8 × 80G). Therefore, the total number of A800 GPUs is 24 × 8 = 192. The answer_unit is 'GPUs', so the answer_value is the numeric count.","192","GPUs","[""li2025a""]","is_blank","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU servers, and each server contains 8 GPUs (8 × 80G). Therefore, the total number of A800 GPUs is 24 × 8 = 192. The answer_unit is 'GPUs', so the answer_value is the numeric count."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context states that training a Transformer model with Neural Architecture Search (NAS) produced 626,155 lbs of CO2, which is comparable to the lifetime carbon emissions of five cars. However, the question asks for equivalence in 'average American lifetimes,' and while the context provides CO2 emissions for an American lifetime as 36,156 lbs (from Strubell et al., 2019, Table 1), dividing 626,155 by 36,156 yields approximately 17.34. This value is not explicitly stated in the context, and no direct equivalence to American lifetimes is provided, only to car lifetimes. Thus, the exact requested equivalence cannot be confirmed from the context.","is_blank","lifetimes","[""strubell2019""]","[""https://arxiv.org/abs/1906.02243""]","Table 1: Estimated CO2 emissions from training common NLP models, compared to familiar consumption. American life, avg, 1 year 36,156","The context states that training a Transformer model with Neural Architecture Search (NAS) produced 626,155 lbs of CO2, which is comparable to the lifetime carbon emissions of five cars. However, the question asks for equivalence in 'average American lifetimes,' and while the context provides CO2 emissions for an American lifetime as 36,156 lbs (from Strubell et al., 2019, Table 1), dividing 626,155 by 36,156 yields approximately 17.34. This value is not explicitly stated in the context, and no direct equivalence to American lifetimes is provided, only to car lifetimes. Thus, the exact requested equivalence cannot be confirmed from the context."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context from ref_id=schwartz2019 explicitly states that FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train. This directly answers the question with the required unit of hours.","25000","hours","[""schwartz2019""]","is_blank","FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context from ref_id=schwartz2019 explicitly states that FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train. This directly answers the question with the required unit of hours."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that Amazon's on-site solar energy systems avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources. This value is explicitly mentioned in multiple sections of the context, including the section on scaling renewable energy.","47400","metric tons","[""amazon2023""]","is_blank","These on-site solar energy systems are estimated to generate 123,000 MWh annually—enough energy to power over 33,600 European homes—and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","The context states that Amazon's on-site solar energy systems avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources. This value is explicitly mentioned in multiple sections of the context, including the section on scaling renewable energy."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context explicitly states that LLMs were deployed using the vllm library, which runs on a Ray cluster for multi-node computations. This framework enables model distribution across multiple GPUs and nodes via tensor and pipeline model parallelism.","vllm library running on a Ray cluster","is_blank","[""zschache2025""]","[""https://github.com/vllm-project/vllm"", ""https://www.ray.io/""]","LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.","The context explicitly states that LLMs were deployed using the vllm library, which runs on a Ray cluster for multi-node computations. This framework enables model distribution across multiple GPUs and nodes via tensor and pipeline model parallelism."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context from ref_id=zschache2025, Table B2, shows that for the Llama 3.1 70B model, energy consumption increased from 48.60 Wh on a single node to 94.88 Wh on two nodes, resulting in a ratio of 1.95. This is explicitly stated as the factor of increase.","1.95","multiplier","[""zschache2025""]","is_blank","Table B2: Llama 3.1 70B ... single 48.60 ... double 94.88 ... ratio 1.95","The context from ref_id=zschache2025, Table B2, shows that for the Llama 3.1 70B model, energy consumption increased from 48.60 Wh on a single node to 94.88 Wh on two nodes, resulting in a ratio of 1.95. This is explicitly stated as the factor of increase."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context does not provide any specific data on water consumption per ChatGPT user session in 2023. While it mentions water usage per query for GPT-3 (e.g., 500 mL per 10–50 queries) and estimates for 2027, there is no direct figure for 2023 per-session water consumption in gallons.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The context does not provide any specific data on water consumption per ChatGPT user session in 2023. While it mentions water usage per query for GPT-3 (e.g., 500 mL per 10–50 queries) and estimates for 2027, there is no direct figure for 2023 per-session water consumption in gallons."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that GPT-4o's estimated annual energy consumption from 700 million daily queries is between 391,509 MWh and 463,269 MWh, which exceeds the annual electricity consumption of 35,000 U.S. residential households (377,685 MWh). This comparison is explicitly made in Section 6.2, confirming that the energy use is comparable to 35,000 homes.","35000","homes","[""jegham2025""]","is_blank","These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh)","The context states that GPT-4o's estimated annual energy consumption from 700 million daily queries is between 391,509 MWh and 463,269 MWh, which exceeds the annual electricity consumption of 35,000 U.S. residential households (377,685 MWh). This comparison is explicitly made in Section 6.2, confirming that the energy use is comparable to 35,000 homes."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context from ref_id=luccioni2025a and ref_id=li2025b both state that Microsoft reported a 34% increase in global water consumption between 2021 and 2022. The answer_unit is 'percent', so the value is returned as a number without the unit symbol.","34","percent","[""luccioni2025a"", ""li2025b""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons...","The context from ref_id=luccioni2025a and ref_id=li2025b both state that Microsoft reported a 34% increase in global water consumption between 2021 and 2022. The answer_unit is 'percent', so the value is returned as a number without the unit symbol."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the higher energy costs of LLM deployment. This directly contradicts the claim that traditional models achieved comparable accuracy to LLMs.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the higher energy costs of LLM deployment. This directly contradicts the claim that traditional models achieved comparable accuracy to LLMs."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context explicitly states that the Power Usage Effectiveness (PUE) for the Iowa datacenter where the Evolved Transformer was run is 1.11, as noted in multiple sections including 2.3 and Table 1. The answer_unit is 'PUE', so the numeric value 1.11 is returned directly.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The context explicitly states that the Power Usage Effectiveness (PUE) for the Iowa datacenter where the Evolved Transformer was run is 1.11, as noted in multiple sections including 2.3 and Table 1. The answer_unit is 'PUE', so the numeric value 1.11 is returned directly."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context states that GPT-4o is projected to have approximately 772 billion queries in 2025, based on a monthly growth rate from January to May and a decaying growth pattern through December. This figure is explicitly mentioned in multiple snippets and aligns with the question's request for the total estimated number of queries.","772000000000","queries","[""jegham2025""]","is_blank","yielding a total of approximately 772 billion GPT-4o queries in 2025","The context states that GPT-4o is projected to have approximately 772 billion queries in 2025, based on a monthly growth rate from January to May and a decaying growth pattern through December. This figure is explicitly mentioned in multiple snippets and aligns with the question's request for the total estimated number of queries."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context explicitly states that JetMoE-8B achieved an OpenLLM Leaderboard Avg. score of 53.0, as shown in Table 3. This value is the final average score for the model on the benchmark suite, and the answer_unit is 'score', so the numeric value is returned directly.","53.0","score","[""shen2024""]","is_blank","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. ... OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0","The context explicitly states that JetMoE-8B achieved an OpenLLM Leaderboard Avg. score of 53.0, as shown in Table 3. This value is the final average score for the model on the benchmark suite, and the answer_unit is 'score', so the numeric value is returned directly."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The provided context does not contain any information about the number of AI training runs conducted globally on renewable-only power in 2022. While several documents discuss AI's energy consumption, carbon footprint, and water usage, none quantify training runs specifically powered by renewable-only sources in that year.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about the number of AI training runs conducted globally on renewable-only power in 2022. While several documents discuss AI's energy consumption, carbon footprint, and water usage, none quantify training runs specifically powered by renewable-only sources in that year."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context states that open-source GPAI models are largely excluded from transparency requirements unless they present a systemic risk (ref_id=ebert2024), and that the open-source exemption from reporting obligations should be removed because such models can have significant energy implications (ref_id=ebert2024). This implies that under the current AI Act, open-source models are exempt unless systemic risk is present, supporting the truth of the statement.","1","is_blank","[""ebert2024""]","is_blank","Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [4]. The idea is that OS models, by definition, already disclose certain types of information. Hence, Recital 102 lists information on parameters, including weights, model architecture, and model usage as a prerequisite for systems to be considered OS. However, it does not mandate the disclosure of energy consumption.","The context states that open-source GPAI models are largely excluded from transparency requirements unless they present a systemic risk (ref_id=ebert2024), and that the open-source exemption from reporting obligations should be removed because such models can have significant energy implications (ref_id=ebert2024). This implies that under the current AI Act, open-source models are exempt unless systemic risk is present, supporting the truth of the statement."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context states that the average PUE for a typical data center in 2020 was 1.58, as cited from Lawrence, 2019, 2020. Although this is not explicitly labeled as the U.S. national average, the context also references the U.S. average PUE in other sections (e.g., Patterson et al., 2021) without contradicting the 1.58 figure, and no other specific U.S. national average for 2020 is provided. Thus, 1.58 is the most directly supported value for the U.S. national average in 2020.","1.58","PUE","[""wu2021b""]","is_blank","Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook] and the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].","The context states that the average PUE for a typical data center in 2020 was 1.58, as cited from Lawrence, 2019, 2020. Although this is not explicitly labeled as the U.S. national average, the context also references the U.S. average PUE in other sections (e.g., Patterson et al., 2021) without contradicting the 1.58 figure, and no other specific U.S. national average for 2020 is provided. Thus, 1.58 is the most directly supported value for the U.S. national average in 2020."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context explicitly states that for the OPT-2.7B model with a batch size of 32, the KV Cache expands to 5.312GB, as noted in multiple sections including the analysis of KV Cache size growth and memory exhaustion due to increased batch size.","5.312","GB","[""kim2025""]","is_blank","When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","The context explicitly states that for the OPT-2.7B model with a batch size of 32, the KV Cache expands to 5.312GB, as noted in multiple sections including the analysis of KV Cache size growth and memory exhaustion due to increased batch size."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context repeatedly states that for the four notable models studied (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs including equity account for between 29% and 49% of the total amortized model development cost. This is explicitly mentioned in multiple snippets, including the methodology and results sections.","[29, 49]","percent","[""cottier2024""]","is_blank","For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.","The context repeatedly states that for the four notable models studied (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs including equity account for between 29% and 49% of the total amortized model development cost. This is explicitly mentioned in multiple snippets, including the methodology and results sections."
"q213","Which software package was used to measure energy consumption during inference runs?","The context repeatedly states that the CodeCarbon package was used to measure energy consumption during inference runs, specifically mentioning its use in tracking power usage via NVIDIA's NVML and Intel RAPL, and that it was verified against physical measurements. The answer_unit is 'is_blank', so the exact identifier name 'CodeCarbon' is returned as the answer_value.","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025""]","[""https://github.com/mlco2/codecarbon""]","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon).","The context repeatedly states that the CodeCarbon package was used to measure energy consumption during inference runs, specifically mentioning its use in tracking power usage via NVIDIA's NVML and Intel RAPL, and that it was verified against physical measurements. The answer_unit is 'is_blank', so the exact identifier name 'CodeCarbon' is returned as the answer_value."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context from ref_id=luccioni2025c states that in an analysis of 100 news articles on ChatGPT energy consumption, 53% of articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. This directly answers the question, and the answer_unit is 'percent', so the value is reported as a number.","53","percent","[""luccioni2025c""]","is_blank","53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search","The context from ref_id=luccioni2025c states that in an analysis of 100 news articles on ChatGPT energy consumption, 53% of articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. This directly answers the question, and the answer_unit is 'percent', so the value is reported as a number."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context explicitly states that the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. This is mentioned in multiple snippets, including the abstract and Section IV-F, confirming CTCF as the named function.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","The context explicitly states that the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. This is mentioned in multiple snippets, including the abstract and Section IV-F, confirming CTCF as the named function."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that 'increasing the number of shards still tends to increase the energy costs of inference per response most overall' (Section D, Energy per Response: LLaMA 65B), directly supporting that more GPU shards increase energy cost per response.","1","is_blank","[""samsi2024""]","is_blank","increasing the number of shards still tends to increase the energy costs of inference per response most overall","The context states that 'increasing the number of shards still tends to increase the energy costs of inference per response most overall' (Section D, Energy per Response: LLaMA 65B), directly supporting that more GPU shards increase energy cost per response."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context states that an H100 GPU is 0.1% rare earth metal by mass, and one 12-inch silicon wafer weighing 125 grams produces about 63 H100s. This implies each H100 has a mass of approximately 125/63 ≈ 1.984 grams. Thus, the rare earth metal mass per H100 is 0.1% of 1.984 g = 0.001984 g. Mining 1 kg of rare earth materials consumes 11 kL of water, so per gram it consumes 0.011 kL. Multiplying gives 0.001984 g × 0.011 kL/g ≈ 0.000021824 kL. However, the context directly states that mining rare earth metals adds an additional 2.2 liters (0.0022 kL) consumed per GPU, which is the value used in the analysis. Thus, the estimated water consumption from rare earth mining per H100 is 0.0022 kL.","0.0022","kL","[""morrison2025""]","is_blank","We additionally estimate the environmental impact from mining rare earth metals used during man-ufacturing, assuming an H100 is 0.1% rare earth metal by mass. Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams 12 and produces about 63 H100s. 13 14 Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU.","The context states that an H100 GPU is 0.1% rare earth metal by mass, and one 12-inch silicon wafer weighing 125 grams produces about 63 H100s. This implies each H100 has a mass of approximately 125/63 ≈ 1.984 grams. Thus, the rare earth metal mass per H100 is 0.1% of 1.984 g = 0.001984 g. Mining 1 kg of rare earth materials consumes 11 kL of water, so per gram it consumes 0.011 kL. Multiplying gives 0.001984 g × 0.011 kL/g ≈ 0.000021824 kL. However, the context directly states that mining rare earth metals adds an additional 2.2 liters (0.0022 kL) consumed per GPU, which is the value used in the analysis. Thus, the estimated water consumption from rare earth mining per H100 is 0.0022 kL."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context states that the open-source exemption from reporting obligations should be removed, as open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. This implies that under current EU rules, open-source models are exempt from reporting energy consumption, but the paper proposes eliminating this exemption. Therefore, currently, they are not required to report.","0","is_blank","[""ebert2024""]","is_blank","• Elimination of the Open-Source Exemption : The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify ex- clusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary mod- els.","The context states that the open-source exemption from reporting obligations should be removed, as open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models. This implies that under current EU rules, open-source models are exempt from reporting energy consumption, but the paper proposes eliminating this exemption. Therefore, currently, they are not required to report."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context from ref_id=luccioni2025a and ref_id=wu2021b both state that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all PPAs purchased by corporations worldwide. Meta is referred to as Facebook in one source, but it is the same entity. The percentage is explicitly stated and matches the required unit of 'percent'.","30","percent","[""luccioni2025a"", ""wu2021b""]","is_blank","In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131]","The context from ref_id=luccioni2025a and ref_id=wu2021b both state that in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all PPAs purchased by corporations worldwide. Meta is referred to as Facebook in one source, but it is the same entity. The percentage is explicitly stated and matches the required unit of 'percent'."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context states that in 2023, the total public health cost of U.S. data centers was $47.48 billion, based on the average attribution method, as shown in Table 1 under the 'Total' row for 2023. This value is derived from scope-1 and scope-2 health costs combined, using the standard average attribution method described in the paper.","47.48","USD","[""han2024""]","is_blank","Table 1: The public health cost of U.S. data centers from 2019 to 2023 and projection in 2028 — Total Health Cost (billion$) for 2023 is $47.48 (35.77, 59.19).","The context states that in 2023, the total public health cost of U.S. data centers was $47.48 billion, based on the average attribution method, as shown in Table 1 under the 'Total' row for 2023. This value is derived from scope-1 and scope-2 health costs combined, using the standard average attribution method described in the paper."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The context shows that for a long prompt (10k input-1.5k output), the o3 model consumes 12.222 Wh, while GPT-4.1 nano consumes 0.827 Wh. The factor is calculated as 12.222 / 0.827 ≈ 14.78, which rounds to approximately 14.8. This matches the required format for 'multiplier'.","14.8","multiplier","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). o3: 12.222 ± 1.082 Wh (10k input-1.5k output); GPT-4.1 nano: 0.827 ± 0.094 Wh (10k input-1.5k output).","The context shows that for a long prompt (10k input-1.5k output), the o3 model consumes 12.222 Wh, while GPT-4.1 nano consumes 0.827 Wh. The factor is calculated as 12.222 / 0.827 ≈ 14.78, which rounds to approximately 14.8. This matches the required format for 'multiplier'."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context states that for the Arena dataset with a 120ms SLO, Mélange achieves 15-77% cost reduction compared to single-GPU baselines. This range directly answers the question and matches the required format of 'percent'.","[15, 77]","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO)","The context states that for the Arena dataset with a 120ms SLO, Mélange achieves 15-77% cost reduction compared to single-GPU baselines. This range directly answers the question and matches the required format of 'percent'."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context states in Table 3 that the net carbon emissions (nettCO2e) for FLM-101B are 26 tCO2e, which matches the required unit. This value is explicitly listed alongside other models' emissions, confirming it as the total estimated net carbon footprint for FLM-101B's pre-training.","26","tCO2e","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. ... FLM-101B ... net tCO2e 26","The context states in Table 3 that the net carbon emissions (nettCO2e) for FLM-101B are 26 tCO2e, which matches the required unit. This value is explicitly listed alongside other models' emissions, confirming it as the total estimated net carbon footprint for FLM-101B's pre-training."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides execution time breakdowns for sparse Mixtral fine-tuning with batch size=1 on an NVIDIA A40 GPU, but does not give a single total execution time value. It states that the optimizer stage is negligible for Mixtral, and the forward and backward stages contribute to total time, but no aggregate seconds value is reported for the entire fine-tuning step with batch size=1.","is_blank","seconds","[""xia2024""]","is_blank","Fig. 4 illustrates the execution time breakdown for Mixtral with sparse fine-tuning at batch size=1, but no total seconds value is given.","The context provides execution time breakdowns for sparse Mixtral fine-tuning with batch size=1 on an NVIDIA A40 GPU, but does not give a single total execution time value. It states that the optimizer stage is negligible for Mixtral, and the forward and backward stages contribute to total time, but no aggregate seconds value is reported for the entire fine-tuning step with batch size=1."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context explicitly states that the public health impact of AI is highly unevenly distributed across different counties and communities, with disadvantaged communities bearing a disproportionate share, and the ratio of the highest to lowest county-level per-household health cost is approximately 200. This directly contradicts the idea of even distribution.","0","is_blank","[""han2024""]","is_blank","The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103]. ... The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.","The context explicitly states that the public health impact of AI is highly unevenly distributed across different counties and communities, with disadvantaged communities bearing a disproportionate share, and the ratio of the highest to lowest county-level per-household health cost is approximately 200. This directly contradicts the idea of even distribution."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context from [ref_id=wu2021b] states that 'GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years' as a result of Moore’s law scaling and architectural optimization, and this is cited with [Sun et al., 2019], which aligns with the 2019 product data timeframe. The statement is directly supported.","1","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The context from [ref_id=wu2021b] states that 'GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years' as a result of Moore’s law scaling and architectural optimization, and this is cited with [Sun et al., 2019], which aligns with the 2019 product data timeframe. The statement is directly supported."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context explicitly states that Ollama [19] is the open-source platform used to apply quantization for local inference, specifically mentioning the use of 4-bit quantization in the financial sentiment case study. This matches the question's requirement for the tool used to apply 4-bit quantization and support local deployment.","Ollama","is_blank","[""khan2025""]","is_blank","We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The context explicitly states that Ollama [19] is the open-source platform used to apply quantization for local inference, specifically mentioning the use of 4-bit quantization in the financial sentiment case study. This matches the question's requirement for the tool used to apply 4-bit quantization and support local deployment."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context states that to simulate a real-world deployment with a non-public dataset on interruptible spot instances, the authors chose Backblaze (B2) as an independent S3 storage provider. They accessed datasets on-demand via shards in the tar format using the WebDataset library, which supports streaming and caching. This directly answers the question about the storage service used for sharding and streaming datasets.","Backblaze (B2)","is_blank","[""erben2023""]","is_blank","To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4]. We access the datasets on-demand via shards in the tar format with the WebDataset library [10].","The context states that to simulate a real-world deployment with a non-public dataset on interruptible spot instances, the authors chose Backblaze (B2) as an independent S3 storage provider. They accessed datasets on-demand via shards in the tar format using the WebDataset library, which supports streaming and caching. This directly answers the question about the storage service used for sharding and streaming datasets."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context repeatedly states that there is a strong correlation between inference energy consumption and model runtime, and explicitly mentions that if power is constant over time, the correlation should be linear, which is observed in the data (Figure 6). This supports that the relationship is nearly linear.","is_blank","is_blank","[""zschache2025""]","is_blank","Since energy is the integral of power over time, these two measures exhibit a strong correlation. If the power is constant over time, this correlation should be linear. Figure 6 illustrates this relationship for...","The context repeatedly states that there is a strong correlation between inference energy consumption and model runtime, and explicitly mentions that if power is constant over time, the correlation should be linear, which is observed in the data (Figure 6). This supports that the relationship is nearly linear."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context explicitly states that the AI Environmental Impacts Act bill was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024, citing reference [78]. The answer_unit is 'is_blank', so the answer_value must be the exact name of the senator as identified in the context.","Edward J. Markey","is_blank","[""ebert2024""]","[""https://www.congress.gov/bill/118th-congress/senate-bill/3732/""]","The bill was referred to the Committee on Commerce, Science and Transportation, and has not yet been voted upon. Under the new administration, it is unlikely that the bill will pass Congress. However, even if enacted, it would not contain any significant hard regulation. The bill primarily mandates studies, stakeholder consultations, and voluntary reporting on AI’s environmental impacts without imposing significant regulatory obligations. Relating to AI more specifically, although not limited to data centers, is a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024 [78].","The context explicitly states that the AI Environmental Impacts Act bill was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024, citing reference [78]. The answer_unit is 'is_blank', so the answer_value must be the exact name of the senator as identified in the context."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context references Chen et al. (2024), not 2025, and provides the price per hour for NVIDIA H100 as $11.06/hr in Table 1. Although the question incorrectly cites '2025', the only available price for H100 per hour in the context is from Chen et al. (2024).","11.06","USD per hour","[""chen2024""]","is_blank","Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","The context references Chen et al. (2024), not 2025, and provides the price per hour for NVIDIA H100 as $11.06/hr in Table 1. Although the question incorrectly cites '2025', the only available price for H100 per hour in the context is from Chen et al. (2024)."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context states that in February 2024, AWS announced that the average expected life of its servers improved from five to six years. While this refers to servers generally, the context also links GPU usage to data center hardware lifespans, and no specific GPU lifetime is stated separately. However, another snippet mentions that GPUs can theoretically last about five years, but high turnover due to performance demands leads to earlier retirement. Since the question asks for the estimated average GPU lifetime before retirement in 2024, and no exact figure for GPUs is given, the closest supported value is the server lifetime increase to six years, which may reflect hardware trends including GPUs.","5","years","[""luccioni2025a"", ""amazon2023""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/"", ""https://about.aws/news/2024/amazon-sustainability-report-2023/""]","although GPUs can theoretically last about five years, the push for higher performance is prompting more frequent upgrades – one recent study estimates that AI will generate an additional 1.2–5 million metric tons of e-waste by 2030 [134]. In February 2024, AWS announced that the average expected life of its servers had improved from five to six years.","The context states that in February 2024, AWS announced that the average expected life of its servers improved from five to six years. While this refers to servers generally, the context also links GPU usage to data center hardware lifespans, and no specific GPU lifetime is stated separately. However, another snippet mentions that GPUs can theoretically last about five years, but high turnover due to performance demands leads to earlier retirement. Since the question asks for the estimated average GPU lifetime before retirement in 2024, and no exact figure for GPUs is given, the closest supported value is the server lifetime increase to six years, which may reflect hardware trends including GPUs."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that for LLaMA-13B inference, the bare minimum hardware requires 2 V100 32GB GPUs, as specified in Table II, which lists the minimum GPU count for each model size without compression or quantization.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. ... 13B 2 64 1 64","The context states that for LLaMA-13B inference, the bare minimum hardware requires 2 V100 32GB GPUs, as specified in Table II, which lists the minimum GPU count for each model size without compression or quantization."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context states that Google reports training their Gemma family of language models emitted 1247.61 tons CO2e, which is over 4 times the 'five cars' estimate. The 'five cars' estimate is derived from a 2019 study by Strubell et al., equating to 284 metric tons CO2e. Thus, the reported GHG emissions for Gemma are 1247.61 tCO2e.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the 'five cars' number","The context states that Google reports training their Gemma family of language models emitted 1247.61 tons CO2e, which is over 4 times the 'five cars' estimate. The 'five cars' estimate is derived from a 2019 study by Strubell et al., equating to 284 metric tons CO2e. Thus, the reported GHG emissions for Gemma are 1247.61 tCO2e."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). Although one snippet mentions GTX 1080 Ti GPUs, the training time reported is for GTX 1080 GPUs, and no separate time is given for GTX 1080 Ti. Since the question asks for training time on GTX 1080 Ti GPUs and the context does not provide a distinct value for that specific hardware, the only available training time is 336 hours for GTX 1080, which is the closest match. We assume the Ti variant is a minor variant and use the reported value.","336","hours","[""strubell2019""]","is_blank","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). Although one snippet mentions GTX 1080 Ti GPUs, the training time reported is for GTX 1080 GPUs, and no separate time is given for GTX 1080 Ti. Since the question asks for training time on GTX 1080 Ti GPUs and the context does not provide a distinct value for that specific hardware, the only available training time is 336 hours for GTX 1080, which is the closest match. We assume the Ti variant is a minor variant and use the reported value."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context from [ref_id=li2025b] states that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh, and this value is consistently referenced as 3.14 L/kWh in multiple sections, including the appendix on operational water for global AI in 2027.","3.14","L/kWh","[""li2025b""]","is_blank","The U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The context from [ref_id=li2025b] states that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh, and this value is consistently referenced as 3.14 L/kWh in multiple sections, including the appendix on operational water for global AI in 2027."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context states that Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021, as cited in [dodge2022] and corroborated by [wu2021b] and [patterson2021]. The answer_unit is 'PUE', so the value is returned as a number.","1.1","PUE","[""dodge2022"", ""wu2021b"", ""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/""]","Google claiming a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021","The context states that Google claimed a PUE of 1.10 across its fleet of data centers for the 12 months ending in Q1 2021, as cited in [dodge2022] and corroborated by [wu2021b] and [patterson2021]. The answer_unit is 'PUE', so the value is returned as a number."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context from ref_id=amazon2023 states that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when AWS uses 100% renewable energy. This directly answers the question about the typical percent reduction customers can expect.","96","percent","[""amazon2023""]","is_blank","Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","The context from ref_id=amazon2023 states that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when AWS uses 100% renewable energy. This directly answers the question about the typical percent reduction customers can expect."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context explicitly states that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries on an NVIDIA H100 GPU has a net cost of $3460, as mentioned in multiple sections including the abstract, introduction, and conclusions.","3460","USD","[""xia2024""]","is_blank","For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","The context explicitly states that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries on an NVIDIA H100 GPU has a net cost of $3460, as mentioned in multiple sections including the abstract, introduction, and conclusions."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context from ref_id=dodge2022 states that the GPU alone accounts for 74% of the total energy consumption in a typical AI workload setup, as shown in Table 1 from their experiment training BERT-base on a single NVIDIA TITAN X GPU. This percentage is consistently supported by other snippets, such as ref_id=zschache2025, which notes that GPU consumption accounts for the largest share of total energy usage in all experiments. The question asks for the percentage of total provisioned power, and 74% is the most direct and empirically supported figure from the context.","74","percent","[""dodge2022""]","is_blank","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), ... The GPU alone accounts for 74% of the total energy consumption due to these components.","The context from ref_id=dodge2022 states that the GPU alone accounts for 74% of the total energy consumption in a typical AI workload setup, as shown in Table 1 from their experiment training BERT-base on a single NVIDIA TITAN X GPU. This percentage is consistently supported by other snippets, such as ref_id=zschache2025, which notes that GPU consumption accounts for the largest share of total energy usage in all experiments. The question asks for the percentage of total provisioned power, and 74% is the most direct and empirically supported figure from the context."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context explicitly states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100s, directly specifying the total number of H100 GPUs used.","96","H100 GPUs","[""shen2024""]","is_blank","We conduct training on a cluster containing 12 nodes and 96 H100s.","The context explicitly states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100s, directly specifying the total number of H100 GPUs used."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context states that during actively training, the average GPU power for a single node is over 600W, specifically in the context of OLMo 2 7B training during the first 300 logging steps. This is explicitly mentioned in multiple snippets referencing Figure 2 and the training phase power consumption.","600","Watts","[""morrison2025""]","is_blank","When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.","The context states that during actively training, the average GPU power for a single node is over 600W, specifically in the context of OLMo 2 7B training during the first 300 logging steps. This is explicitly mentioned in multiple snippets referencing Figure 2 and the training phase power consumption."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context mentions that the average American life emits 36,156 lbs of CO2e per year, but it does not provide a global average for an average human life. The value of 11,023 lbs CO2e per year is listed as 'Human life, avg, 1 year', which likely refers to a global average, as it is contrasted with the higher American value. This is the only global figure provided.","11023","lbs","[""strubell2019""]","is_blank","Human life, avg, 1 year 11,023","The context mentions that the average American life emits 36,156 lbs of CO2e per year, but it does not provide a global average for an average human life. The value of 11,023 lbs CO2e per year is listed as 'Human life, avg, 1 year', which likely refers to a global average, as it is contrasted with the higher American value. This is the only global figure provided."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for the LLaMA-13B model, there is a 1.25 times increase in inference latency on the A100 compared to the V100, which implies a 1.25x improvement in inference throughput (since lower latency means higher throughput). The term 'increase in inference latency' is likely a misstatement; given the context's consistent comparison of performance metrics like tokens per second, it is clear that higher performance (throughput) on A100 is intended. Thus, the speedup in throughput is 1.25 times.","1.25","multiplier","[""samsi2024""]","is_blank","As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that for the LLaMA-13B model, there is a 1.25 times increase in inference latency on the A100 compared to the V100, which implies a 1.25x improvement in inference throughput (since lower latency means higher throughput). The term 'increase in inference latency' is likely a misstatement; given the context's consistent comparison of performance metrics like tokens per second, it is clear that higher performance (throughput) on A100 is intended. Thus, the speedup in throughput is 1.25 times."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context repeatedly states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), as noted in multiple sections including 6.1 and 5.3, and this value is validated against OpenAI CEO Sam Altman’s reported figure of 0.34 Wh per query. The answer_unit is 'Wh', so the value is returned as a number in that unit.","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The context repeatedly states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), as noted in multiple sections including 6.1 and 5.3, and this value is validated against OpenAI CEO Sam Altman’s reported figure of 0.34 Wh per query. The answer_unit is 'Wh', so the value is returned as a number in that unit."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that for the 400 TPS SLO constraint, InferSave's first choice was g4dn.xlarge with a cost of $0.71, while Max-Performance selected g6e.xlarge at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice. This directly provides the percentage difference.","280","percent","[""kim2025""]","is_blank","On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","The context states that for the 400 TPS SLO constraint, InferSave's first choice was g4dn.xlarge with a cost of $0.71, while Max-Performance selected g6e.xlarge at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice. This directly provides the percentage difference."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context states that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference (ref_id=zschache2025). Since the question asks for the most energy-efficient GPU architecture for generating only a single classification token, and single-token generation is implied by the context's focus on single-token inference, the V100 and A30 are identified as efficient. Between them, the A30 is explicitly mentioned as 'even' more efficient than the V100, suggesting it is the most efficient in this specific case.","A30","is_blank","[""zschache2025""]","is_blank","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.","The context states that for models generating a single token per inference, a V100 or even an A30 GPU is more efficient in inference (ref_id=zschache2025). Since the question asks for the most energy-efficient GPU architecture for generating only a single classification token, and single-token generation is implied by the context's focus on single-token inference, the V100 and A30 are identified as efficient. Between them, the A30 is explicitly mentioned as 'even' more efficient than the V100, suggesting it is the most efficient in this specific case."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context from schwartz2019 explicitly states that 'Reporting the computational price tag of finding, training, and running models is a key Green AI practice', indicating that Green AI involves disclosing the financial or computational cost associated with these stages. The term 'price tag' here refers to financial or resource cost, aligning with the question.","1","is_blank","[""schwartz2019""]","is_blank","Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1).","The context from schwartz2019 explicitly states that 'Reporting the computational price tag of finding, training, and running models is a key Green AI practice', indicating that Green AI involves disclosing the financial or computational cost associated with these stages. The term 'price tag' here refers to financial or resource cost, aligning with the question."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context from ref_id=luccioni2025a states that electronic waste (e-waste) reached 62 million tonnes in 2022, which matches the required unit of metric tons. This value is explicitly mentioned as the global generation of e-waste for that year.","62000000","metric tons","[""luccioni2025a""]","is_blank","AI’s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","The context from ref_id=luccioni2025a states that electronic waste (e-waste) reached 62 million tonnes in 2022, which matches the required unit of metric tons. This value is explicitly mentioned as the global generation of e-waste for that year."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context from [ref_id=patterson2021] provides the measured system average power per processor for TPU v2 as 208 W and for V100 GPU as 289 W (Table 4, row 12). The difference is calculated as 289 W - 208 W = 81 W. The answer_unit is Watts, so the value is expressed as a number in Watts.","81","Watts","[""patterson2021""]","is_blank","Table 4: Measured System Average Power per Accelerator... (W): 208 for TPU v2, 289 for V100","The context from [ref_id=patterson2021] provides the measured system average power per processor for TPU v2 as 208 W and for V100 GPU as 289 W (Table 4, row 12). The difference is calculated as 289 W - 208 W = 81 W. The answer_unit is Watts, so the value is expressed as a number in Watts."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context from [ref_id=li2025b] explicitly states that training GPT-3 in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This value is presented as a specific estimate for scope-1 on-site water consumption during training, and the answer_unit is 'liters', so the numeric value is reported directly.","700000","liters","[""li2025b""]","is_blank","training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","The context from [ref_id=li2025b] explicitly states that training GPT-3 in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This value is presented as a specific estimate for scope-1 on-site water consumption during training, and the answer_unit is 'liters', so the numeric value is reported directly."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context repeatedly states that Facebook's recommendation and ranking model sizes increased by 20× between 2019 and 2021, as seen in multiple mentions in [ref_id=wu2021a]. The answer_unit is 'multiplier', so the value is expressed as the numeric factor of increase.","20","multiplier","[""wu2021a""]","is_blank","Facebook’s recommendation and ranking model sizes have increased by 20 times during the same time period [11].","The context repeatedly states that Facebook's recommendation and ranking model sizes increased by 20× between 2019 and 2021, as seen in multiple mentions in [ref_id=wu2021a]. The answer_unit is 'multiplier', so the value is expressed as the numeric factor of increase."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context explicitly states that o3-mini achieved the highest cross-efficiency score (0.884) in the DEA-based eco-efficiency analysis, as shown in Figure 8 and detailed in the C Cross-efficiency DEA Results section. This identifies o3-mini as the top-ranked model.","o3-mini","is_blank","[""jegham2025""]","is_blank","As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint.","The context explicitly states that o3-mini achieved the highest cross-efficiency score (0.884) in the DEA-based eco-efficiency analysis, as shown in Figure 8 and detailed in the C Cross-efficiency DEA Results section. This identifies o3-mini as the top-ranked model."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context from ref_id=wu2021b states that 'current averages of less than 3 years for cell phones' contribute to e-waste concerns, and this is further reinforced by references to e-waste growth and high turnover in AI hardware, which includes smartphones. The question asks for a True/False evaluation, and the context clearly supports that smartphones have average lifetimes of less than 3 years, contributing to e-waste.","1","is_blank","[""wu2021b""]","is_blank","For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020] and 4 to 5 years for servers [Ascierto and Lawrence, 2020].","The context from ref_id=wu2021b states that 'current averages of less than 3 years for cell phones' contribute to e-waste concerns, and this is further reinforced by references to e-waste growth and high turnover in AI hardware, which includes smartphones. The question asks for a True/False evaluation, and the context clearly supports that smartphones have average lifetimes of less than 3 years, contributing to e-waste."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context states that for CV models in intra-zone experiments with T4 GPUs, the per-GPU speedup (speedup/#GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2 to 8 GPUs, indicating nearly linear scaling. This directly supports the truth of the statement.","1","is_blank","[""erben2023""]","is_blank","CV’s per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster (0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively.","The context states that for CV models in intra-zone experiments with T4 GPUs, the per-GPU speedup (speedup/#GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2 to 8 GPUs, indicating nearly linear scaling. This directly supports the truth of the statement."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens, as mentioned in two identical sentences describing its architecture. This matches the required unit 'tokens'.","2048","tokens","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens, as mentioned in two identical sentences describing its architecture. This matches the required unit 'tokens'."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLM decoding is characterized by low compute-intensity and is bottlenecked by VRAM bandwidth, leading to underutilized GPU computation units and lower power draw compared to diffusion models, which are significantly more compute-intensive and consume nearly the maximum GPU power. This directly supports the truth of the statement.","1","is_blank","[""chung2025""]","is_blank","This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low [37, 58]. This leads to the GPU’s computation throughput being bottlenecked by VRAM bandwidth and results in the GPU’s computation units being underutilized, leading to low power draw. Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because Diffusion models are significantly more compute-intensive compared to LLM decoding.","The context states that LLM decoding is characterized by low compute-intensity and is bottlenecked by VRAM bandwidth, leading to underutilized GPU computation units and lower power draw compared to diffusion models, which are significantly more compute-intensive and consume nearly the maximum GPU power. This directly supports the truth of the statement."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context provides workforce representation data for Amazon in 2023, including gender breakdowns for 'People Managers'. Under the 'People Managers' row in the 'Gender—Global' section, the percentage of women is listed as 46.9% for 2023. This matches the required unit of 'percent'.","46.9","percent","[""amazon2023""]","is_blank","People Managers
Gender—Global
2023
46.9%52.9%","The context provides workforce representation data for Amazon in 2023, including gender breakdowns for 'People Managers'. Under the 'People Managers' row in the 'Gender—Global' section, the percentage of women is listed as 46.9% for 2023. This matches the required unit of 'percent'."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context states that when equity is excluded from R&D staff costs, the fraction of computing hardware costs rises to 61–76% of the total amortized cost for the four key models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra). This range is explicitly provided in the text and matches the question's requirement for percentage range.","[61, 76]","percent","[""cottier2024""]","is_blank","However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.","The context states that when equity is excluded from R&D staff costs, the fraction of computing hardware costs rises to 61–76% of the total amortized cost for the four key models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra). This range is explicitly provided in the text and matches the question's requirement for percentage range."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context states that accuracy and F1 scores are slightly lower after optimization in most cases, with only precision and recall showing minor increases in specific instances. This indicates that accuracy and F1 scores did not always improve — in fact, they generally declined. Therefore, the statement is false.","0","is_blank","[""khan2025""]","is_blank","Metrics such as F1 score and overall accuracy may decline slightly post-optimization, which could be critical for applications requiring high precision, such as medical diagnostics or financial modeling.","The context states that accuracy and F1 scores are slightly lower after optimization in most cases, with only precision and recall showing minor increases in specific instances. This indicates that accuracy and F1 scores did not always improve — in fact, they generally declined. Therefore, the statement is false."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The context from [ref_id=strubell2019] and [ref_id=patterson2021] states that the U.S. Environmental Protection Agency (EPA) provides the average CO2 produced in pounds per kilowatt-hour for power consumed in the U.S., and specifically cites a value of 0.954 lbs/kWh derived from the conversion formula CO2e = 0.954pt, where pt is in kilowatt-hours. This value is consistently referenced as the U.S. average CO2 emission factor.","0.954","lbs/kWh","[""strubell2019"", ""patterson2021""]","[""https://bit.ly/30sGEbi"", ""https://bit.ly/2LObQhV""]","The U.S. Environmental Protection Agency (EPA) provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. ... CO2e = 0.954pt (2)","The context from [ref_id=strubell2019] and [ref_id=patterson2021] states that the U.S. Environmental Protection Agency (EPA) provides the average CO2 produced in pounds per kilowatt-hour for power consumed in the U.S., and specifically cites a value of 0.954 lbs/kWh derived from the conversion formula CO2e = 0.954pt, where pt is in kilowatt-hours. This value is consistently referenced as the U.S. average CO2 emission factor."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context from ref_id=li2025b states that a recent study suggests global AI could consume 85 – 134 TWh of electricity in 2027. This matches the required unit (TWh) and provides a clear numeric range.","[85, 134]","TWh","[""li2025b""]","is_blank","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 [7]","The context from ref_id=li2025b states that a recent study suggests global AI could consume 85 – 134 TWh of electricity in 2027. This matches the required unit (TWh) and provides a clear numeric range."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context states that in 2023, Amazon delivered 150 million packages via EVs in Europe, as explicitly mentioned in the Europe section under 'Last Mile Electric Delivery Vehicles by Region'. The answer_unit is 'packages', so the numeric value is returned without units.","150","packages","[""amazon2023""]","is_blank","• We delivered 150 million packages via EVs.","The context states that in 2023, Amazon delivered 150 million packages via EVs in Europe, as explicitly mentioned in the Europe section under 'Last Mile Electric Delivery Vehicles by Region'. The answer_unit is 'packages', so the numeric value is returned without units."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The online inference workload evaluated 3000 requests, each with 128 input tokens and 512 output tokens. The total tokens per request is 128 + 512 = 640. Multiplying by 3000 requests gives 1,920,000 total tokens. The answer_unit is 'tokens', so the value is expressed as a number.","1920000","tokens","[""kim2025""]","is_blank","• Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. ... The workload evaluates a total of 3000 requests.","The online inference workload evaluated 3000 requests, each with 128 input tokens and 512 output tokens. The total tokens per request is 128 + 512 = 640. Multiplying by 3000 requests gives 1,920,000 total tokens. The answer_unit is 'tokens', so the value is expressed as a number."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context explicitly states that the AI Act fails to address greenhouse gas (GHG) emissions generated by AI applications, including those used in oil and gas exploration. Multiple snippets from ref_id=ebert2024 confirm this omission, noting it creates a 'notable reporting gap'. Therefore, the AI Act does not mandate such disclosure.","0","is_blank","[""ebert2024""]","is_blank","5) The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37].","The context explicitly states that the AI Act fails to address greenhouse gas (GHG) emissions generated by AI applications, including those used in oil and gas exploration. Multiple snippets from ref_id=ebert2024 confirm this omission, noting it creates a 'notable reporting gap'. Therefore, the AI Act does not mandate such disclosure."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context states that for the DenseNet 201 experiment, a short job, the Flexible Start optimization led to up to 80% CO2 emissions reduction in the West US region. This is explicitly mentioned in the caption of Figure 3 and reiterated in the text.","80","percent","[""dodge2022""]","is_blank","For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;","The context states that for the DenseNet 201 experiment, a short job, the Flexible Start optimization led to up to 80% CO2 emissions reduction in the West US region. This is explicitly mentioned in the caption of Figure 3 and reiterated in the text."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context states that the least energy-intensive task is text classification with a mean consumption of 0.002 kWh per 1,000 inferences, and the most energy-intensive task is image generation with a mean consumption of 2.9 kWh per 1,000 inferences. The ratio is calculated as 2.9 / 0.002 = 1450, and the text explicitly confirms that the variation is 'over 1450 in terms of the energy required'. The answer_unit is 'times', so the value is reported as a number.","1450","times","[""luccioni2024""]","is_blank","Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis. We can also observe that there is a large variation in the amount of energy used, from the least energy-intensive task, text classification, with mean consumption of 0.002 KwH per 1,000 inferences, to the most energy-intensive one, image generation, whose mean consumption is 2.9kWh. This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The context states that the least energy-intensive task is text classification with a mean consumption of 0.002 kWh per 1,000 inferences, and the most energy-intensive task is image generation with a mean consumption of 2.9 kWh per 1,000 inferences. The ratio is calculated as 2.9 / 0.002 = 1450, and the text explicitly confirms that the variation is 'over 1450 in terms of the energy required'. The answer_unit is 'times', so the value is reported as a number."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the higher energy costs of LLM deployment. This directly contradicts the claim that traditional models achieved comparable accuracy to LLMs.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that in sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the higher energy costs of LLM deployment. This directly contradicts the claim that traditional models achieved comparable accuracy to LLMs."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context states that as of January 2024, Amazon had announced 244 renewable energy projects in the United States, as listed in the 'Project Location' table with 'United States' having 244 projects and total MW capacity of 17,706. This is the only explicit count of projects announced in the U.S. as of January 2024.","244","projects","[""amazon2023""]","is_blank","Project Location
Number 
of Projects
Total MW 
Capacity†
... United States 244 17,706","The context states that as of January 2024, Amazon had announced 244 renewable energy projects in the United States, as listed in the 'Project Location' table with 'United States' having 244 projects and total MW capacity of 17,706. This is the only explicit count of projects announced in the U.S. as of January 2024."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context provides no information about Amazon's AWS renewable energy usage percentage in 2018. The earliest year mentioned for AWS or Amazon's renewable energy coverage is 2019 (42%), and no data is provided for 2018.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context provides no information about Amazon's AWS renewable energy usage percentage in 2018. The earliest year mentioned for AWS or Amazon's renewable energy coverage is 2019 (42%), and no data is provided for 2018."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The context from ref_id=ebert2024 explicitly recommends that energy consumption should be reported at the 'cumulative server level' to balance accuracy and feasibility, as this captures total computation-related power usage and allows for better optimization of AI models. The text also notes that estimations may be used only when direct measurements are unavailable, but the preferred and recommended level is the cumulative server level.","cumulative server level","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","Energy consumption should be reported at the cumulative server level (see also [4]). In this endeavor, estimations may be used only when direct measurements are unavailable.","The context from ref_id=ebert2024 explicitly recommends that energy consumption should be reported at the 'cumulative server level' to balance accuracy and feasibility, as this captures total computation-related power usage and allows for better optimization of AI models. The text also notes that estimations may be used only when direct measurements are unavailable, but the preferred and recommended level is the cumulative server level."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context from [ref_id=dodge2022] explicitly states that during the training of a BERT-base model on a single NVIDIA TITAN X GPU, the GPU accounted for 74% of the total electricity consumption, as shown in Table 1. This matches the required answer_unit of 'percent'.","74","percent","[""dodge2022""]","is_blank","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption due to these components.","The context from [ref_id=dodge2022] explicitly states that during the training of a BERT-base model on a single NVIDIA TITAN X GPU, the GPU accounted for 74% of the total electricity consumption, as shown in Table 1. This matches the required answer_unit of 'percent'."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context from ref_id=griggs2024 explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. The answer_unit specifies the unit as 'NVIDIA A100-80GB GPUs', so the answer_value is the numeric count.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context from ref_id=griggs2024 explicitly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. The answer_unit specifies the unit as 'NVIDIA A100-80GB GPUs', so the answer_value is the numeric count."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context repeatedly states that iterative hardware-software optimization led to a 28.5% operational energy footprint reduction over a two-year time period, specifically citing Figure 8 and Section III-B. This value is consistently mentioned across multiple snippets as the net effect of optimizations despite increasing AI infrastructure demand.","28.5","percent","[""wu2021a""]","is_blank","The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).","The context repeatedly states that iterative hardware-software optimization led to a 28.5% operational energy footprint reduction over a two-year time period, specifically citing Figure 8 and Section III-B. This value is consistently mentioned across multiple snippets as the net effect of optimizations despite increasing AI infrastructure demand."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The provided context discusses energy consumption, carbon emissions, water usage, and health impacts related to data centers and AI training, but it does not mention any data regarding the installation of fiber optic cable globally in 2023. Therefore, there is no supporting evidence to determine the number of kilometers of fiber optic cable installed.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The provided context discusses energy consumption, carbon emissions, water usage, and health impacts related to data centers and AI training, but it does not mention any data regarding the installation of fiber optic cable globally in 2023. Therefore, there is no supporting evidence to determine the number of kilometers of fiber optic cable installed."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context from [ref_id=cottier2024] explicitly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800M, distinguishing it from the $40M amortized hardware CapEx + energy cost. This value is directly provided as the acquisition cost based on hardware purchase prices and training hardware quantities.","800000000","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context from [ref_id=cottier2024] explicitly states that the estimated upfront hardware acquisition cost to train GPT-4 is $800M, distinguishing it from the $40M amortized hardware CapEx + energy cost. This value is directly provided as the acquisition cost based on hardware purchase prices and training hardware quantities."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context states that 'Sustainable AI' was proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. This means it encompasses two aspects, not just one. Therefore, the statement that it was proposed to only encompass using AI in climate-positive applications is false.","0","is_blank","[""luccioni2025b""]","is_blank","The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a ﬁeld of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].","The context states that 'Sustainable AI' was proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. This means it encompasses two aspects, not just one. Therefore, the statement that it was proposed to only encompass using AI in climate-positive applications is false."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context states that for NVIDIA A100-40GB GPU, the maximum batch size supported for fine-tuning Mixtral is empirically determined and shown in Table III and Fig. 13. Table III lists the maximum batch size for Mixtral-D (dense) on CS dataset as 2 and on MATH as 1, but these are dataset-specific. Fig. 13 and the surrounding text project maximum batch sizes for different GPUs, including A100-40GB, and while the exact number isn't explicitly stated for A100-40GB in the projection, the ground truth values are referenced in Fig. 13. However, the exact numerical value for A100-40GB is not directly provided in the text. The only explicit batch size values given for A100-40GB are inferred from the figure caption and context, but no numeric value is listed for it in the text. Thus, the ground truth value cannot be extracted directly.","is_blank","samples","[""xia2024""]","is_blank","Fig. 13 shows our projected maximum batch size and correlate it with experimented ground truth. While the maximum memory capacity available in NVIDIA GPUs today is 80GB, we use our analytical model to project the maximum batch size that future GPUs might support. For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.","The context states that for NVIDIA A100-40GB GPU, the maximum batch size supported for fine-tuning Mixtral is empirically determined and shown in Table III and Fig. 13. Table III lists the maximum batch size for Mixtral-D (dense) on CS dataset as 2 and on MATH as 1, but these are dataset-specific. Fig. 13 and the surrounding text project maximum batch sizes for different GPUs, including A100-40GB, and while the exact number isn't explicitly stated for A100-40GB in the projection, the ground truth values are referenced in Fig. 13. However, the exact numerical value for A100-40GB is not directly provided in the text. The only explicit batch size values given for A100-40GB are inferred from the figure caption and context, but no numeric value is listed for it in the text. Thus, the ground truth value cannot be extracted directly."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context explicitly states that when the server is overloaded, Swapping consistently consumes less energy than Recomputation because Recomputation requires extra computation upon restoration, while Swapping only involves data movement, which consumes less energy.","Swapping","is_blank","[""chung2025""]","is_blank","It can be seen that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.","The context explicitly states that when the server is overloaded, Swapping consistently consumes less energy than Recomputation because Recomputation requires extra computation upon restoration, while Swapping only involves data movement, which consumes less energy."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context from ref_id=luccioni2025a explicitly states that Google reports a 48% increase in GHG emissions since 2019, attributing it to increases in data center energy consumption. The answer_unit is 'percent', so the value is returned as a number without the unit name.","48","percent","[""luccioni2025a""]","[""https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf""]","For example, in their 2024 annual environmental sustainability report (ESG), Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to ""increases in data center energy consumption"" [42]","The context from ref_id=luccioni2025a explicitly states that Google reports a 48% increase in GHG emissions since 2019, attributing it to increases in data center energy consumption. The answer_unit is 'percent', so the value is returned as a number without the unit name."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context cites McKinsey & Company's report (ref_id=chung2025) and other sources (ref_id=fernandez2025, ref_id=morrison2025) that project data centers will consume between 9.1% and 11.7% of total U.S. energy demand by 2030. While McKinsey is mentioned, the specific percentage range (9.1–11.7%) is attributed to multiple sources including Aljbour et al., Shehabi et al., and Green et al., which are cited alongside McKinsey. Since the exact McKinsey projection is not isolated, the range from the combined context is used as the best available estimate.","[9.1, 11.7]","percent","[""chung2025"", ""fernandez2025"", ""morrison2025""]","[""https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/investing-in-the-rising-data-center-economy""]","Projections estimate that data centers consume between 9.1% and 11.7% of the total US energy demand by 2030 (Aljbour et al., 2024; Shehabi et al., 2024; Green et al., 2024).","The context cites McKinsey & Company's report (ref_id=chung2025) and other sources (ref_id=fernandez2025, ref_id=morrison2025) that project data centers will consume between 9.1% and 11.7% of total U.S. energy demand by 2030. While McKinsey is mentioned, the specific percentage range (9.1–11.7%) is attributed to multiple sources including Aljbour et al., Shehabi et al., and Green et al., which are cited alongside McKinsey. Since the exact McKinsey projection is not isolated, the range from the combined context is used as the best available estimate."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context states that for the 6 billion parameter transformer training run, which lasted 8 days, doubling the duration via Pause and Resume optimization can lead to significant emissions savings of up to about 25%. This is explicitly mentioned in Figure 4 and supporting text, and confirmed in Table 5 where the Pause and Resume optimization for the 6B transformer shows an 11.4% reduction under 100% duration increase, but the maximum potential saving is capped at 25% as stated in the figure caption.","25","percent","[""dodge2022""]","is_blank","Fig. 4. What proportion of emissions can we expect to save if we pause an AI workload when emissions in a region are high and resume when emissions are low, increasing the total duration by up to double the original duration? ... doubling the duration can lead to significant savings up to about 25%.","The context states that for the 6 billion parameter transformer training run, which lasted 8 days, doubling the duration via Pause and Resume optimization can lead to significant emissions savings of up to about 25%. This is explicitly mentioned in Figure 4 and supporting text, and confirmed in Table 5 where the Pause and Resume optimization for the 6B transformer shows an 11.4% reduction under 100% duration increase, but the maximum potential saving is capped at 25% as stated in the figure caption."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B, as mentioned in multiple snippets. The answer_unit is 'percent', so the value is returned as a number without the unit name.","70","percent","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context explicitly states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B, as mentioned in multiple snippets. The answer_unit is 'percent', so the value is returned as a number without the unit name."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The seminal 2019 study by Strubell et al. quantified the carbon footprint of training BERT as 626,155 pounds of CO2 emissions, as cited in both [ref_id=luccioni2025b] and [ref_id=luccioni2023]. The answer_unit is specified as 'lbs CO2e', so the numeric value is directly extracted.","626155","lbs CO2e","[""luccioni2025b"", ""luccioni2023""]","is_blank","which quantiﬁed the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].","The seminal 2019 study by Strubell et al. quantified the carbon footprint of training BERT as 626,155 pounds of CO2 emissions, as cited in both [ref_id=luccioni2025b] and [ref_id=luccioni2023]. The answer_unit is specified as 'lbs CO2e', so the numeric value is directly extracted."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context from ref_id=li2025b explicitly states that GPT-3 was trained with an estimated training energy of 1287 MWh. This value is directly provided and matches the required unit (MWh).","1287","MWh","[""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context from ref_id=li2025b explicitly states that GPT-3 was trained with an estimated training energy of 1287 MWh. This value is directly provided and matches the required unit (MWh)."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context explicitly states that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning and that optimizing the MoE layer is crucial for improving performance. This directly supports that the MoE layer is a targeted layer for enhancing performance.","1","is_blank","[""xia2024""]","is_blank","Our study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fine-tuning.","The context explicitly states that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning and that optimizing the MoE layer is crucial for improving performance. This directly supports that the MoE layer is a targeted layer for enhancing performance."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context states in Table III that the maximum batch size supported by Mixtral-D (dense) on the Hellaswag (HE) dataset using an NVIDIA A40 GPU is 2. This value is explicitly listed under the 'CS' column (Hellaswag is referred to as CS in the context) for Mixtral-D.","2","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE AND S:SPARSE .
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20","The context states in Table III that the maximum batch size supported by Mixtral-D (dense) on the Hellaswag (HE) dataset using an NVIDIA A40 GPU is 2. This value is explicitly listed under the 'CS' column (Hellaswag is referred to as CS in the context) for Mixtral-D."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context explicitly states that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7% compared to the fully local experiment (A-8). This directly supports the truth of the statement.","1","is_blank","[""erben2023""]","is_blank","In summary, while local compute is the best choice for maximum throughput, for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%.","The context explicitly states that for high granularity tasks like CV, distributing VMs over four continents only slows down performance by 7% compared to the fully local experiment (A-8). This directly supports the truth of the statement."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The provided context discusses energy consumption, water footprint, air pollution, and health impacts of AI and data centers, but does not mention any data regarding the land area (in hectares) occupied by new AI data centers globally in 2022.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The provided context discusses energy consumption, water footprint, air pollution, and health impacts of AI and data centers, but does not mention any data regarding the land area (in hectares) occupied by new AI data centers globally in 2022."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context from ref_id=luccioni2024 explicitly states that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries in text classification. This matches the required answer_unit of 'g CO2eq', so the numeric value is directly extracted.","0.32","g CO2eq","[""luccioni2024""]","is_blank","for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The context from ref_id=luccioni2024 explicitly states that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries in text classification. This matches the required answer_unit of 'g CO2eq', so the numeric value is directly extracted."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context from ref_id=dodge2022 states that for training BERT on 8 V100 GPUs for 36 hours, the carbon emissions range from 7k grams in the most efficient region to 26k grams in the least efficient region. This directly provides the range in grams as requested.","[7000, 26000]","grams","[""dodge2022""]","is_blank","There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful (7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","The context from ref_id=dodge2022 states that for training BERT on 8 V100 GPUs for 36 hours, the carbon emissions range from 7k grams in the most efficient region to 26k grams in the least efficient region. This directly provides the range in grams as requested."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context states that 'The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information,' and that this was followed by a decline due to the rise of commercial and proprietary models after 2022. This directly identifies 2022 as the peak year.","2022","year","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","The context states that 'The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information,' and that this was followed by a decline due to the rise of commercial and proprietary models after 2022. This directly identifies 2022 as the peak year."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context from [ref_id=morrison2025] states that training the OLMo 60M model consumed 1.6 kL of water, which is equivalent to 5 days of water usage by the average person in the U.S. This directly provides the answer in the required unit of days.","5","days","[""morrison2025""]","[""https://www.epa.gov/watersense/statistics-and-facts""]","OLMo 60M† 1.2 0.4 1 month 1.6 5 days","The context from [ref_id=morrison2025] states that training the OLMo 60M model consumed 1.6 kL of water, which is equivalent to 5 days of water usage by the average person in the U.S. This directly provides the answer in the required unit of days."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context mentions that DeepMind AI reduced Google's data center cooling bill by 40%, but it does not provide any specific data on freshwater consumption by DeepMind's AlphaFold servers in 2023. No numeric value or estimate is given for AlphaFold's water usage.","is_blank","liters of freshwater","[""wu2021b""]","[""https://deepmind.com/blog/article/deepmind-ai-reduces-google-data-centre-cooling-bill-40""]","DeepMind AI Reduces Google Data Centre Cooling Bill by 40%.","The context mentions that DeepMind AI reduced Google's data center cooling bill by 40%, but it does not provide any specific data on freshwater consumption by DeepMind's AlphaFold servers in 2023. No numeric value or estimate is given for AlphaFold's water usage."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that 'a way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers,' implying that adding compute resources reduces cost, not increases it. Therefore, the statement that adding compute resources increases costs is false.","0","is_blank","[""xia2024""]","is_blank","A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.","The context states that 'a way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers,' implying that adding compute resources reduces cost, not increases it. Therefore, the statement that adding compute resources increases costs is false."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The context states that the energy consumption for training FLM-101B is 40 MkWh (mega-kilowatt-hours), which equals 40,000,000 kWh. This is derived from Table 3, which lists 'Energy (MkWh)' for FLM-101B as 40. Since the required unit is kWh, the value is converted accordingly.","40000000","kWh","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. ... Energy (MkWh) ... FLM-101B 40","The context states that the energy consumption for training FLM-101B is 40 MkWh (mega-kilowatt-hours), which equals 40,000,000 kWh. This is derived from Table 3, which lists 'Energy (MkWh)' for FLM-101B as 40. Since the required unit is kWh, the value is converted accordingly."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context provides projections for the public health burden of U.S. data centers in 2028, estimating up to more than $20 billion under a high-growth scenario. However, no projection for 2030 is mentioned in any of the provided snippets. Therefore, the requested value for 2030 cannot be determined from the context.","is_blank","USD","[""han2024""]","is_blank","is_blank","The context provides projections for the public health burden of U.S. data centers in 2028, estimating up to more than $20 billion under a high-growth scenario. However, no projection for 2030 is mentioned in any of the provided snippets. Therefore, the requested value for 2030 cannot be determined from the context."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context provides a table (TABLE IV) that estimates the cost of fine-tuning Mixtral on the GSM8K (GS) dataset with sparse MoE using an NVIDIA A40-48GB GPU, listing the total cost as $32.7. This matches the question's requirement for the total cost in USD.","32.7","USD","[""xia2024""]","is_blank","TABLE IV
ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE
BASED ON OUR ANALYTICAL MODEL
GPU Mem MBS Throughput Cost ($/hr) Cost ($)
A40 48GB 4 1.01 0.79 32.7","The context provides a table (TABLE IV) that estimates the cost of fine-tuning Mixtral on the GSM8K (GS) dataset with sparse MoE using an NVIDIA A40-48GB GPU, listing the total cost as $32.7. This matches the question's requirement for the total cost in USD."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context states that for Mixtral with sparse MoE on NVIDIA A40 (48GB), the maximum batch size supported is 8 samples (Table III). The MoE layer is the most time-consuming component, and the longest-running MoE layer would occur at the maximum batch size. Thus, the batch size for the longest-running MoE layer is 8 samples.","8","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE AND S:SPARSE .
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20","The context states that for Mixtral with sparse MoE on NVIDIA A40 (48GB), the maximum batch size supported is 8 samples (Table III). The MoE layer is the most time-consuming component, and the longest-running MoE layer would occur at the maximum batch size. Thus, the batch size for the longest-running MoE layer is 8 samples."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context provides execution time breakdowns for Mixtral fine-tuning with batch sizes of 1 and maximum batch size, but does not specify the exact total execution time for a sparse Mixtral model with batch size 10 on an A40-48GB GPU. While Table III and Fig. 4 show batch sizes and execution time components, no direct value for this specific configuration is given.","is_blank","seconds","[""xia2024""]","is_blank","Fig. 4 illustrates execution time breakdown for Mixtral with batch sizes of 1 and maximum batch size, but no value is given for batch size 10 on A40-48GB.","The context provides execution time breakdowns for Mixtral fine-tuning with batch sizes of 1 and maximum batch size, but does not specify the exact total execution time for a sparse Mixtral model with batch size 10 on an A40-48GB GPU. While Table III and Fig. 4 show batch sizes and execution time components, no direct value for this specific configuration is given."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context explicitly states that GPU-level power consumption monitoring substantially under-represents actual energy consumption because it measures only a single component, and advocates against using GPU-level or other component-based tracking for overall energy measurements. Therefore, it is not recommended as the preferred method.","0","is_blank","[""ebert2024""]","is_blank","We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.","The context explicitly states that GPU-level power consumption monitoring substantially under-represents actual energy consumption because it measures only a single component, and advocates against using GPU-level or other component-based tracking for overall energy measurements. Therefore, it is not recommended as the preferred method."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context from ref_id=luccioni2025b states that in a 2023 article estimating the carbon footprint of BLOOM, training accounted for only half of the model’s overall emissions. This directly provides the percentage as 50%.","50","percent","[""luccioni2025b""]","is_blank","In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, since it takes into account different stages of the model life cycle including the manufacturing of computing hardware, idle energy usage, and model deployment, finding that training accounted for only half of the model’s overall emissions [121]","The context from ref_id=luccioni2025b states that in a 2023 article estimating the carbon footprint of BLOOM, training accounted for only half of the model’s overall emissions. This directly provides the percentage as 50%."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context from [ref_id=samsi2024] Table II explicitly states that the bare minimum hardware requirement for LLaMA-7B inference on V100 32GB GPUs is 1 GPU, with a maximum batch size of 64 and a maximum generation length of 256. No compression or quantization is assumed, matching the question's conditions.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64","The context from [ref_id=samsi2024] Table II explicitly states that the bare minimum hardware requirement for LLaMA-7B inference on V100 32GB GPUs is 1 GPU, with a maximum batch size of 64 and a maximum generation length of 256. No compression or quantization is assumed, matching the question's conditions."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context states that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10–50 medium-length responses, depending on deployment location. Although the data center location (Arizona) is mentioned in the context as having specific water efficiency characteristics, no location-specific adjustment to the 10–50 range is provided. Thus, the baseline range from the context applies.","[10, 50]","requests","[""li2025b""]","is_blank","Additionally, GPT-3 needs to “drink” (i.e., consume) a500ml bottle of waterfor roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10–50 medium-length responses, depending on deployment location. Although the data center location (Arizona) is mentioned in the context as having specific water efficiency characteristics, no location-specific adjustment to the 10–50 range is provided. Thus, the baseline range from the context applies."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context from ref_id=dodge2022 explicitly states that the emissions from one year of average US home energy use (including electricity, natural gas, liquid petroleum gas, and fuel oil) total 8.3 metric tons of CO2. This matches the required answer unit of 'metric tons'.","8.3","metric tons","[""dodge2022""]","is_blank","one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)","The context from ref_id=dodge2022 explicitly states that the emissions from one year of average US home energy use (including electricity, natural gas, liquid petroleum gas, and fuel oil) total 8.3 metric tons of CO2. This matches the required answer unit of 'metric tons'."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context from ref_id=shen2024 includes Table 3, which shows the performance of JetMoE-8B on the GSM8k benchmark with a score of 27.8. This score represents the model's performance on grade school math problem-solving, which is the exact metric asked for in the question.","27.8","score","[""shen2024""]","is_blank","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. ... GSM8k 14.5 17.3 16.9 27.8","The context from ref_id=shen2024 includes Table 3, which shows the performance of JetMoE-8B on the GSM8k benchmark with a score of 27.8. This score represents the model's performance on grade school math problem-solving, which is the exact metric asked for in the question."
