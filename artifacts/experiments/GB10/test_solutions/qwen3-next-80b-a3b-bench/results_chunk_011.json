[
  {
    "id": "q317",
    "question": "What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?",
    "gt_value": "4",
    "gt_unit": "seconds",
    "gt_ref": "['xia2024']",
    "pred_value": "is_blank",
    "pred_unit": "seconds",
    "pred_ref": "[\"xia2024\"]",
    "pred_explanation": "The context provides execution time breakdowns for Mixtral fine-tuning with batch sizes of 1 and maximum batch size, but does not specify the exact total execution time for a sparse Mixtral model with batch size 10 on an A40-48GB GPU. While Table III and Fig. 4 show batch sizes and execution time components, no direct value for this specific configuration is given.",
    "raw_response": "{\n  \"explanation\": \"The context provides execution time breakdowns for Mixtral fine-tuning with batch sizes of 1 and maximum batch size, but does not specify the exact total execution time for a sparse Mixtral model with batch size 10 on an A40-48GB GPU. While Table III and Fig. 4 show batch sizes and execution time components, no direct value for this specific configuration is given.\",\n  \"answer\": \"The total execution time for a sparse Mixtral model fine-tuned with batch size 10 on an A40-48GB GPU is not explicitly provided in the context.\",\n  \"answer_value\": \"is_blank\",\n  \"ref_id\": [\"xia2024\"],\n  \"ref_url\": [],\n  \"supporting_materials\": \"Fig. 4 illustrates execution time breakdown for Mixtral with batch sizes of 1 and maximum batch size, but no value is given for batch size 10 on A40-48GB.\"\n}",
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true,
    "weighted_score": 0.25,
    "latency_seconds": 270.3035261631012,
    "retrieval_seconds": 19.657551765441895,
    "generation_seconds": 89.88864254951477,
    "error": null,
    "pred_ref_url": [],
    "pred_supporting_materials": "Fig. 4 illustrates execution time breakdown for Mixtral with batch sizes of 1 and maximum batch size, but no value is given for batch size 10 on A40-48GB.",
    "rendered_prompt": "You will answer a question using ONLY the provided context snippets.\nIf the context does not clearly support an answer, use \"is_blank\".\n\nContext snippets from documents:\n[ref_id=xia2024] For example, our model predicted that fine-\ntuning a sparse Mixtral model using a realistic data size of\n2M queries can be done with NVIDIA H100 GPU with a cost\nof $3460.\n---\n[ref_id=xia2024] 0 2 4 6 8 100.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n RMSE=0.05\nMixtral-CS\nDense Sparse\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n RMSE=0.02\nMixtral-MATH\n0 5 10 15 200\n2\n4\n6\n8\n10\n12\n14\n16\n RMSE=0.79\nMamba-CS\n0 2 4 6 8 10 120\n2\n4\n6\n8\n10\n12\n RMSE=0.42\nThroughput (queries/sec)\nBatch size\nMamba-MATH\nFig. 14. Estimation and validation of LLM fine-tuning throughput for\ndifferent models, datasets for A40 GPU. Dots represent ground truth and\nlines present the estimation.\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nRMSE=0.03\nMixtral-CS-A100-40GB\nDense Sparse\n0 5 10 15 200.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n RMSE=0.09\nMixtral-CS-A100-80GB\n0 5 10 15 200\n1\n2\n3\n4\n5\n RMSE=0.55\nMixtral-CS-H100\nThroughput (queries/sec)\nBatch size\nFig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS\nfor different GPUs: A100 and H100.\nresource renting per hour is calculated based on CUDO\ncompute [33], as other popular cloud providers do not offer\ncost/hour rates for the NVIDIA A40 GPU. However, one\ncan easily adjust the GPU renting cost per hour to estimate\nthe cost on other clouds such as Amazon AWS [34] or\nLambda [35]. Table IV estimates the cost for fine-tuning\nMixtral on the MATH dataset with a sparse setup, using\n10 epochs on different GPUs for a realistic cost estimate.\nEnterprises may use larger datasets for fine-tuning, such as,\nOpenOrca [36] and LaMini-instruction [37] containing more\nthan 2M queries. For OpenOrca, by scaling the cost by\nnumber of queries, our model predicts that the most cost-\neffective option to rent GPU resources on CUDO compute\nis NVIDIA H100 with a net cost of $3460.\nD. Generalization of the Analytical Model\nThe analytical models for estimating maximum batch size\nand throughput can be generalized to various LLM models\nand datasets. These models consider the characteristics of\nthe LLM, dataset, and GPU. Specifically, the maximum\nbatch size model combines GPU memory and LLM model\nsize to determine available memory for input data, while\ndataset sequence length and LLM sparsity determine space\nneeded per batch. In throughput estimation, based on the\nobservation we made (\u00a7IV-B4 Takeaway 5), GPU shifts from\nmemory-bound to compute-bound as batch size increases.\nThis characteristic generally applies to all GPUs due to the\nresource constraint, so the logarithmic relation between batch\nsize and throughput persists. The sparsity in (2) is model\ndependent, the influence of GPU, LLM model, and dataset\nare embedded in the coefficients C2, C3, and C4 in (2).\nThe coefficients in (1) and (2) are dependent on GPU,\nLLM model, and dataset; however, the underlying models\nare generalizable to unseen GPU, LLM model, and datasets.\nAlthough it takes some effort to sweep batch sizes and collect\nthroughput data points to fit our models, the benefits greatly\noutweigh the cost. Once the models are fit, our model can\nhelp choose the most cost-efficient GPU for fine-tuning LLM\nmodels, greatly saving resources and money.\nVI. R ELATED WORKS\nParameter-Efficient Fine-Tuning (PEFT) has been widely\nadopted to fine-tune LLM model for specialized tasks [15],\n[38]\u2013[43]. MoE additioally train specialized experts for dif-\nferent areas and the dynamic selection of experts makes\nit possible to scale the fine-tuning workload to different\nexperts in parallel. [44]\u2013[47] show that MoE models can\nimprove the ability to process knowledge for specific tasks,\nwhile maintaining the world knowledge in LLM. Kim et\nal. [48] construct an analytical model to estimate GPU\nmemory consumption for distributed fine-tuning. The model\nalso provides insights into optimizing memory usage through\ntensor, model, and pipeline parallelism.\nVII. C ONCLUSIONS\nFine-tuning LLMs is an attractive technique for tailoring\nmodern language models using domain-specific knowledge in\na cost-effective manner. This paper delved into understanding\nthe performance of fine-tuning MoE LLM models on a single\nGPU. Our profiling demonstrated that sparse MoE layers\noffer the best bang-for-buck trade-off. Using our profiling\nresults, we developed and validated an accurate analytical\nmodel to estimate the cost of LLM fine-tuning. Using this\nmodel, we showed the dollar amount that needs to be\nbudgeted for fine-tuning LLMs, which is much lower than\npre-training. For example, our model predicted that fine-\ntuning a sparse Mixtral model using a realistic data size of\n2M queries can be done with NVIDIA H100 GPU with a cost\nof $3460. A way to further reduce cost based on our study\nis to add compute resources to accelerate the MoE layers.\nWhile we showcase our study on fine-tuning LLMs using a\nsingle GPU, extending this model to multi-GPU systems is\nleft for future exploration.\nACKNOWLEDGMENTS\nThis work was supported in part by Semiconductor Re-\nsearch Corporation (SRC). We thank all the anonymous\nreviewers for their valuable comments and suggestions.\n---\n[ref_id=xia2024] under 10% on BlackMamba. We assess accuracy improve-\nments post-fine-tuning and compare the learning capabilities\nof dense and sparse versions of both models.\nFig. 3 shows the testing accuracy of Mixtral and Black-\nMamba on two datasets Hellaswag (HE) and GSM8K (GS).\nWe fine-tune both models using the sparse and dense setups\ndescribed in \u00a7III for 10 epochs, and test the accuracy of\nthe fine-tuned model at each epoch. We make the following\nobservations in Fig. 3. (1) Fine-tuning converges relatively\nquickly. Typically, 10 epochs are enough for fine-tune models\nto stabilize at or close to their peak accuracy. On GS, both\nmodels are close to their peak accuracy at the first epoch.\n(2) The smaller model BlackMamba takes relatively more\nepochs to reach its peak accuracy, as it took BlackMamba 5\nepochs to converge on HE. (3) The larger model Mixtral has\nbetter accuracy compared to BlackMamba on both datasets.\n(4) Both models perform better on the CS dataset HE than\non the GS dataset GS. This is because math is harder for\nsmaller LLMs to learn [24]. The BlackMamba model is\ninadequate for fine-tuning GS. This is likely attributed to\nthe complexity of mathematical tasks and the smaller model\nsize of BlackMamba. Additionally, Mamba is specifically\nengineered for long sequence modeling, potentially resulting\nin unsatisfactory arithmetic reasoning ability [25]. Thus, in\nour characterization study in later sections, we will not show\nthe results for BlackMamba fine-tuned on MATH. (5) The\nperformance of sparse fine-tuning is close to that of dense\nfine-tuning, with the exception of Mixtral on HE. However,\neven for this outlier, sparse fine-tuning achieves similar peak\naccuracy compared to dense; we see a drop of accuracy\nbetween the epoch 4 and 5, and indicates sparse fine-tuning is\nmore vulnerable to over-fitting, especially for easy tasks [26].\nFollowing the above insights, the key take-away of this\nanalysis can be summarized as follows.\nTakeaway 1. Sparse model can be trained as well\nas its dense counterpart.\nTakeaway 2. Fine-tuning generally takes less ten\nepochs to reach peak accuracy.\nB. Analysis of Runtime Performance\nAfter confirming that both Mixtral and BlackMamba can\nbe fine-tuned to achieve acceptable accuracy, we examine\ntheir performance in a resource-constrained environment us-\ning a single GPU. This setup highlights unique runtime char-\nacteristics such as execution time breakdown, throughput,\nmaximum batch size, compute and memory utilization, load\nimbalance, and sensitivity analysis. We also compare sparse\nand dense models. Insights from this study will help develop\na robust analytical model for estimating fine-tuning costs.\n1) Maximum Batch Size Support: The maximum batch\nsize in fine-tuning is determined by GPU memory size,\nmodel size, sequence length, and MoE sparsity. The LLM\nTABLE III\nMAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE\nAND S:SPARSE .\nMixtral-D Mixtral-S BlackMamba-D BlackMamba-S\nCS 2 8 6 20\nMATH 1 3 2 8\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)\n0.0\n2.0\n4.0\n6.0\n8.0\nForward Backward Optimizer\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)\n0.0\n0.5\n1.0\n1.5\n2.0\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig. 4. Execution time breakdown.\noccupies a certain amount of GPU memory, with the re-\nmainder available for intermediate data during fine-tuning.\nLonger sequence lengths consume more memory, and denser\nMoE configurations require additional memory space. We\ndiscuss the heuristic for determining the maximum batch size\nin \u00a7V. Based on our experimental study on NVIDIA A40\nGPU with 48GB memory, we empirically find and report\nthe maximum batch size supported by different model and\ndataset combinations in Table III.\n2) Execution Time Breakdown: We first analyze the high-\nlevel execution time breakdown for Mixtral and Black-\nMamba. The purpose of this study is to understand where\ndoes this workload spend most of its time. As discussed in\n\u00a7III, we conduct this study using a sequence length of 128.\nAt a high-level, the fine-tuning workload can be divided\ninto three stages: (1) forward, (2) backward, and (3) opti-\nmizer. We use a batch size of 1 and the maximum batch size\nsupported by a model-dataset combination to show workload\ncharacteristics. Fig. 4 illustrates the following insights. (1)\nThe optimizer stage in BlackMamba fine-tuning takes a\nconsiderable portion of the running time (up to 53% when\nconducting sparse fine-tuning with batch size = 1), while\nthe execution time share of the optimizer stage in Mixtral\nfine-tuning is negligible. The running time of the optimizer\nstage depends only on the number of parameters that need\nto be updated during fine-tuning. This difference is primarily\ndue to the different fine-tuning strategies applied to these two\nmodels: only the parameters in the LoRA module are updated\nfor Mixtral fine-tuning, whereas BlackMamba undergoes full\nfine-tuning. (2) The runtime of the forward and backward\nstages increases with sparsity and batch size due to the\nincreased amount of computation. (3) The backward stage\ntypically takes more time than the forward stage. In Black-\nMamba, the backward stage demands more computation than\n---\n[ref_id=xia2024] Page 4\n---\n[ref_id=xia2024] under 10% on BlackMamba.\n---\n[ref_id=xia2024] We assess accuracy improve-\nments post-fine-tuning and compare the learning capabilities\nof dense and sparse versions of both models.\n---\n[ref_id=xia2024] Fig.\n---\n[ref_id=xia2024] 3 shows the testing accuracy of Mixtral and Black-\nMamba on two datasets Hellaswag (HE) and GSM8K (GS).\n---\n[ref_id=xia2024] We fine-tune both models using the sparse and dense setups\ndescribed in \u00a7III for 10 epochs, and test the accuracy of\nthe fine-tuned model at each epoch.\n---\n[ref_id=xia2024] We make the following\nobservations in Fig.\n---\n[ref_id=xia2024] 3.\n---\n[ref_id=xia2024] (1) Fine-tuning converges relatively\nquickly.\n---\n[ref_id=xia2024] Typically, 10 epochs are enough for fine-tune models\nto stabilize at or close to their peak accuracy.\n---\n[ref_id=xia2024] On GS, both\nmodels are close to their peak accuracy at the first epoch.\n---\n[ref_id=xia2024] (2) The smaller model BlackMamba takes relatively more\nepochs to reach its peak accuracy, as it took BlackMamba 5\nepochs to converge on HE.\n---\n[ref_id=xia2024] (3) The larger model Mixtral has\nbetter accuracy compared to BlackMamba on both datasets.\n---\n[ref_id=xia2024] (4) Both models perform better on the CS dataset HE than\non the GS dataset GS.\n---\n[ref_id=xia2024] This is because math is harder for\nsmaller LLMs to learn [24].\n---\n[ref_id=xia2024] The BlackMamba model is\ninadequate for fine-tuning GS.\n---\n[ref_id=xia2024] This is likely attributed to\nthe complexity of mathematical tasks and the smaller model\nsize of BlackMamba.\n---\n[ref_id=xia2024] Additionally, Mamba is specifically\nengineered for long sequence modeling, potentially resulting\nin unsatisfactory arithmetic reasoning ability [25].\n---\n[ref_id=xia2024] Thus, in\nour characterization study in later sections, we will not show\nthe results for BlackMamba fine-tuned on MATH.\n---\n[ref_id=xia2024] (5) The\nperformance of sparse fine-tuning is close to that of dense\nfine-tuning, with the exception of Mixtral on HE.\n---\n[ref_id=xia2024] However,\neven for this outlier, sparse fine-tuning achieves similar peak\naccuracy compared to dense;\n---\n[ref_id=xia2024] we see a drop of accuracy\nbetween the epoch 4 and 5, and indicates sparse fine-tuning is\nmore vulnerable to over-fitting, especially for easy tasks [26].\n---\n[ref_id=xia2024] Following the above insights, the key take-away of this\nanalysis can be summarized as follows.\n---\n[ref_id=xia2024] Takeaway 1.\n---\n[ref_id=xia2024] Sparse model can be trained as well\nas its dense counterpart.\n---\n[ref_id=xia2024] Takeaway 2.\n---\n[ref_id=xia2024] Fine-tuning generally takes less ten\nepochs to reach peak accuracy.\n---\n[ref_id=xia2024] B.\n---\n[ref_id=xia2024] Analysis of Runtime Performance\nAfter confirming that both Mixtral and BlackMamba can\nbe fine-tuned to achieve acceptable accuracy, we examine\ntheir performance in a resource-constrained environment us-\ning a single GPU.\n---\n[ref_id=xia2024] This setup highlights unique runtime char-\nacteristics such as execution time breakdown, throughput,\nmaximum batch size, compute and memory utilization, load\nimbalance, and sensitivity analysis.\n---\n[ref_id=xia2024] We also compare sparse\nand dense models.\n---\n[ref_id=xia2024] Insights from this study will help develop\na robust analytical model for estimating fine-tuning costs.\n---\n[ref_id=xia2024] 1) Maximum Batch Size Support: The maximum batch\nsize in fine-tuning is determined by GPU memory size,\nmodel size, sequence length, and MoE sparsity.\n---\n[ref_id=xia2024] The LLM\nTABLE III\nMAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ;\n---\n[ref_id=xia2024] D: DENSE\nAND S:SPARSE .\n---\n[ref_id=xia2024] Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S\nCS 2 8 6 20\nMATH 1 3 2 8\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)\n0.0\n2.0\n4.0\n6.0\n8.0\nForward Backward Optimizer\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)\n0.0\n0.5\n1.0\n1.5\n2.0\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig.\n---\n[ref_id=xia2024] 4.\n---\n[ref_id=xia2024] Execution time breakdown.\n---\n[ref_id=xia2024] occupies a certain amount of GPU memory, with the re-\nmainder available for intermediate data during fine-tuning.\n---\n[ref_id=xia2024] Longer sequence lengths consume more memory, and denser\nMoE configurations require additional memory space.\n---\n[ref_id=xia2024] We\ndiscuss the heuristic for determining the maximum batch size\nin \u00a7V.\n---\n[ref_id=xia2024] Based on our experimental study on NVIDIA A40\nGPU with 48GB memory, we empirically find and report\nthe maximum batch size supported by different model and\ndataset combinations in Table III.\n---\n[ref_id=xia2024] 2) Execution Time Breakdown: We first analyze the high-\nlevel execution time breakdown for Mixtral and Black-\nMamba.\n---\n[ref_id=xia2024] The purpose of this study is to understand where\ndoes this workload spend most of its time.\n---\n[ref_id=xia2024] As discussed in\n\u00a7III, we conduct this study using a sequence length of 128.\n---\n[ref_id=xia2024] At a high-level, the fine-tuning workload can be divided\ninto three stages: (1) forward, (2) backward, and (3) opti-\nmizer.\n---\n[ref_id=xia2024] We use a batch size of 1 and the maximum batch size\nsupported by a model-dataset combination to show workload\ncharacteristics.\n---\n[ref_id=xia2024] Fig.\n---\n[ref_id=xia2024] 4 illustrates the following insights.\n---\n[ref_id=xia2024] (1)\nThe optimizer stage in BlackMamba fine-tuning takes a\nconsiderable portion of the running time (up to 53% when\nconducting sparse fine-tuning with batch size = 1), while\nthe execution time share of the optimizer stage in Mixtral\nfine-tuning is negligible.\n---\n[ref_id=xia2024] The running time of the optimizer\nstage depends only on the number of parameters that need\nto be updated during fine-tuning.\n---\n[ref_id=xia2024] This difference is primarily\ndue to the different fine-tuning strategies applied to these two\nmodels: only the parameters in the LoRA module are updated\nfor Mixtral fine-tuning, whereas BlackMamba undergoes full\nfine-tuning.\n---\n[ref_id=xia2024] (2) The runtime of the forward and backward\nstages increases with sparsity and batch size due to the\nincreased amount of computation.\n---\n[ref_id=xia2024] (3) The backward stage\ntypically takes more time than the forward stage.\n---\n[ref_id=xia2024] In Black-\nMamba, the backward stage demands more computation than\n---\n[ref_id=xia2024] 1) Maximum Batch Size Support: The maximum batch\nsize in fine-tuning is determined by GPU memory size,\nmodel size, sequence length, and MoE sparsity.\n---\n[ref_id=xia2024] under 10% on BlackMamba. We assess accuracy improve-\nments post-fine-tuning and compare the learning capabilities\nof dense and sparse versions of both models.\nFig. 3 shows the testing accuracy of Mixtral and Black-\nMamba on two datasets Hellaswag (HE) and GSM8K (GS).\nWe fine-tune both models using the sparse and dense setups\ndescribed in \u00a7III for 10 epochs, and test the accuracy of\nthe fine-tuned model at each epoch. We make the following\nobservations in Fig. 3. (1) Fine-tuning converges relatively\nquickly. Typically, 10 epochs are enough for fine-tune models\nto stabilize at or close to their peak accuracy. On GS, both\nmodels are close to their peak accuracy at the first epoch.\n(2) The smaller model BlackMamba takes relatively more\nepochs to reach its peak accuracy, as it took BlackMamba 5\nepochs to converge on HE. (3) The larger model Mixtral has\nbetter accuracy compared to BlackMamba on both datasets.\n(4) Both models perform better on the CS dataset HE than\non the GS dataset GS. This is because math is harder for\nsmaller LLMs to learn [24]. The BlackMamba model is\ninadequate for fine-tuning GS. This is likely attributed to\nthe complexity of mathematical tasks and the smaller model\nsize of BlackMamba. Additionally, Mamba is specifically\nengineered for long sequence modeling, potentially resulting\nin unsatisfactory arithmetic reasoning ability [25]. Thus, in\nour characterization study in later sections, we will not show\nthe results for BlackMamba fine-tuned on MATH. (5) The\nperformance of sparse fine-tuning is close to that of dense\nfine-tuning, with the exception of Mixtral on HE. However,\neven for this outlier, sparse fine-tuning achieves similar peak\naccuracy compared to dense; we see a drop of accuracy\nbetween the epoch 4 and 5, and indicates sparse fine-tuning is\nmore vulnerable to over-fitting, especially for easy tasks [26].\nFollowing the above insights, the key take-away of this\nanalysis can be summarized as follows.\nTakeaway 1. Sparse model can be trained as well\nas its dense counterpart.\nTakeaway 2. Fine-tuning generally takes less ten\nepochs to reach peak accuracy.\nB. Analysis of Runtime Performance\nAfter confirming that both Mixtral and BlackMamba can\nbe fine-tuned to achieve acceptable accuracy, we examine\ntheir performance in a resource-constrained environment us-\ning a single GPU. This setup highlights unique runtime char-\nacteristics such as execution time breakdown, throughput,\nmaximum batch size, compute and memory utilization, load\nimbalance, and sensitivity analysis. We also compare sparse\nand dense models. Insights from this study will help develop\na robust analytical model for estimating fine-tuning costs.\n1) Maximum Batch Size Support: The maximum batch\nsize in fine-tuning is determined by GPU memory size,\nmodel size, sequence length, and MoE sparsity. The LLM\nTABLE III\nMAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE\nAND S:SPARSE .\nMixtral-D Mixtral-S BlackMamba-D BlackMamba-S\nCS 2 8 6 20\nMATH 1 3 2 8\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)\n0.0\n2.0\n4.0\n6.0\n8.0\nForward Backward Optimizer\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)\n0.0\n0.5\n1.0\n1.5\n2.0\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig. 4. Execution time breakdown.\noccupies a certain amount of GPU memory, with the re-\nmainder available for intermediate data during fine-tuning.\nLonger sequence lengths consume more memory, and denser\nMoE configurations require additional memory space. We\ndiscuss the heuristic for determining the maximum batch size\nin \u00a7V. Based on our experimental study on NVIDIA A40\nGPU with 48GB memory, we empirically find and report\nthe maximum batch size supported by different model and\ndataset combinations in Table III.\n2) Execution Time Breakdown: We first analyze the high-\nlevel execution time breakdown for Mixtral and Black-\nMamba. The purpose of this study is to understand where\ndoes this workload spend most of its time. As discussed in\n\u00a7III, we conduct this study using a sequence length of 128.\nAt a high-level, the fine-tuning workload can be divided\ninto three stages: (1) forward, (2) backward, and (3) opti-\nmizer. We use a batch size of 1 and the maximum batch size\nsupported by a model-dataset combination to show workload\ncharacteristics. Fig. 4 illustrates the following insights. (1)\nThe optimizer stage in BlackMamba fine-tuning takes a\nconsiderable portion of the running time (up to 53% when\nconducting sparse fine-tuning with batch size = 1), while\nthe execution time share of the optimizer stage in Mixtral\nfine-tuning is negligible. The running time of the optimizer\nstage depends only on the number of parameters that need\nto be updated during fine-tuning. This difference is primarily\ndue to the different fine-tuning strategies applied to these two\nmodels: only the parameters in the LoRA module are updated\nfor Mixtral fine-tuning, whereas BlackMamba undergoes full\nfine-tuning. (2) The runtime of the forward and backward\nstages increases with sparsity and batch size due to the\nincreased amount of computation. (3) The backward stage\ntypically takes more time than the forward stage. In Black-\nMamba, the backward stage demands more computation than\n---\n[ref_id=xia2024] 0 20 40 60 80 100 1200\n5\n10\n15\n20\n25\n30\n35\n40\nA100-40GB\nA100-80GB\nA40\nH100\nbsz=28\nbsz=35\nProjected GPU capacity\nGround Truth Projection\nMax batch size\nGPU DRAM capacity\nFig. 13. Projected maximum batch size of Mixtral for different GPUs.\namount of GPU memory, and need to be subtracted in the\nanalytical model. Fig. 8 supports this by showing that on the\nsame dataset, BlackMamba can support larger batch size than\nMixtral because of its smaller model size.\nMoreover, the sequence length and sparsity also affect the\nmaximum batch size. Because the sparsity only affects the\nMoE part of the LLM, we multiply its influence by C1,\nwhich we call MoE coefficient. We apply the sequence length\nand the sparsity in the denominator as they are inversely\nrelated to batch size. Then, we multiply the result by C0,\nthe scaling coefficient , which scales the batch size by a\nconstant. The scaling coefficient is different across LLM\nmodels, because different models have different architecture\n(\u00a7III), and generate different amounts of intermediate data\nfor each query. The scaling coefficient for BlackMamba is\nhigher than that of Mixtral because it is a smaller model.\nFinally, we use floor to round it to the maximum integer.\nThe MoE coefficient and scaling coefficient vary across\nmodels. These coefficients are independent of GPU microar-\nchitectural parameters. We find the maximum batch size for\nboth LLM models on NVIDIA A40 (48GB), A100 (40GB),\nA100 (80GB), and H100 (80GB), and apply our model to\nfind the optimal coefficients. For Mixtral, C0 = 82 and\nC1 = 0 .95, and for BlackMamba, C0 = 83 and C1 = 0 .88.\nWhile we showcase these parameters for the models eval-\nuated, \u00a7V-D discusses how to generalize this approach for\nother models.\nUsing our analytical model, we demonstrate the maximum\nbatch sizes for fine-tuning on four different NVIDIA GPUs:\nA40, A100-40GB, A100-80GB and H100 with memory\ncapacities of 48GB, 40GB, 80GB, and 80GB, respectively.\nFig. 13 shows our projected maximum batch size and corre-\nlate it with experimented ground truth. While the maximum\nmemory capacity available in NVIDIA GPUs today is 80GB,\nwe use our analytical model to project the maximum batch\nsize that future GPUs might support. For GPU memory\ncapacities of 100GB and 120GB, our model predicts that\nthe maximum batch sizes supported for fine-tuning Mixtral\nwill be 28 and 35, respectively. Due to space limitations, we\nonly show the projection of Mixtral model.\nTABLE IV\nESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE\nBASED ON OUR ANALYTICAL MODEL\nGPU Mem MBS Throughput Cost ($/hr) Cost ($)\nA40 48GB 4 1.01 0.79 32.7\nA100 80GB 17 2.74 1.67 25.4\nH100 80GB 17 4.90 2.1 17.9\nB. Estimating Throughput\nAs discussed in \u00a7IV-B4, when the batch size increases, the\nLLM fine-tuning gradually switches from memory bound to\ncompute bound. When the compute resources are abundant,\nthe throughput increases almost linearly with batch size.\nHowever, when compute resources become constrained, the\nthroughput improvement gradually saturates. We model this\nbehavior using a logarithmic relation between batch size and\nthroughput. Our analytical model for maximum batch size is\nshown in (2).\nT hroughput = C2 \u2217 log( batch size\nsparsity \u2217 C3 ) + C4 (2)\nIn the equation, in addition to the basic logarithmic part, we\nhave three coefficients C2, C3, and C4. C2 is the scaling\ncoefficient, which depends on the LLM model, GPU archi-\ntecture, and the dataset. The higher the compute capability a\nGPU can provide, and the lower the LLM model and dataset\ncompute requirement is, the higher the scaling coefficient will\nbe. C3 is the MoE attenuation coefficient , which tunes how\nmuch the MoE sparsity affects the throughput. MoE sparsity\nonly affects the MoE part in LLM model, and thus should\nbe attenuated to avoid over compensation. This coefficient is\nonly LLM model dependent, because once the model is fixed,\nthe influence of sparsity is determined. C4 is the intercept,\nconceptually it equals to the throughput when batch size\nequals one, because the logarithmic part in (2) is zero when\nbatch size is one. Using scipy [32] to fit the model and\ngenerate four sets (C2, C3, C4), for each model and dataset\ncombination.\nTo estimate the accuracy of this model, we correlate the\nmodel output with experimental data from our study. Fig. 14\nshows this correlation study, where discrete data points (dots)\nrepresent experimental values, and the line represents output\nof our analytical model. We use both dense and sparse\nMixtral and BlackMamba for both datasets used in our study.\nThe figure clearly shows that our model accurately predicts\nLLM fine-tuning throughput with a Root Mean Squared Error\n(RMSE) of less than 0.8. Fig. 15 shows the correlation study\nof the analytical model of three other GPUs, A100 (40GB),\nA100 (80GB), and H100. The RMSE is less than 0.6, close\nto that of A40.\nC. Estimating the Total Fine-Tuning Cost\nUsing the throughput estimation, we calculate the cost\nof fine-tuning LLMs for different GPUs. The cost of GPU\n---\n[ref_id=xia2024] Page 9\n---\n[ref_id=xia2024] 0 20 40 60 80 100 1200\n5\n10\n15\n20\n25\n30\n35\n40\nA100-40GB\nA100-80GB\nA40\nH100\nbsz=28\nbsz=35\nProjected GPU capacity\nGround Truth Projection\nMax batch size\nGPU DRAM capacity\nFig.\n---\n[ref_id=xia2024] 13.\n---\n[ref_id=xia2024] Projected maximum batch size of Mixtral for different GPUs.\n---\n[ref_id=xia2024] amount of GPU memory, and need to be subtracted in the\nanalytical model.\n---\n[ref_id=xia2024] Fig.\n---\n[ref_id=xia2024] 8 supports this by showing that on the\nsame dataset, BlackMamba can support larger batch size than\nMixtral because of its smaller model size.\n---\n[ref_id=xia2024] Moreover, the sequence length and sparsity also affect the\nmaximum batch size.\n---\n[ref_id=xia2024] Because the sparsity only affects the\nMoE part of the LLM, we multiply its influence by C1,\nwhich we call MoE coefficient.\n---\n[ref_id=xia2024] We apply the sequence length\nand the sparsity in the denominator as they are inversely\nrelated to batch size.\n---\n[ref_id=xia2024] Then, we multiply the result by C0,\nthe scaling coefficient , which scales the batch size by a\nconstant.\n---\n[ref_id=xia2024] The scaling coefficient is different across LLM\nmodels, because different models have different architecture\n(\u00a7III), and generate different amounts of intermediate data\nfor each query.\n---\n[ref_id=xia2024] The scaling coefficient for BlackMamba is\nhigher than that of Mixtral because it is a smaller model.\n---\n[ref_id=xia2024] Finally, we use floor to round it to the maximum integer.\n---\n[ref_id=xia2024] The MoE coefficient and scaling coefficient vary across\nmodels.\n---\n[ref_id=xia2024] These coefficients are independent of GPU microar-\nchitectural parameters.\n---\n[ref_id=xia2024] We find the maximum batch size for\nboth LLM models on NVIDIA A40 (48GB), A100 (40GB),\nA100 (80GB), and H100 (80GB), and apply our model to\nfind the optimal coefficients.\n---\n[ref_id=xia2024] For Mixtral, C0 = 82 and\nC1 = 0 .95, and for BlackMamba, C0 = 83 and C1 = 0 .88.\n---\n[ref_id=xia2024] While we showcase these parameters for the models eval-\nuated, \u00a7V-D discusses how to generalize this approach for\nother models.\n---\n[ref_id=xia2024] Using our analytical model, we demonstrate the maximum\nbatch sizes for fine-tuning on four different NVIDIA GPUs:\nA40, A100-40GB, A100-80GB and H100 with memory\ncapacities of 48GB, 40GB, 80GB, and 80GB, respectively.\n---\n[ref_id=xia2024] Fig.\n---\n[ref_id=xia2024] 13 shows our projected maximum batch size and corre-\nlate it with experimented ground truth.\n---\n[ref_id=xia2024] While the maximum\nmemory capacity available in NVIDIA GPUs today is 80GB,\nwe use our analytical model to project the maximum batch\nsize that future GPUs might support.\n---\n[ref_id=xia2024] For GPU memory\ncapacities of 100GB and 120GB, our model predicts that\nthe maximum batch sizes supported for fine-tuning Mixtral\nwill be 28 and 35, respectively.\n---\n[ref_id=xia2024] Due to space limitations, we\nonly show the projection of Mixtral model.\n---\n[ref_id=xia2024] TABLE IV\nESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE\nBASED ON OUR ANALYTICAL MODEL\nGPU Mem MBS Throughput Cost ($/hr) Cost ($)\nA40 48GB 4 1.01 0.79 32.7\nA100 80GB 17 2.74 1.67 25.4\nH100 80GB 17 4.90 2.1 17.9\nB.\n---\n[ref_id=xia2024] Estimating Throughput\nAs discussed in \u00a7IV-B4, when the batch size increases, the\nLLM fine-tuning gradually switches from memory bound to\ncompute bound.\n---\n[ref_id=xia2024] When the compute resources are abundant,\nthe throughput increases almost linearly with batch size.\n---\n[ref_id=xia2024] However, when compute resources become constrained, the\nthroughput improvement gradually saturates.\n---\n[ref_id=xia2024] We model this\nbehavior using a logarithmic relation between batch size and\nthroughput.\n---\n[ref_id=xia2024] Our analytical model for maximum batch size is\nshown in (2).\n---\n[ref_id=xia2024] T hroughput = C2 \u2217 log( batch size\nsparsity \u2217 C3 ) + C4 (2)\nIn the equation, in addition to the basic logarithmic part, we\nhave three coefficients C2, C3, and C4.\n---\n[ref_id=xia2024] C2 is the scaling\ncoefficient, which depends on the LLM model, GPU archi-\ntecture, and the dataset.\n---\n[ref_id=xia2024] The higher the compute capability a\nGPU can provide, and the lower the LLM model and dataset\ncompute requirement is, the higher the scaling coefficient will\nbe.\n---\n[ref_id=xia2024] C3 is the MoE attenuation coefficient , which tunes how\nmuch the MoE sparsity affects the throughput.\n---\n[ref_id=xia2024] MoE sparsity\nonly affects the MoE part in LLM model, and thus should\nbe attenuated to avoid over compensation.\n---\n[ref_id=xia2024] This coefficient is\nonly LLM model dependent, because once the model is fixed,\nthe influence of sparsity is determined.\n---\n[ref_id=xia2024] C4 is the intercept,\nconceptually it equals to the throughput when batch size\nequals one, because the logarithmic part in (2) is zero when\nbatch size is one.\n---\n[ref_id=xia2024] Using scipy [32] to fit the model and\ngenerate four sets (C2, C3, C4), for each model and dataset\ncombination.\n---\n[ref_id=xia2024] To estimate the accuracy of this model, we correlate the\nmodel output with experimental data from our study.\n---\n[ref_id=xia2024] Fig.\n---\n[ref_id=xia2024] 14\nshows this correlation study, where discrete data points (dots)\nrepresent experimental values, and the line represents output\nof our analytical model.\n---\n[ref_id=xia2024] We use both dense and sparse\nMixtral and BlackMamba for both datasets used in our study.\n---\n[ref_id=xia2024] The figure clearly shows that our model accurately predicts\nLLM fine-tuning throughput with a Root Mean Squared Error\n(RMSE) of less than 0.8.\n---\n[ref_id=xia2024] Fig.\n---\n[ref_id=xia2024] 15 shows the correlation study\nof the analytical model of three other GPUs, A100 (40GB),\nA100 (80GB), and H100.\n---\n[ref_id=xia2024] The RMSE is less than 0.6, close\nto that of A40.\n---\n[ref_id=xia2024] C.\n---\n[ref_id=xia2024] Estimating the Total Fine-Tuning Cost\nUsing the throughput estimation, we calculate the cost\nof fine-tuning LLMs for different GPUs.\n---\n[ref_id=xia2024] The cost of GPU\n---\n[ref_id=xia2024] Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0.0\n1.0\n2.0\n3.0\n4.0\n5.0\nInput normalization Attention Post attention norm. MoE\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0.0\n0.5\n1.0\n1.5\nRMS layernorm Mamba MoE\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig. 5. Execution time breakdown in terms of different model layers.\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0\n2000\n4000\n6000\nmatmul(w2)\nw2_dequant\nmatmul(w3)\nw3_dequant\nmatmul(w1)\nw1_dequant\nsoftmax\ntopk\nmatmul(router)\nrouter_dequant\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0\n400\n800\n1200\n1600\n2000\nmatmul(w1)\ngelu\nmatmul(w2)\nelementwise_mult\ntop_k\nsigmoid\nmatmul(router)\n    Execution Time Breakdown (\u03bcs)\nMixtral\nMamba\nFig. 6. Execution breakdown of the MoE layer for different kernels.\nthe forward stage due to the need for gradient calculation\nand propagation, resulting in two matrix multiplication op-\nerations. In Mixtral fine-tuning, gradient calculation adds\nminimal computation as only a small portion of parameters\nneed it. However, gradient checkpointing in Mixtral saves\nmemory but increases the backward stage runtime due to the\nre-computation of intermediate values.\nWe further investigate the execution breakdown based\non various layers in two LLM models. For Mixtral, these\nlayers include input normalization, attention, post-attention\nnormalization, and MoE. In contrast, BlackMamba comprises\nthe Mamba layer, Root Mean Squared (RMS) layer nor-\nmalization, and MoE. As shown in Fig. 5, the MoE layer\nis the most time-consuming, accounting for 85% of the\noverall execution time on average. The execution time for\nthe MoE layer encompasses both the forward and backward\npasses during fine-tuning. Consequently, MoE is the costliest\nlayer and a prime target for optimization to enhance the\nperformance of LLM fine-tuning.\nTo concretely understand the opportunity for improving\nMoE layer performance, we also perform a kernel-level anal-\nysis within the MoE layer. Fig. 7 illustrates the architecture\nof the MoE layer in both Mixtral and BlackMamba models.\nEach expert in BlackMamba consists of a standard Feed-\nForward Network (FFN) layer with two serially connected\nweight matrices (W1 and W2) and a Gelu activation layer\nbetween. In contrast, experts in Mixtral are FFN layers with\nSwish-Gated Linear Units, involving an additional weight\nFig. 7. Expert architectures for Mixtral (top) and BlackMamba (bottom).\nmatrix (W3) in parallel with W1.\nFig. 6 shows the kernel-level MoE time breakdown. The\nfigure clearly shows that matrix multiplication (W1, W2,\nand W3) is the largest component of the MoE layer for\nboth BlackMamba and Mixtral. As batch size and sparsity\nincrease, so does computational demand, prolonging matrix\nmultiplication latency. The de-quantization operation in Mix-\ntral fine-tuning also becomes significant, especially with low\nsparsity and small batch sizes. While quantization reduces\nmodel size and memory footprint, it can increase computation\ntime due to de-quantization. This highlights the need to\nevaluate trade-offs between memory savings and computation\ntime, particularly in scenarios with small batch sizes and\nsequence lengths.\nTakeaway 3. Matrix multiplication operations in the\nMoE layer contribute significantly to the end-to-end\nexecution time, making the MoE layer the costliest\ncomponent in LLM fine-tuning.\n3) Fine-Tuning Throughput: Next, we present the fine-\ntuning throughput of Mixtral and BlackMamba on the MATH\nand CS datasets separately in Fig. 8. We use a throughput\nmetric of queries/second processed, where a query includes a\nprompt and a ground-truth answer for fine-tuning. To obtain\nthese results, we extract 1000 examples from each dataset\nand fine-tuned Mixtral and BlackMamba on them using the\nsmallest batch size (batch size = 1) and the largest batch size\nthat would fill the GPU memory.\nAs illustrated in Fig. 8, sparse fine-tuning achieves higher\nthroughput than dense fine-tuning. This is because the sparse\nfine-tuning baseline consumes less memory to store interme-\ndiate values, which allows for higher batch sizes compared\nto its dense counterpart. Additionally, with the same batch\nsize, sparse fine-tuning achieves higher throughput because\n---\n[ref_id=xia2024] Page 5\n---\n[ref_id=xia2024] Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0.0\n1.0\n2.0\n3.0\n4.0\n5.0\nInput normalization Attention Post attention norm.\n---\n[ref_id=xia2024] MoE\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0.0\n0.5\n1.0\n1.5\nRMS layernorm Mamba MoE\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig.\n---\n[ref_id=xia2024] 5.\n---\n[ref_id=xia2024] Execution time breakdown in terms of different model layers.\n---\n[ref_id=xia2024] Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0\n2000\n4000\n6000\nmatmul(w2)\nw2_dequant\nmatmul(w3)\nw3_dequant\nmatmul(w1)\nw1_dequant\nsoftmax\ntopk\nmatmul(router)\nrouter_dequant\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0\n400\n800\n1200\n1600\n2000\nmatmul(w1)\ngelu\nmatmul(w2)\nelementwise_mult\ntop_k\nsigmoid\nmatmul(router)\n    Execution Time Breakdown (\u03bcs)\nMixtral\nMamba\nFig.\n---\n[ref_id=xia2024] 6.\n---\n[ref_id=xia2024] Execution breakdown of the MoE layer for different kernels.\n---\n[ref_id=xia2024] the forward stage due to the need for gradient calculation\nand propagation, resulting in two matrix multiplication op-\nerations.\n---\n[ref_id=xia2024] In Mixtral fine-tuning, gradient calculation adds\nminimal computation as only a small portion of parameters\nneed it.\n---\n[ref_id=xia2024] However, gradient checkpointing in Mixtral saves\nmemory but increases the backward stage runtime due to the\nre-computation of intermediate values.\n---\n[ref_id=xia2024] We further investigate the execution breakdown based\non various layers in two LLM models.\n---\n[ref_id=xia2024] For Mixtral, these\nlayers include input normalization, attention, post-attention\nnormalization, and MoE.\n---\n[ref_id=xia2024] In contrast, BlackMamba comprises\nthe Mamba layer, Root Mean Squared (RMS) layer nor-\nmalization, and MoE.\n---\n[ref_id=xia2024] As shown in Fig.\n---\n[ref_id=xia2024] 5, the MoE layer\nis the most time-consuming, accounting for 85% of the\noverall execution time on average.\n---\n[ref_id=xia2024] The execution time for\nthe MoE layer encompasses both the forward and backward\npasses during fine-tuning.\n---\n[ref_id=xia2024] Consequently, MoE is the costliest\nlayer and a prime target for optimization to enhance the\nperformance of LLM fine-tuning.\n---\n[ref_id=xia2024] To concretely understand the opportunity for improving\nMoE layer performance, we also perform a kernel-level anal-\nysis within the MoE layer.\n---\n[ref_id=xia2024] Fig.\n---\n[ref_id=xia2024] 7 illustrates the architecture\nof the MoE layer in both Mixtral and BlackMamba models.\n---\n[ref_id=xia2024] Each expert in BlackMamba consists of a standard Feed-\nForward Network (FFN) layer with two serially connected\nweight matrices (W1 and W2) and a Gelu activation layer\nbetween.\n---\n[ref_id=xia2024] In contrast, experts in Mixtral are FFN layers with\nSwish-Gated Linear Units, involving an additional weight\nFig.\n---\n[ref_id=xia2024] 7.\n---\n[ref_id=xia2024] Expert architectures for Mixtral (top) and BlackMamba (bottom).\n---\n[ref_id=xia2024] matrix (W3) in parallel with W1.\n---\n[ref_id=xia2024] Fig.\n---\n[ref_id=xia2024] 6 shows the kernel-level MoE time breakdown.\n---\n[ref_id=xia2024] The\nfigure clearly shows that matrix multiplication (W1, W2,\nand W3) is the largest component of the MoE layer for\nboth BlackMamba and Mixtral.\n---\n[ref_id=xia2024] As batch size and sparsity\nincrease, so does computational demand, prolonging matrix\nmultiplication latency.\n---\n[ref_id=xia2024] The de-quantization operation in Mix-\ntral fine-tuning also becomes significant, especially with low\nsparsity and small batch sizes.\n---\n[ref_id=xia2024] While quantization reduces\nmodel size and memory footprint, it can increase computation\ntime due to de-quantization.\n---\n[ref_id=xia2024] This highlights the need to\nevaluate trade-offs between memory savings and computation\ntime, particularly in scenarios with small batch sizes and\nsequence lengths.\n---\n[ref_id=xia2024] Takeaway 3.\n---\n[ref_id=xia2024] Matrix multiplication operations in the\nMoE layer contribute significantly to the end-to-end\nexecution time, making the MoE layer the costliest\ncomponent in LLM fine-tuning.\n---\n[ref_id=xia2024] 3) Fine-Tuning Throughput: Next, we present the fine-\ntuning throughput of Mixtral and BlackMamba on the MATH\nand CS datasets separately in Fig.\n---\n[ref_id=xia2024] 8.\n---\n[ref_id=xia2024] We use a throughput\nmetric of queries/second processed, where a query includes a\nprompt and a ground-truth answer for fine-tuning.\n---\n[ref_id=xia2024] To obtain\nthese results, we extract 1000 examples from each dataset\nand fine-tuned Mixtral and BlackMamba on them using the\nsmallest batch size (batch size = 1) and the largest batch size\nthat would fill the GPU memory.\n---\n[ref_id=xia2024] As illustrated in Fig.\n---\n[ref_id=xia2024] 8, sparse fine-tuning achieves higher\nthroughput than dense fine-tuning.\n---\n[ref_id=xia2024] This is because the sparse\nfine-tuning baseline consumes less memory to store interme-\ndiate values, which allows for higher batch sizes compared\nto its dense counterpart.\n---\n[ref_id=xia2024] Additionally, with the same batch\nsize, sparse fine-tuning achieves higher throughput because\n---\n[ref_id=xia2024] Understanding the Performance and Estimating the\nCost of LLM Fine-Tuning\nYuchen Xia1 Jiho Kim 2 Yuhan Chen1 Haojie Ye1 Souvik Kundu 3\nCong (Callie) Hao 2 Nishil Talati1\n1University of Michigan 2Georgia Institute of Technology 3Intel Labs\nAbstract\u2014Due to the cost-prohibitive nature of training Large\nLanguage Models (LLMs), fine-tuning has emerged as an attrac-\ntive alternative for specializing LLMs for specific tasks using\nlimited compute resources in a cost-effective manner. In this\npaper, we characterize sparse Mixture of Experts (MoE) based\nLLM fine-tuning to understand their accuracy and runtime\nperformance on a single GPU. Our evaluation provides unique\ninsights into the training efficacy of sparse and dense versions of\nMoE models, as well as their runtime characteristics, including\nmaximum batch size, execution time breakdown, end-to-end\nthroughput, GPU hardware utilization, and load distribution.\nOur study identifies the optimization of the MoE layer as crucial\nfor further improving the performance of LLM fine-tuning.\nUsing our profiling results, we also develop and validate an\nanalytical model to estimate the cost of LLM fine-tuning on\nthe cloud. This model, based on parameters of the model and\nGPU architecture, estimates LLM throughput and the cost\nof training, aiding practitioners in industry and academia to\nbudget the cost of fine-tuning a specific model.\nI. I NTRODUCTION\nLarge Language Models (LLMs) are widely utilized in\nNatural Language Processing (NLP) [1]. Modern LLMs\ntypically possess billions to trillions of parameters, neces-\nsitating extensive time and resources for training. For in-\nstance, the estimated cost of training OpenAI\u2019s GPT-4 model\nexceeds $100 million, rendering it financially prohibitive\nfor most small-to-medium size enterprises and the academic\ncommunity [2]. Given the open-sourcing of numerous pre-\ntrained LLMs (e.g., LLAMA [3] and Mixtral [4]), fine-\ntuning has emerged as an attractive alternative for further\nspecializing these models in a cost-effective manner [5].\nGiven the learning ability of pre-trained models, it is feasible\nto use a domain-specific dataset to align the desired behav-\niors of LLMs through supervised fine-tuning on instruction-\nfollowing tasks [6]. Unlike pre-training, fine-tuning can be\nconducted in a resource-constrained environment, typically\nusing one or a few GPUs. Consequently, fine-tuning presents\na compelling case for applications such as specialized ques-\ntion answering within enterprises, legal document analysis\nand drafting, healthcare/medical research, technical and IT\nsupport, among others [7].\nThis paper characterizes LLM fine-tuning with two pri-\nmary objectives: (1) understanding the performance charac-\nteristics of LLM fine-tuning, and (2) developing an analytical\nmodel to estimate the cost of fine-tuning on the cloud. Given\nour focus on cost-efficient LLM fine-tuning, we concen-\ntrate on fine-tuning sparse Mixture-of-Expert (MoE) models.\nSpecifically, we employ an attention-based MoE model, Mix-\ntral [4], and a state-space MoE model, BlackMamba [8]. Us-\ning these models and two domain-specific datasets for math-\nematics and common-sense question-answering, we conduct\nan in-depth profiling study to understand their performance\ncharacteristics with a single GPU. We compare the dense\nand sparse counterparts of the investigated MoE models to\nevaluate their learning rates and runtime performance. Our\ninvestigation covers memory consumption, maximum batch\nsize supported within a single GPU memory budget, exe-\ncution time breakdown and bottlenecks, overall throughput,\nmicroarchitectural performance counters, and runtime load\ndistribution. The insights gained from our study are used to\ndevelop and validate an analytical model to estimate the cost.\nOur characterization uncovers the following unique in-\nsights. (1) Fine-tuning can be achieved in less than 10 epochs,\nand sparse MoE model that activates a subset of experts\ncan learn as well as its dense counterparts. (2) MoE layer\nconsumes the highest fraction of execution time in LLM\nfine-tuning; optimizing MoE layer performance is key to\nimproving the overall cost of LLM fine-tuning. (3) Sparse\nMoE model improves end-to-end throughput by supporting\na larger batch size. Given similar learning abilities of sparse\nand dense models, it is desired to use a sparse MoE model\nfor cost-effective fine-tuning. (4) The workload becomes\ncompute bound by increasing batch size; improving compute\nresources will increase performance. (5) Fine-tuning sparse\nmodel leads to more load imbalance.\nBased on these insights, we create an analytical model\nto estimate the cost of LLM fine-tuning based on model\nsize, dataset size, and GPU architecture. First, we estimate\nthe maximum batch size for a given GPU memory, then\ncompute fine-tuning throughput. We validate this throughput\nwith experimental results, showing an RMSE of less than\n0.55. Using the estimated throughput, our model calculates\nthe fine-tuning cost for different cloud providers.\nThe contributions of this paper are as follows.\n\u2022 Make a case for LLM fine-tuning for specializing pre-\ntrained models in a cost-effective manner.\narXiv:2408.04693v1  [cs.CL]  8 Aug 2024\n---\n[ref_id=xia2024] Page 1\n---\n[ref_id=xia2024] Understanding the Performance and Estimating the\nCost of LLM Fine-Tuning\nYuchen Xia1 Jiho Kim 2 Yuhan Chen1 Haojie Ye1 Souvik Kundu 3\nCong (Callie) Hao 2 Nishil Talati1\n1University of Michigan 2Georgia Institute of Technology 3Intel Labs\nAbstract\u2014Due to the cost-prohibitive nature of training Large\nLanguage Models (LLMs), fine-tuning has emerged as an attrac-\ntive alternative for specializing LLMs for specific tasks using\nlimited compute resources in a cost-effective manner.\n---\n[ref_id=xia2024] In this\npaper, we characterize sparse Mixture of Experts (MoE) based\nLLM fine-tuning to understand their accuracy and runtime\nperformance on a single GPU.\n---\n[ref_id=xia2024] Our evaluation provides unique\ninsights into the training efficacy of sparse and dense versions of\nMoE models, as well as their runtime characteristics, including\nmaximum batch size, execution time breakdown, end-to-end\nthroughput, GPU hardware utilization, and load distribution.\n---\n[ref_id=xia2024] Our study identifies the optimization of the MoE layer as crucial\nfor further improving the performance of LLM fine-tuning.\n---\n[ref_id=xia2024] Using our profiling results, we also develop and validate an\nanalytical model to estimate the cost of LLM fine-tuning on\nthe cloud.\n---\n[ref_id=xia2024] This model, based on parameters of the model and\nGPU architecture, estimates LLM throughput and the cost\nof training, aiding practitioners in industry and academia to\nbudget the cost of fine-tuning a specific model.\n---\n[ref_id=xia2024] I.\n---\n[ref_id=xia2024] I NTRODUCTION\nLarge Language Models (LLMs) are widely utilized in\nNatural Language Processing (NLP) [1].\n---\n[ref_id=xia2024] Modern LLMs\ntypically possess billions to trillions of parameters, neces-\nsitating extensive time and resources for training.\n---\n[ref_id=xia2024] For in-\nstance, the estimated cost of training OpenAI\u2019s GPT-4 model\nexceeds $100 million, rendering it financially prohibitive\nfor most small-to-medium size enterprises and the academic\ncommunity [2].\n---\n[ref_id=xia2024] Given the open-sourcing of numerous pre-\ntrained LLMs (e.g., LLAMA [3] and Mixtral [4]), fine-\ntuning has emerged as an attractive alternative for further\nspecializing these models in a cost-effective manner [5].\n---\n[ref_id=xia2024] Given the learning ability of pre-trained models, it is feasible\nto use a domain-specific dataset to align the desired behav-\niors of LLMs through supervised fine-tuning on instruction-\nfollowing tasks [6].\n---\n[ref_id=xia2024] Unlike pre-training, fine-tuning can be\nconducted in a resource-constrained environment, typically\nusing one or a few GPUs.\n---\n[ref_id=xia2024] Consequently, fine-tuning presents\na compelling case for applications such as specialized ques-\ntion answering within enterprises, legal document analysis\nand drafting, healthcare/medical research, technical and IT\nsupport, among others [7].\n---\n[ref_id=xia2024] This paper characterizes LLM fine-tuning with two pri-\nmary objectives: (1) understanding the performance charac-\nteristics of LLM fine-tuning, and (2) developing an analytical\nmodel to estimate the cost of fine-tuning on the cloud.\n---\n[ref_id=xia2024] Given\nour focus on cost-efficient LLM fine-tuning, we concen-\ntrate on fine-tuning sparse Mixture-of-Expert (MoE) models.\n---\n[ref_id=xia2024] Specifically, we employ an attention-based MoE model, Mix-\ntral [4], and a state-space MoE model, BlackMamba [8].\n---\n[ref_id=xia2024] Us-\ning these models and two domain-specific datasets for math-\nematics and common-sense question-answering, we conduct\nan in-depth profiling study to understand their performance\ncharacteristics with a single GPU.\n---\n[ref_id=xia2024] We compare the dense\nand sparse counterparts of the investigated MoE models to\nevaluate their learning rates and runtime performance.\n---\n[ref_id=xia2024] Our\ninvestigation covers memory consumption, maximum batch\nsize supported within a single GPU memory budget, exe-\ncution time breakdown and bottlenecks, overall throughput,\nmicroarchitectural performance counters, and runtime load\ndistribution.\n---\n[ref_id=xia2024] The insights gained from our study are used to\ndevelop and validate an analytical model to estimate the cost.\n---\n[ref_id=xia2024] Our characterization uncovers the following unique in-\nsights.\n---\n[ref_id=xia2024] (1) Fine-tuning can be achieved in less than 10 epochs,\nand sparse MoE model that activates a subset of experts\ncan learn as well as its dense counterparts.\n---\n[ref_id=xia2024] (2) MoE layer\nconsumes the highest fraction of execution time in LLM\nfine-tuning;\n---\n[ref_id=xia2024] optimizing MoE layer performance is key to\nimproving the overall cost of LLM fine-tuning.\n---\n[ref_id=xia2024] (3) Sparse\nMoE model improves end-to-end throughput by supporting\na larger batch size.\n---\n[ref_id=xia2024] Given similar learning abilities of sparse\nand dense models, it is desired to use a sparse MoE model\nfor cost-effective fine-tuning.\n---\n[ref_id=xia2024] (4) The workload becomes\ncompute bound by increasing batch size;\n---\n[ref_id=xia2024] improving compute\nresources will increase performance.\n---\n[ref_id=xia2024] (5) Fine-tuning sparse\nmodel leads to more load imbalance.\n---\n[ref_id=xia2024] Based on these insights, we create an analytical model\nto estimate the cost of LLM fine-tuning based on model\nsize, dataset size, and GPU architecture.\n---\n[ref_id=xia2024] First, we estimate\nthe maximum batch size for a given GPU memory, then\ncompute fine-tuning throughput.\n---\n[ref_id=xia2024] We validate this throughput\nwith experimental results, showing an RMSE of less than\n0.55.\n---\n[ref_id=xia2024] Using the estimated throughput, our model calculates\nthe fine-tuning cost for different cloud providers.\n---\n[ref_id=xia2024] The contributions of this paper are as follows.\n---\n[ref_id=xia2024] \u2022 Make a case for LLM fine-tuning for specializing pre-\ntrained models in a cost-effective manner.\n---\n[ref_id=xia2024] arXiv:2408.04693v1  [cs.CL]  8 Aug 2024\n---\n[ref_id=xia2024] Table IV estimates the cost for fine-tuning\nMixtral on the MATH dataset with a sparse setup, using\n10 epochs on different GPUs for a realistic cost estimate.\n---\n[ref_id=xia2024] 0 2 4 6 8 100.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n RMSE=0.05\nMixtral-CS\nDense Sparse\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n RMSE=0.02\nMixtral-MATH\n0 5 10 15 200\n2\n4\n6\n8\n10\n12\n14\n16\n RMSE=0.79\nMamba-CS\n0 2 4 6 8 10 120\n2\n4\n6\n8\n10\n12\n RMSE=0.42\nThroughput (queries/sec)\nBatch size\nMamba-MATH\nFig. 14. Estimation and validation of LLM fine-tuning throughput for\ndifferent models, datasets for A40 GPU. Dots represent ground truth and\nlines present the estimation.\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nRMSE=0.03\nMixtral-CS-A100-40GB\nDense Sparse\n0 5 10 15 200.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n RMSE=0.09\nMixtral-CS-A100-80GB\n0 5 10 15 200\n1\n2\n3\n4\n5\n RMSE=0.55\nMixtral-CS-H100\nThroughput (queries/sec)\nBatch size\nFig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS\nfor different GPUs: A100 and H100.\nresource renting per hour is calculated based on CUDO\ncompute [33], as other popular cloud providers do not offer\ncost/hour rates for the NVIDIA A40 GPU. However, one\ncan easily adjust the GPU renting cost per hour to estimate\nthe cost on other clouds such as Amazon AWS [34] or\nLambda [35]. Table IV estimates the cost for fine-tuning\nMixtral on the MATH dataset with a sparse setup, using\n10 epochs on different GPUs for a realistic cost estimate.\nEnterprises may use larger datasets for fine-tuning, such as,\nOpenOrca [36] and LaMini-instruction [37] containing more\nthan 2M queries. For OpenOrca, by scaling the cost by\nnumber of queries, our model predicts that the most cost-\neffective option to rent GPU resources on CUDO compute\nis NVIDIA H100 with a net cost of $3460.\nD. Generalization of the Analytical Model\nThe analytical models for estimating maximum batch size\nand throughput can be generalized to various LLM models\nand datasets. These models consider the characteristics of\nthe LLM, dataset, and GPU. Specifically, the maximum\nbatch size model combines GPU memory and LLM model\nsize to determine available memory for input data, while\ndataset sequence length and LLM sparsity determine space\nneeded per batch. In throughput estimation, based on the\nobservation we made (\u00a7IV-B4 Takeaway 5), GPU shifts from\nmemory-bound to compute-bound as batch size increases.\nThis characteristic generally applies to all GPUs due to the\nresource constraint, so the logarithmic relation between batch\nsize and throughput persists. The sparsity in (2) is model\ndependent, the influence of GPU, LLM model, and dataset\nare embedded in the coefficients C2, C3, and C4 in (2).\nThe coefficients in (1) and (2) are dependent on GPU,\nLLM model, and dataset; however, the underlying models\nare generalizable to unseen GPU, LLM model, and datasets.\nAlthough it takes some effort to sweep batch sizes and collect\nthroughput data points to fit our models, the benefits greatly\noutweigh the cost. Once the models are fit, our model can\nhelp choose the most cost-efficient GPU for fine-tuning LLM\nmodels, greatly saving resources and money.\nVI. R ELATED WORKS\nParameter-Efficient Fine-Tuning (PEFT) has been widely\nadopted to fine-tune LLM model for specialized tasks [15],\n[38]\u2013[43]. MoE additioally train specialized experts for dif-\nferent areas and the dynamic selection of experts makes\nit possible to scale the fine-tuning workload to different\nexperts in parallel. [44]\u2013[47] show that MoE models can\nimprove the ability to process knowledge for specific tasks,\nwhile maintaining the world knowledge in LLM. Kim et\nal. [48] construct an analytical model to estimate GPU\nmemory consumption for distributed fine-tuning. The model\nalso provides insights into optimizing memory usage through\ntensor, model, and pipeline parallelism.\nVII. C ONCLUSIONS\nFine-tuning LLMs is an attractive technique for tailoring\nmodern language models using domain-specific knowledge in\na cost-effective manner. This paper delved into understanding\nthe performance of fine-tuning MoE LLM models on a single\nGPU. Our profiling demonstrated that sparse MoE layers\noffer the best bang-for-buck trade-off. Using our profiling\nresults, we developed and validated an accurate analytical\nmodel to estimate the cost of LLM fine-tuning. Using this\nmodel, we showed the dollar amount that needs to be\nbudgeted for fine-tuning LLMs, which is much lower than\npre-training. For example, our model predicted that fine-\ntuning a sparse Mixtral model using a realistic data size of\n2M queries can be done with NVIDIA H100 GPU with a cost\nof $3460. A way to further reduce cost based on our study\nis to add compute resources to accelerate the MoE layers.\nWhile we showcase our study on fine-tuning LLMs using a\nsingle GPU, extending this model to multi-GPU systems is\nleft for future exploration.\nACKNOWLEDGMENTS\nThis work was supported in part by Semiconductor Re-\nsearch Corporation (SRC). We thank all the anonymous\nreviewers for their valuable comments and suggestions.\n---\n[ref_id=xia2024] Mixtral-CS0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.5 0.3 0.7\n1.7\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\nMixtral-MATH0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.3\n1.0\nDense(bsz=1)\nSparse(bsz=1)\nSparse(bsz=3)\nBlackmamba-CS0\n5\n10\n15\n20\n2.3\n7.9\n2.4\n10.5\n14.9\nDense(bsz=1)\nDense(bsz=6)\nSparse(bsz=1)\nSparse(bsz=6)\nSparse(bsz=20)\nBlackmamba-MATH0\n5\n10\n15\n20\n2.2 5.3 2.2\n6.5\n11.6\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\n      Throughput (quries/second)\nFig. 8. Query throughput of Mixtral and BlackMamba.\nmatmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n      SM Utilization (%)\nFig. 9. GPU SM utilization of different kernels in the MoE layer for different batch sizes.\nit involves fewer computational demands, resulting in lower\nlatency. This is evident when comparing the throughput of\nbatch size of 2 in Mixtral-CS for dense (0.5 qps) vs. sparse\n(0.7 qps) models.\nFig. 8 also shows that throughput does not increase linearly\nwith batch size. For instance, sparse fine-tuning of Mixtral-\nCS improves throughput by 1.9 \u00d7 when increasing the batch\nsize from 1 to 2, but only by 4.8 \u00d7 when increasing from\n1 to 8. With smaller batch sizes, the SM utilization rate\nis lower, providing enough computational resources to feed\nmore operations in parallel. However, as the batch size con-\ntinues to increase, the SMs become saturated (more details in\n\u00a7IV-B4), and we can no longer hide latency by better utilizing\ncomputational resources.\nTakeaway 4. Sparse model significantly improves\nthroughput, reducing end-to-end cost of fine-tuning.\n4) Hardware characterization: As shown in Fig. 4, the\nexecution time of LLM fine-tuning is dominated by the MoE\nlayer. To offer further insights, we use detailed microarchi-\ntecture hardware metrics on the GPU to further understand\nexecution bottlenecks in the MoE layer. The goal of this\nstudy is to identify whether various kernels in the MoE layers\nare bound by compute or memory resources, and how future\nGPU designs can further scale performance.\nCompute resource utilization study. Fig. 9 shows the\nkernel-level breakdown of GPU Streaming Multi-processor\n(SM) utilization for the MoE layer. This utilization is\n---\n[ref_id=xia2024] Page 6\n---\n[ref_id=xia2024] Mixtral-CS0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.5 0.3 0.7\n1.7\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\nMixtral-MATH0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.3\n1.0\nDense(bsz=1)\nSparse(bsz=1)\nSparse(bsz=3)\nBlackmamba-CS0\n5\n10\n15\n20\n2.3\n7.9\n2.4\n10.5\n14.9\nDense(bsz=1)\nDense(bsz=6)\nSparse(bsz=1)\nSparse(bsz=6)\nSparse(bsz=20)\nBlackmamba-MATH0\n5\n10\n15\n20\n2.2 5.3 2.2\n6.5\n11.6\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\n      Throughput (quries/second)\nFig.\n---\n[ref_id=xia2024] 8.\n---\n[ref_id=xia2024] Query throughput of Mixtral and BlackMamba.\n---\n[ref_id=xia2024] matmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n      SM Utilization (%)\nFig.\n---\n[ref_id=xia2024] 9.\n---\n[ref_id=xia2024] GPU SM utilization of different kernels in the MoE layer for different batch sizes.\n---\n[ref_id=xia2024] it involves fewer computational demands, resulting in lower\nlatency.\n---\n[ref_id=xia2024] This is evident when comparing the throughput of\nbatch size of 2 in Mixtral-CS for dense (0.5 qps) vs.\n---\n[ref_id=xia2024] sparse\n(0.7 qps) models.\n---\n[ref_id=xia2024] Fig.\n---\n[ref_id=xia2024] 8 also shows that throughput does not increase linearly\nwith batch size.\n---\n[ref_id=xia2024] For instance, sparse fine-tuning of Mixtral-\nCS improves throughput by 1.9 \u00d7 when increasing the batch\nsize from 1 to 2, but only by 4.8 \u00d7 when increasing from\n1 to 8.\n---\n[ref_id=xia2024] With smaller batch sizes, the SM utilization rate\nis lower, providing enough computational resources to feed\nmore operations in parallel.\n---\n[ref_id=xia2024] However, as the batch size con-\ntinues to increase, the SMs become saturated (more details in\n\u00a7IV-B4), and we can no longer hide latency by better utilizing\ncomputational resources.\n---\n[ref_id=xia2024] Takeaway 4.\n---\n[ref_id=xia2024] Sparse model significantly improves\nthroughput, reducing end-to-end cost of fine-tuning.\n---\n[ref_id=xia2024] 4) Hardware characterization: As shown in Fig.\n---\n[ref_id=xia2024] 4, the\nexecution time of LLM fine-tuning is dominated by the MoE\nlayer.\n---\n[ref_id=xia2024] To offer further insights, we use detailed microarchi-\ntecture hardware metrics on the GPU to further understand\nexecution bottlenecks in the MoE layer.\n---\n[ref_id=xia2024] The goal of this\nstudy is to identify whether various kernels in the MoE layers\nare bound by compute or memory resources, and how future\nGPU designs can further scale performance.\n---\n[ref_id=xia2024] Compute resource utilization study.\n---\n[ref_id=xia2024] Fig.\n---\n[ref_id=xia2024] 9 shows the\nkernel-level breakdown of GPU Streaming Multi-processor\n(SM) utilization for the MoE layer.\n---\n[ref_id=xia2024] This utilization is\n---\n[ref_id=xia2024] 0 2 4 6 8 100.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n RMSE=0.05\nMixtral-CS\nDense Sparse\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n RMSE=0.02\nMixtral-MATH\n0 5 10 15 200\n2\n4\n6\n8\n10\n12\n14\n16\n RMSE=0.79\nMamba-CS\n0 2 4 6 8 10 120\n2\n4\n6\n8\n10\n12\n RMSE=0.42\nThroughput (queries/sec)\nBatch size\nMamba-MATH\nFig. 14. Estimation and validation of LLM fine-tuning throughput for\ndifferent models, datasets for A40 GPU. Dots represent ground truth and\nlines present the estimation.\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nRMSE=0.03\nMixtral-CS-A100-40GB\nDense Sparse\n0 5 10 15 200.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n RMSE=0.09\nMixtral-CS-A100-80GB\n0 5 10 15 200\n1\n2\n3\n4\n5\n RMSE=0.55\nMixtral-CS-H100\nThroughput (queries/sec)\nBatch size\nFig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS\nfor different GPUs: A100 and H100.\nresource renting per hour is calculated based on CUDO\ncompute [33], as other popular cloud providers do not offer\ncost/hour rates for the NVIDIA A40 GPU. However, one\ncan easily adjust the GPU renting cost per hour to estimate\nthe cost on other clouds such as Amazon AWS [34] or\nLambda [35]. Table IV estimates the cost for fine-tuning\nMixtral on the MATH dataset with a sparse setup, using\n10 epochs on different GPUs for a realistic cost estimate.\nEnterprises may use larger datasets for fine-tuning, such as,\nOpenOrca [36] and LaMini-instruction [37] containing more\nthan 2M queries. For OpenOrca, by scaling the cost by\nnumber of queries, our model predicts that the most cost-\neffective option to rent GPU resources on CUDO compute\nis NVIDIA H100 with a net cost of $3460.\nD. Generalization of the Analytical Model\nThe analytical models for estimating maximum batch size\nand throughput can be generalized to various LLM models\nand datasets. These models consider the characteristics of\nthe LLM, dataset, and GPU. Specifically, the maximum\nbatch size model combines GPU memory and LLM model\nsize to determine available memory for input data, while\ndataset sequence length and LLM sparsity determine space\nneeded per batch. In throughput estimation, based on the\nobservation we made (\u00a7IV-B4 Takeaway 5), GPU shifts from\nmemory-bound to compute-bound as batch size increases.\nThis characteristic generally applies to all GPUs due to the\nresource constraint, so the logarithmic relation between batch\nsize and throughput persists. The sparsity in (2) is model\ndependent, the influence of GPU, LLM model, and dataset\nare embedded in the coefficients C2, C3, and C4 in (2).\nThe coefficients in (1) and (2) are dependent on GPU,\nLLM model, and dataset; however, the underlying models\nare generalizable to unseen GPU, LLM model, and datasets.\nAlthough it takes some effort to sweep batch sizes and collect\nthroughput data points to fit our models, the benefits greatly\noutweigh the cost. Once the models are fit, our model can\nhelp choose the most cost-efficient GPU for fine-tuning LLM\nmodels, greatly saving resources and money.\nVI. R ELATED WORKS\nParameter-Efficient Fine-Tuning (PEFT) has been widely\nadopted to fine-tune LLM model for specialized tasks [15],\n[38]\u2013[43]. MoE additioally train specialized experts for dif-\nferent areas and the dynamic selection of experts makes\nit possible to scale the fine-tuning workload to different\nexperts in parallel. [44]\u2013[47] show that MoE models can\nimprove the ability to process knowledge for specific tasks,\nwhile maintaining the world knowledge in LLM. Kim et\nal. [48] construct an analytical model to estimate GPU\nmemory consumption for distributed fine-tuning. The model\nalso provides insights into optimizing memory usage through\ntensor, model, and pipeline parallelism.\nVII. C ONCLUSIONS\nFine-tuning LLMs is an attractive technique for tailoring\nmodern language models using domain-specific knowledge in\na cost-effective manner. This paper delved into understanding\nthe performance of fine-tuning MoE LLM models on a single\nGPU. Our profiling demonstrated that sparse MoE layers\noffer the best bang-for-buck trade-off. Using our profiling\nresults, we developed and validated an accurate analytical\nmodel to estimate the cost of LLM fine-tuning. Using this\nmodel, we showed the dollar amount that needs to be\nbudgeted for fine-tuning LLMs, which is much lower than\npre-training. For example, our model predicted that fine-\ntuning a sparse Mixtral model using a realistic data size of\n2M queries can be done with NVIDIA H100 GPU with a cost\nof $3460. A way to further reduce cost based on our study\nis to add compute resources to accelerate the MoE layers.\nWhile we showcase our study on fine-tuning LLMs using a\nsingle GPU, extending this model to multi-GPU systems is\nleft for future exploration.\nACKNOWLEDGMENTS\nThis work was supported in part by Semiconductor Re-\nsearch Corporation (SRC). We thank all the anonymous\nreviewers for their valuable comments and suggestions.\n---\n[ref_id=xia2024] Page 10\n---\n[ref_id=xia2024] 0 2 4 6 8 100.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n RMSE=0.05\nMixtral-CS\nDense Sparse\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n RMSE=0.02\nMixtral-MATH\n0 5 10 15 200\n2\n4\n6\n8\n10\n12\n14\n16\n RMSE=0.79\nMamba-CS\n0 2 4 6 8 10 120\n2\n4\n6\n8\n10\n12\n RMSE=0.42\nThroughput (queries/sec)\nBatch size\nMamba-MATH\nFig.\n---\n[ref_id=xia2024] 14.\n---\n[ref_id=xia2024] Estimation and validation of LLM fine-tuning throughput for\ndifferent models, datasets for A40 GPU.\n---\n[ref_id=xia2024] Dots represent ground truth and\nlines present the estimation.\n---\n[ref_id=xia2024] 0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nRMSE=0.03\nMixtral-CS-A100-40GB\nDense Sparse\n0 5 10 15 200.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n RMSE=0.09\nMixtral-CS-A100-80GB\n0 5 10 15 200\n1\n2\n3\n4\n5\n RMSE=0.55\nMixtral-CS-H100\nThroughput (queries/sec)\nBatch size\nFig.\n---\n[ref_id=xia2024] 15.\n---\n[ref_id=xia2024] Estimation and validation of fine-tuning throughput for Mixtral GS\nfor different GPUs: A100 and H100.\n---\n[ref_id=xia2024] resource renting per hour is calculated based on CUDO\ncompute [33], as other popular cloud providers do not offer\ncost/hour rates for the NVIDIA A40 GPU.\n---\n[ref_id=xia2024] However, one\ncan easily adjust the GPU renting cost per hour to estimate\nthe cost on other clouds such as Amazon AWS [34] or\nLambda [35].\n---\n[ref_id=xia2024] Table IV estimates the cost for fine-tuning\nMixtral on the MATH dataset with a sparse setup, using\n10 epochs on different GPUs for a realistic cost estimate.\n---\n[ref_id=xia2024] Enterprises may use larger datasets for fine-tuning, such as,\nOpenOrca [36] and LaMini-instruction [37] containing more\nthan 2M queries.\n---\n[ref_id=xia2024] For OpenOrca, by scaling the cost by\nnumber of queries, our model predicts that the most cost-\neffective option to rent GPU resources on CUDO compute\nis NVIDIA H100 with a net cost of $3460.\n---\n[ref_id=xia2024] D.\n---\n[ref_id=xia2024] Generalization of the Analytical Model\nThe analytical models for estimating maximum batch size\nand throughput can be generalized to various LLM models\nand datasets.\n---\n[ref_id=xia2024] These models consider the characteristics of\nthe LLM, dataset, and GPU.\n---\n[ref_id=xia2024] Specifically, the maximum\nbatch size model combines GPU memory and LLM model\nsize to determine available memory for input data, while\ndataset sequence length and LLM sparsity determine space\nneeded per batch.\n---\n[ref_id=xia2024] In throughput estimation, based on the\nobservation we made (\u00a7IV-B4 Takeaway 5), GPU shifts from\nmemory-bound to compute-bound as batch size increases.\n---\n[ref_id=xia2024] This characteristic generally applies to all GPUs due to the\nresource constraint, so the logarithmic relation between batch\nsize and throughput persists.\n---\n[ref_id=xia2024] The sparsity in (2) is model\ndependent, the influence of GPU, LLM model, and dataset\nare embedded in the coefficients C2, C3, and C4 in (2).\n---\n[ref_id=xia2024] The coefficients in (1) and (2) are dependent on GPU,\nLLM model, and dataset;\n---\n[ref_id=xia2024] however, the underlying models\nare generalizable to unseen GPU, LLM model, and datasets.\n---\n[ref_id=xia2024] Although it takes some effort to sweep batch sizes and collect\nthroughput data points to fit our models, the benefits greatly\noutweigh the cost.\n---\n[ref_id=xia2024] Once the models are fit, our model can\nhelp choose the most cost-efficient GPU for fine-tuning LLM\nmodels, greatly saving resources and money.\n---\n[ref_id=xia2024] VI.\n---\n[ref_id=xia2024] R ELATED WORKS\nParameter-Efficient Fine-Tuning (PEFT) has been widely\nadopted to fine-tune LLM model for specialized tasks [15],\n[38]\u2013[43].\n---\n[ref_id=xia2024] MoE additioally train specialized experts for dif-\nferent areas and the dynamic selection of experts makes\nit possible to scale the fine-tuning workload to different\nexperts in parallel.\n---\n[ref_id=xia2024] [44]\u2013[47] show that MoE models can\nimprove the ability to process knowledge for specific tasks,\nwhile maintaining the world knowledge in LLM.\n---\n[ref_id=xia2024] Kim et\nal.\n---\n[ref_id=xia2024] [48] construct an analytical model to estimate GPU\nmemory consumption for distributed fine-tuning.\n---\n[ref_id=xia2024] The model\nalso provides insights into optimizing memory usage through\ntensor, model, and pipeline parallelism.\n---\n[ref_id=xia2024] VII.\n---\n[ref_id=xia2024] C ONCLUSIONS\nFine-tuning LLMs is an attractive technique for tailoring\nmodern language models using domain-specific knowledge in\na cost-effective manner.\n---\n[ref_id=xia2024] This paper delved into understanding\nthe performance of fine-tuning MoE LLM models on a single\nGPU.\n---\n[ref_id=xia2024] Our profiling demonstrated that sparse MoE layers\noffer the best bang-for-buck trade-off.\n---\n[ref_id=xia2024] Using our profiling\nresults, we developed and validated an accurate analytical\nmodel to estimate the cost of LLM fine-tuning.\n---\n[ref_id=xia2024] Using this\nmodel, we showed the dollar amount that needs to be\nbudgeted for fine-tuning LLMs, which is much lower than\npre-training.\n---\n[ref_id=xia2024] For example, our model predicted that fine-\ntuning a sparse Mixtral model using a realistic data size of\n2M queries can be done with NVIDIA H100 GPU with a cost\nof $3460.\n---\n[ref_id=xia2024] A way to further reduce cost based on our study\nis to add compute resources to accelerate the MoE layers.\n---\n[ref_id=xia2024] While we showcase our study on fine-tuning LLMs using a\nsingle GPU, extending this model to multi-GPU systems is\nleft for future exploration.\n---\n[ref_id=xia2024] ACKNOWLEDGMENTS\nThis work was supported in part by Semiconductor Re-\nsearch Corporation (SRC).\n---\n[ref_id=xia2024] We thank all the anonymous\nreviewers for their valuable comments and suggestions.\n---\n[ref_id=xia2024] TABLE IV\nESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE\nBASED ON OUR ANALYTICAL MODEL\nGPU Mem MBS Throughput Cost ($/hr) Cost ($)\nA40 48GB 4 1.01 0.79 32.7\nA100 80GB 17 2.74 1.67 25.4\nH100 80GB 17 4.90 2.1 17.9\nB.\n---\n[ref_id=xia2024] 0 20 40 60 80 100 1200\n5\n10\n15\n20\n25\n30\n35\n40\nA100-40GB\nA100-80GB\nA40\nH100\nbsz=28\nbsz=35\nProjected GPU capacity\nGround Truth Projection\nMax batch size\nGPU DRAM capacity\nFig. 13. Projected maximum batch size of Mixtral for different GPUs.\namount of GPU memory, and need to be subtracted in the\nanalytical model. Fig. 8 supports this by showing that on the\nsame dataset, BlackMamba can support larger batch size than\nMixtral because of its smaller model size.\nMoreover, the sequence length and sparsity also affect the\nmaximum batch size. Because the sparsity only affects the\nMoE part of the LLM, we multiply its influence by C1,\nwhich we call MoE coefficient. We apply the sequence length\nand the sparsity in the denominator as they are inversely\nrelated to batch size. Then, we multiply the result by C0,\nthe scaling coefficient , which scales the batch size by a\nconstant. The scaling coefficient is different across LLM\nmodels, because different models have different architecture\n(\u00a7III), and generate different amounts of intermediate data\nfor each query. The scaling coefficient for BlackMamba is\nhigher than that of Mixtral because it is a smaller model.\nFinally, we use floor to round it to the maximum integer.\nThe MoE coefficient and scaling coefficient vary across\nmodels. These coefficients are independent of GPU microar-\nchitectural parameters. We find the maximum batch size for\nboth LLM models on NVIDIA A40 (48GB), A100 (40GB),\nA100 (80GB), and H100 (80GB), and apply our model to\nfind the optimal coefficients. For Mixtral, C0 = 82 and\nC1 = 0 .95, and for BlackMamba, C0 = 83 and C1 = 0 .88.\nWhile we showcase these parameters for the models eval-\nuated, \u00a7V-D discusses how to generalize this approach for\nother models.\nUsing our analytical model, we demonstrate the maximum\nbatch sizes for fine-tuning on four different NVIDIA GPUs:\nA40, A100-40GB, A100-80GB and H100 with memory\ncapacities of 48GB, 40GB, 80GB, and 80GB, respectively.\nFig. 13 shows our projected maximum batch size and corre-\nlate it with experimented ground truth. While the maximum\nmemory capacity available in NVIDIA GPUs today is 80GB,\nwe use our analytical model to project the maximum batch\nsize that future GPUs might support. For GPU memory\ncapacities of 100GB and 120GB, our model predicts that\nthe maximum batch sizes supported for fine-tuning Mixtral\nwill be 28 and 35, respectively. Due to space limitations, we\nonly show the projection of Mixtral model.\nTABLE IV\nESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE\nBASED ON OUR ANALYTICAL MODEL\nGPU Mem MBS Throughput Cost ($/hr) Cost ($)\nA40 48GB 4 1.01 0.79 32.7\nA100 80GB 17 2.74 1.67 25.4\nH100 80GB 17 4.90 2.1 17.9\nB. Estimating Throughput\nAs discussed in \u00a7IV-B4, when the batch size increases, the\nLLM fine-tuning gradually switches from memory bound to\ncompute bound. When the compute resources are abundant,\nthe throughput increases almost linearly with batch size.\nHowever, when compute resources become constrained, the\nthroughput improvement gradually saturates. We model this\nbehavior using a logarithmic relation between batch size and\nthroughput. Our analytical model for maximum batch size is\nshown in (2).\nT hroughput = C2 \u2217 log( batch size\nsparsity \u2217 C3 ) + C4 (2)\nIn the equation, in addition to the basic logarithmic part, we\nhave three coefficients C2, C3, and C4. C2 is the scaling\ncoefficient, which depends on the LLM model, GPU archi-\ntecture, and the dataset. The higher the compute capability a\nGPU can provide, and the lower the LLM model and dataset\ncompute requirement is, the higher the scaling coefficient will\nbe. C3 is the MoE attenuation coefficient , which tunes how\nmuch the MoE sparsity affects the throughput. MoE sparsity\nonly affects the MoE part in LLM model, and thus should\nbe attenuated to avoid over compensation. This coefficient is\nonly LLM model dependent, because once the model is fixed,\nthe influence of sparsity is determined. C4 is the intercept,\nconceptually it equals to the throughput when batch size\nequals one, because the logarithmic part in (2) is zero when\nbatch size is one. Using scipy [32] to fit the model and\ngenerate four sets (C2, C3, C4), for each model and dataset\ncombination.\nTo estimate the accuracy of this model, we correlate the\nmodel output with experimental data from our study. Fig. 14\nshows this correlation study, where discrete data points (dots)\nrepresent experimental values, and the line represents output\nof our analytical model. We use both dense and sparse\nMixtral and BlackMamba for both datasets used in our study.\nThe figure clearly shows that our model accurately predicts\nLLM fine-tuning throughput with a Root Mean Squared Error\n(RMSE) of less than 0.8. Fig. 15 shows the correlation study\nof the analytical model of three other GPUs, A100 (40GB),\nA100 (80GB), and H100. The RMSE is less than 0.6, close\nto that of A40.\nC. Estimating the Total Fine-Tuning Cost\nUsing the throughput estimation, we calculate the cost\nof fine-tuning LLMs for different GPUs. The cost of GPU\n---\n[ref_id=xia2024] (5) The\nperformance of sparse fine-tuning is close to that of dense\nfine-tuning, with the exception of Mixtral on HE.\n---\n[ref_id=xia2024] under 10% on BlackMamba. We assess accuracy improve-\nments post-fine-tuning and compare the learning capabilities\nof dense and sparse versions of both models.\nFig. 3 shows the testing accuracy of Mixtral and Black-\nMamba on two datasets Hellaswag (HE) and GSM8K (GS).\nWe fine-tune both models using the sparse and dense setups\ndescribed in \u00a7III for 10 epochs, and test the accuracy of\nthe fine-tuned model at each epoch. We make the following\nobservations in Fig. 3. (1) Fine-tuning converges relatively\nquickly. Typically, 10 epochs are enough for fine-tune models\nto stabilize at or close to their peak accuracy. On GS, both\nmodels are close to their peak accuracy at the first epoch.\n(2) The smaller model BlackMamba takes relatively more\nepochs to reach its peak accuracy, as it took BlackMamba 5\nepochs to converge on HE. (3) The larger model Mixtral has\nbetter accuracy compared to BlackMamba on both datasets.\n(4) Both models perform better on the CS dataset HE than\non the GS dataset GS. This is because math is harder for\nsmaller LLMs to learn [24]. The BlackMamba model is\ninadequate for fine-tuning GS. This is likely attributed to\nthe complexity of mathematical tasks and the smaller model\nsize of BlackMamba. Additionally, Mamba is specifically\nengineered for long sequence modeling, potentially resulting\nin unsatisfactory arithmetic reasoning ability [25]. Thus, in\nour characterization study in later sections, we will not show\nthe results for BlackMamba fine-tuned on MATH. (5) The\nperformance of sparse fine-tuning is close to that of dense\nfine-tuning, with the exception of Mixtral on HE. However,\neven for this outlier, sparse fine-tuning achieves similar peak\naccuracy compared to dense; we see a drop of accuracy\nbetween the epoch 4 and 5, and indicates sparse fine-tuning is\nmore vulnerable to over-fitting, especially for easy tasks [26].\nFollowing the above insights, the key take-away of this\nanalysis can be summarized as follows.\nTakeaway 1. Sparse model can be trained as well\nas its dense counterpart.\nTakeaway 2. Fine-tuning generally takes less ten\nepochs to reach peak accuracy.\nB. Analysis of Runtime Performance\nAfter confirming that both Mixtral and BlackMamba can\nbe fine-tuned to achieve acceptable accuracy, we examine\ntheir performance in a resource-constrained environment us-\ning a single GPU. This setup highlights unique runtime char-\nacteristics such as execution time breakdown, throughput,\nmaximum batch size, compute and memory utilization, load\nimbalance, and sensitivity analysis. We also compare sparse\nand dense models. Insights from this study will help develop\na robust analytical model for estimating fine-tuning costs.\n1) Maximum Batch Size Support: The maximum batch\nsize in fine-tuning is determined by GPU memory size,\nmodel size, sequence length, and MoE sparsity. The LLM\nTABLE III\nMAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE\nAND S:SPARSE .\nMixtral-D Mixtral-S BlackMamba-D BlackMamba-S\nCS 2 8 6 20\nMATH 1 3 2 8\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)\n0.0\n2.0\n4.0\n6.0\n8.0\nForward Backward Optimizer\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)\n0.0\n0.5\n1.0\n1.5\n2.0\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig. 4. Execution time breakdown.\noccupies a certain amount of GPU memory, with the re-\nmainder available for intermediate data during fine-tuning.\nLonger sequence lengths consume more memory, and denser\nMoE configurations require additional memory space. We\ndiscuss the heuristic for determining the maximum batch size\nin \u00a7V. Based on our experimental study on NVIDIA A40\nGPU with 48GB memory, we empirically find and report\nthe maximum batch size supported by different model and\ndataset combinations in Table III.\n2) Execution Time Breakdown: We first analyze the high-\nlevel execution time breakdown for Mixtral and Black-\nMamba. The purpose of this study is to understand where\ndoes this workload spend most of its time. As discussed in\n\u00a7III, we conduct this study using a sequence length of 128.\nAt a high-level, the fine-tuning workload can be divided\ninto three stages: (1) forward, (2) backward, and (3) opti-\nmizer. We use a batch size of 1 and the maximum batch size\nsupported by a model-dataset combination to show workload\ncharacteristics. Fig. 4 illustrates the following insights. (1)\nThe optimizer stage in BlackMamba fine-tuning takes a\nconsiderable portion of the running time (up to 53% when\nconducting sparse fine-tuning with batch size = 1), while\nthe execution time share of the optimizer stage in Mixtral\nfine-tuning is negligible. The running time of the optimizer\nstage depends only on the number of parameters that need\nto be updated during fine-tuning. This difference is primarily\ndue to the different fine-tuning strategies applied to these two\nmodels: only the parameters in the LoRA module are updated\nfor Mixtral fine-tuning, whereas BlackMamba undergoes full\nfine-tuning. (2) The runtime of the forward and backward\nstages increases with sparsity and batch size due to the\nincreased amount of computation. (3) The backward stage\ntypically takes more time than the forward stage. In Black-\nMamba, the backward stage demands more computation than\n---\n[ref_id=xia2024] Our evaluation (the figure is omitted from the paper\ndue to page limitation) shows that the latency for Mixtral\nremains almost constant across different sequence lengths,\nwhile BlackMamba fine-tuning exhibited a slight reduction\nin latency as sequence length increased, with approximately\n19% and 25% decreases for sparse and dense fine-tuning,\nrespectively.\n---\n[ref_id=xia2024] Fig. 12. Pseudo code for MoE layers.\nciency [27]. During the training process of MoE models,\neach token is dynamically assigned to the top-k experts based\non routing scores. This strategy often leads to most tokens\nbeing assigned to a small number of experts, resulting in load\nimbalance and slower training. Additionally, some experts\nreceive insufficient training, which degrades overall model\nperformance [28]. A na\u00a8\u0131ve approach to address this imbalance\nis to use token dropping and padding to ensure that the\nnumber of tokens assigned to each expert is equal [29].\nHowever, this method sacrifices model quality or leads to\nwasted computation. In this section, we analyze how fine-\ntuning influences the token distribution among experts. We\ncompare the token distribution of Mixtral and BlackMamba\nbefore and after fine-tuning to understand the impact of this\nprocess.\nWe extract 1,000 examples from both the CS and MATH\ndatasets to test the original models without tuning and the\nmodels after 10 epochs of tuning on these datasets. Fig. 12\nprovides the pseudo code for MoE layers with top-k gating.\nIn this process, the hidden states are first sent to the router\nof the MoE layer, which generates router logits. These logits\ndetermine the priority of each expert for each token. Based on\nthe router score for each token, tokens are grouped together\nand sent to their assigned experts. This top-k routing strategy\ncan lead to load imbalance if the model has not been pre-\ntrained for balance.\nFig. 11 evidently shows that fine-tuning causes load im-\nbalance in Mixtral for both datasets. Comparing variance\nbefore and after fine-tuning (e.g., HE vs. HE tuned), the\ntoken assignment variance increased from 55 to 112 for\nCS and from 21 to 79 for GS. Expert 3 became the\nmost frequently used and important expert post fine-tuning.\nConversely, there is a decrease in the variance of token\ndistribution for BlackMamba on the CS dataset, dropping\nfrom 150 to 93. For the GS dataset, the token distribution\nvariance for BlackMamba remains almost unchanged after\nfine-tuning. This suggests that load-imbalance has a less\ndisruptive impact on fine-tuning for BlackMamba compared\nto Mixtral. From Fig. 11, we can also observe that Mixtral\ndemonstrates better load balance in both tasks compared to\nBlackMamba, despite the increased load imbalance after fine-\ntuning. The increased level of imbalance after fine-tuning\nsuggests GPU load balancing techniques can be helpful.\nBoth single GPU load balancing [30] and multi-GPU load\nbalancing [31] have been proposed to address this issue.\nTakeaway 6. The effect of fine-tuning on expert\nload imbalance in the MoE layer is LLM model and\ndataset dependent.\n6) Sensitivity Study on Sequence Length: To further ana-\nlyze the effect of sequence length on the fine-tuning process,\nwe chose the batch size that would maximize the memory\nfor each sequence length (64, 128, 256, 512, and 1024) and\ncompared the latency, SM utilization, and DRAM utiliza-\ntion. Our evaluation (the figure is omitted from the paper\ndue to page limitation) shows that the latency for Mixtral\nremains almost constant across different sequence lengths,\nwhile BlackMamba fine-tuning exhibited a slight reduction\nin latency as sequence length increased, with approximately\n19% and 25% decreases for sparse and dense fine-tuning,\nrespectively. This is due to the varying maximum batch sizes\nsupported by each sequence length, resulting in a similar\nnumber of tokens in each batch. Because latency remains\nconsistent with increasing sequence length and we can use\nlarger batch sizes, throughput is higher for shorter sequences.\nV. A NALYTICAL MODEL TO ESTIMATE THE COST OF\nFINE -T UNING LLM S\nWhile training LLMs from scratch is a cost-prohibitive\nprocess, fine-tuning LLMs offers an attractive solution to\nalign LLMs to desired behaviors. One such example is fine-\ntuning LLMs to a domain-specific use-cases, for example, to\nanswer math questions. \u00a7IV-A shows that it is possible to\nfine-tune pre-trained LLMs on domain-specific tasks to sig-\nnificant improve accuracy. While this is a desired approach,\ncurrently, no model exists that can predict the cost of fine-\ntuning LLMs.\nFine-tuning LLMs is complex, influenced by factors like\nmodel size, GPU memory, dataset sequence length, and MoE\nsparsity, all affecting batch size and throughput. By integrat-\ning these factors with GPU costs, we can identify the most\ncost-efficient GPU for pre-tuning tasks. This section presents\nan analytical model based on previous characterization.\nThis model estimates cloud-based fine-tuning costs for a\ngiven dataset and LLM. Developed from previous sections,\nit can be adapted for other LLMs by adjusting parameters. It\nassumes using the maximum batch size supported by GPU\nmemory to optimize cost. We first estimate this batch size,\nthen use it to evaluate throughput and fine-tuning costs.\nA. Estimating Maximum Batch Size\nThe maximum batch size is the maximum number of\nqueries that can fit in GPU memory at once. Our analytical\nmodel for maximum batch size is shown in (1).\nM ax BSZ = \u230aC0\u2217 GP U mem \u2212 model mem\nseq len \u2217 ((1 \u2212 C1) + C1 \u2217 sparsity) \u230b\n(1)\nIntuitively, larger GPU memory allows for higher batch\nsizes. In the meantime, the LLM model will take up a certain\n---\n[ref_id=xia2024] In this\npaper, we characterize sparse Mixture of Experts (MoE) based\nLLM fine-tuning to understand their accuracy and runtime\nperformance on a single GPU.\n---\n[ref_id=xia2024] Understanding the Performance and Estimating the\nCost of LLM Fine-Tuning\nYuchen Xia1 Jiho Kim 2 Yuhan Chen1 Haojie Ye1 Souvik Kundu 3\nCong (Callie) Hao 2 Nishil Talati1\n1University of Michigan 2Georgia Institute of Technology 3Intel Labs\nAbstract\u2014Due to the cost-prohibitive nature of training Large\nLanguage Models (LLMs), fine-tuning has emerged as an attrac-\ntive alternative for specializing LLMs for specific tasks using\nlimited compute resources in a cost-effective manner. In this\npaper, we characterize sparse Mixture of Experts (MoE) based\nLLM fine-tuning to understand their accuracy and runtime\nperformance on a single GPU. Our evaluation provides unique\ninsights into the training efficacy of sparse and dense versions of\nMoE models, as well as their runtime characteristics, including\nmaximum batch size, execution time breakdown, end-to-end\nthroughput, GPU hardware utilization, and load distribution.\nOur study identifies the optimization of the MoE layer as crucial\nfor further improving the performance of LLM fine-tuning.\nUsing our profiling results, we also develop and validate an\nanalytical model to estimate the cost of LLM fine-tuning on\nthe cloud. This model, based on parameters of the model and\nGPU architecture, estimates LLM throughput and the cost\nof training, aiding practitioners in industry and academia to\nbudget the cost of fine-tuning a specific model.\nI. I NTRODUCTION\nLarge Language Models (LLMs) are widely utilized in\nNatural Language Processing (NLP) [1]. Modern LLMs\ntypically possess billions to trillions of parameters, neces-\nsitating extensive time and resources for training. For in-\nstance, the estimated cost of training OpenAI\u2019s GPT-4 model\nexceeds $100 million, rendering it financially prohibitive\nfor most small-to-medium size enterprises and the academic\ncommunity [2]. Given the open-sourcing of numerous pre-\ntrained LLMs (e.g., LLAMA [3] and Mixtral [4]), fine-\ntuning has emerged as an attractive alternative for further\nspecializing these models in a cost-effective manner [5].\nGiven the learning ability of pre-trained models, it is feasible\nto use a domain-specific dataset to align the desired behav-\niors of LLMs through supervised fine-tuning on instruction-\nfollowing tasks [6]. Unlike pre-training, fine-tuning can be\nconducted in a resource-constrained environment, typically\nusing one or a few GPUs. Consequently, fine-tuning presents\na compelling case for applications such as specialized ques-\ntion answering within enterprises, legal document analysis\nand drafting, healthcare/medical research, technical and IT\nsupport, among others [7].\nThis paper characterizes LLM fine-tuning with two pri-\nmary objectives: (1) understanding the performance charac-\nteristics of LLM fine-tuning, and (2) developing an analytical\nmodel to estimate the cost of fine-tuning on the cloud. Given\nour focus on cost-efficient LLM fine-tuning, we concen-\ntrate on fine-tuning sparse Mixture-of-Expert (MoE) models.\nSpecifically, we employ an attention-based MoE model, Mix-\ntral [4], and a state-space MoE model, BlackMamba [8]. Us-\ning these models and two domain-specific datasets for math-\nematics and common-sense question-answering, we conduct\nan in-depth profiling study to understand their performance\ncharacteristics with a single GPU. We compare the dense\nand sparse counterparts of the investigated MoE models to\nevaluate their learning rates and runtime performance. Our\ninvestigation covers memory consumption, maximum batch\nsize supported within a single GPU memory budget, exe-\ncution time breakdown and bottlenecks, overall throughput,\nmicroarchitectural performance counters, and runtime load\ndistribution. The insights gained from our study are used to\ndevelop and validate an analytical model to estimate the cost.\nOur characterization uncovers the following unique in-\nsights. (1) Fine-tuning can be achieved in less than 10 epochs,\nand sparse MoE model that activates a subset of experts\ncan learn as well as its dense counterparts. (2) MoE layer\nconsumes the highest fraction of execution time in LLM\nfine-tuning; optimizing MoE layer performance is key to\nimproving the overall cost of LLM fine-tuning. (3) Sparse\nMoE model improves end-to-end throughput by supporting\na larger batch size. Given similar learning abilities of sparse\nand dense models, it is desired to use a sparse MoE model\nfor cost-effective fine-tuning. (4) The workload becomes\ncompute bound by increasing batch size; improving compute\nresources will increase performance. (5) Fine-tuning sparse\nmodel leads to more load imbalance.\nBased on these insights, we create an analytical model\nto estimate the cost of LLM fine-tuning based on model\nsize, dataset size, and GPU architecture. First, we estimate\nthe maximum batch size for a given GPU memory, then\ncompute fine-tuning throughput. We validate this throughput\nwith experimental results, showing an RMSE of less than\n0.55. Using the estimated throughput, our model calculates\nthe fine-tuning cost for different cloud providers.\nThe contributions of this paper are as follows.\n\u2022 Make a case for LLM fine-tuning for specializing pre-\ntrained models in a cost-effective manner.\narXiv:2408.04693v1  [cs.CL]  8 Aug 2024\n---\n[ref_id=xia2024] The de-quantization operation in Mix-\ntral fine-tuning also becomes significant, especially with low\nsparsity and small batch sizes.\n---\n[ref_id=xia2024] Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0.0\n1.0\n2.0\n3.0\n4.0\n5.0\nInput normalization Attention Post attention norm. MoE\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0.0\n0.5\n1.0\n1.5\nRMS layernorm Mamba MoE\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig. 5. Execution time breakdown in terms of different model layers.\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0\n2000\n4000\n6000\nmatmul(w2)\nw2_dequant\nmatmul(w3)\nw3_dequant\nmatmul(w1)\nw1_dequant\nsoftmax\ntopk\nmatmul(router)\nrouter_dequant\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0\n400\n800\n1200\n1600\n2000\nmatmul(w1)\ngelu\nmatmul(w2)\nelementwise_mult\ntop_k\nsigmoid\nmatmul(router)\n    Execution Time Breakdown (\u03bcs)\nMixtral\nMamba\nFig. 6. Execution breakdown of the MoE layer for different kernels.\nthe forward stage due to the need for gradient calculation\nand propagation, resulting in two matrix multiplication op-\nerations. In Mixtral fine-tuning, gradient calculation adds\nminimal computation as only a small portion of parameters\nneed it. However, gradient checkpointing in Mixtral saves\nmemory but increases the backward stage runtime due to the\nre-computation of intermediate values.\nWe further investigate the execution breakdown based\non various layers in two LLM models. For Mixtral, these\nlayers include input normalization, attention, post-attention\nnormalization, and MoE. In contrast, BlackMamba comprises\nthe Mamba layer, Root Mean Squared (RMS) layer nor-\nmalization, and MoE. As shown in Fig. 5, the MoE layer\nis the most time-consuming, accounting for 85% of the\noverall execution time on average. The execution time for\nthe MoE layer encompasses both the forward and backward\npasses during fine-tuning. Consequently, MoE is the costliest\nlayer and a prime target for optimization to enhance the\nperformance of LLM fine-tuning.\nTo concretely understand the opportunity for improving\nMoE layer performance, we also perform a kernel-level anal-\nysis within the MoE layer. Fig. 7 illustrates the architecture\nof the MoE layer in both Mixtral and BlackMamba models.\nEach expert in BlackMamba consists of a standard Feed-\nForward Network (FFN) layer with two serially connected\nweight matrices (W1 and W2) and a Gelu activation layer\nbetween. In contrast, experts in Mixtral are FFN layers with\nSwish-Gated Linear Units, involving an additional weight\nFig. 7. Expert architectures for Mixtral (top) and BlackMamba (bottom).\nmatrix (W3) in parallel with W1.\nFig. 6 shows the kernel-level MoE time breakdown. The\nfigure clearly shows that matrix multiplication (W1, W2,\nand W3) is the largest component of the MoE layer for\nboth BlackMamba and Mixtral. As batch size and sparsity\nincrease, so does computational demand, prolonging matrix\nmultiplication latency. The de-quantization operation in Mix-\ntral fine-tuning also becomes significant, especially with low\nsparsity and small batch sizes. While quantization reduces\nmodel size and memory footprint, it can increase computation\ntime due to de-quantization. This highlights the need to\nevaluate trade-offs between memory savings and computation\ntime, particularly in scenarios with small batch sizes and\nsequence lengths.\nTakeaway 3. Matrix multiplication operations in the\nMoE layer contribute significantly to the end-to-end\nexecution time, making the MoE layer the costliest\ncomponent in LLM fine-tuning.\n3) Fine-Tuning Throughput: Next, we present the fine-\ntuning throughput of Mixtral and BlackMamba on the MATH\nand CS datasets separately in Fig. 8. We use a throughput\nmetric of queries/second processed, where a query includes a\nprompt and a ground-truth answer for fine-tuning. To obtain\nthese results, we extract 1000 examples from each dataset\nand fine-tuned Mixtral and BlackMamba on them using the\nsmallest batch size (batch size = 1) and the largest batch size\nthat would fill the GPU memory.\nAs illustrated in Fig. 8, sparse fine-tuning achieves higher\nthroughput than dense fine-tuning. This is because the sparse\nfine-tuning baseline consumes less memory to store interme-\ndiate values, which allows for higher batch sizes compared\nto its dense counterpart. Additionally, with the same batch\nsize, sparse fine-tuning achieves higher throughput because\n---\n[ref_id=xia2024] matmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n   DRAM Bandwidth Utilization (%)\nFig. 10. GPU DRAM bandwidth utilization of different kernels in the MoE layer for different batch sizes.\nweighted by the amount of time each kernel takes. We\nuse a sequence length of 128 (\u00a7III). Sequence length will\ninfluence the choice of batch size, and we discuss the effects\nof sequence length on runtime, throughput, SM utilization,\nand memory utilization in \u00a7IV-B6. For dense fine-tuning, we\nshow the SM utilization of batch size 1 and the maximum\nbatch size that fits into memory; for sparse fine-tuning,\nwe use the two batch sizes for dense fine-tuning, and the\nmaximum batch size that fits into memory.\nFig. 9 shows the SM utilization of different kernels in the\nMoE layer, which offers the following insights. (1) For both\nsparse and dense fine-tuning, SM utilization increases with\nbatch size due to higher parallelism and GPU activity. (2)\nSparse fine-tuning has lower SM utilization than dense fine-\ntuning at the same batch size because it activates only 2\nout of 8 experts, reducing parallelism. Consequently, sparse\nfine-tuning supports a higher maximum batch size. Both\nachieve similar maximum SM utilization at their peak batch\nsizes. (3) The de-quantization kernel maintains high SM\nutilization regardless of batch size. (4) Matrix multiplication\nkernels achieve higher SM utilization with larger batch sizes,\nleveraging the GPU\u2019s parallel processing capabilities.\nMemory resource utilization study. Fig. 10 shows the\nkernel-level breakdown of GPU memory bandwidth utiliza-\ntion. We use the same experimental setup as in the evalua-\ntion of SM utilization, and find the following insights. (1)\nFor both sparse and dense fine-tuning, the time-weighted\nmemory utilization decreases with increasing batch size.\nThis is because the model parameters are loaded once and\nshared by all queries in a batch. However, a larger batch\nincreases the execution time (as discussed in \u00a7IV-B6),\nleading to a lower average memory bandwidth utilization.\nHE HE_tuned GS GS_tuned0\n25\n50\n75\n100\nExpert 0\nExpert 1\nExpert 2\nExpert 3\nExpert 4\nExpert 5\nExpert 6\nExpert 7\nHE HE_tuned GS GS_tuned0\n25\n50\n75\n100                          Avg Num. of Token Per Query\nMixtral\nMamba\nvar=55.5 var=112.3\nvar=21.2\nvar=79.2\nvar=150.7 var=93.3\nvar=186.5 var=187.9\nFig. 11. Token distribution to different experts.\n(2) For the same batch size, sparse fine-tuning achieves\nhigher memory bandwidth utilization than dense fine-tuning\ndue to shorter execution times. (3) Dequant layers\u2019 memory\nutilization is batch-size-independent, while matmul layers\u2019\nutilization decreases with larger batch sizes. To maximize\nGPU memory usage, a sufficiently large batch size should be\nused. With large batch sizes, fine-tuning becomes compute-\nbound, indicating a need for improved compute resources in\nfuture hardware to better utilize memory bandwidth.\nTakeaway 5 . As the batch size increases, LLM\nfine-tuning transitions from being memory-bound to\ncompute-bound.\n5) Effect of Load Imbalance Due to Fine-Tuning: Recent\ntrends in deploying expert parallelism in MoE models have\nhighlighted load-imbalanced computation among experts as\na significant issue impacting inference and training effi-\n---\n[ref_id=xia2024] Page 7\n---\n[ref_id=xia2024] matmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n   DRAM Bandwidth Utilization (%)\nFig.\n---\n[ref_id=xia2024] 10.\n---\n[ref_id=xia2024] GPU DRAM bandwidth utilization of different kernels in the MoE layer for different batch sizes.\n---\n[ref_id=xia2024] weighted by the amount of time each kernel takes.\n---\n[ref_id=xia2024] We\nuse a sequence length of 128 (\u00a7III).\n---\n[ref_id=xia2024] Sequence length will\ninfluence the choice of batch size, and we discuss the effects\nof sequence length on runtime, throughput, SM utilization,\nand memory utilization in \u00a7IV-B6.\n---\n[ref_id=xia2024] For dense fine-tuning, we\nshow the SM utilization of batch size 1 and the maximum\nbatch size that fits into memory;\n---\n[ref_id=xia2024] for sparse fine-tuning,\nwe use the two batch sizes for dense fine-tuning, and the\nmaximum batch size that fits into memory.\n---\n[ref_id=xia2024] Fig.\n---\n[ref_id=xia2024] 9 shows the SM utilization of different kernels in the\nMoE layer, which offers the following insights.\n---\n[ref_id=xia2024] (1) For both\nsparse and dense fine-tuning, SM utilization increases with\nbatch size due to higher parallelism and GPU activity.\n---\n[ref_id=xia2024] (2)\nSparse fine-tuning has lower SM utilization than dense fine-\ntuning at the same batch size because it activates only 2\nout of 8 experts, reducing parallelism.\n---\n[ref_id=xia2024] Consequently, sparse\nfine-tuning supports a higher maximum batch size.\n---\n[ref_id=xia2024] Both\nachieve similar maximum SM utilization at their peak batch\nsizes.\n---\n[ref_id=xia2024] (3) The de-quantization kernel maintains high SM\nutilization regardless of batch size.\n---\n[ref_id=xia2024] (4) Matrix multiplication\nkernels achieve higher SM utilization with larger batch sizes,\nleveraging the GPU\u2019s parallel processing capabilities.\n---\n[ref_id=xia2024] Memory resource utilization study.\n---\n[ref_id=xia2024] Fig.\n---\n[ref_id=xia2024] 10 shows the\nkernel-level breakdown of GPU memory bandwidth utiliza-\ntion.\n---\n[ref_id=xia2024] We use the same experimental setup as in the evalua-\ntion of SM utilization, and find the following insights.\n---\n[ref_id=xia2024] (1)\nFor both sparse and dense fine-tuning, the time-weighted\nmemory utilization decreases with increasing batch size.\n---\n[ref_id=xia2024] This is because the model parameters are loaded once and\nshared by all queries in a batch.\n---\n[ref_id=xia2024] However, a larger batch\nincreases the execution time (as discussed in \u00a7IV-B6),\nleading to a lower average memory bandwidth utilization.\n---\n[ref_id=xia2024] HE HE_tuned GS GS_tuned0\n25\n50\n75\n100\nExpert 0\nExpert 1\nExpert 2\nExpert 3\nExpert 4\nExpert 5\nExpert 6\nExpert 7\nHE HE_tuned GS GS_tuned0\n25\n50\n75\n100                          Avg Num.\n---\n[ref_id=xia2024] of Token Per Query\nMixtral\nMamba\nvar=55.5 var=112.3\nvar=21.2\nvar=79.2\nvar=150.7 var=93.3\nvar=186.5 var=187.9\nFig.\n---\n[ref_id=xia2024] 11.\n---\n[ref_id=xia2024] Token distribution to different experts.\n---\n[ref_id=xia2024] (2) For the same batch size, sparse fine-tuning achieves\nhigher memory bandwidth utilization than dense fine-tuning\ndue to shorter execution times.\n---\n[ref_id=xia2024] (3) Dequant layers\u2019 memory\nutilization is batch-size-independent, while matmul layers\u2019\nutilization decreases with larger batch sizes.\n---\n[ref_id=xia2024] To maximize\nGPU memory usage, a sufficiently large batch size should be\nused.\n---\n[ref_id=xia2024] With large batch sizes, fine-tuning becomes compute-\nbound, indicating a need for improved compute resources in\nfuture hardware to better utilize memory bandwidth.\n---\n[ref_id=xia2024] Takeaway 5 .\n---\n[ref_id=xia2024] As the batch size increases, LLM\nfine-tuning transitions from being memory-bound to\ncompute-bound.\n---\n[ref_id=xia2024] 5) Effect of Load Imbalance Due to Fine-Tuning: Recent\ntrends in deploying expert parallelism in MoE models have\nhighlighted load-imbalanced computation among experts as\na significant issue impacting inference and training effi-\n---\n[ref_id=xia2024] (1) Fine-tuning can be achieved in less than 10 epochs,\nand sparse MoE model that activates a subset of experts\ncan learn as well as its dense counterparts.\n---\n[ref_id=xia2024] Understanding the Performance and Estimating the\nCost of LLM Fine-Tuning\nYuchen Xia1 Jiho Kim 2 Yuhan Chen1 Haojie Ye1 Souvik Kundu 3\nCong (Callie) Hao 2 Nishil Talati1\n1University of Michigan 2Georgia Institute of Technology 3Intel Labs\nAbstract\u2014Due to the cost-prohibitive nature of training Large\nLanguage Models (LLMs), fine-tuning has emerged as an attrac-\ntive alternative for specializing LLMs for specific tasks using\nlimited compute resources in a cost-effective manner. In this\npaper, we characterize sparse Mixture of Experts (MoE) based\nLLM fine-tuning to understand their accuracy and runtime\nperformance on a single GPU. Our evaluation provides unique\ninsights into the training efficacy of sparse and dense versions of\nMoE models, as well as their runtime characteristics, including\nmaximum batch size, execution time breakdown, end-to-end\nthroughput, GPU hardware utilization, and load distribution.\nOur study identifies the optimization of the MoE layer as crucial\nfor further improving the performance of LLM fine-tuning.\nUsing our profiling results, we also develop and validate an\nanalytical model to estimate the cost of LLM fine-tuning on\nthe cloud. This model, based on parameters of the model and\nGPU architecture, estimates LLM throughput and the cost\nof training, aiding practitioners in industry and academia to\nbudget the cost of fine-tuning a specific model.\nI. I NTRODUCTION\nLarge Language Models (LLMs) are widely utilized in\nNatural Language Processing (NLP) [1]. Modern LLMs\ntypically possess billions to trillions of parameters, neces-\nsitating extensive time and resources for training. For in-\nstance, the estimated cost of training OpenAI\u2019s GPT-4 model\nexceeds $100 million, rendering it financially prohibitive\nfor most small-to-medium size enterprises and the academic\ncommunity [2]. Given the open-sourcing of numerous pre-\ntrained LLMs (e.g., LLAMA [3] and Mixtral [4]), fine-\ntuning has emerged as an attractive alternative for further\nspecializing these models in a cost-effective manner [5].\nGiven the learning ability of pre-trained models, it is feasible\nto use a domain-specific dataset to align the desired behav-\niors of LLMs through supervised fine-tuning on instruction-\nfollowing tasks [6]. Unlike pre-training, fine-tuning can be\nconducted in a resource-constrained environment, typically\nusing one or a few GPUs. Consequently, fine-tuning presents\na compelling case for applications such as specialized ques-\ntion answering within enterprises, legal document analysis\nand drafting, healthcare/medical research, technical and IT\nsupport, among others [7].\nThis paper characterizes LLM fine-tuning with two pri-\nmary objectives: (1) understanding the performance charac-\nteristics of LLM fine-tuning, and (2) developing an analytical\nmodel to estimate the cost of fine-tuning on the cloud. Given\nour focus on cost-efficient LLM fine-tuning, we concen-\ntrate on fine-tuning sparse Mixture-of-Expert (MoE) models.\nSpecifically, we employ an attention-based MoE model, Mix-\ntral [4], and a state-space MoE model, BlackMamba [8]. Us-\ning these models and two domain-specific datasets for math-\nematics and common-sense question-answering, we conduct\nan in-depth profiling study to understand their performance\ncharacteristics with a single GPU. We compare the dense\nand sparse counterparts of the investigated MoE models to\nevaluate their learning rates and runtime performance. Our\ninvestigation covers memory consumption, maximum batch\nsize supported within a single GPU memory budget, exe-\ncution time breakdown and bottlenecks, overall throughput,\nmicroarchitectural performance counters, and runtime load\ndistribution. The insights gained from our study are used to\ndevelop and validate an analytical model to estimate the cost.\nOur characterization uncovers the following unique in-\nsights. (1) Fine-tuning can be achieved in less than 10 epochs,\nand sparse MoE model that activates a subset of experts\ncan learn as well as its dense counterparts. (2) MoE layer\nconsumes the highest fraction of execution time in LLM\nfine-tuning; optimizing MoE layer performance is key to\nimproving the overall cost of LLM fine-tuning. (3) Sparse\nMoE model improves end-to-end throughput by supporting\na larger batch size. Given similar learning abilities of sparse\nand dense models, it is desired to use a sparse MoE model\nfor cost-effective fine-tuning. (4) The workload becomes\ncompute bound by increasing batch size; improving compute\nresources will increase performance. (5) Fine-tuning sparse\nmodel leads to more load imbalance.\nBased on these insights, we create an analytical model\nto estimate the cost of LLM fine-tuning based on model\nsize, dataset size, and GPU architecture. First, we estimate\nthe maximum batch size for a given GPU memory, then\ncompute fine-tuning throughput. We validate this throughput\nwith experimental results, showing an RMSE of less than\n0.55. Using the estimated throughput, our model calculates\nthe fine-tuning cost for different cloud providers.\nThe contributions of this paper are as follows.\n\u2022 Make a case for LLM fine-tuning for specializing pre-\ntrained models in a cost-effective manner.\narXiv:2408.04693v1  [cs.CL]  8 Aug 2024\n---\n[ref_id=xia2024] Sparse model significantly improves\nthroughput, reducing end-to-end cost of fine-tuning.\n---\n[ref_id=xia2024] Mixtral-CS0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.5 0.3 0.7\n1.7\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\nMixtral-MATH0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.3\n1.0\nDense(bsz=1)\nSparse(bsz=1)\nSparse(bsz=3)\nBlackmamba-CS0\n5\n10\n15\n20\n2.3\n7.9\n2.4\n10.5\n14.9\nDense(bsz=1)\nDense(bsz=6)\nSparse(bsz=1)\nSparse(bsz=6)\nSparse(bsz=20)\nBlackmamba-MATH0\n5\n10\n15\n20\n2.2 5.3 2.2\n6.5\n11.6\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\n      Throughput (quries/second)\nFig. 8. Query throughput of Mixtral and BlackMamba.\nmatmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n      SM Utilization (%)\nFig. 9. GPU SM utilization of different kernels in the MoE layer for different batch sizes.\nit involves fewer computational demands, resulting in lower\nlatency. This is evident when comparing the throughput of\nbatch size of 2 in Mixtral-CS for dense (0.5 qps) vs. sparse\n(0.7 qps) models.\nFig. 8 also shows that throughput does not increase linearly\nwith batch size. For instance, sparse fine-tuning of Mixtral-\nCS improves throughput by 1.9 \u00d7 when increasing the batch\nsize from 1 to 2, but only by 4.8 \u00d7 when increasing from\n1 to 8. With smaller batch sizes, the SM utilization rate\nis lower, providing enough computational resources to feed\nmore operations in parallel. However, as the batch size con-\ntinues to increase, the SMs become saturated (more details in\n\u00a7IV-B4), and we can no longer hide latency by better utilizing\ncomputational resources.\nTakeaway 4. Sparse model significantly improves\nthroughput, reducing end-to-end cost of fine-tuning.\n4) Hardware characterization: As shown in Fig. 4, the\nexecution time of LLM fine-tuning is dominated by the MoE\nlayer. To offer further insights, we use detailed microarchi-\ntecture hardware metrics on the GPU to further understand\nexecution bottlenecks in the MoE layer. The goal of this\nstudy is to identify whether various kernels in the MoE layers\nare bound by compute or memory resources, and how future\nGPU designs can further scale performance.\nCompute resource utilization study. Fig. 9 shows the\nkernel-level breakdown of GPU Streaming Multi-processor\n(SM) utilization for the MoE layer. This utilization is\n---\n[ref_id=xia2024] For instance, sparse fine-tuning of Mixtral-\nCS improves throughput by 1.9 \u00d7 when increasing the batch\nsize from 1 to 2, but only by 4.8 \u00d7 when increasing from\n1 to 8.\n---\n[ref_id=xia2024] Mixtral-CS0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.5 0.3 0.7\n1.7\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\nMixtral-MATH0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.3\n1.0\nDense(bsz=1)\nSparse(bsz=1)\nSparse(bsz=3)\nBlackmamba-CS0\n5\n10\n15\n20\n2.3\n7.9\n2.4\n10.5\n14.9\nDense(bsz=1)\nDense(bsz=6)\nSparse(bsz=1)\nSparse(bsz=6)\nSparse(bsz=20)\nBlackmamba-MATH0\n5\n10\n15\n20\n2.2 5.3 2.2\n6.5\n11.6\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\n      Throughput (quries/second)\nFig. 8. Query throughput of Mixtral and BlackMamba.\nmatmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n      SM Utilization (%)\nFig. 9. GPU SM utilization of different kernels in the MoE layer for different batch sizes.\nit involves fewer computational demands, resulting in lower\nlatency. This is evident when comparing the throughput of\nbatch size of 2 in Mixtral-CS for dense (0.5 qps) vs. sparse\n(0.7 qps) models.\nFig. 8 also shows that throughput does not increase linearly\nwith batch size. For instance, sparse fine-tuning of Mixtral-\nCS improves throughput by 1.9 \u00d7 when increasing the batch\nsize from 1 to 2, but only by 4.8 \u00d7 when increasing from\n1 to 8. With smaller batch sizes, the SM utilization rate\nis lower, providing enough computational resources to feed\nmore operations in parallel. However, as the batch size con-\ntinues to increase, the SMs become saturated (more details in\n\u00a7IV-B4), and we can no longer hide latency by better utilizing\ncomputational resources.\nTakeaway 4. Sparse model significantly improves\nthroughput, reducing end-to-end cost of fine-tuning.\n4) Hardware characterization: As shown in Fig. 4, the\nexecution time of LLM fine-tuning is dominated by the MoE\nlayer. To offer further insights, we use detailed microarchi-\ntecture hardware metrics on the GPU to further understand\nexecution bottlenecks in the MoE layer. The goal of this\nstudy is to identify whether various kernels in the MoE layers\nare bound by compute or memory resources, and how future\nGPU designs can further scale performance.\nCompute resource utilization study. Fig. 9 shows the\nkernel-level breakdown of GPU Streaming Multi-processor\n(SM) utilization for the MoE layer. This utilization is\n---\n[ref_id=xia2024] 0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nRMSE=0.03\nMixtral-CS-A100-40GB\nDense Sparse\n0 5 10 15 200.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n RMSE=0.09\nMixtral-CS-A100-80GB\n0 5 10 15 200\n1\n2\n3\n4\n5\n RMSE=0.55\nMixtral-CS-H100\nThroughput (queries/sec)\nBatch size\nFig.\n---\n[ref_id=xia2024] 0 2 4 6 8 100.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n RMSE=0.05\nMixtral-CS\nDense Sparse\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n RMSE=0.02\nMixtral-MATH\n0 5 10 15 200\n2\n4\n6\n8\n10\n12\n14\n16\n RMSE=0.79\nMamba-CS\n0 2 4 6 8 10 120\n2\n4\n6\n8\n10\n12\n RMSE=0.42\nThroughput (queries/sec)\nBatch size\nMamba-MATH\nFig. 14. Estimation and validation of LLM fine-tuning throughput for\ndifferent models, datasets for A40 GPU. Dots represent ground truth and\nlines present the estimation.\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nRMSE=0.03\nMixtral-CS-A100-40GB\nDense Sparse\n0 5 10 15 200.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n RMSE=0.09\nMixtral-CS-A100-80GB\n0 5 10 15 200\n1\n2\n3\n4\n5\n RMSE=0.55\nMixtral-CS-H100\nThroughput (queries/sec)\nBatch size\nFig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS\nfor different GPUs: A100 and H100.\nresource renting per hour is calculated based on CUDO\ncompute [33], as other popular cloud providers do not offer\ncost/hour rates for the NVIDIA A40 GPU. However, one\ncan easily adjust the GPU renting cost per hour to estimate\nthe cost on other clouds such as Amazon AWS [34] or\nLambda [35]. Table IV estimates the cost for fine-tuning\nMixtral on the MATH dataset with a sparse setup, using\n10 epochs on different GPUs for a realistic cost estimate.\nEnterprises may use larger datasets for fine-tuning, such as,\nOpenOrca [36] and LaMini-instruction [37] containing more\nthan 2M queries. For OpenOrca, by scaling the cost by\nnumber of queries, our model predicts that the most cost-\neffective option to rent GPU resources on CUDO compute\nis NVIDIA H100 with a net cost of $3460.\nD. Generalization of the Analytical Model\nThe analytical models for estimating maximum batch size\nand throughput can be generalized to various LLM models\nand datasets. These models consider the characteristics of\nthe LLM, dataset, and GPU. Specifically, the maximum\nbatch size model combines GPU memory and LLM model\nsize to determine available memory for input data, while\ndataset sequence length and LLM sparsity determine space\nneeded per batch. In throughput estimation, based on the\nobservation we made (\u00a7IV-B4 Takeaway 5), GPU shifts from\nmemory-bound to compute-bound as batch size increases.\nThis characteristic generally applies to all GPUs due to the\nresource constraint, so the logarithmic relation between batch\nsize and throughput persists. The sparsity in (2) is model\ndependent, the influence of GPU, LLM model, and dataset\nare embedded in the coefficients C2, C3, and C4 in (2).\nThe coefficients in (1) and (2) are dependent on GPU,\nLLM model, and dataset; however, the underlying models\nare generalizable to unseen GPU, LLM model, and datasets.\nAlthough it takes some effort to sweep batch sizes and collect\nthroughput data points to fit our models, the benefits greatly\noutweigh the cost. Once the models are fit, our model can\nhelp choose the most cost-efficient GPU for fine-tuning LLM\nmodels, greatly saving resources and money.\nVI. R ELATED WORKS\nParameter-Efficient Fine-Tuning (PEFT) has been widely\nadopted to fine-tune LLM model for specialized tasks [15],\n[38]\u2013[43]. MoE additioally train specialized experts for dif-\nferent areas and the dynamic selection of experts makes\nit possible to scale the fine-tuning workload to different\nexperts in parallel. [44]\u2013[47] show that MoE models can\nimprove the ability to process knowledge for specific tasks,\nwhile maintaining the world knowledge in LLM. Kim et\nal. [48] construct an analytical model to estimate GPU\nmemory consumption for distributed fine-tuning. The model\nalso provides insights into optimizing memory usage through\ntensor, model, and pipeline parallelism.\nVII. C ONCLUSIONS\nFine-tuning LLMs is an attractive technique for tailoring\nmodern language models using domain-specific knowledge in\na cost-effective manner. This paper delved into understanding\nthe performance of fine-tuning MoE LLM models on a single\nGPU. Our profiling demonstrated that sparse MoE layers\noffer the best bang-for-buck trade-off. Using our profiling\nresults, we developed and validated an accurate analytical\nmodel to estimate the cost of LLM fine-tuning. Using this\nmodel, we showed the dollar amount that needs to be\nbudgeted for fine-tuning LLMs, which is much lower than\npre-training. For example, our model predicted that fine-\ntuning a sparse Mixtral model using a realistic data size of\n2M queries can be done with NVIDIA H100 GPU with a cost\nof $3460. A way to further reduce cost based on our study\nis to add compute resources to accelerate the MoE layers.\nWhile we showcase our study on fine-tuning LLMs using a\nsingle GPU, extending this model to multi-GPU systems is\nleft for future exploration.\nACKNOWLEDGMENTS\nThis work was supported in part by Semiconductor Re-\nsearch Corporation (SRC). We thank all the anonymous\nreviewers for their valuable comments and suggestions.\n---\n[ref_id=xia2024] Additionally, with the same batch\nsize, sparse fine-tuning achieves higher throughput because\n---\n[ref_id=xia2024] Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0.0\n1.0\n2.0\n3.0\n4.0\n5.0\nInput normalization Attention Post attention norm. MoE\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0.0\n0.5\n1.0\n1.5\nRMS layernorm Mamba MoE\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig. 5. Execution time breakdown in terms of different model layers.\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0\n2000\n4000\n6000\nmatmul(w2)\nw2_dequant\nmatmul(w3)\nw3_dequant\nmatmul(w1)\nw1_dequant\nsoftmax\ntopk\nmatmul(router)\nrouter_dequant\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0\n400\n800\n1200\n1600\n2000\nmatmul(w1)\ngelu\nmatmul(w2)\nelementwise_mult\ntop_k\nsigmoid\nmatmul(router)\n    Execution Time Breakdown (\u03bcs)\nMixtral\nMamba\nFig. 6. Execution breakdown of the MoE layer for different kernels.\nthe forward stage due to the need for gradient calculation\nand propagation, resulting in two matrix multiplication op-\nerations. In Mixtral fine-tuning, gradient calculation adds\nminimal computation as only a small portion of parameters\nneed it. However, gradient checkpointing in Mixtral saves\nmemory but increases the backward stage runtime due to the\nre-computation of intermediate values.\nWe further investigate the execution breakdown based\non various layers in two LLM models. For Mixtral, these\nlayers include input normalization, attention, post-attention\nnormalization, and MoE. In contrast, BlackMamba comprises\nthe Mamba layer, Root Mean Squared (RMS) layer nor-\nmalization, and MoE. As shown in Fig. 5, the MoE layer\nis the most time-consuming, accounting for 85% of the\noverall execution time on average. The execution time for\nthe MoE layer encompasses both the forward and backward\npasses during fine-tuning. Consequently, MoE is the costliest\nlayer and a prime target for optimization to enhance the\nperformance of LLM fine-tuning.\nTo concretely understand the opportunity for improving\nMoE layer performance, we also perform a kernel-level anal-\nysis within the MoE layer. Fig. 7 illustrates the architecture\nof the MoE layer in both Mixtral and BlackMamba models.\nEach expert in BlackMamba consists of a standard Feed-\nForward Network (FFN) layer with two serially connected\nweight matrices (W1 and W2) and a Gelu activation layer\nbetween. In contrast, experts in Mixtral are FFN layers with\nSwish-Gated Linear Units, involving an additional weight\nFig. 7. Expert architectures for Mixtral (top) and BlackMamba (bottom).\nmatrix (W3) in parallel with W1.\nFig. 6 shows the kernel-level MoE time breakdown. The\nfigure clearly shows that matrix multiplication (W1, W2,\nand W3) is the largest component of the MoE layer for\nboth BlackMamba and Mixtral. As batch size and sparsity\nincrease, so does computational demand, prolonging matrix\nmultiplication latency. The de-quantization operation in Mix-\ntral fine-tuning also becomes significant, especially with low\nsparsity and small batch sizes. While quantization reduces\nmodel size and memory footprint, it can increase computation\ntime due to de-quantization. This highlights the need to\nevaluate trade-offs between memory savings and computation\ntime, particularly in scenarios with small batch sizes and\nsequence lengths.\nTakeaway 3. Matrix multiplication operations in the\nMoE layer contribute significantly to the end-to-end\nexecution time, making the MoE layer the costliest\ncomponent in LLM fine-tuning.\n3) Fine-Tuning Throughput: Next, we present the fine-\ntuning throughput of Mixtral and BlackMamba on the MATH\nand CS datasets separately in Fig. 8. We use a throughput\nmetric of queries/second processed, where a query includes a\nprompt and a ground-truth answer for fine-tuning. To obtain\nthese results, we extract 1000 examples from each dataset\nand fine-tuned Mixtral and BlackMamba on them using the\nsmallest batch size (batch size = 1) and the largest batch size\nthat would fill the GPU memory.\nAs illustrated in Fig. 8, sparse fine-tuning achieves higher\nthroughput than dense fine-tuning. This is because the sparse\nfine-tuning baseline consumes less memory to store interme-\ndiate values, which allows for higher batch sizes compared\nto its dense counterpart. Additionally, with the same batch\nsize, sparse fine-tuning achieves higher throughput because\n---\n[ref_id=xia2024] For GPU memory\ncapacities of 100GB and 120GB, our model predicts that\nthe maximum batch sizes supported for fine-tuning Mixtral\nwill be 28 and 35, respectively.\n---\n[ref_id=xia2024] 0 20 40 60 80 100 1200\n5\n10\n15\n20\n25\n30\n35\n40\nA100-40GB\nA100-80GB\nA40\nH100\nbsz=28\nbsz=35\nProjected GPU capacity\nGround Truth Projection\nMax batch size\nGPU DRAM capacity\nFig. 13. Projected maximum batch size of Mixtral for different GPUs.\namount of GPU memory, and need to be subtracted in the\nanalytical model. Fig. 8 supports this by showing that on the\nsame dataset, BlackMamba can support larger batch size than\nMixtral because of its smaller model size.\nMoreover, the sequence length and sparsity also affect the\nmaximum batch size. Because the sparsity only affects the\nMoE part of the LLM, we multiply its influence by C1,\nwhich we call MoE coefficient. We apply the sequence length\nand the sparsity in the denominator as they are inversely\nrelated to batch size. Then, we multiply the result by C0,\nthe scaling coefficient , which scales the batch size by a\nconstant. The scaling coefficient is different across LLM\nmodels, because different models have different architecture\n(\u00a7III), and generate different amounts of intermediate data\nfor each query. The scaling coefficient for BlackMamba is\nhigher than that of Mixtral because it is a smaller model.\nFinally, we use floor to round it to the maximum integer.\nThe MoE coefficient and scaling coefficient vary across\nmodels. These coefficients are independent of GPU microar-\nchitectural parameters. We find the maximum batch size for\nboth LLM models on NVIDIA A40 (48GB), A100 (40GB),\nA100 (80GB), and H100 (80GB), and apply our model to\nfind the optimal coefficients. For Mixtral, C0 = 82 and\nC1 = 0 .95, and for BlackMamba, C0 = 83 and C1 = 0 .88.\nWhile we showcase these parameters for the models eval-\nuated, \u00a7V-D discusses how to generalize this approach for\nother models.\nUsing our analytical model, we demonstrate the maximum\nbatch sizes for fine-tuning on four different NVIDIA GPUs:\nA40, A100-40GB, A100-80GB and H100 with memory\ncapacities of 48GB, 40GB, 80GB, and 80GB, respectively.\nFig. 13 shows our projected maximum batch size and corre-\nlate it with experimented ground truth. While the maximum\nmemory capacity available in NVIDIA GPUs today is 80GB,\nwe use our analytical model to project the maximum batch\nsize that future GPUs might support. For GPU memory\ncapacities of 100GB and 120GB, our model predicts that\nthe maximum batch sizes supported for fine-tuning Mixtral\nwill be 28 and 35, respectively. Due to space limitations, we\nonly show the projection of Mixtral model.\nTABLE IV\nESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE\nBASED ON OUR ANALYTICAL MODEL\nGPU Mem MBS Throughput Cost ($/hr) Cost ($)\nA40 48GB 4 1.01 0.79 32.7\nA100 80GB 17 2.74 1.67 25.4\nH100 80GB 17 4.90 2.1 17.9\nB. Estimating Throughput\nAs discussed in \u00a7IV-B4, when the batch size increases, the\nLLM fine-tuning gradually switches from memory bound to\ncompute bound. When the compute resources are abundant,\nthe throughput increases almost linearly with batch size.\nHowever, when compute resources become constrained, the\nthroughput improvement gradually saturates. We model this\nbehavior using a logarithmic relation between batch size and\nthroughput. Our analytical model for maximum batch size is\nshown in (2).\nT hroughput = C2 \u2217 log( batch size\nsparsity \u2217 C3 ) + C4 (2)\nIn the equation, in addition to the basic logarithmic part, we\nhave three coefficients C2, C3, and C4. C2 is the scaling\ncoefficient, which depends on the LLM model, GPU archi-\ntecture, and the dataset. The higher the compute capability a\nGPU can provide, and the lower the LLM model and dataset\ncompute requirement is, the higher the scaling coefficient will\nbe. C3 is the MoE attenuation coefficient , which tunes how\nmuch the MoE sparsity affects the throughput. MoE sparsity\nonly affects the MoE part in LLM model, and thus should\nbe attenuated to avoid over compensation. This coefficient is\nonly LLM model dependent, because once the model is fixed,\nthe influence of sparsity is determined. C4 is the intercept,\nconceptually it equals to the throughput when batch size\nequals one, because the logarithmic part in (2) is zero when\nbatch size is one. Using scipy [32] to fit the model and\ngenerate four sets (C2, C3, C4), for each model and dataset\ncombination.\nTo estimate the accuracy of this model, we correlate the\nmodel output with experimental data from our study. Fig. 14\nshows this correlation study, where discrete data points (dots)\nrepresent experimental values, and the line represents output\nof our analytical model. We use both dense and sparse\nMixtral and BlackMamba for both datasets used in our study.\nThe figure clearly shows that our model accurately predicts\nLLM fine-tuning throughput with a Root Mean Squared Error\n(RMSE) of less than 0.8. Fig. 15 shows the correlation study\nof the analytical model of three other GPUs, A100 (40GB),\nA100 (80GB), and H100. The RMSE is less than 0.6, close\nto that of A40.\nC. Estimating the Total Fine-Tuning Cost\nUsing the throughput estimation, we calculate the cost\nof fine-tuning LLMs for different GPUs. The cost of GPU\n---\n[ref_id=xia2024] for sparse fine-tuning,\nwe use the two batch sizes for dense fine-tuning, and the\nmaximum batch size that fits into memory.\n---\n[ref_id=xia2024] matmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n   DRAM Bandwidth Utilization (%)\nFig. 10. GPU DRAM bandwidth utilization of different kernels in the MoE layer for different batch sizes.\nweighted by the amount of time each kernel takes. We\nuse a sequence length of 128 (\u00a7III). Sequence length will\ninfluence the choice of batch size, and we discuss the effects\nof sequence length on runtime, throughput, SM utilization,\nand memory utilization in \u00a7IV-B6. For dense fine-tuning, we\nshow the SM utilization of batch size 1 and the maximum\nbatch size that fits into memory; for sparse fine-tuning,\nwe use the two batch sizes for dense fine-tuning, and the\nmaximum batch size that fits into memory.\nFig. 9 shows the SM utilization of different kernels in the\nMoE layer, which offers the following insights. (1) For both\nsparse and dense fine-tuning, SM utilization increases with\nbatch size due to higher parallelism and GPU activity. (2)\nSparse fine-tuning has lower SM utilization than dense fine-\ntuning at the same batch size because it activates only 2\nout of 8 experts, reducing parallelism. Consequently, sparse\nfine-tuning supports a higher maximum batch size. Both\nachieve similar maximum SM utilization at their peak batch\nsizes. (3) The de-quantization kernel maintains high SM\nutilization regardless of batch size. (4) Matrix multiplication\nkernels achieve higher SM utilization with larger batch sizes,\nleveraging the GPU\u2019s parallel processing capabilities.\nMemory resource utilization study. Fig. 10 shows the\nkernel-level breakdown of GPU memory bandwidth utiliza-\ntion. We use the same experimental setup as in the evalua-\ntion of SM utilization, and find the following insights. (1)\nFor both sparse and dense fine-tuning, the time-weighted\nmemory utilization decreases with increasing batch size.\nThis is because the model parameters are loaded once and\nshared by all queries in a batch. However, a larger batch\nincreases the execution time (as discussed in \u00a7IV-B6),\nleading to a lower average memory bandwidth utilization.\nHE HE_tuned GS GS_tuned0\n25\n50\n75\n100\nExpert 0\nExpert 1\nExpert 2\nExpert 3\nExpert 4\nExpert 5\nExpert 6\nExpert 7\nHE HE_tuned GS GS_tuned0\n25\n50\n75\n100                          Avg Num. of Token Per Query\nMixtral\nMamba\nvar=55.5 var=112.3\nvar=21.2\nvar=79.2\nvar=150.7 var=93.3\nvar=186.5 var=187.9\nFig. 11. Token distribution to different experts.\n(2) For the same batch size, sparse fine-tuning achieves\nhigher memory bandwidth utilization than dense fine-tuning\ndue to shorter execution times. (3) Dequant layers\u2019 memory\nutilization is batch-size-independent, while matmul layers\u2019\nutilization decreases with larger batch sizes. To maximize\nGPU memory usage, a sufficiently large batch size should be\nused. With large batch sizes, fine-tuning becomes compute-\nbound, indicating a need for improved compute resources in\nfuture hardware to better utilize memory bandwidth.\nTakeaway 5 . As the batch size increases, LLM\nfine-tuning transitions from being memory-bound to\ncompute-bound.\n5) Effect of Load Imbalance Due to Fine-Tuning: Recent\ntrends in deploying expert parallelism in MoE models have\nhighlighted load-imbalanced computation among experts as\na significant issue impacting inference and training effi-\n---\n[ref_id=xia2024] parameters. We fine-tune the full BlackMamba model (i.e.,\noriginal weight matrices), whereas employed QLoRA [15]\nfor parameter-efficient fine-tuning (PEFT) on Mixtral due to\nGPU memory capacity budget. For QLoRA, we target the\nMoE layers, including the routers, and set the rank of the\nLoRA modules to 16. We enable FlashAttention2 [17] during\nMixtral fine-tuning for enhanced efficiency. Moreover, we use\ngradient checkpointing [18] to save memory usage.\nDatasets. Our fine-tuning process is implemented in Py-\nTorch using the LLaMA-Factory framework [19], with a\nlearning rate of 5e-5 and 10 epochs. Both models were fine-\ntuned on two datasets focused on different tasks: common-\nsense 15k (CS) and Math 14k (MATH), which address com-\nmonsense reasoning and arithmetic reasoning respectively\n(provided by LLM-adapters [20]). The details of datasets\nare used in Table II. For evaluation, we tested the models\non GSM8K [21] for arithmetic reasoning and HE [22] for\ncommonsense reasoning. Each dataset consists of thousands\nof queries. We define a query as the concatenation of a\nprompt and its ground-truth answer, which is feed to LLMs\nfor fine-tuning.\nProfiling experiments. We evaluate the fine-tuning pro-\ncess from both software and hardware perspectives. The\nsoftware evaluation includes an end-to-end assessment of\nthe fine-tuning process and measures the performance of\nthe two models on various tasks post-fine-tuning. Using\nPyTorch, we provide essential algorithm-level information\nsuch as test accuracy, training throughput, and layer-level\nlatency breakdown. The hardware evaluation offers a detailed\nanalysis of GPU performance. Utilizing NVIDIA Nsight\nCompute [23], we gather kernel-level information, including\nSM utilization, memory utilization, and kernel latency. These\nmetrics collectively offer a comprehensive overview of the\nmodels\u2019 performance, capturing both high-level algorithmic\nefficiency and detailed hardware utilization. Software evalu-\nation is dataset-dependent, and we will show the test accu-\nracy and fine-tuning throughput by utilizing both datasets.\nIn contrast, hardware evaluation is dataset-independent as\nthese workload characteristics do not depend on runtime\ndata. Because profiling is time-consuming (approximately\n10,000\u00d7 costlier compared to a native run without the profiler\nenabled), we manually set the batch size and sequence length\nto facilitate a more direct and efficient profiling process.\nWe present the sequence length distribution for the CS and\nMATH datasets in Fig. 2. The median sequence length is 79\nfor CS and 174 for MATH. Therefore, we select a sequence\nlength of 128 for the hardware evaluation section to achieve\nan approximate profiling effect. We also show a sensitivity\nstudy by varying sequence length to demonstrate its effect\non performance.\nGPU platform. Our study is focused on characterizing the\nLLM fine-tuning process on a resource-constrained environ-\nment. Therefore, we focus on fine-tuning these models on a\nsingle GPU. Specifically, we conduct our experiments using\n0 50 100 150 200 250 300 350 4000\n100\n200\n300\n400\n500\n0 50 100 150 200 250 300 350 4000\n20\n40\n60\n80\n100\n120\nCS\nMATH\nMedian=79\nMedian=174\nFrequency\nSequence Length\nFig. 2. Sequence length distribution for evaluated datasets.\n0 2 4 6 8 10\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nMixtral-dense-HE\nMixtral-sparse-HE\nMixtral-dense-GS\nMixtral-sparse-GS\n0 2 4 6 8 10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nBlackmamba -dense-HE\nBlackmamba -sparse-HE\nBlackmamba -dense-GS\nBlackmamba -sparse-GS\nEpoch\nAccuracy\nFig. 3. Testing accuracy of Mixtral and BlackMamba. Both models are\nevaluated on two datasets Hellaswag (HE) and GSM8K (GS), using dense\nand sparse fine-tuning.\nNVIDIA A40 GPU with Ampere architecture. The GPU has\n48GB memory. While our profiling study is based on this\nparticular GPU, we show the versatility of our analytical\nmodel by validating our model against three other GPU\nwith different sizes of compute and memory resources: (1)\nA100 GPU with 40GB memory, (2) A100 GPU with 80GB\nmemory, and (3) H100 GPU with 80GB memory. We use\nPython v3.8.10, PyTorch v2.1.0, and CUDA v11.8.\nIV. C HARACTERIZATION STUDY\nUsing the experimental setup discussed above, next, we\nconduct an in-depth characterization of LLM fine-tuning to\nunderstand both accuracy and runtime behaviors.\nA. Analysis of Model Trainability\nWe first evaluate if fine-tuning sparse LLM models can\nachieve the desired accuracy levels. Pre-trained models show\nlow accuracy: HE and GS have under 25% on Mixtral and\n---\n[ref_id=xia2024] Page 3\n---\n[ref_id=xia2024] parameters.\n---\n[ref_id=xia2024] We fine-tune the full BlackMamba model (i.e.,\noriginal weight matrices), whereas employed QLoRA [15]\nfor parameter-efficient fine-tuning (PEFT) on Mixtral due to\nGPU memory capacity budget.\n---\n[ref_id=xia2024] For QLoRA, we target the\nMoE layers, including the routers, and set the rank of the\nLoRA modules to 16.\n---\n[ref_id=xia2024] We enable FlashAttention2 [17] during\nMixtral fine-tuning for enhanced efficiency.\n---\n[ref_id=xia2024] Moreover, we use\ngradient checkpointing [18] to save memory usage.\n---\n[ref_id=xia2024] Datasets.\n---\n[ref_id=xia2024] Our fine-tuning process is implemented in Py-\nTorch using the LLaMA-Factory framework [19], with a\nlearning rate of 5e-5 and 10 epochs.\n---\n[ref_id=xia2024] Both models were fine-\ntuned on two datasets focused on different tasks: common-\nsense 15k (CS) and Math 14k (MATH), which address com-\nmonsense reasoning and arithmetic reasoning respectively\n(provided by LLM-adapters [20]).\n---\n[ref_id=xia2024] The details of datasets\nare used in Table II.\n---\n[ref_id=xia2024] For evaluation, we tested the models\non GSM8K [21] for arithmetic reasoning and HE [22] for\ncommonsense reasoning.\n---\n[ref_id=xia2024] Each dataset consists of thousands\nof queries.\n---\n[ref_id=xia2024] We define a query as the concatenation of a\nprompt and its ground-truth answer, which is feed to LLMs\nfor fine-tuning.\n---\n[ref_id=xia2024] Profiling experiments.\n---\n[ref_id=xia2024] We evaluate the fine-tuning pro-\ncess from both software and hardware perspectives.\n---\n[ref_id=xia2024] The\nsoftware evaluation includes an end-to-end assessment of\nthe fine-tuning process and measures the performance of\nthe two models on various tasks post-fine-tuning.\n---\n[ref_id=xia2024] Using\nPyTorch, we provide essential algorithm-level information\nsuch as test accuracy, training throughput, and layer-level\nlatency breakdown.\n---\n[ref_id=xia2024] The hardware evaluation offers a detailed\nanalysis of GPU performance.\n---\n[ref_id=xia2024] Utilizing NVIDIA Nsight\nCompute [23], we gather kernel-level information, including\nSM utilization, memory utilization, and kernel latency.\n---\n[ref_id=xia2024] These\nmetrics collectively offer a comprehensive overview of the\nmodels\u2019 performance, capturing both high-level algorithmic\nefficiency and detailed hardware utilization.\n---\n[ref_id=xia2024] Software evalu-\nation is dataset-dependent, and we will show the test accu-\nracy and fine-tuning throughput by utilizing both datasets.\n---\n[ref_id=xia2024] In contrast, hardware evaluation is dataset-independent as\nthese workload characteristics do not depend on runtime\ndata.\n---\n[ref_id=xia2024] Because profiling is time-consuming (approximately\n10,000\u00d7 costlier compared to a native run without the profiler\nenabled), we manually set the batch size and sequence length\nto facilitate a more direct and efficient profiling process.\n---\n[ref_id=xia2024] We present the sequence length distribution for the CS and\nMATH datasets in Fig.\n---\n[ref_id=xia2024] 2.\n---\n[ref_id=xia2024] The median sequence length is 79\nfor CS and 174 for MATH.\n---\n[ref_id=xia2024] Therefore, we select a sequence\nlength of 128 for the hardware evaluation section to achieve\nan approximate profiling effect.\n---\n[ref_id=xia2024] We also show a sensitivity\nstudy by varying sequence length to demonstrate its effect\non performance.\n---\n[ref_id=xia2024] GPU platform.\n---\n[ref_id=xia2024] Our study is focused on characterizing the\nLLM fine-tuning process on a resource-constrained environ-\nment.\n---\n[ref_id=xia2024] Therefore, we focus on fine-tuning these models on a\nsingle GPU.\n---\n[ref_id=xia2024] Specifically, we conduct our experiments using\n0 50 100 150 200 250 300 350 4000\n100\n200\n300\n400\n500\n0 50 100 150 200 250 300 350 4000\n20\n40\n60\n80\n100\n120\nCS\nMATH\nMedian=79\nMedian=174\nFrequency\nSequence Length\nFig.\n---\n[ref_id=xia2024] 2.\n---\n[ref_id=xia2024] Sequence length distribution for evaluated datasets.\n---\n[ref_id=xia2024] 0 2 4 6 8 10\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nMixtral-dense-HE\nMixtral-sparse-HE\nMixtral-dense-GS\nMixtral-sparse-GS\n0 2 4 6 8 10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nBlackmamba -dense-HE\nBlackmamba -sparse-HE\nBlackmamba -dense-GS\nBlackmamba -sparse-GS\nEpoch\nAccuracy\nFig.\n---\n[ref_id=xia2024] 3.\n---\n[ref_id=xia2024] Testing accuracy of Mixtral and BlackMamba.\n---\n[ref_id=xia2024] Both models are\nevaluated on two datasets Hellaswag (HE) and GSM8K (GS), using dense\nand sparse fine-tuning.\n---\n[ref_id=xia2024] NVIDIA A40 GPU with Ampere architecture.\n---\n[ref_id=xia2024] The GPU has\n48GB memory.\n---\n[ref_id=xia2024] While our profiling study is based on this\nparticular GPU, we show the versatility of our analytical\nmodel by validating our model against three other GPU\nwith different sizes of compute and memory resources: (1)\nA100 GPU with 40GB memory, (2) A100 GPU with 80GB\nmemory, and (3) H100 GPU with 80GB memory.\n---\n[ref_id=xia2024] We use\nPython v3.8.10, PyTorch v2.1.0, and CUDA v11.8.\n---\n[ref_id=xia2024] IV.\n---\n[ref_id=xia2024] C HARACTERIZATION STUDY\nUsing the experimental setup discussed above, next, we\nconduct an in-depth characterization of LLM fine-tuning to\nunderstand both accuracy and runtime behaviors.\n---\n[ref_id=xia2024] A.\n---\n[ref_id=xia2024] Analysis of Model Trainability\nWe first evaluate if fine-tuning sparse LLM models can\nachieve the desired accuracy levels.\n---\n[ref_id=xia2024] Pre-trained models show\nlow accuracy: HE and GS have under 25% on Mixtral and\n---\n[ref_id=samsi2024] responses and the total time needed to run inference over the\ninput data.\nWe monitor GPUs using the nvidia-smi [23] and\nNVIDIA DCGM [24] utilities to study GPU utilization, energy,\npower draw, etc. during our experiments. The nvidia-smi\nutility is used to capture GPU usage over time at 100ms\nintervals and the DCGM monitoring tool is used to capture\naggregate GPU energy in Joules for the rank-0 node. For a\nmulti-node, multi-GPU model, we multiply the rank-0 energy\nby the number of nodes used. Maximum power draw on GPUs\nis capped at 250 Watts unless otherwise stated. Due to limits\non resource availability, we mainly use V100 GPUs for larger-\nscale distributed experiments (i.e., for 8, 16, and 32 shards)\nand A100 GPUs for smaller scale experiments.\nInference energy metrics are calculated by combining the\ninference metrics above with the energy data collected from\nour GPUs using NVIDIA\u2019s utilities described above. Specif-\nically, energy per second is defined as the total aggregate\nGPU energy spent from a single experiment/job (across all\nshards) divided by the total run time of that experiment/job in\nseconds. A single experiment/job denotes a single run through\nall 4,096 prompts under a specified batch size. Energy per\ntoken and energy per response are similarly defined as total\nenergy divided by the number of decoded output tokens and\nthe number of responses as defined above, respectively.\nIV. R ESULTS\nA. Baselines: LLaMA 7B, 13B, & 65B\n1) Inference Performance: We begin our analysis with\na baseline comparison of LLaMA 65B with smaller-scale\nLLaMA models: LLaMA 7B and 13B. The goal is to under-\nstand the following: what do inference performance and energy\ntrade-offs look like for the different sizes of LLaMA under the\nbare-minimum set of resources required to have them running\ninference? This question can be important for researchers\nand users who have may not have limitless computational\nresources and hardware acceleration or may be constrained\nin terms of GPU memory, etc.\nGiven the sizes of the models, the size of the data, and the\nhardware memory limits, we only show results from experi-\nments that were possible for a given combination of parameters\n(i.e., for some models, certain combinations of batch size and\nnumber of shards are infeasible due to memory limits of the\nunderlying GPUs). Table II shows the bare minimum hardware\nrequirements for each LLaMA variant and the maximum batch\nsize possible for each combination, assuming no further model\ncompression, optimization, quantization, distillation etc.\nWith these limits in mind, we present the inference per-\nformance of LLaMA 7B, 13B, and 65B on the Alpaca and\nGSM8K datasets with the bare minimum hardware settings in\nFigure 2. The plots in Figure 2 show a baseline comparison of\ninference performance of the three LLaMA variants on both\nthe V100 and A100 GPUs respectively. For each model, in\nline with the spirit of the bare minimum settings, inference\nis done with a batch size of 64 and an maximum generation\nlength of 256. The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256. These limits are imposed by a combination of GPU\nmemory, model size, response length and the number of GPUs.\nWhile the 65B model can sharded across 6 V100 GPUs, we\nuse 8 since the model architecture makes it better suited for\nbalanced sharding across 8 GPUs.\nModel Size V100 32GB A100 80GB\nCount Max. Batch size Count Max. Batch size\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n13B on two GPUs in each case whereas the 65B model was\nrun on 8 V100 GPUs and 4 A100 GPUs respectively due to\nthe size of the model and available memory on the GPU(s).\nAs expected, we observe that the A100 outperforms V100\non both the Alpaca and GSM8K datasets: particularly for the\nsmaller LLaMA 7B and 13B, we see anywhere from a 2\ntimes (7B) to a 1.25 times increase (13B) in inference latency\non the A100 when compared to the V100 across words per\nsecond, tokens per second, and responses per second. Faster\nresponse rates and inference are likely due to the fact that\nthe number of computations, directly related to the number\nof parameters of said model, involved in the 7B and 13B\nmodels are significantly lower than the 65B model. We do note\nthat for LLaMA 65B, we see a much smaller improvement in\nusing the A100 over the V100; however, since the 65B model\nrequires sharding across two (A100) or four (V100) compute\nnodes at the mininum, this could result in additional latency\nto each forward pass of the model, explaining the smaller\nimprovements. We also observe that while LLaMA 7B exhibits\na considerable improvement in inference throughput on both\nAlpaca and GSM8K with the A100, the improvement is much\nlarger for Alpaca than GSM8K. This can also be attributed to\nthe different complexities of inputs from each dataset.\n2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above. For both the Alpaca and\nGSM8K datasets, we see that there is a considerable increase\nin the energy per second across all LLaMA sizes when using\nthe A100 over the V100 where the most considerable increase\nis for the smallest 7B model. Although Figure 2 shows a\nconsiderable increase in inference throughput from using the\nA100, Figure 3 shows us that this improvement does not come\nfor free: it comes at an increased energy cost per second.\nMoreover, for the largest LLaMA 65B, it is less clear whether\nthe increased inference energy per second (Figure 3) is worth\nthe small improvement in inference throughput in terms of\nwords/token/responses per second (Figure 2).\n---\n[ref_id=samsi2024] Page 4\n---\n[ref_id=samsi2024] responses and the total time needed to run inference over the\ninput data.\n---\n[ref_id=samsi2024] We monitor GPUs using the nvidia-smi [23] and\nNVIDIA DCGM [24] utilities to study GPU utilization, energy,\npower draw, etc.\n---\n[ref_id=samsi2024] during our experiments.\n---\n[ref_id=samsi2024] The nvidia-smi\nutility is used to capture GPU usage over time at 100ms\nintervals and the DCGM monitoring tool is used to capture\naggregate GPU energy in Joules for the rank-0 node.\n---\n[ref_id=samsi2024] For a\nmulti-node, multi-GPU model, we multiply the rank-0 energy\nby the number of nodes used.\n---\n[ref_id=samsi2024] Maximum power draw on GPUs\nis capped at 250 Watts unless otherwise stated.\n---\n[ref_id=samsi2024] Due to limits\non resource availability, we mainly use V100 GPUs for larger-\nscale distributed experiments (i.e., for 8, 16, and 32 shards)\nand A100 GPUs for smaller scale experiments.\n---\n[ref_id=samsi2024] Inference energy metrics are calculated by combining the\ninference metrics above with the energy data collected from\nour GPUs using NVIDIA\u2019s utilities described above.\n---\n[ref_id=samsi2024] Specif-\nically, energy per second is defined as the total aggregate\nGPU energy spent from a single experiment/job (across all\nshards) divided by the total run time of that experiment/job in\nseconds.\n---\n[ref_id=samsi2024] A single experiment/job denotes a single run through\nall 4,096 prompts under a specified batch size.\n---\n[ref_id=samsi2024] Energy per\ntoken and energy per response are similarly defined as total\nenergy divided by the number of decoded output tokens and\nthe number of responses as defined above, respectively.\n---\n[ref_id=samsi2024] IV.\n---\n[ref_id=samsi2024] R ESULTS\nA.\n---\n[ref_id=samsi2024] Baselines: LLaMA 7B, 13B, & 65B\n1) Inference Performance: We begin our analysis with\na baseline comparison of LLaMA 65B with smaller-scale\nLLaMA models: LLaMA 7B and 13B.\n---\n[ref_id=samsi2024] The goal is to under-\nstand the following: what do inference performance and energy\ntrade-offs look like for the different sizes of LLaMA under the\nbare-minimum set of resources required to have them running\ninference?\n---\n[ref_id=samsi2024] This question can be important for researchers\nand users who have may not have limitless computational\nresources and hardware acceleration or may be constrained\nin terms of GPU memory, etc.\n---\n[ref_id=samsi2024] Given the sizes of the models, the size of the data, and the\nhardware memory limits, we only show results from experi-\nments that were possible for a given combination of parameters\n(i.e., for some models, certain combinations of batch size and\nnumber of shards are infeasible due to memory limits of the\nunderlying GPUs).\n---\n[ref_id=samsi2024] Table II shows the bare minimum hardware\nrequirements for each LLaMA variant and the maximum batch\nsize possible for each combination, assuming no further model\ncompression, optimization, quantization, distillation etc.\n---\n[ref_id=samsi2024] With these limits in mind, we present the inference per-\nformance of LLaMA 7B, 13B, and 65B on the Alpaca and\nGSM8K datasets with the bare minimum hardware settings in\nFigure 2.\n---\n[ref_id=samsi2024] The plots in Figure 2 show a baseline comparison of\ninference performance of the three LLaMA variants on both\nthe V100 and A100 GPUs respectively.\n---\n[ref_id=samsi2024] For each model, in\nline with the spirit of the bare minimum settings, inference\nis done with a batch size of 64 and an maximum generation\nlength of 256.\n---\n[ref_id=samsi2024] The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256.\n---\n[ref_id=samsi2024] These limits are imposed by a combination of GPU\nmemory, model size, response length and the number of GPUs.\n---\n[ref_id=samsi2024] While the 65B model can sharded across 6 V100 GPUs, we\nuse 8 since the model architecture makes it better suited for\nbalanced sharding across 8 GPUs.\n---\n[ref_id=samsi2024] Model Size V100 32GB A100 80GB\nCount Max.\n---\n[ref_id=samsi2024] Batch size Count Max.\n---\n[ref_id=samsi2024] Batch size\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n13B on two GPUs in each case whereas the 65B model was\nrun on 8 V100 GPUs and 4 A100 GPUs respectively due to\nthe size of the model and available memory on the GPU(s).\n---\n[ref_id=samsi2024] As expected, we observe that the A100 outperforms V100\non both the Alpaca and GSM8K datasets: particularly for the\nsmaller LLaMA 7B and 13B, we see anywhere from a 2\ntimes (7B) to a 1.25 times increase (13B) in inference latency\non the A100 when compared to the V100 across words per\nsecond, tokens per second, and responses per second.\n---\n[ref_id=samsi2024] Faster\nresponse rates and inference are likely due to the fact that\nthe number of computations, directly related to the number\nof parameters of said model, involved in the 7B and 13B\nmodels are significantly lower than the 65B model.\n---\n[ref_id=samsi2024] We do note\nthat for LLaMA 65B, we see a much smaller improvement in\nusing the A100 over the V100;\n---\n[ref_id=samsi2024] however, since the 65B model\nrequires sharding across two (A100) or four (V100) compute\nnodes at the mininum, this could result in additional latency\nto each forward pass of the model, explaining the smaller\nimprovements.\n---\n[ref_id=samsi2024] We also observe that while LLaMA 7B exhibits\na considerable improvement in inference throughput on both\nAlpaca and GSM8K with the A100, the improvement is much\nlarger for Alpaca than GSM8K.\n---\n[ref_id=samsi2024] This can also be attributed to\nthe different complexities of inputs from each dataset.\n---\n[ref_id=samsi2024] 2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above.\n---\n[ref_id=samsi2024] For both the Alpaca and\nGSM8K datasets, we see that there is a considerable increase\nin the energy per second across all LLaMA sizes when using\nthe A100 over the V100 where the most considerable increase\nis for the smallest 7B model.\n---\n[ref_id=samsi2024] Although Figure 2 shows a\nconsiderable increase in inference throughput from using the\nA100, Figure 3 shows us that this improvement does not come\nfor free: it comes at an increased energy cost per second.\n---\n[ref_id=samsi2024] Moreover, for the largest LLaMA 65B, it is less clear whether\nthe increased inference energy per second (Figure 3) is worth\nthe small improvement in inference throughput in terms of\nwords/token/responses per second (Figure 2).\n---\n[ref_id=kim2025] and statistical analysis. Our system is built on top of FlexGen,\na cutting-edge framework for LLM inference that provides\nrobust KV cache offloading capabilities. A key advantage\nof InferSave is its minimal computational overhead and\nexceptional speed in determining optimal resource config-\nurations. Once user parameters and SLO requirements are\nprovided, our system quickly performs TPS predictions and\ncost-efficiency calculations, enabling rapid and precise GPU\ninstance recommendations. The complete source code of\nInferSave, along with all associated tools and algorithms,\nis publicly available for download at https://github.com/lass-\nlab/InferSave.\nVI. Evaluation\nA. Experimental setup\nFor our evaluation, we conducted two contrasting inference\ntasks representative of online and offline inference scenarios\nto comprehensively assess the impact of offloading strategies\non cost and performance across various cloud-based GPU\ninstances. The objective of the evaluation is to quantitatively\nanalyze the effects of offloading and the impacts it has\non cost and performance efficiency, as well as to pick the\noptimal instance given a SLO as input. Online inferencing\nfocuses on finding the most price-effective inference while\nmeeting the strict SLO requirement, while offline inferencing\nrelaxes the SLO requirement, allowing for strategies such as\noffloading and used lower priced instances. All experiments\nwere performed 3 times for each instance to maintain result\nintegrity, and the average of each result were used for analysis.\nWorkload Definition: For a holistic evaluation of\nInferSave\u2019s ability to select the optimal instance in a variety\nof scenarios, we perform two contrasting inference workloads.\n\u2022 Online Inference workload: To model a real-time chatbot\nsystem, we use a pattern of 128 input tokens and a 512\noutput tokens. This simulates a common AI LLM chatbot\nscenario of a user asking short questions, with the chatbot\nproviding detailed answers. The workload evaluates a total\nof 3000 requests.\n\u2022 Offline Inference workload: To model a batch processing\ntask, an input size of 1024 tokens and an output size\nof 128 tokens was used. This takes into account tasks\nsuch as document summarization and data wrangling. To\nsimulate a batch processing task, the workload evaluates\nthe performance of completing 1000 requests.\nA WS Cloud Experiment Setup : To maintain uniform\nexperimental conditions and reduce potential disruptions\ncaused by fluctuating cloud workloads, all experiments were\ncarried out on AWS in the us-east-1 (Northern Virginia)\nregion between 9:00 AM and 10:00 PM KST, spanning\nthe period from December 2024 to March 2025. To avoid\nperformance variations due to regional resource contention,\ntesting was evenly distributed across availability zones us-\neast-1a through us-east-1f. For the GPU-VMs, we utilized\ng4dn.xlarge(NVIDIA T4), g5.xlarge(NVIDIA A10G),\ng6.xlarge(NVIDIA L4) and g6e.xlarge(NVIDIA L40s)\nA detailed specification of the instances are specified in Table\nIV.\nTABLE IV\nSpecifications of VM instances, including 4 GPU-VMs based on\nA WS specifications.\nInstanceGPU-TypeOn-Demand PriceGPU MemoryFP16 PCIe B/W($/hr) (GB) (TFLOPS)(GB/s)\ng6e.xlarge L40s 2.699 48 91.61 12g6.xlarge L4 1.167 24 30.29 12g5.xlarge A10G 1.466 24 31.52 12g4dn.xlargeT4 0.71 16 8.24 6\nTo validate the effectiveness of InferSave, major\ntransformer-based LLM models such as OPT-1.3B, OPT-2.7B,\nOPT-6.7B were used for testing in an in-house benchmark\nsuite. To find the optimal performance configuration, tests\nwere conducted by varying the batch size from 1 to 64 under\ndifferent conditions for single GPU processing.\nPolicy To Select Instance : As stated in Section II-D,\nthere are no clear state of the art methodologies for GPU\ninstance selection for inferencing. Therefore, in our evaluation,\nwe compared the following two baseline approaches with\nInferSave.\n\u2022 Most powerful instance(Max-Performance) : This policy\nsimply chooses the GPU instance that offers the most per-\nformance, and aims to lower latency and raise throughput\nas much as possible. However, this methodology does not\ntake into consideration price, and therefore running costs\ncan be raised needlessly.\n\u2022 Simple performance prediction( InferSave (without\nKV Cache offloading)) : This policy uses theoretical\nperformance metrics (FLOPS, memory bandwidth) to predict\nperformance and select an instance. However, it does not\ntake into consideration the effects of KV Cache offloading,\nand may not be able to find the most optimal instance.\nB. CTCF Validation\nInferSave proposes the Compute Time Calibration Func-\ntion (CTCF) to accurately determine the optimal instance\nbased on user requirements. To validate the accuracy of\nCTCF, experiments were conducted on two GPU instances,\ng4dn.xlarge and g6.xlarge. The experiments utilized the OPT-\n2.7B model, with an input token length of 512 and an output\ntoken length of 128. The model\u2019s key computational units,\nincluding a hidden size of 2560 and an intermediate size of\n2560 \u00d7 4, were applied, and the total number of layers (32)\nwas incorporated to measure computation time. For FLOPS\nestimation, the theoretical FLOPS values provided by GPU\nmanufacturers were used: g4dn.xlarge with NVIDIA T4 (8.24\nTFLOPS) and g6.xlarge with NVIDIA L4 (30.29 TFLOPS).\nAfter applying CTCF, the corrected prediction times were\ncomputed and compared with actual measurements to analyze\nthe error rate. As shown in Figure 4, the CTCF-adjusted values\nclosely matched the actual measurements. Specifically, in the\nDecode stage of g4dn.xlarge, the corrected values exhibited\nan average error rate of 1% compared to actual measurements,\nwhile in the Prefill stage of g6.xlarge, the average error rate\n---\n[ref_id=kim2025] Page 8\n---\n[ref_id=kim2025] and statistical analysis.\n---\n[ref_id=kim2025] Our system is built on top of FlexGen,\na cutting-edge framework for LLM inference that provides\nrobust KV cache offloading capabilities.\n---\n[ref_id=kim2025] A key advantage\nof InferSave is its minimal computational overhead and\nexceptional speed in determining optimal resource config-\nurations.\n---\n[ref_id=kim2025] Once user parameters and SLO requirements are\nprovided, our system quickly performs TPS predictions and\ncost-efficiency calculations, enabling rapid and precise GPU\ninstance recommendations.\n---\n[ref_id=kim2025] The complete source code of\nInferSave, along with all associated tools and algorithms,\nis publicly available for download at https://github.com/lass-\nlab/InferSave.\n---\n[ref_id=kim2025] VI.\n---\n[ref_id=kim2025] Evaluation\nA.\n---\n[ref_id=kim2025] Experimental setup\nFor our evaluation, we conducted two contrasting inference\ntasks representative of online and offline inference scenarios\nto comprehensively assess the impact of offloading strategies\non cost and performance across various cloud-based GPU\ninstances.\n---\n[ref_id=kim2025] The objective of the evaluation is to quantitatively\nanalyze the effects of offloading and the impacts it has\non cost and performance efficiency, as well as to pick the\noptimal instance given a SLO as input.\n---\n[ref_id=kim2025] Online inferencing\nfocuses on finding the most price-effective inference while\nmeeting the strict SLO requirement, while offline inferencing\nrelaxes the SLO requirement, allowing for strategies such as\noffloading and used lower priced instances.\n---\n[ref_id=kim2025] All experiments\nwere performed 3 times for each instance to maintain result\nintegrity, and the average of each result were used for analysis.\n---\n[ref_id=kim2025] Workload Definition: For a holistic evaluation of\nInferSave\u2019s ability to select the optimal instance in a variety\nof scenarios, we perform two contrasting inference workloads.\n---\n[ref_id=kim2025] \u2022 Online Inference workload: To model a real-time chatbot\nsystem, we use a pattern of 128 input tokens and a 512\noutput tokens.\n---\n[ref_id=kim2025] This simulates a common AI LLM chatbot\nscenario of a user asking short questions, with the chatbot\nproviding detailed answers.\n---\n[ref_id=kim2025] The workload evaluates a total\nof 3000 requests.\n---\n[ref_id=kim2025] \u2022 Offline Inference workload: To model a batch processing\ntask, an input size of 1024 tokens and an output size\nof 128 tokens was used.\n---\n[ref_id=kim2025] This takes into account tasks\nsuch as document summarization and data wrangling.\n---\n[ref_id=kim2025] To\nsimulate a batch processing task, the workload evaluates\nthe performance of completing 1000 requests.\n---\n[ref_id=kim2025] A WS Cloud Experiment Setup : To maintain uniform\nexperimental conditions and reduce potential disruptions\ncaused by fluctuating cloud workloads, all experiments were\ncarried out on AWS in the us-east-1 (Northern Virginia)\nregion between 9:00 AM and 10:00 PM KST, spanning\nthe period from December 2024 to March 2025.\n---\n[ref_id=kim2025] To avoid\nperformance variations due to regional resource contention,\ntesting was evenly distributed across availability zones us-\neast-1a through us-east-1f.\n---\n[ref_id=kim2025] For the GPU-VMs, we utilized\ng4dn.xlarge(NVIDIA T4), g5.xlarge(NVIDIA A10G),\ng6.xlarge(NVIDIA L4) and g6e.xlarge(NVIDIA L40s)\nA detailed specification of the instances are specified in Table\nIV.\n---\n[ref_id=kim2025] TABLE IV\nSpecifications of VM instances, including 4 GPU-VMs based on\nA WS specifications.\n---\n[ref_id=kim2025] InstanceGPU-TypeOn-Demand PriceGPU MemoryFP16 PCIe B/W($/hr) (GB) (TFLOPS)(GB/s)\ng6e.xlarge L40s 2.699 48 91.61 12g6.xlarge L4 1.167 24 30.29 12g5.xlarge A10G 1.466 24 31.52 12g4dn.xlargeT4 0.71 16 8.24 6\nTo validate the effectiveness of InferSave, major\ntransformer-based LLM models such as OPT-1.3B, OPT-2.7B,\nOPT-6.7B were used for testing in an in-house benchmark\nsuite.\n---\n[ref_id=kim2025] To find the optimal performance configuration, tests\nwere conducted by varying the batch size from 1 to 64 under\ndifferent conditions for single GPU processing.\n---\n[ref_id=kim2025] Policy To Select Instance : As stated in Section II-D,\nthere are no clear state of the art methodologies for GPU\ninstance selection for inferencing.\n---\n[ref_id=kim2025] Therefore, in our evaluation,\nwe compared the following two baseline approaches with\nInferSave.\n---\n[ref_id=kim2025] \u2022 Most powerful instance(Max-Performance) : This policy\nsimply chooses the GPU instance that offers the most per-\nformance, and aims to lower latency and raise throughput\nas much as possible.\n---\n[ref_id=kim2025] However, this methodology does not\ntake into consideration price, and therefore running costs\ncan be raised needlessly.\n---\n[ref_id=kim2025] \u2022 Simple performance prediction( InferSave (without\nKV Cache offloading)) : This policy uses theoretical\nperformance metrics (FLOPS, memory bandwidth) to predict\nperformance and select an instance.\n---\n[ref_id=kim2025] However, it does not\ntake into consideration the effects of KV Cache offloading,\nand may not be able to find the most optimal instance.\n---\n[ref_id=kim2025] B.\n---\n[ref_id=kim2025] CTCF Validation\nInferSave proposes the Compute Time Calibration Func-\ntion (CTCF) to accurately determine the optimal instance\nbased on user requirements.\n---\n[ref_id=kim2025] To validate the accuracy of\nCTCF, experiments were conducted on two GPU instances,\ng4dn.xlarge and g6.xlarge.\n---\n[ref_id=kim2025] The experiments utilized the OPT-\n2.7B model, with an input token length of 512 and an output\ntoken length of 128.\n---\n[ref_id=kim2025] The model\u2019s key computational units,\nincluding a hidden size of 2560 and an intermediate size of\n2560 \u00d7 4, were applied, and the total number of layers (32)\nwas incorporated to measure computation time.\n---\n[ref_id=kim2025] For FLOPS\nestimation, the theoretical FLOPS values provided by GPU\nmanufacturers were used: g4dn.xlarge with NVIDIA T4 (8.24\nTFLOPS) and g6.xlarge with NVIDIA L4 (30.29 TFLOPS).\n---\n[ref_id=kim2025] After applying CTCF, the corrected prediction times were\ncomputed and compared with actual measurements to analyze\nthe error rate.\n---\n[ref_id=kim2025] As shown in Figure 4, the CTCF-adjusted values\nclosely matched the actual measurements.\n---\n[ref_id=kim2025] Specifically, in the\nDecode stage of g4dn.xlarge, the corrected values exhibited\nan average error rate of 1% compared to actual measurements,\nwhile in the Prefill stage of g6.xlarge, the average error rate\n---\n[ref_id=jegham2025] Li + Output Length\nRi\n3600\n!\n| {z }\nTotal inference time (Ti, hours)\n\u00d7\n\uf8ee\n\uf8ef\uf8f0PGPU \u00d7 UGPU,{min,max}| {z }\nGPU power (kW)\n+ Pnon-GPU \u00d7 Unon-GPU| {z }\nNon-GPU power (kW)\n\uf8f9\n\uf8fa\uf8fb \u00d7 PUE\n(1)\nWe also define an expected per-query energy as a weighted combination of both scenarios (wmax =\n0.5), and the framework aggregates all Monte Carlo draws to produce a distribution of per-query\nenergy outcomes. The final metrics are reported as the sample mean and standard deviation:\nEi,exp = wmaxEi,max + (1 \u2212 wmax)Ei,min, \u00afEquery = E[Ei,exp], \u03c3 Equery =\nq\nVar[Ei,exp] (2)\nThis stochastic formulation captures variability in runtime, hardware utilization, and data-center\nefficiency, enabling robust and reproducible estimation of per-query energy consumption across\ndiverse inference conditions.\n4.3 Hardware-Class Attribution\nWe stratify LLMs into five hardware classes based on model size: Nano (<7B), Micro (7\u201320B),\nSmall (20\u201340B), Medium (40\u201370B), and Large (>70B), assigning 1, 2, 4, or 8 GPUs accordingly.\nModels that do not disclose parameter counts, such as OpenAI and Anthropic flagship models (e.g.,\nGPT-4o, Claude-3.7 Sonnet), are classified as Large, OpenAI Mini variants (e.g., GPT-4o mini)\nas Medium, and models labeled \u201cNano\u201d such as GPT-4.1 nano asSmall based on reported model\nperformance (e.g., TPS, latency, and reasoning capabilities) [51].\nAI companies and cloud providers typically rely on dynamic batching to optimize GPU utilization\nwhile maintaining low latency [52]. Although actual batch sizes fluctuate depending on incoming\ndemand, they are generally constrained to a narrow range below 16 to preserve responsiveness.\nBenchmarks [51] show that even for large prompts, most models maintain a first-token latency below\none second. Moreover, prior studies [53, 54] show that these latency values are consistent with batch\nsizes in the range of 4 to 16. This suggests that real-world deployments prioritize small, latency-\nsensitive batches over maximal throughput. Accordingly, we adopt a batch size of 8 for all primary\ncalculations, as it represents a practical midpoint between common deployment scenarios. A detailed\nsensitivity analysis exploring the impact of alternative batch sizes is provided in Appendix A. The\nnumber of GPUs and their allocated power draw utilization rates for H100 systems are estimated from\nSplitwise [54], the Latency Processing Unit study [55], and LLM-Inference-Bench [53]. For A100\nsystems, we adopt measurements from Patel et al. and Kakolyris et al.\u2019s work [56, 57]. Per-request\nGPU and non-GPU utilization rates are calculated as:\nUGPU total = G \u00d7 DGPU\nN \u00d7 B , U non-GPU total = G \u00d7 Dnon-GPU\nN \u00d7 B (3)\nwhere G is the number of GPUs assigned per model, N = 8 is the number of GPUs per node, and\nB = 8 is the batch size. DGPU denotes the assigned GPUs\u2019 power draw, expressed as a fraction of their\nmaximum power draw, while Dnon-GPU = 0.5 represents the conservatively assigned fixed utilization\nfraction for non-GPU components (e.g., CPU, memory, storage, cooling), relative to their peak power\ndraw [32]. We exclude idle power consumption from unutilized GPUs in partially loaded nodes,\nas deployment-specific telemetry is unavailable to determine whether such capacity is reassigned,\nload-balanced, or remains idle. Table 2 summarizes GPU and non-GPU power utilization rates across\nmodel classes. Values are rounded to typical intervals observed during inference, accounting for input\nprocessing spikes, output length, decoding complexity, and a batch size of 8 parallel requests.\n5\n---\n[ref_id=jegham2025] Page 5\n---\n[ref_id=jegham2025] Li + Output Length\nRi\n3600\n!\n---\n[ref_id=jegham2025] | {z }\nTotal inference time (Ti, hours)\n\u00d7\n\uf8ee\n\uf8ef\uf8f0PGPU \u00d7 UGPU,{min,max}| {z }\nGPU power (kW)\n+ Pnon-GPU \u00d7 Unon-GPU| {z }\nNon-GPU power (kW)\n\uf8f9\n\uf8fa\uf8fb \u00d7 PUE\n(1)\nWe also define an expected per-query energy as a weighted combination of both scenarios (wmax =\n0.5), and the framework aggregates all Monte Carlo draws to produce a distribution of per-query\nenergy outcomes.\n---\n[ref_id=jegham2025] The final metrics are reported as the sample mean and standard deviation:\nEi,exp = wmaxEi,max + (1 \u2212 wmax)Ei,min, \u00afEquery = E[Ei,exp], \u03c3 Equery =\nq\nVar[Ei,exp] (2)\nThis stochastic formulation captures variability in runtime, hardware utilization, and data-center\nefficiency, enabling robust and reproducible estimation of per-query energy consumption across\ndiverse inference conditions.\n---\n[ref_id=jegham2025] 4.3 Hardware-Class Attribution\nWe stratify LLMs into five hardware classes based on model size: Nano (<7B), Micro (7\u201320B),\nSmall (20\u201340B), Medium (40\u201370B), and Large (>70B), assigning 1, 2, 4, or 8 GPUs accordingly.\n---\n[ref_id=jegham2025] Models that do not disclose parameter counts, such as OpenAI and Anthropic flagship models (e.g.,\nGPT-4o, Claude-3.7 Sonnet), are classified as Large, OpenAI Mini variants (e.g., GPT-4o mini)\nas Medium, and models labeled \u201cNano\u201d such as GPT-4.1 nano asSmall based on reported model\nperformance (e.g., TPS, latency, and reasoning capabilities) [51].\n---\n[ref_id=jegham2025] AI companies and cloud providers typically rely on dynamic batching to optimize GPU utilization\nwhile maintaining low latency [52].\n---\n[ref_id=jegham2025] Although actual batch sizes fluctuate depending on incoming\ndemand, they are generally constrained to a narrow range below 16 to preserve responsiveness.\n---\n[ref_id=jegham2025] Benchmarks [51] show that even for large prompts, most models maintain a first-token latency below\none second.\n---\n[ref_id=jegham2025] Moreover, prior studies [53, 54] show that these latency values are consistent with batch\nsizes in the range of 4 to 16.\n---\n[ref_id=jegham2025] This suggests that real-world deployments prioritize small, latency-\nsensitive batches over maximal throughput.\n---\n[ref_id=jegham2025] Accordingly, we adopt a batch size of 8 for all primary\ncalculations, as it represents a practical midpoint between common deployment scenarios.\n---\n[ref_id=jegham2025] A detailed\nsensitivity analysis exploring the impact of alternative batch sizes is provided in Appendix A.\n---\n[ref_id=jegham2025] The\nnumber of GPUs and their allocated power draw utilization rates for H100 systems are estimated from\nSplitwise [54], the Latency Processing Unit study [55], and LLM-Inference-Bench [53].\n---\n[ref_id=jegham2025] For A100\nsystems, we adopt measurements from Patel et al.\n---\n[ref_id=jegham2025] and Kakolyris et al.\u2019s work [56, 57].\n---\n[ref_id=jegham2025] Per-request\nGPU and non-GPU utilization rates are calculated as:\nUGPU total = G \u00d7 DGPU\nN \u00d7 B , U non-GPU total = G \u00d7 Dnon-GPU\nN \u00d7 B (3)\nwhere G is the number of GPUs assigned per model, N = 8 is the number of GPUs per node, and\nB = 8 is the batch size.\n---\n[ref_id=jegham2025] DGPU denotes the assigned GPUs\u2019 power draw, expressed as a fraction of their\nmaximum power draw, while Dnon-GPU = 0.5 represents the conservatively assigned fixed utilization\nfraction for non-GPU components (e.g., CPU, memory, storage, cooling), relative to their peak power\ndraw [32].\n---\n[ref_id=jegham2025] We exclude idle power consumption from unutilized GPUs in partially loaded nodes,\nas deployment-specific telemetry is unavailable to determine whether such capacity is reassigned,\nload-balanced, or remains idle.\n---\n[ref_id=jegham2025] Table 2 summarizes GPU and non-GPU power utilization rates across\nmodel classes.\n---\n[ref_id=jegham2025] Values are rounded to typical intervals observed during inference, accounting for input\nprocessing spikes, output length, decoding complexity, and a batch size of 8 parallel requests.\n---\n[ref_id=jegham2025] 5\n---\n[ref_id=griggs2024] 6 Evaluation\nWe assess M\u00e9lange\u2019s performance across diverse hardware, request sizes, rates, and SLOs. M\u00e9lange\nconsistently achieves significant cost savings (up to 77%) compared to single-GPU-type strategies,\nand the selected allocations successfully attain TPOT SLO for over 99.5% of requests.\n6.1 Experiment Setup\nEnvironment. We use four NVIDIA GPU types that capture a broad range of prices and specifications,\nwith details in Tab. 1. In increasing price order, we use L4, A10G, A100-80G, and H100. To\ndetermine the GPU cost, we select the lowest on-demand price available from major cloud providers\n(AWS, Azure, and GCP). Since on-demand H100 is not offered by these major providers, we defer to\nthe pricing from RunPod [39] due to its popularity and availability. To ensure fair cost comparisons,\nwe normalize RunPod\u2019s H100 pricing to match the pricing structures of major platforms. We\ncalculate this by comparing RunPod\u2019s H100 cost ($4.69) to RunPod\u2019s A100-80G cost ($2.29), then\nadjusting relative to the A100\u2019s price on major clouds ($3.67), resulting in a normalized price of\n(4.69/2.29) \u00d7 3.67 = $7 .516 for H100. In each experiment, we serve Llama2-7b [44] with vLLM\n0.2.7 [19].\nType L4 A10G (PCIe) A100-80G (SXM) H100 (SXM)\nOn-demand Price ($/h) 0.7 1.01 3.67 7.5164\nInstance Provider GCP AWS Azure RunPod\nInstance Name g2-standard-4 g5.xlarge NC24ads_A100_v4/N.A. N.A.\nMemory (GB) 24 24 80 80\nMemory Bandwidth (GB/s) 300 600 1935 3350\nFP16 (TFLOPS) 242 125 312 1979\nTable 1: Specifications of four NVIDIA GPUs: L4, A10G, A100, and H100.\nDatasets and SLOs. We evaluate across three datasets to cover a wide range of application scenarios.\nFor short-context tasks (interactive chats) we use the Chatbot Arena dataset [55], for long-context\ntasks (document summarization) we use the PubMed dataset [ 7], and for a mixed-context-length\nsetting we create a synthetic dataset by sampling 80% from Chatbot Arena and 20% from PubMed.\nThe input and output length distributions are shown in Fig. 10. We follow standard LLM inference\nbenchmarks [3] to set reasonable TPOT SLOs, and use 40ms to simulate services where swift\nresponses are essential, and 120ms where longer response times are acceptable. Both selected SLOs\nsurpass the average human reading speed, ensuring the SLOs satisfy practical user experience.\n0 2500 5000 7500 10000 12500\nInput Length (tokens)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Fraction\nDataset\nMixed (mean=1278.04)\nArena (mean=329.43)\nPubmed (mean=4174.13)\n(a) Input length distributions.\n0 250 500 750 1000\nOutput Length (tokens)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125Fraction\nDataset\nMixed (mean=219.87)\nArena (mean=195.66)\nPubmed (mean=314.1) (b) Output length distributions.\nFigure 10: Dataset input and output length distributions.\nM\u00e9lange Configuration. Bucket size ranges correspond to Figure 5, comprising of 10 input length\nranges and 6 output length ranges (60 total buckets). The slice factor is set to 8 for a total of\n60 \u00b7 8 = 480 slices.\nBaselines. We compare M\u00e9lange to allocations that use a single GPU type. To derive baseline\nallocations, we use M\u00e9lange\u2019s ILP formulation (\u00a7 5.4.3) but restrict the solver to a single GPU type.\n9\n---\n[ref_id=griggs2024] Page 9\n---\n[ref_id=griggs2024] 6 Evaluation\nWe assess M\u00e9lange\u2019s performance across diverse hardware, request sizes, rates, and SLOs.\n---\n[ref_id=griggs2024] M\u00e9lange\nconsistently achieves significant cost savings (up to 77%) compared to single-GPU-type strategies,\nand the selected allocations successfully attain TPOT SLO for over 99.5% of requests.\n---\n[ref_id=griggs2024] 6.1 Experiment Setup\nEnvironment.\n---\n[ref_id=griggs2024] We use four NVIDIA GPU types that capture a broad range of prices and specifications,\nwith details in Tab.\n---\n[ref_id=griggs2024] 1.\n---\n[ref_id=griggs2024] In increasing price order, we use L4, A10G, A100-80G, and H100.\n---\n[ref_id=griggs2024] To\ndetermine the GPU cost, we select the lowest on-demand price available from major cloud providers\n(AWS, Azure, and GCP).\n---\n[ref_id=griggs2024] Since on-demand H100 is not offered by these major providers, we defer to\nthe pricing from RunPod [39] due to its popularity and availability.\n---\n[ref_id=griggs2024] To ensure fair cost comparisons,\nwe normalize RunPod\u2019s H100 pricing to match the pricing structures of major platforms.\n---\n[ref_id=griggs2024] We\ncalculate this by comparing RunPod\u2019s H100 cost ($4.69) to RunPod\u2019s A100-80G cost ($2.29), then\nadjusting relative to the A100\u2019s price on major clouds ($3.67), resulting in a normalized price of\n(4.69/2.29) \u00d7 3.67 = $7 .516 for H100.\n---\n[ref_id=griggs2024] In each experiment, we serve Llama2-7b [44] with vLLM\n0.2.7 [19].\n---\n[ref_id=griggs2024] Type L4 A10G (PCIe) A100-80G (SXM) H100 (SXM)\nOn-demand Price ($/h) 0.7 1.01 3.67 7.5164\nInstance Provider GCP AWS Azure RunPod\nInstance Name g2-standard-4 g5.xlarge NC24ads_A100_v4/N.A.\n---\n[ref_id=griggs2024] N.A.\n---\n[ref_id=griggs2024] Memory (GB) 24 24 80 80\nMemory Bandwidth (GB/s) 300 600 1935 3350\nFP16 (TFLOPS) 242 125 312 1979\nTable 1: Specifications of four NVIDIA GPUs: L4, A10G, A100, and H100.\n---\n[ref_id=griggs2024] Datasets and SLOs.\n---\n[ref_id=griggs2024] We evaluate across three datasets to cover a wide range of application scenarios.\n---\n[ref_id=griggs2024] For short-context tasks (interactive chats) we use the Chatbot Arena dataset [55], for long-context\ntasks (document summarization) we use the PubMed dataset [ 7], and for a mixed-context-length\nsetting we create a synthetic dataset by sampling 80% from Chatbot Arena and 20% from PubMed.\n---\n[ref_id=griggs2024] The input and output length distributions are shown in Fig.\n---\n[ref_id=griggs2024] 10.\n---\n[ref_id=griggs2024] We follow standard LLM inference\nbenchmarks [3] to set reasonable TPOT SLOs, and use 40ms to simulate services where swift\nresponses are essential, and 120ms where longer response times are acceptable.\n---\n[ref_id=griggs2024] Both selected SLOs\nsurpass the average human reading speed, ensuring the SLOs satisfy practical user experience.\n---\n[ref_id=griggs2024] 0 2500 5000 7500 10000 12500\nInput Length (tokens)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Fraction\nDataset\nMixed (mean=1278.04)\nArena (mean=329.43)\nPubmed (mean=4174.13)\n(a) Input length distributions.\n---\n[ref_id=griggs2024] 0 250 500 750 1000\nOutput Length (tokens)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125Fraction\nDataset\nMixed (mean=219.87)\nArena (mean=195.66)\nPubmed (mean=314.1) (b) Output length distributions.\n---\n[ref_id=griggs2024] Figure 10: Dataset input and output length distributions.\n---\n[ref_id=griggs2024] M\u00e9lange Configuration.\n---\n[ref_id=griggs2024] Bucket size ranges correspond to Figure 5, comprising of 10 input length\nranges and 6 output length ranges (60 total buckets).\n---\n[ref_id=griggs2024] The slice factor is set to 8 for a total of\n60 \u00b7 8 = 480 slices.\n---\n[ref_id=griggs2024] Baselines.\n---\n[ref_id=griggs2024] We compare M\u00e9lange to allocations that use a single GPU type.\n---\n[ref_id=griggs2024] To derive baseline\nallocations, we use M\u00e9lange\u2019s ILP formulation (\u00a7 5.4.3) but restrict the solver to a single GPU type.\n---\n[ref_id=griggs2024] 9\n---\n[ref_id=samsi2024] III. E XPERIMENTAL SETUP\nWe conducted our experiments on the MIT Supercloud\nhigh-performance computing (HPC) system [18]. This het-\nerogeneous HPC cluster consists of 448 compute nodes with\ndual Intel Xeon Gold 6248 CPUs with 384 GB of RAM\nand two NVIDIA V olta V100 GPUs with 32 GB of memory\nper node. Each node on the system has two independent\nback-end fabrics: a 100 Gb/s Intel Omnipath as well as a\n25 Gb/s Ethernet interconnect using Mellanox ConnectX-4\nadapters with all servers connected to a single, non-blocking\nArista DCS-7516 Ethernet core switch. The GPUs, Omnipath,\nand Ethernet cards are all connected to PCIe slots that route\ndirectly to the Xeon processors without any intermediary PCIe\nswitches. All experiments in this paper exclusively used the\n25 Gb/s Ethernet interconnect. The system also includes 480\nCPU-only nodes with Intel Xeon Platinum 8260 processors.\nIn addition, four nodes with NVIDIA A100 GPUs were also\navailable for experiments described in this paper. A summary\nof the hardware is shown in Table I. All experiments described\nin this paper were run exclusively on NVIDIA GPUs.\nTABLE I: Compute node configurations: This table lists\nthe types of hardware used for inference evaluations in our\nexperiments. Each node consists of 2 CPUs and 2 GPUs in\nthe configuration listed below. All GPUs are from NVIDIA.\nCPU GPU\nType Memory TDP Type Memory TDP\n(GB) (W) (GB) (W)\nIntel Xeon\nGold 6248 384 150 V100 32 250\nIntel Xeon\nPlatinum 8358 503 240 A100 80 300\nA. Models\nExperiments were performed using open-source implemen-\ntation of the pre-trained LLaMA 65B model available via\nrequest from Meta [14] and evaluation scripts available via\nGitHub [19]. This implementation of the model uses Pytorch\nand the FairScale [20] library to enable model sharding across\nmultiple GPUs and nodes. For the models, we use a decoder\ntemperature setting \u03c4 = 0 .8 and a top- p value of 0.95 in\nattempts to align our settings with the general range of values\nthat are typically used. In future work, we aim to study\nhow varying decoding temperature, top- p, and other hyper-\nparameters may affect compute performance and energy usage\nduring inference. While our main focus is on LLaMA 65B,\nwe also examine LLaMA 7B and LLaMA 13B to characterize\ninference performance and energy under the bare minimum\nsettings/resources required to run these models.\nB. Datasets\nWe used two datasets to evaluate inference performance.\nThe first is an instruction following dataset used to fine-tune\nthe Alpaca [21] model (from here on, this dataset is referred\nto as \u201cAlpaca\u201d in our paper which is not to be confused with\nthe Alpaca model). This Alpaca dataset consists of 52,000\ninstruction-following tasks, instructions/questions where some\nhave example inputs and some do not, that the model is asked\nto answer. The second dataset is GSM8K [22], consisting of\n8,500 human crafted grade school math problems. The goal\nof using these two datasets is two-fold: (1) to evaluate the\nmodel on a diverse set of tasks in NLP and (2) evaluate\nhow different types of data and its underlying dynamics\ncan impact energy and inference performance. While natural\nlanguage is more common in LLM usage and in LLM training\ndata, increasingly new capabilities have been demonstrated\nin LLMs, including the ability to solve simple mathematical\nproblems, provide/correct examples of code, and more. Math\nquestions also differ considerably from questions posed in\nnatural language which can result in smaller context windows,\ninputs/outputs of differing lengths, number of decoded tokens,\netc. This, in turn, may impact inference performance in\neither throughput rates or energy costs. For this reason, our\nbenchmarking experiments are conducted on both datasets.\nFor both datasets, we sample 4,096 inputs for our inference\nexperiments. Using the entirety of the datasets would only\nserve to increase inference time and energy used for the ex-\nperimentation unreasonably and did not provide any significant\nbenefits to the study.\nC. Evaluation\nOur goal is to evaluate the inference performance, latency,\nand inference energy costs of LLaMA 65B as a representative\nlarge language model that requires sharding across multiple\nGPUs. We intend this to be a preliminary analysis that will\nhelp guide more in-depth experiments and benchmarking for\nour future work. Our analysis also includes limited analysis of\nsmaller LLaMA variants to illustrate inference performance\nand energy trade-offs in bare-minimum hardware settings:\nnamely, LLaMA 7B and 13B. While we do not control for\nthe correctness/quality of the outputs or the complexity of the\ninputs/outputs in studying trade-offs between inference energy\nand performance, we hope to account for this as an ablative\nstudy in future work. Similarly, we do not perform a com-\nprehensive evaluation with different optimization techniques\nor inference settings available for LLMs such as modeling\nquery arrival rates, model quantization, continuous batching,\netc. which we also leave for future work.\nInference performance is measured in terms of rates: words,\ntokens, and responses per second or, equivalently, the number\nof words, tokens, and responses generated per second. When\nrunning inference with LLaMA, the model generates a string\nof text for each input until the length of the text hits a\nmaximum generation length or a stop-word is encountered.\nThe number of words are calculated by counting the number\nof words present in the output by splitting each output string\non spaces. The number of tokens is calculated using LLaMA\u2019s\nown default tokenizer by counting the number of tokens in the\ntokenized output. Lastly, the number of responses per second\nor the response rate is calculated using the total number of\n---\n[ref_id=samsi2024] Page 3\n---\n[ref_id=samsi2024] III.\n---\n[ref_id=samsi2024] E XPERIMENTAL SETUP\nWe conducted our experiments on the MIT Supercloud\nhigh-performance computing (HPC) system [18].\n---\n[ref_id=samsi2024] This het-\nerogeneous HPC cluster consists of 448 compute nodes with\ndual Intel Xeon Gold 6248 CPUs with 384 GB of RAM\nand two NVIDIA V olta V100 GPUs with 32 GB of memory\nper node.\n---\n[ref_id=samsi2024] Each node on the system has two independent\nback-end fabrics: a 100 Gb/s Intel Omnipath as well as a\n25 Gb/s Ethernet interconnect using Mellanox ConnectX-4\nadapters with all servers connected to a single, non-blocking\nArista DCS-7516 Ethernet core switch.\n---\n[ref_id=samsi2024] The GPUs, Omnipath,\nand Ethernet cards are all connected to PCIe slots that route\ndirectly to the Xeon processors without any intermediary PCIe\nswitches.\n---\n[ref_id=samsi2024] All experiments in this paper exclusively used the\n25 Gb/s Ethernet interconnect.\n---\n[ref_id=samsi2024] The system also includes 480\nCPU-only nodes with Intel Xeon Platinum 8260 processors.\n---\n[ref_id=samsi2024] In addition, four nodes with NVIDIA A100 GPUs were also\navailable for experiments described in this paper.\n---\n[ref_id=samsi2024] A summary\nof the hardware is shown in Table I.\n---\n[ref_id=samsi2024] All experiments described\nin this paper were run exclusively on NVIDIA GPUs.\n---\n[ref_id=samsi2024] TABLE I: Compute node configurations: This table lists\nthe types of hardware used for inference evaluations in our\nexperiments.\n---\n[ref_id=samsi2024] Each node consists of 2 CPUs and 2 GPUs in\nthe configuration listed below.\n---\n[ref_id=samsi2024] All GPUs are from NVIDIA.\n---\n[ref_id=samsi2024] CPU GPU\nType Memory TDP Type Memory TDP\n(GB) (W) (GB) (W)\nIntel Xeon\nGold 6248 384 150 V100 32 250\nIntel Xeon\nPlatinum 8358 503 240 A100 80 300\nA.\n---\n[ref_id=samsi2024] Models\nExperiments were performed using open-source implemen-\ntation of the pre-trained LLaMA 65B model available via\nrequest from Meta [14] and evaluation scripts available via\nGitHub [19].\n---\n[ref_id=samsi2024] This implementation of the model uses Pytorch\nand the FairScale [20] library to enable model sharding across\nmultiple GPUs and nodes.\n---\n[ref_id=samsi2024] For the models, we use a decoder\ntemperature setting \u03c4 = 0 .8 and a top- p value of 0.95 in\nattempts to align our settings with the general range of values\nthat are typically used.\n---\n[ref_id=samsi2024] In future work, we aim to study\nhow varying decoding temperature, top- p, and other hyper-\nparameters may affect compute performance and energy usage\nduring inference.\n---\n[ref_id=samsi2024] While our main focus is on LLaMA 65B,\nwe also examine LLaMA 7B and LLaMA 13B to characterize\ninference performance and energy under the bare minimum\nsettings/resources required to run these models.\n---\n[ref_id=samsi2024] B.\n---\n[ref_id=samsi2024] Datasets\nWe used two datasets to evaluate inference performance.\n---\n[ref_id=samsi2024] The first is an instruction following dataset used to fine-tune\nthe Alpaca [21] model (from here on, this dataset is referred\nto as \u201cAlpaca\u201d in our paper which is not to be confused with\nthe Alpaca model).\n---\n[ref_id=samsi2024] This Alpaca dataset consists of 52,000\ninstruction-following tasks, instructions/questions where some\nhave example inputs and some do not, that the model is asked\nto answer.\n---\n[ref_id=samsi2024] The second dataset is GSM8K [22], consisting of\n8,500 human crafted grade school math problems.\n---\n[ref_id=samsi2024] The goal\nof using these two datasets is two-fold: (1) to evaluate the\nmodel on a diverse set of tasks in NLP and (2) evaluate\nhow different types of data and its underlying dynamics\ncan impact energy and inference performance.\n---\n[ref_id=samsi2024] While natural\nlanguage is more common in LLM usage and in LLM training\ndata, increasingly new capabilities have been demonstrated\nin LLMs, including the ability to solve simple mathematical\nproblems, provide/correct examples of code, and more.\n---\n[ref_id=samsi2024] Math\nquestions also differ considerably from questions posed in\nnatural language which can result in smaller context windows,\ninputs/outputs of differing lengths, number of decoded tokens,\netc.\n---\n[ref_id=samsi2024] This, in turn, may impact inference performance in\neither throughput rates or energy costs.\n---\n[ref_id=samsi2024] For this reason, our\nbenchmarking experiments are conducted on both datasets.\n---\n[ref_id=samsi2024] For both datasets, we sample 4,096 inputs for our inference\nexperiments.\n---\n[ref_id=samsi2024] Using the entirety of the datasets would only\nserve to increase inference time and energy used for the ex-\nperimentation unreasonably and did not provide any significant\nbenefits to the study.\n---\n[ref_id=samsi2024] C.\n---\n[ref_id=samsi2024] Evaluation\nOur goal is to evaluate the inference performance, latency,\nand inference energy costs of LLaMA 65B as a representative\nlarge language model that requires sharding across multiple\nGPUs.\n---\n[ref_id=samsi2024] We intend this to be a preliminary analysis that will\nhelp guide more in-depth experiments and benchmarking for\nour future work.\n---\n[ref_id=samsi2024] Our analysis also includes limited analysis of\nsmaller LLaMA variants to illustrate inference performance\nand energy trade-offs in bare-minimum hardware settings:\nnamely, LLaMA 7B and 13B.\n---\n[ref_id=samsi2024] While we do not control for\nthe correctness/quality of the outputs or the complexity of the\ninputs/outputs in studying trade-offs between inference energy\nand performance, we hope to account for this as an ablative\nstudy in future work.\n---\n[ref_id=samsi2024] Similarly, we do not perform a com-\nprehensive evaluation with different optimization techniques\nor inference settings available for LLMs such as modeling\nquery arrival rates, model quantization, continuous batching,\netc.\n---\n[ref_id=samsi2024] which we also leave for future work.\n---\n[ref_id=samsi2024] Inference performance is measured in terms of rates: words,\ntokens, and responses per second or, equivalently, the number\nof words, tokens, and responses generated per second.\n---\n[ref_id=samsi2024] When\nrunning inference with LLaMA, the model generates a string\nof text for each input until the length of the text hits a\nmaximum generation length or a stop-word is encountered.\n---\n[ref_id=samsi2024] The number of words are calculated by counting the number\nof words present in the output by splitting each output string\non spaces.\n---\n[ref_id=samsi2024] The number of tokens is calculated using LLaMA\u2019s\nown default tokenizer by counting the number of tokens in the\ntokenized output.\n---\n[ref_id=samsi2024] Lastly, the number of responses per second\nor the response rate is calculated using the total number of\n---\n[ref_id=xia2024] Projected maximum batch size of Mixtral for different GPUs.\n---\n[ref_id=xia2024] 0 20 40 60 80 100 1200\n5\n10\n15\n20\n25\n30\n35\n40\nA100-40GB\nA100-80GB\nA40\nH100\nbsz=28\nbsz=35\nProjected GPU capacity\nGround Truth Projection\nMax batch size\nGPU DRAM capacity\nFig. 13. Projected maximum batch size of Mixtral for different GPUs.\namount of GPU memory, and need to be subtracted in the\nanalytical model. Fig. 8 supports this by showing that on the\nsame dataset, BlackMamba can support larger batch size than\nMixtral because of its smaller model size.\nMoreover, the sequence length and sparsity also affect the\nmaximum batch size. Because the sparsity only affects the\nMoE part of the LLM, we multiply its influence by C1,\nwhich we call MoE coefficient. We apply the sequence length\nand the sparsity in the denominator as they are inversely\nrelated to batch size. Then, we multiply the result by C0,\nthe scaling coefficient , which scales the batch size by a\nconstant. The scaling coefficient is different across LLM\nmodels, because different models have different architecture\n(\u00a7III), and generate different amounts of intermediate data\nfor each query. The scaling coefficient for BlackMamba is\nhigher than that of Mixtral because it is a smaller model.\nFinally, we use floor to round it to the maximum integer.\nThe MoE coefficient and scaling coefficient vary across\nmodels. These coefficients are independent of GPU microar-\nchitectural parameters. We find the maximum batch size for\nboth LLM models on NVIDIA A40 (48GB), A100 (40GB),\nA100 (80GB), and H100 (80GB), and apply our model to\nfind the optimal coefficients. For Mixtral, C0 = 82 and\nC1 = 0 .95, and for BlackMamba, C0 = 83 and C1 = 0 .88.\nWhile we showcase these parameters for the models eval-\nuated, \u00a7V-D discusses how to generalize this approach for\nother models.\nUsing our analytical model, we demonstrate the maximum\nbatch sizes for fine-tuning on four different NVIDIA GPUs:\nA40, A100-40GB, A100-80GB and H100 with memory\ncapacities of 48GB, 40GB, 80GB, and 80GB, respectively.\nFig. 13 shows our projected maximum batch size and corre-\nlate it with experimented ground truth. While the maximum\nmemory capacity available in NVIDIA GPUs today is 80GB,\nwe use our analytical model to project the maximum batch\nsize that future GPUs might support. For GPU memory\ncapacities of 100GB and 120GB, our model predicts that\nthe maximum batch sizes supported for fine-tuning Mixtral\nwill be 28 and 35, respectively. Due to space limitations, we\nonly show the projection of Mixtral model.\nTABLE IV\nESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE\nBASED ON OUR ANALYTICAL MODEL\nGPU Mem MBS Throughput Cost ($/hr) Cost ($)\nA40 48GB 4 1.01 0.79 32.7\nA100 80GB 17 2.74 1.67 25.4\nH100 80GB 17 4.90 2.1 17.9\nB. Estimating Throughput\nAs discussed in \u00a7IV-B4, when the batch size increases, the\nLLM fine-tuning gradually switches from memory bound to\ncompute bound. When the compute resources are abundant,\nthe throughput increases almost linearly with batch size.\nHowever, when compute resources become constrained, the\nthroughput improvement gradually saturates. We model this\nbehavior using a logarithmic relation between batch size and\nthroughput. Our analytical model for maximum batch size is\nshown in (2).\nT hroughput = C2 \u2217 log( batch size\nsparsity \u2217 C3 ) + C4 (2)\nIn the equation, in addition to the basic logarithmic part, we\nhave three coefficients C2, C3, and C4. C2 is the scaling\ncoefficient, which depends on the LLM model, GPU archi-\ntecture, and the dataset. The higher the compute capability a\nGPU can provide, and the lower the LLM model and dataset\ncompute requirement is, the higher the scaling coefficient will\nbe. C3 is the MoE attenuation coefficient , which tunes how\nmuch the MoE sparsity affects the throughput. MoE sparsity\nonly affects the MoE part in LLM model, and thus should\nbe attenuated to avoid over compensation. This coefficient is\nonly LLM model dependent, because once the model is fixed,\nthe influence of sparsity is determined. C4 is the intercept,\nconceptually it equals to the throughput when batch size\nequals one, because the logarithmic part in (2) is zero when\nbatch size is one. Using scipy [32] to fit the model and\ngenerate four sets (C2, C3, C4), for each model and dataset\ncombination.\nTo estimate the accuracy of this model, we correlate the\nmodel output with experimental data from our study. Fig. 14\nshows this correlation study, where discrete data points (dots)\nrepresent experimental values, and the line represents output\nof our analytical model. We use both dense and sparse\nMixtral and BlackMamba for both datasets used in our study.\nThe figure clearly shows that our model accurately predicts\nLLM fine-tuning throughput with a Root Mean Squared Error\n(RMSE) of less than 0.8. Fig. 15 shows the correlation study\nof the analytical model of three other GPUs, A100 (40GB),\nA100 (80GB), and H100. The RMSE is less than 0.6, close\nto that of A40.\nC. Estimating the Total Fine-Tuning Cost\nUsing the throughput estimation, we calculate the cost\nof fine-tuning LLMs for different GPUs. The cost of GPU\n\n---\n\nNow answer the following question based ONLY on the context above.\n\nQuestion: What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?\n\nAdditional info (JSON): {\"answer_unit\": \"seconds\", \"question_id\": \"q317\"}\n\nIMPORTANT: The \"answer_unit\" field specifies the expected format/unit for answer_value.\n- If answer_unit is a unit (e.g., \"kW\", \"USD\"), express answer_value as a number in that unit (no unit name).\n- If answer_unit is \"is_blank\", answer_value should be the exact identifier/name from context that answers the question.\n- If the answer is a numeric range, format as [lower,upper].\n\nReturn STRICT JSON with these keys in order:\n- explanation          (1-3 sentences explaining how context supports the answer and how you applied answer_unit; or \"is_blank\")\n- answer               (short natural-language response)\n- answer_value         (the value matching the expected format; or \"is_blank\")\n- ref_id               (list of document ids from context used as evidence; or \"is_blank\")\n- ref_url              (list of URLs for the cited documents; or \"is_blank\")\n- supporting_materials (verbatim quote, table reference, or figure reference from the cited document; or \"is_blank\")\n\nJSON Answer:",
    "retrieved_snippets": [
      {
        "node_id": "xia2024:sec10:p13:s426",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "For example, our model predicted that fine-\ntuning a sparse Mixtral model using a realistic data size of\n2M queries can be done with NVIDIA H100 GPU with a cost\nof $3460.",
        "score": 0.7549,
        "rank": 1,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 426
        }
      },
      {
        "node_id": "xia2024:sec10:p13",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "0 2 4 6 8 100.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n RMSE=0.05\nMixtral-CS\nDense Sparse\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n RMSE=0.02\nMixtral-MATH\n0 5 10 15 200\n2\n4\n6\n8\n10\n12\n14\n16\n RMSE=0.79\nMamba-CS\n0 2 4 6 8 10 120\n2\n4\n6\n8\n10\n12\n RMSE=0.42\nThroughput (queries/sec)\nBatch size\nMamba-MATH\nFig. 14. Estimation and validation of LLM fine-tuning throughput for\ndifferent models, datasets for A40 GPU. Dots represent ground truth and\nlines present the estimation.\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nRMSE=0.03\nMixtral-CS-A100-40GB\nDense Sparse\n0 5 10 15 200.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n RMSE=0.09\nMixtral-CS-A100-80GB\n0 5 10 15 200\n1\n2\n3\n4\n5\n RMSE=0.55\nMixtral-CS-H100\nThroughput (queries/sec)\nBatch size\nFig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS\nfor different GPUs: A100 and H100.\nresource renting per hour is calculated based on CUDO\ncompute [33], as other popular cloud providers do not offer\ncost/hour rates for the NVIDIA A40 GPU. However, one\ncan easily adjust the GPU renting cost per hour to estimate\nthe cost on other clouds such as Amazon AWS [34] or\nLambda [35]. Table IV estimates the cost for fine-tuning\nMixtral on the MATH dataset with a sparse setup, using\n10 epochs on different GPUs for a realistic cost estimate.\nEnterprises may use larger datasets for fine-tuning, such as,\nOpenOrca [36] and LaMini-instruction [37] containing more\nthan 2M queries. For OpenOrca, by scaling the cost by\nnumber of queries, our model predicts that the most cost-\neffective option to rent GPU resources on CUDO compute\nis NVIDIA H100 with a net cost of $3460.\nD. Generalization of the Analytical Model\nThe analytical models for estimating maximum batch size\nand throughput can be generalized to various LLM models\nand datasets. These models consider the characteristics of\nthe LLM, dataset, and GPU. Specifically, the maximum\nbatch size model combines GPU memory and LLM model\nsize to determine available memory for input data, while\ndataset sequence length and LLM sparsity determine space\nneeded per batch. In throughput estimation, based on the\nobservation we made (\u00a7IV-B4 Takeaway 5), GPU shifts from\nmemory-bound to compute-bound as batch size increases.\nThis characteristic generally applies to all GPUs due to the\nresource constraint, so the logarithmic relation between batch\nsize and throughput persists. The sparsity in (2) is model\ndependent, the influence of GPU, LLM model, and dataset\nare embedded in the coefficients C2, C3, and C4 in (2).\nThe coefficients in (1) and (2) are dependent on GPU,\nLLM model, and dataset; however, the underlying models\nare generalizable to unseen GPU, LLM model, and datasets.\nAlthough it takes some effort to sweep batch sizes and collect\nthroughput data points to fit our models, the benefits greatly\noutweigh the cost. Once the models are fit, our model can\nhelp choose the most cost-efficient GPU for fine-tuning LLM\nmodels, greatly saving resources and money.\nVI. R ELATED WORKS\nParameter-Efficient Fine-Tuning (PEFT) has been widely\nadopted to fine-tune LLM model for specialized tasks [15],\n[38]\u2013[43]. MoE additioally train specialized experts for dif-\nferent areas and the dynamic selection of experts makes\nit possible to scale the fine-tuning workload to different\nexperts in parallel. [44]\u2013[47] show that MoE models can\nimprove the ability to process knowledge for specific tasks,\nwhile maintaining the world knowledge in LLM. Kim et\nal. [48] construct an analytical model to estimate GPU\nmemory consumption for distributed fine-tuning. The model\nalso provides insights into optimizing memory usage through\ntensor, model, and pipeline parallelism.\nVII. C ONCLUSIONS\nFine-tuning LLMs is an attractive technique for tailoring\nmodern language models using domain-specific knowledge in\na cost-effective manner. This paper delved into understanding\nthe performance of fine-tuning MoE LLM models on a single\nGPU. Our profiling demonstrated that sparse MoE layers\noffer the best bang-for-buck trade-off. Using our profiling\nresults, we developed and validated an accurate analytical\nmodel to estimate the cost of LLM fine-tuning. Using this\nmodel, we showed the dollar amount that needs to be\nbudgeted for fine-tuning LLMs, which is much lower than\npre-training. For example, our model predicted that fine-\ntuning a sparse Mixtral model using a realistic data size of\n2M queries can be done with NVIDIA H100 GPU with a cost\nof $3460. A way to further reduce cost based on our study\nis to add compute resources to accelerate the MoE layers.\nWhile we showcase our study on fine-tuning LLMs using a\nsingle GPU, extending this model to multi-GPU systems is\nleft for future exploration.\nACKNOWLEDGMENTS\nThis work was supported in part by Semiconductor Re-\nsearch Corporation (SRC). We thank all the anonymous\nreviewers for their valuable comments and suggestions.",
        "score": 0.7549,
        "rank": 1,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 10,
          "paragraph_index": 13,
          "section_id": "xia2024:sec10",
          "section_index": 10
        }
      },
      {
        "node_id": "xia2024:sec4:p5",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "under 10% on BlackMamba. We assess accuracy improve-\nments post-fine-tuning and compare the learning capabilities\nof dense and sparse versions of both models.\nFig. 3 shows the testing accuracy of Mixtral and Black-\nMamba on two datasets Hellaswag (HE) and GSM8K (GS).\nWe fine-tune both models using the sparse and dense setups\ndescribed in \u00a7III for 10 epochs, and test the accuracy of\nthe fine-tuned model at each epoch. We make the following\nobservations in Fig. 3. (1) Fine-tuning converges relatively\nquickly. Typically, 10 epochs are enough for fine-tune models\nto stabilize at or close to their peak accuracy. On GS, both\nmodels are close to their peak accuracy at the first epoch.\n(2) The smaller model BlackMamba takes relatively more\nepochs to reach its peak accuracy, as it took BlackMamba 5\nepochs to converge on HE. (3) The larger model Mixtral has\nbetter accuracy compared to BlackMamba on both datasets.\n(4) Both models perform better on the CS dataset HE than\non the GS dataset GS. This is because math is harder for\nsmaller LLMs to learn [24]. The BlackMamba model is\ninadequate for fine-tuning GS. This is likely attributed to\nthe complexity of mathematical tasks and the smaller model\nsize of BlackMamba. Additionally, Mamba is specifically\nengineered for long sequence modeling, potentially resulting\nin unsatisfactory arithmetic reasoning ability [25]. Thus, in\nour characterization study in later sections, we will not show\nthe results for BlackMamba fine-tuned on MATH. (5) The\nperformance of sparse fine-tuning is close to that of dense\nfine-tuning, with the exception of Mixtral on HE. However,\neven for this outlier, sparse fine-tuning achieves similar peak\naccuracy compared to dense; we see a drop of accuracy\nbetween the epoch 4 and 5, and indicates sparse fine-tuning is\nmore vulnerable to over-fitting, especially for easy tasks [26].\nFollowing the above insights, the key take-away of this\nanalysis can be summarized as follows.\nTakeaway 1. Sparse model can be trained as well\nas its dense counterpart.\nTakeaway 2. Fine-tuning generally takes less ten\nepochs to reach peak accuracy.\nB. Analysis of Runtime Performance\nAfter confirming that both Mixtral and BlackMamba can\nbe fine-tuned to achieve acceptable accuracy, we examine\ntheir performance in a resource-constrained environment us-\ning a single GPU. This setup highlights unique runtime char-\nacteristics such as execution time breakdown, throughput,\nmaximum batch size, compute and memory utilization, load\nimbalance, and sensitivity analysis. We also compare sparse\nand dense models. Insights from this study will help develop\na robust analytical model for estimating fine-tuning costs.\n1) Maximum Batch Size Support: The maximum batch\nsize in fine-tuning is determined by GPU memory size,\nmodel size, sequence length, and MoE sparsity. The LLM\nTABLE III\nMAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE\nAND S:SPARSE .\nMixtral-D Mixtral-S BlackMamba-D BlackMamba-S\nCS 2 8 6 20\nMATH 1 3 2 8\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)\n0.0\n2.0\n4.0\n6.0\n8.0\nForward Backward Optimizer\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)\n0.0\n0.5\n1.0\n1.5\n2.0\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig. 4. Execution time breakdown.\noccupies a certain amount of GPU memory, with the re-\nmainder available for intermediate data during fine-tuning.\nLonger sequence lengths consume more memory, and denser\nMoE configurations require additional memory space. We\ndiscuss the heuristic for determining the maximum batch size\nin \u00a7V. Based on our experimental study on NVIDIA A40\nGPU with 48GB memory, we empirically find and report\nthe maximum batch size supported by different model and\ndataset combinations in Table III.\n2) Execution Time Breakdown: We first analyze the high-\nlevel execution time breakdown for Mixtral and Black-\nMamba. The purpose of this study is to understand where\ndoes this workload spend most of its time. As discussed in\n\u00a7III, we conduct this study using a sequence length of 128.\nAt a high-level, the fine-tuning workload can be divided\ninto three stages: (1) forward, (2) backward, and (3) opti-\nmizer. We use a batch size of 1 and the maximum batch size\nsupported by a model-dataset combination to show workload\ncharacteristics. Fig. 4 illustrates the following insights. (1)\nThe optimizer stage in BlackMamba fine-tuning takes a\nconsiderable portion of the running time (up to 53% when\nconducting sparse fine-tuning with batch size = 1), while\nthe execution time share of the optimizer stage in Mixtral\nfine-tuning is negligible. The running time of the optimizer\nstage depends only on the number of parameters that need\nto be updated during fine-tuning. This difference is primarily\ndue to the different fine-tuning strategies applied to these two\nmodels: only the parameters in the LoRA module are updated\nfor Mixtral fine-tuning, whereas BlackMamba undergoes full\nfine-tuning. (2) The runtime of the forward and backward\nstages increases with sparsity and batch size due to the\nincreased amount of computation. (3) The backward stage\ntypically takes more time than the forward stage. In Black-\nMamba, the backward stage demands more computation than",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 4,
          "paragraph_index": 5,
          "section_id": "xia2024:sec4",
          "section_index": 4
        }
      },
      {
        "node_id": "xia2024:sec4",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Page 4",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 4,
          "section_index": 4
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s132",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "under 10% on BlackMamba.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 132
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s133",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We assess accuracy improve-\nments post-fine-tuning and compare the learning capabilities\nof dense and sparse versions of both models.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 133
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s134",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Fig.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 134
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s135",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "3 shows the testing accuracy of Mixtral and Black-\nMamba on two datasets Hellaswag (HE) and GSM8K (GS).",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 135
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s136",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We fine-tune both models using the sparse and dense setups\ndescribed in \u00a7III for 10 epochs, and test the accuracy of\nthe fine-tuned model at each epoch.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 136
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s137",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We make the following\nobservations in Fig.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 137
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s138",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "3.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 138
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s139",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(1) Fine-tuning converges relatively\nquickly.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 139
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s140",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Typically, 10 epochs are enough for fine-tune models\nto stabilize at or close to their peak accuracy.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 140
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s141",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "On GS, both\nmodels are close to their peak accuracy at the first epoch.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 141
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s142",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(2) The smaller model BlackMamba takes relatively more\nepochs to reach its peak accuracy, as it took BlackMamba 5\nepochs to converge on HE.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 142
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s143",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(3) The larger model Mixtral has\nbetter accuracy compared to BlackMamba on both datasets.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 143
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s144",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(4) Both models perform better on the CS dataset HE than\non the GS dataset GS.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 144
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s145",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "This is because math is harder for\nsmaller LLMs to learn [24].",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 145
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s146",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The BlackMamba model is\ninadequate for fine-tuning GS.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 146
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s147",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "This is likely attributed to\nthe complexity of mathematical tasks and the smaller model\nsize of BlackMamba.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 147
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s148",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Additionally, Mamba is specifically\nengineered for long sequence modeling, potentially resulting\nin unsatisfactory arithmetic reasoning ability [25].",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 148
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s149",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Thus, in\nour characterization study in later sections, we will not show\nthe results for BlackMamba fine-tuned on MATH.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 149
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s150",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(5) The\nperformance of sparse fine-tuning is close to that of dense\nfine-tuning, with the exception of Mixtral on HE.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 150
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s151",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "However,\neven for this outlier, sparse fine-tuning achieves similar peak\naccuracy compared to dense;",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 151
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s152",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "we see a drop of accuracy\nbetween the epoch 4 and 5, and indicates sparse fine-tuning is\nmore vulnerable to over-fitting, especially for easy tasks [26].",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 152
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s153",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Following the above insights, the key take-away of this\nanalysis can be summarized as follows.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 153
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s154",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Takeaway 1.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 154
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s155",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Sparse model can be trained as well\nas its dense counterpart.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 155
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s156",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Takeaway 2.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 156
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s157",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Fine-tuning generally takes less ten\nepochs to reach peak accuracy.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 157
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s158",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "B.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 158
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s159",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Analysis of Runtime Performance\nAfter confirming that both Mixtral and BlackMamba can\nbe fine-tuned to achieve acceptable accuracy, we examine\ntheir performance in a resource-constrained environment us-\ning a single GPU.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 159
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s160",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "This setup highlights unique runtime char-\nacteristics such as execution time breakdown, throughput,\nmaximum batch size, compute and memory utilization, load\nimbalance, and sensitivity analysis.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 160
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s161",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We also compare sparse\nand dense models.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 161
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s162",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Insights from this study will help develop\na robust analytical model for estimating fine-tuning costs.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 162
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s163",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "1) Maximum Batch Size Support: The maximum batch\nsize in fine-tuning is determined by GPU memory size,\nmodel size, sequence length, and MoE sparsity.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 163
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s164",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The LLM\nTABLE III\nMAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ;",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 164
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s165",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "D: DENSE\nAND S:SPARSE .",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 165
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s166",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S\nCS 2 8 6 20\nMATH 1 3 2 8\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)\n0.0\n2.0\n4.0\n6.0\n8.0\nForward Backward Optimizer\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)\n0.0\n0.5\n1.0\n1.5\n2.0\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 166
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s167",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "4.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 167
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s168",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Execution time breakdown.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 168
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s169",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "occupies a certain amount of GPU memory, with the re-\nmainder available for intermediate data during fine-tuning.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 169
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s170",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Longer sequence lengths consume more memory, and denser\nMoE configurations require additional memory space.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 170
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s171",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We\ndiscuss the heuristic for determining the maximum batch size\nin \u00a7V.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 171
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s172",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Based on our experimental study on NVIDIA A40\nGPU with 48GB memory, we empirically find and report\nthe maximum batch size supported by different model and\ndataset combinations in Table III.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 172
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s173",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "2) Execution Time Breakdown: We first analyze the high-\nlevel execution time breakdown for Mixtral and Black-\nMamba.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 173
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s174",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The purpose of this study is to understand where\ndoes this workload spend most of its time.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 174
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s175",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "As discussed in\n\u00a7III, we conduct this study using a sequence length of 128.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 175
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s176",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "At a high-level, the fine-tuning workload can be divided\ninto three stages: (1) forward, (2) backward, and (3) opti-\nmizer.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 176
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s177",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We use a batch size of 1 and the maximum batch size\nsupported by a model-dataset combination to show workload\ncharacteristics.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 177
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s178",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Fig.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 178
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s179",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "4 illustrates the following insights.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 179
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s180",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(1)\nThe optimizer stage in BlackMamba fine-tuning takes a\nconsiderable portion of the running time (up to 53% when\nconducting sparse fine-tuning with batch size = 1), while\nthe execution time share of the optimizer stage in Mixtral\nfine-tuning is negligible.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 180
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s181",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The running time of the optimizer\nstage depends only on the number of parameters that need\nto be updated during fine-tuning.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 181
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s182",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "This difference is primarily\ndue to the different fine-tuning strategies applied to these two\nmodels: only the parameters in the LoRA module are updated\nfor Mixtral fine-tuning, whereas BlackMamba undergoes full\nfine-tuning.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 182
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s183",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(2) The runtime of the forward and backward\nstages increases with sparsity and batch size due to the\nincreased amount of computation.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 183
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s184",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(3) The backward stage\ntypically takes more time than the forward stage.",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 184
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s185",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "In Black-\nMamba, the backward stage demands more computation than",
        "score": 0.7503,
        "rank": 2,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 185
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s163",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "1) Maximum Batch Size Support: The maximum batch\nsize in fine-tuning is determined by GPU memory size,\nmodel size, sequence length, and MoE sparsity.",
        "score": 0.7488,
        "rank": 3,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 163
        }
      },
      {
        "node_id": "xia2024:sec4:p5",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "under 10% on BlackMamba. We assess accuracy improve-\nments post-fine-tuning and compare the learning capabilities\nof dense and sparse versions of both models.\nFig. 3 shows the testing accuracy of Mixtral and Black-\nMamba on two datasets Hellaswag (HE) and GSM8K (GS).\nWe fine-tune both models using the sparse and dense setups\ndescribed in \u00a7III for 10 epochs, and test the accuracy of\nthe fine-tuned model at each epoch. We make the following\nobservations in Fig. 3. (1) Fine-tuning converges relatively\nquickly. Typically, 10 epochs are enough for fine-tune models\nto stabilize at or close to their peak accuracy. On GS, both\nmodels are close to their peak accuracy at the first epoch.\n(2) The smaller model BlackMamba takes relatively more\nepochs to reach its peak accuracy, as it took BlackMamba 5\nepochs to converge on HE. (3) The larger model Mixtral has\nbetter accuracy compared to BlackMamba on both datasets.\n(4) Both models perform better on the CS dataset HE than\non the GS dataset GS. This is because math is harder for\nsmaller LLMs to learn [24]. The BlackMamba model is\ninadequate for fine-tuning GS. This is likely attributed to\nthe complexity of mathematical tasks and the smaller model\nsize of BlackMamba. Additionally, Mamba is specifically\nengineered for long sequence modeling, potentially resulting\nin unsatisfactory arithmetic reasoning ability [25]. Thus, in\nour characterization study in later sections, we will not show\nthe results for BlackMamba fine-tuned on MATH. (5) The\nperformance of sparse fine-tuning is close to that of dense\nfine-tuning, with the exception of Mixtral on HE. However,\neven for this outlier, sparse fine-tuning achieves similar peak\naccuracy compared to dense; we see a drop of accuracy\nbetween the epoch 4 and 5, and indicates sparse fine-tuning is\nmore vulnerable to over-fitting, especially for easy tasks [26].\nFollowing the above insights, the key take-away of this\nanalysis can be summarized as follows.\nTakeaway 1. Sparse model can be trained as well\nas its dense counterpart.\nTakeaway 2. Fine-tuning generally takes less ten\nepochs to reach peak accuracy.\nB. Analysis of Runtime Performance\nAfter confirming that both Mixtral and BlackMamba can\nbe fine-tuned to achieve acceptable accuracy, we examine\ntheir performance in a resource-constrained environment us-\ning a single GPU. This setup highlights unique runtime char-\nacteristics such as execution time breakdown, throughput,\nmaximum batch size, compute and memory utilization, load\nimbalance, and sensitivity analysis. We also compare sparse\nand dense models. Insights from this study will help develop\na robust analytical model for estimating fine-tuning costs.\n1) Maximum Batch Size Support: The maximum batch\nsize in fine-tuning is determined by GPU memory size,\nmodel size, sequence length, and MoE sparsity. The LLM\nTABLE III\nMAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE\nAND S:SPARSE .\nMixtral-D Mixtral-S BlackMamba-D BlackMamba-S\nCS 2 8 6 20\nMATH 1 3 2 8\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)\n0.0\n2.0\n4.0\n6.0\n8.0\nForward Backward Optimizer\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)\n0.0\n0.5\n1.0\n1.5\n2.0\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig. 4. Execution time breakdown.\noccupies a certain amount of GPU memory, with the re-\nmainder available for intermediate data during fine-tuning.\nLonger sequence lengths consume more memory, and denser\nMoE configurations require additional memory space. We\ndiscuss the heuristic for determining the maximum batch size\nin \u00a7V. Based on our experimental study on NVIDIA A40\nGPU with 48GB memory, we empirically find and report\nthe maximum batch size supported by different model and\ndataset combinations in Table III.\n2) Execution Time Breakdown: We first analyze the high-\nlevel execution time breakdown for Mixtral and Black-\nMamba. The purpose of this study is to understand where\ndoes this workload spend most of its time. As discussed in\n\u00a7III, we conduct this study using a sequence length of 128.\nAt a high-level, the fine-tuning workload can be divided\ninto three stages: (1) forward, (2) backward, and (3) opti-\nmizer. We use a batch size of 1 and the maximum batch size\nsupported by a model-dataset combination to show workload\ncharacteristics. Fig. 4 illustrates the following insights. (1)\nThe optimizer stage in BlackMamba fine-tuning takes a\nconsiderable portion of the running time (up to 53% when\nconducting sparse fine-tuning with batch size = 1), while\nthe execution time share of the optimizer stage in Mixtral\nfine-tuning is negligible. The running time of the optimizer\nstage depends only on the number of parameters that need\nto be updated during fine-tuning. This difference is primarily\ndue to the different fine-tuning strategies applied to these two\nmodels: only the parameters in the LoRA module are updated\nfor Mixtral fine-tuning, whereas BlackMamba undergoes full\nfine-tuning. (2) The runtime of the forward and backward\nstages increases with sparsity and batch size due to the\nincreased amount of computation. (3) The backward stage\ntypically takes more time than the forward stage. In Black-\nMamba, the backward stage demands more computation than",
        "score": 0.7488,
        "rank": 3,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 4,
          "paragraph_index": 5,
          "section_id": "xia2024:sec4",
          "section_index": 4
        }
      },
      {
        "node_id": "xia2024:sec9:p12",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "0 20 40 60 80 100 1200\n5\n10\n15\n20\n25\n30\n35\n40\nA100-40GB\nA100-80GB\nA40\nH100\nbsz=28\nbsz=35\nProjected GPU capacity\nGround Truth Projection\nMax batch size\nGPU DRAM capacity\nFig. 13. Projected maximum batch size of Mixtral for different GPUs.\namount of GPU memory, and need to be subtracted in the\nanalytical model. Fig. 8 supports this by showing that on the\nsame dataset, BlackMamba can support larger batch size than\nMixtral because of its smaller model size.\nMoreover, the sequence length and sparsity also affect the\nmaximum batch size. Because the sparsity only affects the\nMoE part of the LLM, we multiply its influence by C1,\nwhich we call MoE coefficient. We apply the sequence length\nand the sparsity in the denominator as they are inversely\nrelated to batch size. Then, we multiply the result by C0,\nthe scaling coefficient , which scales the batch size by a\nconstant. The scaling coefficient is different across LLM\nmodels, because different models have different architecture\n(\u00a7III), and generate different amounts of intermediate data\nfor each query. The scaling coefficient for BlackMamba is\nhigher than that of Mixtral because it is a smaller model.\nFinally, we use floor to round it to the maximum integer.\nThe MoE coefficient and scaling coefficient vary across\nmodels. These coefficients are independent of GPU microar-\nchitectural parameters. We find the maximum batch size for\nboth LLM models on NVIDIA A40 (48GB), A100 (40GB),\nA100 (80GB), and H100 (80GB), and apply our model to\nfind the optimal coefficients. For Mixtral, C0 = 82 and\nC1 = 0 .95, and for BlackMamba, C0 = 83 and C1 = 0 .88.\nWhile we showcase these parameters for the models eval-\nuated, \u00a7V-D discusses how to generalize this approach for\nother models.\nUsing our analytical model, we demonstrate the maximum\nbatch sizes for fine-tuning on four different NVIDIA GPUs:\nA40, A100-40GB, A100-80GB and H100 with memory\ncapacities of 48GB, 40GB, 80GB, and 80GB, respectively.\nFig. 13 shows our projected maximum batch size and corre-\nlate it with experimented ground truth. While the maximum\nmemory capacity available in NVIDIA GPUs today is 80GB,\nwe use our analytical model to project the maximum batch\nsize that future GPUs might support. For GPU memory\ncapacities of 100GB and 120GB, our model predicts that\nthe maximum batch sizes supported for fine-tuning Mixtral\nwill be 28 and 35, respectively. Due to space limitations, we\nonly show the projection of Mixtral model.\nTABLE IV\nESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE\nBASED ON OUR ANALYTICAL MODEL\nGPU Mem MBS Throughput Cost ($/hr) Cost ($)\nA40 48GB 4 1.01 0.79 32.7\nA100 80GB 17 2.74 1.67 25.4\nH100 80GB 17 4.90 2.1 17.9\nB. Estimating Throughput\nAs discussed in \u00a7IV-B4, when the batch size increases, the\nLLM fine-tuning gradually switches from memory bound to\ncompute bound. When the compute resources are abundant,\nthe throughput increases almost linearly with batch size.\nHowever, when compute resources become constrained, the\nthroughput improvement gradually saturates. We model this\nbehavior using a logarithmic relation between batch size and\nthroughput. Our analytical model for maximum batch size is\nshown in (2).\nT hroughput = C2 \u2217 log( batch size\nsparsity \u2217 C3 ) + C4 (2)\nIn the equation, in addition to the basic logarithmic part, we\nhave three coefficients C2, C3, and C4. C2 is the scaling\ncoefficient, which depends on the LLM model, GPU archi-\ntecture, and the dataset. The higher the compute capability a\nGPU can provide, and the lower the LLM model and dataset\ncompute requirement is, the higher the scaling coefficient will\nbe. C3 is the MoE attenuation coefficient , which tunes how\nmuch the MoE sparsity affects the throughput. MoE sparsity\nonly affects the MoE part in LLM model, and thus should\nbe attenuated to avoid over compensation. This coefficient is\nonly LLM model dependent, because once the model is fixed,\nthe influence of sparsity is determined. C4 is the intercept,\nconceptually it equals to the throughput when batch size\nequals one, because the logarithmic part in (2) is zero when\nbatch size is one. Using scipy [32] to fit the model and\ngenerate four sets (C2, C3, C4), for each model and dataset\ncombination.\nTo estimate the accuracy of this model, we correlate the\nmodel output with experimental data from our study. Fig. 14\nshows this correlation study, where discrete data points (dots)\nrepresent experimental values, and the line represents output\nof our analytical model. We use both dense and sparse\nMixtral and BlackMamba for both datasets used in our study.\nThe figure clearly shows that our model accurately predicts\nLLM fine-tuning throughput with a Root Mean Squared Error\n(RMSE) of less than 0.8. Fig. 15 shows the correlation study\nof the analytical model of three other GPUs, A100 (40GB),\nA100 (80GB), and H100. The RMSE is less than 0.6, close\nto that of A40.\nC. Estimating the Total Fine-Tuning Cost\nUsing the throughput estimation, we calculate the cost\nof fine-tuning LLMs for different GPUs. The cost of GPU",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 9,
          "paragraph_index": 12,
          "section_id": "xia2024:sec9",
          "section_index": 9
        }
      },
      {
        "node_id": "xia2024:sec9",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Page 9",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 9,
          "section_index": 9
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s341",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "0 20 40 60 80 100 1200\n5\n10\n15\n20\n25\n30\n35\n40\nA100-40GB\nA100-80GB\nA40\nH100\nbsz=28\nbsz=35\nProjected GPU capacity\nGround Truth Projection\nMax batch size\nGPU DRAM capacity\nFig.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 341
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s342",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "13.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 342
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s343",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Projected maximum batch size of Mixtral for different GPUs.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 343
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s344",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "amount of GPU memory, and need to be subtracted in the\nanalytical model.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 344
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s345",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Fig.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 345
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s346",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "8 supports this by showing that on the\nsame dataset, BlackMamba can support larger batch size than\nMixtral because of its smaller model size.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 346
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s347",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Moreover, the sequence length and sparsity also affect the\nmaximum batch size.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 347
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s348",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Because the sparsity only affects the\nMoE part of the LLM, we multiply its influence by C1,\nwhich we call MoE coefficient.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 348
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s349",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We apply the sequence length\nand the sparsity in the denominator as they are inversely\nrelated to batch size.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 349
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s350",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Then, we multiply the result by C0,\nthe scaling coefficient , which scales the batch size by a\nconstant.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 350
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s351",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The scaling coefficient is different across LLM\nmodels, because different models have different architecture\n(\u00a7III), and generate different amounts of intermediate data\nfor each query.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 351
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s352",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The scaling coefficient for BlackMamba is\nhigher than that of Mixtral because it is a smaller model.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 352
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s353",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Finally, we use floor to round it to the maximum integer.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 353
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s354",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The MoE coefficient and scaling coefficient vary across\nmodels.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 354
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s355",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "These coefficients are independent of GPU microar-\nchitectural parameters.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 355
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s356",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We find the maximum batch size for\nboth LLM models on NVIDIA A40 (48GB), A100 (40GB),\nA100 (80GB), and H100 (80GB), and apply our model to\nfind the optimal coefficients.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 356
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s357",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "For Mixtral, C0 = 82 and\nC1 = 0 .95, and for BlackMamba, C0 = 83 and C1 = 0 .88.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 357
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s358",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "While we showcase these parameters for the models eval-\nuated, \u00a7V-D discusses how to generalize this approach for\nother models.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 358
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s359",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Using our analytical model, we demonstrate the maximum\nbatch sizes for fine-tuning on four different NVIDIA GPUs:\nA40, A100-40GB, A100-80GB and H100 with memory\ncapacities of 48GB, 40GB, 80GB, and 80GB, respectively.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 359
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s360",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Fig.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 360
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s361",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "13 shows our projected maximum batch size and corre-\nlate it with experimented ground truth.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 361
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s362",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "While the maximum\nmemory capacity available in NVIDIA GPUs today is 80GB,\nwe use our analytical model to project the maximum batch\nsize that future GPUs might support.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 362
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s363",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "For GPU memory\ncapacities of 100GB and 120GB, our model predicts that\nthe maximum batch sizes supported for fine-tuning Mixtral\nwill be 28 and 35, respectively.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 363
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s364",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Due to space limitations, we\nonly show the projection of Mixtral model.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 364
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s365",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "TABLE IV\nESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE\nBASED ON OUR ANALYTICAL MODEL\nGPU Mem MBS Throughput Cost ($/hr) Cost ($)\nA40 48GB 4 1.01 0.79 32.7\nA100 80GB 17 2.74 1.67 25.4\nH100 80GB 17 4.90 2.1 17.9\nB.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 365
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s366",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Estimating Throughput\nAs discussed in \u00a7IV-B4, when the batch size increases, the\nLLM fine-tuning gradually switches from memory bound to\ncompute bound.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 366
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s367",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "When the compute resources are abundant,\nthe throughput increases almost linearly with batch size.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 367
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s368",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "However, when compute resources become constrained, the\nthroughput improvement gradually saturates.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 368
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s369",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We model this\nbehavior using a logarithmic relation between batch size and\nthroughput.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 369
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s370",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Our analytical model for maximum batch size is\nshown in (2).",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 370
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s371",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "T hroughput = C2 \u2217 log( batch size\nsparsity \u2217 C3 ) + C4 (2)\nIn the equation, in addition to the basic logarithmic part, we\nhave three coefficients C2, C3, and C4.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 371
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s372",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "C2 is the scaling\ncoefficient, which depends on the LLM model, GPU archi-\ntecture, and the dataset.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 372
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s373",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The higher the compute capability a\nGPU can provide, and the lower the LLM model and dataset\ncompute requirement is, the higher the scaling coefficient will\nbe.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 373
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s374",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "C3 is the MoE attenuation coefficient , which tunes how\nmuch the MoE sparsity affects the throughput.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 374
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s375",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "MoE sparsity\nonly affects the MoE part in LLM model, and thus should\nbe attenuated to avoid over compensation.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 375
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s376",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "This coefficient is\nonly LLM model dependent, because once the model is fixed,\nthe influence of sparsity is determined.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 376
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s377",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "C4 is the intercept,\nconceptually it equals to the throughput when batch size\nequals one, because the logarithmic part in (2) is zero when\nbatch size is one.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 377
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s378",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Using scipy [32] to fit the model and\ngenerate four sets (C2, C3, C4), for each model and dataset\ncombination.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 378
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s379",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "To estimate the accuracy of this model, we correlate the\nmodel output with experimental data from our study.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 379
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s380",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Fig.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 380
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s381",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "14\nshows this correlation study, where discrete data points (dots)\nrepresent experimental values, and the line represents output\nof our analytical model.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 381
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s382",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We use both dense and sparse\nMixtral and BlackMamba for both datasets used in our study.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 382
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s383",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The figure clearly shows that our model accurately predicts\nLLM fine-tuning throughput with a Root Mean Squared Error\n(RMSE) of less than 0.8.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 383
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s384",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Fig.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 384
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s385",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "15 shows the correlation study\nof the analytical model of three other GPUs, A100 (40GB),\nA100 (80GB), and H100.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 385
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s386",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The RMSE is less than 0.6, close\nto that of A40.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 386
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s387",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "C.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 387
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s388",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Estimating the Total Fine-Tuning Cost\nUsing the throughput estimation, we calculate the cost\nof fine-tuning LLMs for different GPUs.",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 388
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s389",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The cost of GPU",
        "score": 0.7353,
        "rank": 4,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 389
        }
      },
      {
        "node_id": "xia2024:sec5:p6",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0.0\n1.0\n2.0\n3.0\n4.0\n5.0\nInput normalization Attention Post attention norm. MoE\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0.0\n0.5\n1.0\n1.5\nRMS layernorm Mamba MoE\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig. 5. Execution time breakdown in terms of different model layers.\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0\n2000\n4000\n6000\nmatmul(w2)\nw2_dequant\nmatmul(w3)\nw3_dequant\nmatmul(w1)\nw1_dequant\nsoftmax\ntopk\nmatmul(router)\nrouter_dequant\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0\n400\n800\n1200\n1600\n2000\nmatmul(w1)\ngelu\nmatmul(w2)\nelementwise_mult\ntop_k\nsigmoid\nmatmul(router)\n    Execution Time Breakdown (\u03bcs)\nMixtral\nMamba\nFig. 6. Execution breakdown of the MoE layer for different kernels.\nthe forward stage due to the need for gradient calculation\nand propagation, resulting in two matrix multiplication op-\nerations. In Mixtral fine-tuning, gradient calculation adds\nminimal computation as only a small portion of parameters\nneed it. However, gradient checkpointing in Mixtral saves\nmemory but increases the backward stage runtime due to the\nre-computation of intermediate values.\nWe further investigate the execution breakdown based\non various layers in two LLM models. For Mixtral, these\nlayers include input normalization, attention, post-attention\nnormalization, and MoE. In contrast, BlackMamba comprises\nthe Mamba layer, Root Mean Squared (RMS) layer nor-\nmalization, and MoE. As shown in Fig. 5, the MoE layer\nis the most time-consuming, accounting for 85% of the\noverall execution time on average. The execution time for\nthe MoE layer encompasses both the forward and backward\npasses during fine-tuning. Consequently, MoE is the costliest\nlayer and a prime target for optimization to enhance the\nperformance of LLM fine-tuning.\nTo concretely understand the opportunity for improving\nMoE layer performance, we also perform a kernel-level anal-\nysis within the MoE layer. Fig. 7 illustrates the architecture\nof the MoE layer in both Mixtral and BlackMamba models.\nEach expert in BlackMamba consists of a standard Feed-\nForward Network (FFN) layer with two serially connected\nweight matrices (W1 and W2) and a Gelu activation layer\nbetween. In contrast, experts in Mixtral are FFN layers with\nSwish-Gated Linear Units, involving an additional weight\nFig. 7. Expert architectures for Mixtral (top) and BlackMamba (bottom).\nmatrix (W3) in parallel with W1.\nFig. 6 shows the kernel-level MoE time breakdown. The\nfigure clearly shows that matrix multiplication (W1, W2,\nand W3) is the largest component of the MoE layer for\nboth BlackMamba and Mixtral. As batch size and sparsity\nincrease, so does computational demand, prolonging matrix\nmultiplication latency. The de-quantization operation in Mix-\ntral fine-tuning also becomes significant, especially with low\nsparsity and small batch sizes. While quantization reduces\nmodel size and memory footprint, it can increase computation\ntime due to de-quantization. This highlights the need to\nevaluate trade-offs between memory savings and computation\ntime, particularly in scenarios with small batch sizes and\nsequence lengths.\nTakeaway 3. Matrix multiplication operations in the\nMoE layer contribute significantly to the end-to-end\nexecution time, making the MoE layer the costliest\ncomponent in LLM fine-tuning.\n3) Fine-Tuning Throughput: Next, we present the fine-\ntuning throughput of Mixtral and BlackMamba on the MATH\nand CS datasets separately in Fig. 8. We use a throughput\nmetric of queries/second processed, where a query includes a\nprompt and a ground-truth answer for fine-tuning. To obtain\nthese results, we extract 1000 examples from each dataset\nand fine-tuned Mixtral and BlackMamba on them using the\nsmallest batch size (batch size = 1) and the largest batch size\nthat would fill the GPU memory.\nAs illustrated in Fig. 8, sparse fine-tuning achieves higher\nthroughput than dense fine-tuning. This is because the sparse\nfine-tuning baseline consumes less memory to store interme-\ndiate values, which allows for higher batch sizes compared\nto its dense counterpart. Additionally, with the same batch\nsize, sparse fine-tuning achieves higher throughput because",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 5,
          "paragraph_index": 6,
          "section_id": "xia2024:sec5",
          "section_index": 5
        }
      },
      {
        "node_id": "xia2024:sec5",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Page 5",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 5,
          "section_index": 5
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s186",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0.0\n1.0\n2.0\n3.0\n4.0\n5.0\nInput normalization Attention Post attention norm.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 186
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s187",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "MoE\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0.0\n0.5\n1.0\n1.5\nRMS layernorm Mamba MoE\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 187
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s188",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "5.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 188
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s189",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Execution time breakdown in terms of different model layers.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 189
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s190",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0\n2000\n4000\n6000\nmatmul(w2)\nw2_dequant\nmatmul(w3)\nw3_dequant\nmatmul(w1)\nw1_dequant\nsoftmax\ntopk\nmatmul(router)\nrouter_dequant\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0\n400\n800\n1200\n1600\n2000\nmatmul(w1)\ngelu\nmatmul(w2)\nelementwise_mult\ntop_k\nsigmoid\nmatmul(router)\n    Execution Time Breakdown (\u03bcs)\nMixtral\nMamba\nFig.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 190
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s191",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "6.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 191
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s192",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Execution breakdown of the MoE layer for different kernels.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 192
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s193",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "the forward stage due to the need for gradient calculation\nand propagation, resulting in two matrix multiplication op-\nerations.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 193
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s194",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "In Mixtral fine-tuning, gradient calculation adds\nminimal computation as only a small portion of parameters\nneed it.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 194
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s195",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "However, gradient checkpointing in Mixtral saves\nmemory but increases the backward stage runtime due to the\nre-computation of intermediate values.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 195
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s196",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We further investigate the execution breakdown based\non various layers in two LLM models.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 196
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s197",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "For Mixtral, these\nlayers include input normalization, attention, post-attention\nnormalization, and MoE.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 197
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s198",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "In contrast, BlackMamba comprises\nthe Mamba layer, Root Mean Squared (RMS) layer nor-\nmalization, and MoE.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 198
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s199",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "As shown in Fig.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 199
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s200",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "5, the MoE layer\nis the most time-consuming, accounting for 85% of the\noverall execution time on average.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 200
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s201",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The execution time for\nthe MoE layer encompasses both the forward and backward\npasses during fine-tuning.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 201
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s202",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Consequently, MoE is the costliest\nlayer and a prime target for optimization to enhance the\nperformance of LLM fine-tuning.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 202
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s203",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "To concretely understand the opportunity for improving\nMoE layer performance, we also perform a kernel-level anal-\nysis within the MoE layer.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 203
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s204",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Fig.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 204
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s205",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "7 illustrates the architecture\nof the MoE layer in both Mixtral and BlackMamba models.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 205
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s206",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Each expert in BlackMamba consists of a standard Feed-\nForward Network (FFN) layer with two serially connected\nweight matrices (W1 and W2) and a Gelu activation layer\nbetween.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 206
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s207",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "In contrast, experts in Mixtral are FFN layers with\nSwish-Gated Linear Units, involving an additional weight\nFig.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 207
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s208",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "7.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 208
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s209",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Expert architectures for Mixtral (top) and BlackMamba (bottom).",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 209
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s210",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "matrix (W3) in parallel with W1.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 210
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s211",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Fig.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 211
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s212",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "6 shows the kernel-level MoE time breakdown.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 212
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s213",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The\nfigure clearly shows that matrix multiplication (W1, W2,\nand W3) is the largest component of the MoE layer for\nboth BlackMamba and Mixtral.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 213
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s214",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "As batch size and sparsity\nincrease, so does computational demand, prolonging matrix\nmultiplication latency.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 214
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s215",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The de-quantization operation in Mix-\ntral fine-tuning also becomes significant, especially with low\nsparsity and small batch sizes.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 215
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s216",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "While quantization reduces\nmodel size and memory footprint, it can increase computation\ntime due to de-quantization.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 216
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s217",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "This highlights the need to\nevaluate trade-offs between memory savings and computation\ntime, particularly in scenarios with small batch sizes and\nsequence lengths.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 217
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s218",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Takeaway 3.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 218
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s219",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Matrix multiplication operations in the\nMoE layer contribute significantly to the end-to-end\nexecution time, making the MoE layer the costliest\ncomponent in LLM fine-tuning.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 219
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s220",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "3) Fine-Tuning Throughput: Next, we present the fine-\ntuning throughput of Mixtral and BlackMamba on the MATH\nand CS datasets separately in Fig.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 220
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s221",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "8.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 221
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s222",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We use a throughput\nmetric of queries/second processed, where a query includes a\nprompt and a ground-truth answer for fine-tuning.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 222
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s223",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "To obtain\nthese results, we extract 1000 examples from each dataset\nand fine-tuned Mixtral and BlackMamba on them using the\nsmallest batch size (batch size = 1) and the largest batch size\nthat would fill the GPU memory.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 223
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s224",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "As illustrated in Fig.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 224
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s225",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "8, sparse fine-tuning achieves higher\nthroughput than dense fine-tuning.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 225
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s226",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "This is because the sparse\nfine-tuning baseline consumes less memory to store interme-\ndiate values, which allows for higher batch sizes compared\nto its dense counterpart.",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 226
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s227",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Additionally, with the same batch\nsize, sparse fine-tuning achieves higher throughput because",
        "score": 0.7319,
        "rank": 5,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 227
        }
      },
      {
        "node_id": "xia2024:sec1:p1",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Understanding the Performance and Estimating the\nCost of LLM Fine-Tuning\nYuchen Xia1 Jiho Kim 2 Yuhan Chen1 Haojie Ye1 Souvik Kundu 3\nCong (Callie) Hao 2 Nishil Talati1\n1University of Michigan 2Georgia Institute of Technology 3Intel Labs\nAbstract\u2014Due to the cost-prohibitive nature of training Large\nLanguage Models (LLMs), fine-tuning has emerged as an attrac-\ntive alternative for specializing LLMs for specific tasks using\nlimited compute resources in a cost-effective manner. In this\npaper, we characterize sparse Mixture of Experts (MoE) based\nLLM fine-tuning to understand their accuracy and runtime\nperformance on a single GPU. Our evaluation provides unique\ninsights into the training efficacy of sparse and dense versions of\nMoE models, as well as their runtime characteristics, including\nmaximum batch size, execution time breakdown, end-to-end\nthroughput, GPU hardware utilization, and load distribution.\nOur study identifies the optimization of the MoE layer as crucial\nfor further improving the performance of LLM fine-tuning.\nUsing our profiling results, we also develop and validate an\nanalytical model to estimate the cost of LLM fine-tuning on\nthe cloud. This model, based on parameters of the model and\nGPU architecture, estimates LLM throughput and the cost\nof training, aiding practitioners in industry and academia to\nbudget the cost of fine-tuning a specific model.\nI. I NTRODUCTION\nLarge Language Models (LLMs) are widely utilized in\nNatural Language Processing (NLP) [1]. Modern LLMs\ntypically possess billions to trillions of parameters, neces-\nsitating extensive time and resources for training. For in-\nstance, the estimated cost of training OpenAI\u2019s GPT-4 model\nexceeds $100 million, rendering it financially prohibitive\nfor most small-to-medium size enterprises and the academic\ncommunity [2]. Given the open-sourcing of numerous pre-\ntrained LLMs (e.g., LLAMA [3] and Mixtral [4]), fine-\ntuning has emerged as an attractive alternative for further\nspecializing these models in a cost-effective manner [5].\nGiven the learning ability of pre-trained models, it is feasible\nto use a domain-specific dataset to align the desired behav-\niors of LLMs through supervised fine-tuning on instruction-\nfollowing tasks [6]. Unlike pre-training, fine-tuning can be\nconducted in a resource-constrained environment, typically\nusing one or a few GPUs. Consequently, fine-tuning presents\na compelling case for applications such as specialized ques-\ntion answering within enterprises, legal document analysis\nand drafting, healthcare/medical research, technical and IT\nsupport, among others [7].\nThis paper characterizes LLM fine-tuning with two pri-\nmary objectives: (1) understanding the performance charac-\nteristics of LLM fine-tuning, and (2) developing an analytical\nmodel to estimate the cost of fine-tuning on the cloud. Given\nour focus on cost-efficient LLM fine-tuning, we concen-\ntrate on fine-tuning sparse Mixture-of-Expert (MoE) models.\nSpecifically, we employ an attention-based MoE model, Mix-\ntral [4], and a state-space MoE model, BlackMamba [8]. Us-\ning these models and two domain-specific datasets for math-\nematics and common-sense question-answering, we conduct\nan in-depth profiling study to understand their performance\ncharacteristics with a single GPU. We compare the dense\nand sparse counterparts of the investigated MoE models to\nevaluate their learning rates and runtime performance. Our\ninvestigation covers memory consumption, maximum batch\nsize supported within a single GPU memory budget, exe-\ncution time breakdown and bottlenecks, overall throughput,\nmicroarchitectural performance counters, and runtime load\ndistribution. The insights gained from our study are used to\ndevelop and validate an analytical model to estimate the cost.\nOur characterization uncovers the following unique in-\nsights. (1) Fine-tuning can be achieved in less than 10 epochs,\nand sparse MoE model that activates a subset of experts\ncan learn as well as its dense counterparts. (2) MoE layer\nconsumes the highest fraction of execution time in LLM\nfine-tuning; optimizing MoE layer performance is key to\nimproving the overall cost of LLM fine-tuning. (3) Sparse\nMoE model improves end-to-end throughput by supporting\na larger batch size. Given similar learning abilities of sparse\nand dense models, it is desired to use a sparse MoE model\nfor cost-effective fine-tuning. (4) The workload becomes\ncompute bound by increasing batch size; improving compute\nresources will increase performance. (5) Fine-tuning sparse\nmodel leads to more load imbalance.\nBased on these insights, we create an analytical model\nto estimate the cost of LLM fine-tuning based on model\nsize, dataset size, and GPU architecture. First, we estimate\nthe maximum batch size for a given GPU memory, then\ncompute fine-tuning throughput. We validate this throughput\nwith experimental results, showing an RMSE of less than\n0.55. Using the estimated throughput, our model calculates\nthe fine-tuning cost for different cloud providers.\nThe contributions of this paper are as follows.\n\u2022 Make a case for LLM fine-tuning for specializing pre-\ntrained models in a cost-effective manner.\narXiv:2408.04693v1  [cs.CL]  8 Aug 2024",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 1,
          "paragraph_index": 1,
          "section_id": "xia2024:sec1",
          "section_index": 1
        }
      },
      {
        "node_id": "xia2024:sec1",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Page 1",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 1,
          "section_index": 1
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s1",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Understanding the Performance and Estimating the\nCost of LLM Fine-Tuning\nYuchen Xia1 Jiho Kim 2 Yuhan Chen1 Haojie Ye1 Souvik Kundu 3\nCong (Callie) Hao 2 Nishil Talati1\n1University of Michigan 2Georgia Institute of Technology 3Intel Labs\nAbstract\u2014Due to the cost-prohibitive nature of training Large\nLanguage Models (LLMs), fine-tuning has emerged as an attrac-\ntive alternative for specializing LLMs for specific tasks using\nlimited compute resources in a cost-effective manner.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 1
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s2",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "In this\npaper, we characterize sparse Mixture of Experts (MoE) based\nLLM fine-tuning to understand their accuracy and runtime\nperformance on a single GPU.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 2
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s3",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Our evaluation provides unique\ninsights into the training efficacy of sparse and dense versions of\nMoE models, as well as their runtime characteristics, including\nmaximum batch size, execution time breakdown, end-to-end\nthroughput, GPU hardware utilization, and load distribution.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 3
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s4",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Our study identifies the optimization of the MoE layer as crucial\nfor further improving the performance of LLM fine-tuning.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 4
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s5",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Using our profiling results, we also develop and validate an\nanalytical model to estimate the cost of LLM fine-tuning on\nthe cloud.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 5
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s6",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "This model, based on parameters of the model and\nGPU architecture, estimates LLM throughput and the cost\nof training, aiding practitioners in industry and academia to\nbudget the cost of fine-tuning a specific model.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 6
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s7",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "I.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 7
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s8",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "I NTRODUCTION\nLarge Language Models (LLMs) are widely utilized in\nNatural Language Processing (NLP) [1].",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 8
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s9",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Modern LLMs\ntypically possess billions to trillions of parameters, neces-\nsitating extensive time and resources for training.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 9
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s10",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "For in-\nstance, the estimated cost of training OpenAI\u2019s GPT-4 model\nexceeds $100 million, rendering it financially prohibitive\nfor most small-to-medium size enterprises and the academic\ncommunity [2].",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 10
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s11",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Given the open-sourcing of numerous pre-\ntrained LLMs (e.g., LLAMA [3] and Mixtral [4]), fine-\ntuning has emerged as an attractive alternative for further\nspecializing these models in a cost-effective manner [5].",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 11
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s12",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Given the learning ability of pre-trained models, it is feasible\nto use a domain-specific dataset to align the desired behav-\niors of LLMs through supervised fine-tuning on instruction-\nfollowing tasks [6].",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 12
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s13",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Unlike pre-training, fine-tuning can be\nconducted in a resource-constrained environment, typically\nusing one or a few GPUs.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 13
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s14",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Consequently, fine-tuning presents\na compelling case for applications such as specialized ques-\ntion answering within enterprises, legal document analysis\nand drafting, healthcare/medical research, technical and IT\nsupport, among others [7].",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 14
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s15",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "This paper characterizes LLM fine-tuning with two pri-\nmary objectives: (1) understanding the performance charac-\nteristics of LLM fine-tuning, and (2) developing an analytical\nmodel to estimate the cost of fine-tuning on the cloud.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 15
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s16",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Given\nour focus on cost-efficient LLM fine-tuning, we concen-\ntrate on fine-tuning sparse Mixture-of-Expert (MoE) models.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 16
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s17",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Specifically, we employ an attention-based MoE model, Mix-\ntral [4], and a state-space MoE model, BlackMamba [8].",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 17
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s18",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Us-\ning these models and two domain-specific datasets for math-\nematics and common-sense question-answering, we conduct\nan in-depth profiling study to understand their performance\ncharacteristics with a single GPU.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 18
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s19",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We compare the dense\nand sparse counterparts of the investigated MoE models to\nevaluate their learning rates and runtime performance.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 19
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s20",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Our\ninvestigation covers memory consumption, maximum batch\nsize supported within a single GPU memory budget, exe-\ncution time breakdown and bottlenecks, overall throughput,\nmicroarchitectural performance counters, and runtime load\ndistribution.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 20
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s21",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The insights gained from our study are used to\ndevelop and validate an analytical model to estimate the cost.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 21
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s22",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Our characterization uncovers the following unique in-\nsights.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 22
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s23",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(1) Fine-tuning can be achieved in less than 10 epochs,\nand sparse MoE model that activates a subset of experts\ncan learn as well as its dense counterparts.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 23
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s24",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(2) MoE layer\nconsumes the highest fraction of execution time in LLM\nfine-tuning;",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 24
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s25",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "optimizing MoE layer performance is key to\nimproving the overall cost of LLM fine-tuning.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 25
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s26",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(3) Sparse\nMoE model improves end-to-end throughput by supporting\na larger batch size.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 26
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s27",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Given similar learning abilities of sparse\nand dense models, it is desired to use a sparse MoE model\nfor cost-effective fine-tuning.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 27
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s28",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(4) The workload becomes\ncompute bound by increasing batch size;",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 28
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s29",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "improving compute\nresources will increase performance.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 29
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s30",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(5) Fine-tuning sparse\nmodel leads to more load imbalance.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 30
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s31",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Based on these insights, we create an analytical model\nto estimate the cost of LLM fine-tuning based on model\nsize, dataset size, and GPU architecture.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 31
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s32",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "First, we estimate\nthe maximum batch size for a given GPU memory, then\ncompute fine-tuning throughput.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 32
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s33",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We validate this throughput\nwith experimental results, showing an RMSE of less than\n0.55.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 33
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s34",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Using the estimated throughput, our model calculates\nthe fine-tuning cost for different cloud providers.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 34
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s35",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The contributions of this paper are as follows.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 35
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s36",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "\u2022 Make a case for LLM fine-tuning for specializing pre-\ntrained models in a cost-effective manner.",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 36
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s37",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "arXiv:2408.04693v1  [cs.CL]  8 Aug 2024",
        "score": 0.7312,
        "rank": 6,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 37
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s399",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Table IV estimates the cost for fine-tuning\nMixtral on the MATH dataset with a sparse setup, using\n10 epochs on different GPUs for a realistic cost estimate.",
        "score": 0.7307,
        "rank": 7,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 399
        }
      },
      {
        "node_id": "xia2024:sec10:p13",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "0 2 4 6 8 100.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n RMSE=0.05\nMixtral-CS\nDense Sparse\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n RMSE=0.02\nMixtral-MATH\n0 5 10 15 200\n2\n4\n6\n8\n10\n12\n14\n16\n RMSE=0.79\nMamba-CS\n0 2 4 6 8 10 120\n2\n4\n6\n8\n10\n12\n RMSE=0.42\nThroughput (queries/sec)\nBatch size\nMamba-MATH\nFig. 14. Estimation and validation of LLM fine-tuning throughput for\ndifferent models, datasets for A40 GPU. Dots represent ground truth and\nlines present the estimation.\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nRMSE=0.03\nMixtral-CS-A100-40GB\nDense Sparse\n0 5 10 15 200.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n RMSE=0.09\nMixtral-CS-A100-80GB\n0 5 10 15 200\n1\n2\n3\n4\n5\n RMSE=0.55\nMixtral-CS-H100\nThroughput (queries/sec)\nBatch size\nFig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS\nfor different GPUs: A100 and H100.\nresource renting per hour is calculated based on CUDO\ncompute [33], as other popular cloud providers do not offer\ncost/hour rates for the NVIDIA A40 GPU. However, one\ncan easily adjust the GPU renting cost per hour to estimate\nthe cost on other clouds such as Amazon AWS [34] or\nLambda [35]. Table IV estimates the cost for fine-tuning\nMixtral on the MATH dataset with a sparse setup, using\n10 epochs on different GPUs for a realistic cost estimate.\nEnterprises may use larger datasets for fine-tuning, such as,\nOpenOrca [36] and LaMini-instruction [37] containing more\nthan 2M queries. For OpenOrca, by scaling the cost by\nnumber of queries, our model predicts that the most cost-\neffective option to rent GPU resources on CUDO compute\nis NVIDIA H100 with a net cost of $3460.\nD. Generalization of the Analytical Model\nThe analytical models for estimating maximum batch size\nand throughput can be generalized to various LLM models\nand datasets. These models consider the characteristics of\nthe LLM, dataset, and GPU. Specifically, the maximum\nbatch size model combines GPU memory and LLM model\nsize to determine available memory for input data, while\ndataset sequence length and LLM sparsity determine space\nneeded per batch. In throughput estimation, based on the\nobservation we made (\u00a7IV-B4 Takeaway 5), GPU shifts from\nmemory-bound to compute-bound as batch size increases.\nThis characteristic generally applies to all GPUs due to the\nresource constraint, so the logarithmic relation between batch\nsize and throughput persists. The sparsity in (2) is model\ndependent, the influence of GPU, LLM model, and dataset\nare embedded in the coefficients C2, C3, and C4 in (2).\nThe coefficients in (1) and (2) are dependent on GPU,\nLLM model, and dataset; however, the underlying models\nare generalizable to unseen GPU, LLM model, and datasets.\nAlthough it takes some effort to sweep batch sizes and collect\nthroughput data points to fit our models, the benefits greatly\noutweigh the cost. Once the models are fit, our model can\nhelp choose the most cost-efficient GPU for fine-tuning LLM\nmodels, greatly saving resources and money.\nVI. R ELATED WORKS\nParameter-Efficient Fine-Tuning (PEFT) has been widely\nadopted to fine-tune LLM model for specialized tasks [15],\n[38]\u2013[43]. MoE additioally train specialized experts for dif-\nferent areas and the dynamic selection of experts makes\nit possible to scale the fine-tuning workload to different\nexperts in parallel. [44]\u2013[47] show that MoE models can\nimprove the ability to process knowledge for specific tasks,\nwhile maintaining the world knowledge in LLM. Kim et\nal. [48] construct an analytical model to estimate GPU\nmemory consumption for distributed fine-tuning. The model\nalso provides insights into optimizing memory usage through\ntensor, model, and pipeline parallelism.\nVII. C ONCLUSIONS\nFine-tuning LLMs is an attractive technique for tailoring\nmodern language models using domain-specific knowledge in\na cost-effective manner. This paper delved into understanding\nthe performance of fine-tuning MoE LLM models on a single\nGPU. Our profiling demonstrated that sparse MoE layers\noffer the best bang-for-buck trade-off. Using our profiling\nresults, we developed and validated an accurate analytical\nmodel to estimate the cost of LLM fine-tuning. Using this\nmodel, we showed the dollar amount that needs to be\nbudgeted for fine-tuning LLMs, which is much lower than\npre-training. For example, our model predicted that fine-\ntuning a sparse Mixtral model using a realistic data size of\n2M queries can be done with NVIDIA H100 GPU with a cost\nof $3460. A way to further reduce cost based on our study\nis to add compute resources to accelerate the MoE layers.\nWhile we showcase our study on fine-tuning LLMs using a\nsingle GPU, extending this model to multi-GPU systems is\nleft for future exploration.\nACKNOWLEDGMENTS\nThis work was supported in part by Semiconductor Re-\nsearch Corporation (SRC). We thank all the anonymous\nreviewers for their valuable comments and suggestions.",
        "score": 0.7307,
        "rank": 7,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 10,
          "paragraph_index": 13,
          "section_id": "xia2024:sec10",
          "section_index": 10
        }
      },
      {
        "node_id": "xia2024:sec6:p8",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Mixtral-CS0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.5 0.3 0.7\n1.7\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\nMixtral-MATH0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.3\n1.0\nDense(bsz=1)\nSparse(bsz=1)\nSparse(bsz=3)\nBlackmamba-CS0\n5\n10\n15\n20\n2.3\n7.9\n2.4\n10.5\n14.9\nDense(bsz=1)\nDense(bsz=6)\nSparse(bsz=1)\nSparse(bsz=6)\nSparse(bsz=20)\nBlackmamba-MATH0\n5\n10\n15\n20\n2.2 5.3 2.2\n6.5\n11.6\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\n      Throughput (quries/second)\nFig. 8. Query throughput of Mixtral and BlackMamba.\nmatmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n      SM Utilization (%)\nFig. 9. GPU SM utilization of different kernels in the MoE layer for different batch sizes.\nit involves fewer computational demands, resulting in lower\nlatency. This is evident when comparing the throughput of\nbatch size of 2 in Mixtral-CS for dense (0.5 qps) vs. sparse\n(0.7 qps) models.\nFig. 8 also shows that throughput does not increase linearly\nwith batch size. For instance, sparse fine-tuning of Mixtral-\nCS improves throughput by 1.9 \u00d7 when increasing the batch\nsize from 1 to 2, but only by 4.8 \u00d7 when increasing from\n1 to 8. With smaller batch sizes, the SM utilization rate\nis lower, providing enough computational resources to feed\nmore operations in parallel. However, as the batch size con-\ntinues to increase, the SMs become saturated (more details in\n\u00a7IV-B4), and we can no longer hide latency by better utilizing\ncomputational resources.\nTakeaway 4. Sparse model significantly improves\nthroughput, reducing end-to-end cost of fine-tuning.\n4) Hardware characterization: As shown in Fig. 4, the\nexecution time of LLM fine-tuning is dominated by the MoE\nlayer. To offer further insights, we use detailed microarchi-\ntecture hardware metrics on the GPU to further understand\nexecution bottlenecks in the MoE layer. The goal of this\nstudy is to identify whether various kernels in the MoE layers\nare bound by compute or memory resources, and how future\nGPU designs can further scale performance.\nCompute resource utilization study. Fig. 9 shows the\nkernel-level breakdown of GPU Streaming Multi-processor\n(SM) utilization for the MoE layer. This utilization is",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 6,
          "paragraph_index": 8,
          "section_id": "xia2024:sec6",
          "section_index": 6
        }
      },
      {
        "node_id": "xia2024:sec6",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Page 6",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 6,
          "section_index": 6
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s229",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Mixtral-CS0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.5 0.3 0.7\n1.7\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\nMixtral-MATH0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.3\n1.0\nDense(bsz=1)\nSparse(bsz=1)\nSparse(bsz=3)\nBlackmamba-CS0\n5\n10\n15\n20\n2.3\n7.9\n2.4\n10.5\n14.9\nDense(bsz=1)\nDense(bsz=6)\nSparse(bsz=1)\nSparse(bsz=6)\nSparse(bsz=20)\nBlackmamba-MATH0\n5\n10\n15\n20\n2.2 5.3 2.2\n6.5\n11.6\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\n      Throughput (quries/second)\nFig.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 229
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s230",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "8.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 230
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s231",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Query throughput of Mixtral and BlackMamba.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 231
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s232",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "matmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n      SM Utilization (%)\nFig.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 232
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s233",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "9.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 233
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s234",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "GPU SM utilization of different kernels in the MoE layer for different batch sizes.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 234
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s235",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "it involves fewer computational demands, resulting in lower\nlatency.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 235
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s236",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "This is evident when comparing the throughput of\nbatch size of 2 in Mixtral-CS for dense (0.5 qps) vs.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 236
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s237",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "sparse\n(0.7 qps) models.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 237
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s238",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Fig.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 238
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s239",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "8 also shows that throughput does not increase linearly\nwith batch size.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 239
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s240",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "For instance, sparse fine-tuning of Mixtral-\nCS improves throughput by 1.9 \u00d7 when increasing the batch\nsize from 1 to 2, but only by 4.8 \u00d7 when increasing from\n1 to 8.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 240
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s241",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "With smaller batch sizes, the SM utilization rate\nis lower, providing enough computational resources to feed\nmore operations in parallel.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 241
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s242",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "However, as the batch size con-\ntinues to increase, the SMs become saturated (more details in\n\u00a7IV-B4), and we can no longer hide latency by better utilizing\ncomputational resources.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 242
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s243",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Takeaway 4.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 243
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s244",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Sparse model significantly improves\nthroughput, reducing end-to-end cost of fine-tuning.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 244
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s245",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "4) Hardware characterization: As shown in Fig.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 245
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s246",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "4, the\nexecution time of LLM fine-tuning is dominated by the MoE\nlayer.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 246
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s247",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "To offer further insights, we use detailed microarchi-\ntecture hardware metrics on the GPU to further understand\nexecution bottlenecks in the MoE layer.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 247
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s248",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The goal of this\nstudy is to identify whether various kernels in the MoE layers\nare bound by compute or memory resources, and how future\nGPU designs can further scale performance.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 248
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s249",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Compute resource utilization study.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 249
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s250",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Fig.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 250
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s251",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "9 shows the\nkernel-level breakdown of GPU Streaming Multi-processor\n(SM) utilization for the MoE layer.",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 251
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s252",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "This utilization is",
        "score": 0.7307,
        "rank": 8,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 252
        }
      },
      {
        "node_id": "xia2024:sec10:p13",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "0 2 4 6 8 100.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n RMSE=0.05\nMixtral-CS\nDense Sparse\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n RMSE=0.02\nMixtral-MATH\n0 5 10 15 200\n2\n4\n6\n8\n10\n12\n14\n16\n RMSE=0.79\nMamba-CS\n0 2 4 6 8 10 120\n2\n4\n6\n8\n10\n12\n RMSE=0.42\nThroughput (queries/sec)\nBatch size\nMamba-MATH\nFig. 14. Estimation and validation of LLM fine-tuning throughput for\ndifferent models, datasets for A40 GPU. Dots represent ground truth and\nlines present the estimation.\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nRMSE=0.03\nMixtral-CS-A100-40GB\nDense Sparse\n0 5 10 15 200.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n RMSE=0.09\nMixtral-CS-A100-80GB\n0 5 10 15 200\n1\n2\n3\n4\n5\n RMSE=0.55\nMixtral-CS-H100\nThroughput (queries/sec)\nBatch size\nFig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS\nfor different GPUs: A100 and H100.\nresource renting per hour is calculated based on CUDO\ncompute [33], as other popular cloud providers do not offer\ncost/hour rates for the NVIDIA A40 GPU. However, one\ncan easily adjust the GPU renting cost per hour to estimate\nthe cost on other clouds such as Amazon AWS [34] or\nLambda [35]. Table IV estimates the cost for fine-tuning\nMixtral on the MATH dataset with a sparse setup, using\n10 epochs on different GPUs for a realistic cost estimate.\nEnterprises may use larger datasets for fine-tuning, such as,\nOpenOrca [36] and LaMini-instruction [37] containing more\nthan 2M queries. For OpenOrca, by scaling the cost by\nnumber of queries, our model predicts that the most cost-\neffective option to rent GPU resources on CUDO compute\nis NVIDIA H100 with a net cost of $3460.\nD. Generalization of the Analytical Model\nThe analytical models for estimating maximum batch size\nand throughput can be generalized to various LLM models\nand datasets. These models consider the characteristics of\nthe LLM, dataset, and GPU. Specifically, the maximum\nbatch size model combines GPU memory and LLM model\nsize to determine available memory for input data, while\ndataset sequence length and LLM sparsity determine space\nneeded per batch. In throughput estimation, based on the\nobservation we made (\u00a7IV-B4 Takeaway 5), GPU shifts from\nmemory-bound to compute-bound as batch size increases.\nThis characteristic generally applies to all GPUs due to the\nresource constraint, so the logarithmic relation between batch\nsize and throughput persists. The sparsity in (2) is model\ndependent, the influence of GPU, LLM model, and dataset\nare embedded in the coefficients C2, C3, and C4 in (2).\nThe coefficients in (1) and (2) are dependent on GPU,\nLLM model, and dataset; however, the underlying models\nare generalizable to unseen GPU, LLM model, and datasets.\nAlthough it takes some effort to sweep batch sizes and collect\nthroughput data points to fit our models, the benefits greatly\noutweigh the cost. Once the models are fit, our model can\nhelp choose the most cost-efficient GPU for fine-tuning LLM\nmodels, greatly saving resources and money.\nVI. R ELATED WORKS\nParameter-Efficient Fine-Tuning (PEFT) has been widely\nadopted to fine-tune LLM model for specialized tasks [15],\n[38]\u2013[43]. MoE additioally train specialized experts for dif-\nferent areas and the dynamic selection of experts makes\nit possible to scale the fine-tuning workload to different\nexperts in parallel. [44]\u2013[47] show that MoE models can\nimprove the ability to process knowledge for specific tasks,\nwhile maintaining the world knowledge in LLM. Kim et\nal. [48] construct an analytical model to estimate GPU\nmemory consumption for distributed fine-tuning. The model\nalso provides insights into optimizing memory usage through\ntensor, model, and pipeline parallelism.\nVII. C ONCLUSIONS\nFine-tuning LLMs is an attractive technique for tailoring\nmodern language models using domain-specific knowledge in\na cost-effective manner. This paper delved into understanding\nthe performance of fine-tuning MoE LLM models on a single\nGPU. Our profiling demonstrated that sparse MoE layers\noffer the best bang-for-buck trade-off. Using our profiling\nresults, we developed and validated an accurate analytical\nmodel to estimate the cost of LLM fine-tuning. Using this\nmodel, we showed the dollar amount that needs to be\nbudgeted for fine-tuning LLMs, which is much lower than\npre-training. For example, our model predicted that fine-\ntuning a sparse Mixtral model using a realistic data size of\n2M queries can be done with NVIDIA H100 GPU with a cost\nof $3460. A way to further reduce cost based on our study\nis to add compute resources to accelerate the MoE layers.\nWhile we showcase our study on fine-tuning LLMs using a\nsingle GPU, extending this model to multi-GPU systems is\nleft for future exploration.\nACKNOWLEDGMENTS\nThis work was supported in part by Semiconductor Re-\nsearch Corporation (SRC). We thank all the anonymous\nreviewers for their valuable comments and suggestions.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 10,
          "paragraph_index": 13,
          "section_id": "xia2024:sec10",
          "section_index": 10
        }
      },
      {
        "node_id": "xia2024:sec10",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Page 10",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 10,
          "section_index": 10
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s390",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "0 2 4 6 8 100.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n RMSE=0.05\nMixtral-CS\nDense Sparse\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n RMSE=0.02\nMixtral-MATH\n0 5 10 15 200\n2\n4\n6\n8\n10\n12\n14\n16\n RMSE=0.79\nMamba-CS\n0 2 4 6 8 10 120\n2\n4\n6\n8\n10\n12\n RMSE=0.42\nThroughput (queries/sec)\nBatch size\nMamba-MATH\nFig.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 390
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s391",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "14.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 391
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s392",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Estimation and validation of LLM fine-tuning throughput for\ndifferent models, datasets for A40 GPU.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 392
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s393",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Dots represent ground truth and\nlines present the estimation.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 393
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s394",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nRMSE=0.03\nMixtral-CS-A100-40GB\nDense Sparse\n0 5 10 15 200.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n RMSE=0.09\nMixtral-CS-A100-80GB\n0 5 10 15 200\n1\n2\n3\n4\n5\n RMSE=0.55\nMixtral-CS-H100\nThroughput (queries/sec)\nBatch size\nFig.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 394
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s395",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "15.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 395
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s396",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Estimation and validation of fine-tuning throughput for Mixtral GS\nfor different GPUs: A100 and H100.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 396
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s397",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "resource renting per hour is calculated based on CUDO\ncompute [33], as other popular cloud providers do not offer\ncost/hour rates for the NVIDIA A40 GPU.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 397
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s398",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "However, one\ncan easily adjust the GPU renting cost per hour to estimate\nthe cost on other clouds such as Amazon AWS [34] or\nLambda [35].",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 398
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s399",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Table IV estimates the cost for fine-tuning\nMixtral on the MATH dataset with a sparse setup, using\n10 epochs on different GPUs for a realistic cost estimate.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 399
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s400",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Enterprises may use larger datasets for fine-tuning, such as,\nOpenOrca [36] and LaMini-instruction [37] containing more\nthan 2M queries.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 400
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s401",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "For OpenOrca, by scaling the cost by\nnumber of queries, our model predicts that the most cost-\neffective option to rent GPU resources on CUDO compute\nis NVIDIA H100 with a net cost of $3460.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 401
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s402",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "D.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 402
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s403",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Generalization of the Analytical Model\nThe analytical models for estimating maximum batch size\nand throughput can be generalized to various LLM models\nand datasets.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 403
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s404",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "These models consider the characteristics of\nthe LLM, dataset, and GPU.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 404
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s405",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Specifically, the maximum\nbatch size model combines GPU memory and LLM model\nsize to determine available memory for input data, while\ndataset sequence length and LLM sparsity determine space\nneeded per batch.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 405
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s406",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "In throughput estimation, based on the\nobservation we made (\u00a7IV-B4 Takeaway 5), GPU shifts from\nmemory-bound to compute-bound as batch size increases.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 406
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s407",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "This characteristic generally applies to all GPUs due to the\nresource constraint, so the logarithmic relation between batch\nsize and throughput persists.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 407
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s408",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The sparsity in (2) is model\ndependent, the influence of GPU, LLM model, and dataset\nare embedded in the coefficients C2, C3, and C4 in (2).",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 408
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s409",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The coefficients in (1) and (2) are dependent on GPU,\nLLM model, and dataset;",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 409
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s410",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "however, the underlying models\nare generalizable to unseen GPU, LLM model, and datasets.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 410
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s411",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Although it takes some effort to sweep batch sizes and collect\nthroughput data points to fit our models, the benefits greatly\noutweigh the cost.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 411
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s412",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Once the models are fit, our model can\nhelp choose the most cost-efficient GPU for fine-tuning LLM\nmodels, greatly saving resources and money.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 412
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s413",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "VI.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 413
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s414",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "R ELATED WORKS\nParameter-Efficient Fine-Tuning (PEFT) has been widely\nadopted to fine-tune LLM model for specialized tasks [15],\n[38]\u2013[43].",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 414
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s415",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "MoE additioally train specialized experts for dif-\nferent areas and the dynamic selection of experts makes\nit possible to scale the fine-tuning workload to different\nexperts in parallel.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 415
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s416",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "[44]\u2013[47] show that MoE models can\nimprove the ability to process knowledge for specific tasks,\nwhile maintaining the world knowledge in LLM.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 416
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s417",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Kim et\nal.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 417
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s418",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "[48] construct an analytical model to estimate GPU\nmemory consumption for distributed fine-tuning.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 418
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s419",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The model\nalso provides insights into optimizing memory usage through\ntensor, model, and pipeline parallelism.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 419
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s420",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "VII.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 420
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s421",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "C ONCLUSIONS\nFine-tuning LLMs is an attractive technique for tailoring\nmodern language models using domain-specific knowledge in\na cost-effective manner.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 421
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s422",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "This paper delved into understanding\nthe performance of fine-tuning MoE LLM models on a single\nGPU.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 422
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s423",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Our profiling demonstrated that sparse MoE layers\noffer the best bang-for-buck trade-off.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 423
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s424",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Using our profiling\nresults, we developed and validated an accurate analytical\nmodel to estimate the cost of LLM fine-tuning.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 424
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s425",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Using this\nmodel, we showed the dollar amount that needs to be\nbudgeted for fine-tuning LLMs, which is much lower than\npre-training.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 425
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s426",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "For example, our model predicted that fine-\ntuning a sparse Mixtral model using a realistic data size of\n2M queries can be done with NVIDIA H100 GPU with a cost\nof $3460.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 426
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s427",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "A way to further reduce cost based on our study\nis to add compute resources to accelerate the MoE layers.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 427
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s428",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "While we showcase our study on fine-tuning LLMs using a\nsingle GPU, extending this model to multi-GPU systems is\nleft for future exploration.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 428
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s429",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "ACKNOWLEDGMENTS\nThis work was supported in part by Semiconductor Re-\nsearch Corporation (SRC).",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 429
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s430",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We thank all the anonymous\nreviewers for their valuable comments and suggestions.",
        "score": 0.7289,
        "rank": 9,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 430
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s365",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "TABLE IV\nESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE\nBASED ON OUR ANALYTICAL MODEL\nGPU Mem MBS Throughput Cost ($/hr) Cost ($)\nA40 48GB 4 1.01 0.79 32.7\nA100 80GB 17 2.74 1.67 25.4\nH100 80GB 17 4.90 2.1 17.9\nB.",
        "score": 0.7239,
        "rank": 10,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 365
        }
      },
      {
        "node_id": "xia2024:sec9:p12",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "0 20 40 60 80 100 1200\n5\n10\n15\n20\n25\n30\n35\n40\nA100-40GB\nA100-80GB\nA40\nH100\nbsz=28\nbsz=35\nProjected GPU capacity\nGround Truth Projection\nMax batch size\nGPU DRAM capacity\nFig. 13. Projected maximum batch size of Mixtral for different GPUs.\namount of GPU memory, and need to be subtracted in the\nanalytical model. Fig. 8 supports this by showing that on the\nsame dataset, BlackMamba can support larger batch size than\nMixtral because of its smaller model size.\nMoreover, the sequence length and sparsity also affect the\nmaximum batch size. Because the sparsity only affects the\nMoE part of the LLM, we multiply its influence by C1,\nwhich we call MoE coefficient. We apply the sequence length\nand the sparsity in the denominator as they are inversely\nrelated to batch size. Then, we multiply the result by C0,\nthe scaling coefficient , which scales the batch size by a\nconstant. The scaling coefficient is different across LLM\nmodels, because different models have different architecture\n(\u00a7III), and generate different amounts of intermediate data\nfor each query. The scaling coefficient for BlackMamba is\nhigher than that of Mixtral because it is a smaller model.\nFinally, we use floor to round it to the maximum integer.\nThe MoE coefficient and scaling coefficient vary across\nmodels. These coefficients are independent of GPU microar-\nchitectural parameters. We find the maximum batch size for\nboth LLM models on NVIDIA A40 (48GB), A100 (40GB),\nA100 (80GB), and H100 (80GB), and apply our model to\nfind the optimal coefficients. For Mixtral, C0 = 82 and\nC1 = 0 .95, and for BlackMamba, C0 = 83 and C1 = 0 .88.\nWhile we showcase these parameters for the models eval-\nuated, \u00a7V-D discusses how to generalize this approach for\nother models.\nUsing our analytical model, we demonstrate the maximum\nbatch sizes for fine-tuning on four different NVIDIA GPUs:\nA40, A100-40GB, A100-80GB and H100 with memory\ncapacities of 48GB, 40GB, 80GB, and 80GB, respectively.\nFig. 13 shows our projected maximum batch size and corre-\nlate it with experimented ground truth. While the maximum\nmemory capacity available in NVIDIA GPUs today is 80GB,\nwe use our analytical model to project the maximum batch\nsize that future GPUs might support. For GPU memory\ncapacities of 100GB and 120GB, our model predicts that\nthe maximum batch sizes supported for fine-tuning Mixtral\nwill be 28 and 35, respectively. Due to space limitations, we\nonly show the projection of Mixtral model.\nTABLE IV\nESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE\nBASED ON OUR ANALYTICAL MODEL\nGPU Mem MBS Throughput Cost ($/hr) Cost ($)\nA40 48GB 4 1.01 0.79 32.7\nA100 80GB 17 2.74 1.67 25.4\nH100 80GB 17 4.90 2.1 17.9\nB. Estimating Throughput\nAs discussed in \u00a7IV-B4, when the batch size increases, the\nLLM fine-tuning gradually switches from memory bound to\ncompute bound. When the compute resources are abundant,\nthe throughput increases almost linearly with batch size.\nHowever, when compute resources become constrained, the\nthroughput improvement gradually saturates. We model this\nbehavior using a logarithmic relation between batch size and\nthroughput. Our analytical model for maximum batch size is\nshown in (2).\nT hroughput = C2 \u2217 log( batch size\nsparsity \u2217 C3 ) + C4 (2)\nIn the equation, in addition to the basic logarithmic part, we\nhave three coefficients C2, C3, and C4. C2 is the scaling\ncoefficient, which depends on the LLM model, GPU archi-\ntecture, and the dataset. The higher the compute capability a\nGPU can provide, and the lower the LLM model and dataset\ncompute requirement is, the higher the scaling coefficient will\nbe. C3 is the MoE attenuation coefficient , which tunes how\nmuch the MoE sparsity affects the throughput. MoE sparsity\nonly affects the MoE part in LLM model, and thus should\nbe attenuated to avoid over compensation. This coefficient is\nonly LLM model dependent, because once the model is fixed,\nthe influence of sparsity is determined. C4 is the intercept,\nconceptually it equals to the throughput when batch size\nequals one, because the logarithmic part in (2) is zero when\nbatch size is one. Using scipy [32] to fit the model and\ngenerate four sets (C2, C3, C4), for each model and dataset\ncombination.\nTo estimate the accuracy of this model, we correlate the\nmodel output with experimental data from our study. Fig. 14\nshows this correlation study, where discrete data points (dots)\nrepresent experimental values, and the line represents output\nof our analytical model. We use both dense and sparse\nMixtral and BlackMamba for both datasets used in our study.\nThe figure clearly shows that our model accurately predicts\nLLM fine-tuning throughput with a Root Mean Squared Error\n(RMSE) of less than 0.8. Fig. 15 shows the correlation study\nof the analytical model of three other GPUs, A100 (40GB),\nA100 (80GB), and H100. The RMSE is less than 0.6, close\nto that of A40.\nC. Estimating the Total Fine-Tuning Cost\nUsing the throughput estimation, we calculate the cost\nof fine-tuning LLMs for different GPUs. The cost of GPU",
        "score": 0.7239,
        "rank": 10,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 9,
          "paragraph_index": 12,
          "section_id": "xia2024:sec9",
          "section_index": 9
        }
      },
      {
        "node_id": "xia2024:sec4:p5:s150",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(5) The\nperformance of sparse fine-tuning is close to that of dense\nfine-tuning, with the exception of Mixtral on HE.",
        "score": 0.7227,
        "rank": 11,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec4:p5",
          "section_id": "xia2024:sec4",
          "sentence_index": 150
        }
      },
      {
        "node_id": "xia2024:sec4:p5",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "under 10% on BlackMamba. We assess accuracy improve-\nments post-fine-tuning and compare the learning capabilities\nof dense and sparse versions of both models.\nFig. 3 shows the testing accuracy of Mixtral and Black-\nMamba on two datasets Hellaswag (HE) and GSM8K (GS).\nWe fine-tune both models using the sparse and dense setups\ndescribed in \u00a7III for 10 epochs, and test the accuracy of\nthe fine-tuned model at each epoch. We make the following\nobservations in Fig. 3. (1) Fine-tuning converges relatively\nquickly. Typically, 10 epochs are enough for fine-tune models\nto stabilize at or close to their peak accuracy. On GS, both\nmodels are close to their peak accuracy at the first epoch.\n(2) The smaller model BlackMamba takes relatively more\nepochs to reach its peak accuracy, as it took BlackMamba 5\nepochs to converge on HE. (3) The larger model Mixtral has\nbetter accuracy compared to BlackMamba on both datasets.\n(4) Both models perform better on the CS dataset HE than\non the GS dataset GS. This is because math is harder for\nsmaller LLMs to learn [24]. The BlackMamba model is\ninadequate for fine-tuning GS. This is likely attributed to\nthe complexity of mathematical tasks and the smaller model\nsize of BlackMamba. Additionally, Mamba is specifically\nengineered for long sequence modeling, potentially resulting\nin unsatisfactory arithmetic reasoning ability [25]. Thus, in\nour characterization study in later sections, we will not show\nthe results for BlackMamba fine-tuned on MATH. (5) The\nperformance of sparse fine-tuning is close to that of dense\nfine-tuning, with the exception of Mixtral on HE. However,\neven for this outlier, sparse fine-tuning achieves similar peak\naccuracy compared to dense; we see a drop of accuracy\nbetween the epoch 4 and 5, and indicates sparse fine-tuning is\nmore vulnerable to over-fitting, especially for easy tasks [26].\nFollowing the above insights, the key take-away of this\nanalysis can be summarized as follows.\nTakeaway 1. Sparse model can be trained as well\nas its dense counterpart.\nTakeaway 2. Fine-tuning generally takes less ten\nepochs to reach peak accuracy.\nB. Analysis of Runtime Performance\nAfter confirming that both Mixtral and BlackMamba can\nbe fine-tuned to achieve acceptable accuracy, we examine\ntheir performance in a resource-constrained environment us-\ning a single GPU. This setup highlights unique runtime char-\nacteristics such as execution time breakdown, throughput,\nmaximum batch size, compute and memory utilization, load\nimbalance, and sensitivity analysis. We also compare sparse\nand dense models. Insights from this study will help develop\na robust analytical model for estimating fine-tuning costs.\n1) Maximum Batch Size Support: The maximum batch\nsize in fine-tuning is determined by GPU memory size,\nmodel size, sequence length, and MoE sparsity. The LLM\nTABLE III\nMAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE\nAND S:SPARSE .\nMixtral-D Mixtral-S BlackMamba-D BlackMamba-S\nCS 2 8 6 20\nMATH 1 3 2 8\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)\n0.0\n2.0\n4.0\n6.0\n8.0\nForward Backward Optimizer\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)\n0.0\n0.5\n1.0\n1.5\n2.0\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig. 4. Execution time breakdown.\noccupies a certain amount of GPU memory, with the re-\nmainder available for intermediate data during fine-tuning.\nLonger sequence lengths consume more memory, and denser\nMoE configurations require additional memory space. We\ndiscuss the heuristic for determining the maximum batch size\nin \u00a7V. Based on our experimental study on NVIDIA A40\nGPU with 48GB memory, we empirically find and report\nthe maximum batch size supported by different model and\ndataset combinations in Table III.\n2) Execution Time Breakdown: We first analyze the high-\nlevel execution time breakdown for Mixtral and Black-\nMamba. The purpose of this study is to understand where\ndoes this workload spend most of its time. As discussed in\n\u00a7III, we conduct this study using a sequence length of 128.\nAt a high-level, the fine-tuning workload can be divided\ninto three stages: (1) forward, (2) backward, and (3) opti-\nmizer. We use a batch size of 1 and the maximum batch size\nsupported by a model-dataset combination to show workload\ncharacteristics. Fig. 4 illustrates the following insights. (1)\nThe optimizer stage in BlackMamba fine-tuning takes a\nconsiderable portion of the running time (up to 53% when\nconducting sparse fine-tuning with batch size = 1), while\nthe execution time share of the optimizer stage in Mixtral\nfine-tuning is negligible. The running time of the optimizer\nstage depends only on the number of parameters that need\nto be updated during fine-tuning. This difference is primarily\ndue to the different fine-tuning strategies applied to these two\nmodels: only the parameters in the LoRA module are updated\nfor Mixtral fine-tuning, whereas BlackMamba undergoes full\nfine-tuning. (2) The runtime of the forward and backward\nstages increases with sparsity and batch size due to the\nincreased amount of computation. (3) The backward stage\ntypically takes more time than the forward stage. In Black-\nMamba, the backward stage demands more computation than",
        "score": 0.7227,
        "rank": 11,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 4,
          "paragraph_index": 5,
          "section_id": "xia2024:sec4",
          "section_index": 4
        }
      },
      {
        "node_id": "xia2024:sec8:p10:s320",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Our evaluation (the figure is omitted from the paper\ndue to page limitation) shows that the latency for Mixtral\nremains almost constant across different sequence lengths,\nwhile BlackMamba fine-tuning exhibited a slight reduction\nin latency as sequence length increased, with approximately\n19% and 25% decreases for sparse and dense fine-tuning,\nrespectively.",
        "score": 0.7225,
        "rank": 12,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec8:p10",
          "section_id": "xia2024:sec8",
          "sentence_index": 320
        }
      },
      {
        "node_id": "xia2024:sec8:p10",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Fig. 12. Pseudo code for MoE layers.\nciency [27]. During the training process of MoE models,\neach token is dynamically assigned to the top-k experts based\non routing scores. This strategy often leads to most tokens\nbeing assigned to a small number of experts, resulting in load\nimbalance and slower training. Additionally, some experts\nreceive insufficient training, which degrades overall model\nperformance [28]. A na\u00a8\u0131ve approach to address this imbalance\nis to use token dropping and padding to ensure that the\nnumber of tokens assigned to each expert is equal [29].\nHowever, this method sacrifices model quality or leads to\nwasted computation. In this section, we analyze how fine-\ntuning influences the token distribution among experts. We\ncompare the token distribution of Mixtral and BlackMamba\nbefore and after fine-tuning to understand the impact of this\nprocess.\nWe extract 1,000 examples from both the CS and MATH\ndatasets to test the original models without tuning and the\nmodels after 10 epochs of tuning on these datasets. Fig. 12\nprovides the pseudo code for MoE layers with top-k gating.\nIn this process, the hidden states are first sent to the router\nof the MoE layer, which generates router logits. These logits\ndetermine the priority of each expert for each token. Based on\nthe router score for each token, tokens are grouped together\nand sent to their assigned experts. This top-k routing strategy\ncan lead to load imbalance if the model has not been pre-\ntrained for balance.\nFig. 11 evidently shows that fine-tuning causes load im-\nbalance in Mixtral for both datasets. Comparing variance\nbefore and after fine-tuning (e.g., HE vs. HE tuned), the\ntoken assignment variance increased from 55 to 112 for\nCS and from 21 to 79 for GS. Expert 3 became the\nmost frequently used and important expert post fine-tuning.\nConversely, there is a decrease in the variance of token\ndistribution for BlackMamba on the CS dataset, dropping\nfrom 150 to 93. For the GS dataset, the token distribution\nvariance for BlackMamba remains almost unchanged after\nfine-tuning. This suggests that load-imbalance has a less\ndisruptive impact on fine-tuning for BlackMamba compared\nto Mixtral. From Fig. 11, we can also observe that Mixtral\ndemonstrates better load balance in both tasks compared to\nBlackMamba, despite the increased load imbalance after fine-\ntuning. The increased level of imbalance after fine-tuning\nsuggests GPU load balancing techniques can be helpful.\nBoth single GPU load balancing [30] and multi-GPU load\nbalancing [31] have been proposed to address this issue.\nTakeaway 6. The effect of fine-tuning on expert\nload imbalance in the MoE layer is LLM model and\ndataset dependent.\n6) Sensitivity Study on Sequence Length: To further ana-\nlyze the effect of sequence length on the fine-tuning process,\nwe chose the batch size that would maximize the memory\nfor each sequence length (64, 128, 256, 512, and 1024) and\ncompared the latency, SM utilization, and DRAM utiliza-\ntion. Our evaluation (the figure is omitted from the paper\ndue to page limitation) shows that the latency for Mixtral\nremains almost constant across different sequence lengths,\nwhile BlackMamba fine-tuning exhibited a slight reduction\nin latency as sequence length increased, with approximately\n19% and 25% decreases for sparse and dense fine-tuning,\nrespectively. This is due to the varying maximum batch sizes\nsupported by each sequence length, resulting in a similar\nnumber of tokens in each batch. Because latency remains\nconsistent with increasing sequence length and we can use\nlarger batch sizes, throughput is higher for shorter sequences.\nV. A NALYTICAL MODEL TO ESTIMATE THE COST OF\nFINE -T UNING LLM S\nWhile training LLMs from scratch is a cost-prohibitive\nprocess, fine-tuning LLMs offers an attractive solution to\nalign LLMs to desired behaviors. One such example is fine-\ntuning LLMs to a domain-specific use-cases, for example, to\nanswer math questions. \u00a7IV-A shows that it is possible to\nfine-tune pre-trained LLMs on domain-specific tasks to sig-\nnificant improve accuracy. While this is a desired approach,\ncurrently, no model exists that can predict the cost of fine-\ntuning LLMs.\nFine-tuning LLMs is complex, influenced by factors like\nmodel size, GPU memory, dataset sequence length, and MoE\nsparsity, all affecting batch size and throughput. By integrat-\ning these factors with GPU costs, we can identify the most\ncost-efficient GPU for pre-tuning tasks. This section presents\nan analytical model based on previous characterization.\nThis model estimates cloud-based fine-tuning costs for a\ngiven dataset and LLM. Developed from previous sections,\nit can be adapted for other LLMs by adjusting parameters. It\nassumes using the maximum batch size supported by GPU\nmemory to optimize cost. We first estimate this batch size,\nthen use it to evaluate throughput and fine-tuning costs.\nA. Estimating Maximum Batch Size\nThe maximum batch size is the maximum number of\nqueries that can fit in GPU memory at once. Our analytical\nmodel for maximum batch size is shown in (1).\nM ax BSZ = \u230aC0\u2217 GP U mem \u2212 model mem\nseq len \u2217 ((1 \u2212 C1) + C1 \u2217 sparsity) \u230b\n(1)\nIntuitively, larger GPU memory allows for higher batch\nsizes. In the meantime, the LLM model will take up a certain",
        "score": 0.7225,
        "rank": 12,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 8,
          "paragraph_index": 10,
          "section_id": "xia2024:sec8",
          "section_index": 8
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s2",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "In this\npaper, we characterize sparse Mixture of Experts (MoE) based\nLLM fine-tuning to understand their accuracy and runtime\nperformance on a single GPU.",
        "score": 0.7223,
        "rank": 13,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 2
        }
      },
      {
        "node_id": "xia2024:sec1:p1",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Understanding the Performance and Estimating the\nCost of LLM Fine-Tuning\nYuchen Xia1 Jiho Kim 2 Yuhan Chen1 Haojie Ye1 Souvik Kundu 3\nCong (Callie) Hao 2 Nishil Talati1\n1University of Michigan 2Georgia Institute of Technology 3Intel Labs\nAbstract\u2014Due to the cost-prohibitive nature of training Large\nLanguage Models (LLMs), fine-tuning has emerged as an attrac-\ntive alternative for specializing LLMs for specific tasks using\nlimited compute resources in a cost-effective manner. In this\npaper, we characterize sparse Mixture of Experts (MoE) based\nLLM fine-tuning to understand their accuracy and runtime\nperformance on a single GPU. Our evaluation provides unique\ninsights into the training efficacy of sparse and dense versions of\nMoE models, as well as their runtime characteristics, including\nmaximum batch size, execution time breakdown, end-to-end\nthroughput, GPU hardware utilization, and load distribution.\nOur study identifies the optimization of the MoE layer as crucial\nfor further improving the performance of LLM fine-tuning.\nUsing our profiling results, we also develop and validate an\nanalytical model to estimate the cost of LLM fine-tuning on\nthe cloud. This model, based on parameters of the model and\nGPU architecture, estimates LLM throughput and the cost\nof training, aiding practitioners in industry and academia to\nbudget the cost of fine-tuning a specific model.\nI. I NTRODUCTION\nLarge Language Models (LLMs) are widely utilized in\nNatural Language Processing (NLP) [1]. Modern LLMs\ntypically possess billions to trillions of parameters, neces-\nsitating extensive time and resources for training. For in-\nstance, the estimated cost of training OpenAI\u2019s GPT-4 model\nexceeds $100 million, rendering it financially prohibitive\nfor most small-to-medium size enterprises and the academic\ncommunity [2]. Given the open-sourcing of numerous pre-\ntrained LLMs (e.g., LLAMA [3] and Mixtral [4]), fine-\ntuning has emerged as an attractive alternative for further\nspecializing these models in a cost-effective manner [5].\nGiven the learning ability of pre-trained models, it is feasible\nto use a domain-specific dataset to align the desired behav-\niors of LLMs through supervised fine-tuning on instruction-\nfollowing tasks [6]. Unlike pre-training, fine-tuning can be\nconducted in a resource-constrained environment, typically\nusing one or a few GPUs. Consequently, fine-tuning presents\na compelling case for applications such as specialized ques-\ntion answering within enterprises, legal document analysis\nand drafting, healthcare/medical research, technical and IT\nsupport, among others [7].\nThis paper characterizes LLM fine-tuning with two pri-\nmary objectives: (1) understanding the performance charac-\nteristics of LLM fine-tuning, and (2) developing an analytical\nmodel to estimate the cost of fine-tuning on the cloud. Given\nour focus on cost-efficient LLM fine-tuning, we concen-\ntrate on fine-tuning sparse Mixture-of-Expert (MoE) models.\nSpecifically, we employ an attention-based MoE model, Mix-\ntral [4], and a state-space MoE model, BlackMamba [8]. Us-\ning these models and two domain-specific datasets for math-\nematics and common-sense question-answering, we conduct\nan in-depth profiling study to understand their performance\ncharacteristics with a single GPU. We compare the dense\nand sparse counterparts of the investigated MoE models to\nevaluate their learning rates and runtime performance. Our\ninvestigation covers memory consumption, maximum batch\nsize supported within a single GPU memory budget, exe-\ncution time breakdown and bottlenecks, overall throughput,\nmicroarchitectural performance counters, and runtime load\ndistribution. The insights gained from our study are used to\ndevelop and validate an analytical model to estimate the cost.\nOur characterization uncovers the following unique in-\nsights. (1) Fine-tuning can be achieved in less than 10 epochs,\nand sparse MoE model that activates a subset of experts\ncan learn as well as its dense counterparts. (2) MoE layer\nconsumes the highest fraction of execution time in LLM\nfine-tuning; optimizing MoE layer performance is key to\nimproving the overall cost of LLM fine-tuning. (3) Sparse\nMoE model improves end-to-end throughput by supporting\na larger batch size. Given similar learning abilities of sparse\nand dense models, it is desired to use a sparse MoE model\nfor cost-effective fine-tuning. (4) The workload becomes\ncompute bound by increasing batch size; improving compute\nresources will increase performance. (5) Fine-tuning sparse\nmodel leads to more load imbalance.\nBased on these insights, we create an analytical model\nto estimate the cost of LLM fine-tuning based on model\nsize, dataset size, and GPU architecture. First, we estimate\nthe maximum batch size for a given GPU memory, then\ncompute fine-tuning throughput. We validate this throughput\nwith experimental results, showing an RMSE of less than\n0.55. Using the estimated throughput, our model calculates\nthe fine-tuning cost for different cloud providers.\nThe contributions of this paper are as follows.\n\u2022 Make a case for LLM fine-tuning for specializing pre-\ntrained models in a cost-effective manner.\narXiv:2408.04693v1  [cs.CL]  8 Aug 2024",
        "score": 0.7223,
        "rank": 13,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 1,
          "paragraph_index": 1,
          "section_id": "xia2024:sec1",
          "section_index": 1
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s215",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The de-quantization operation in Mix-\ntral fine-tuning also becomes significant, especially with low\nsparsity and small batch sizes.",
        "score": 0.7213,
        "rank": 14,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 215
        }
      },
      {
        "node_id": "xia2024:sec5:p6",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0.0\n1.0\n2.0\n3.0\n4.0\n5.0\nInput normalization Attention Post attention norm. MoE\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0.0\n0.5\n1.0\n1.5\nRMS layernorm Mamba MoE\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig. 5. Execution time breakdown in terms of different model layers.\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0\n2000\n4000\n6000\nmatmul(w2)\nw2_dequant\nmatmul(w3)\nw3_dequant\nmatmul(w1)\nw1_dequant\nsoftmax\ntopk\nmatmul(router)\nrouter_dequant\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0\n400\n800\n1200\n1600\n2000\nmatmul(w1)\ngelu\nmatmul(w2)\nelementwise_mult\ntop_k\nsigmoid\nmatmul(router)\n    Execution Time Breakdown (\u03bcs)\nMixtral\nMamba\nFig. 6. Execution breakdown of the MoE layer for different kernels.\nthe forward stage due to the need for gradient calculation\nand propagation, resulting in two matrix multiplication op-\nerations. In Mixtral fine-tuning, gradient calculation adds\nminimal computation as only a small portion of parameters\nneed it. However, gradient checkpointing in Mixtral saves\nmemory but increases the backward stage runtime due to the\nre-computation of intermediate values.\nWe further investigate the execution breakdown based\non various layers in two LLM models. For Mixtral, these\nlayers include input normalization, attention, post-attention\nnormalization, and MoE. In contrast, BlackMamba comprises\nthe Mamba layer, Root Mean Squared (RMS) layer nor-\nmalization, and MoE. As shown in Fig. 5, the MoE layer\nis the most time-consuming, accounting for 85% of the\noverall execution time on average. The execution time for\nthe MoE layer encompasses both the forward and backward\npasses during fine-tuning. Consequently, MoE is the costliest\nlayer and a prime target for optimization to enhance the\nperformance of LLM fine-tuning.\nTo concretely understand the opportunity for improving\nMoE layer performance, we also perform a kernel-level anal-\nysis within the MoE layer. Fig. 7 illustrates the architecture\nof the MoE layer in both Mixtral and BlackMamba models.\nEach expert in BlackMamba consists of a standard Feed-\nForward Network (FFN) layer with two serially connected\nweight matrices (W1 and W2) and a Gelu activation layer\nbetween. In contrast, experts in Mixtral are FFN layers with\nSwish-Gated Linear Units, involving an additional weight\nFig. 7. Expert architectures for Mixtral (top) and BlackMamba (bottom).\nmatrix (W3) in parallel with W1.\nFig. 6 shows the kernel-level MoE time breakdown. The\nfigure clearly shows that matrix multiplication (W1, W2,\nand W3) is the largest component of the MoE layer for\nboth BlackMamba and Mixtral. As batch size and sparsity\nincrease, so does computational demand, prolonging matrix\nmultiplication latency. The de-quantization operation in Mix-\ntral fine-tuning also becomes significant, especially with low\nsparsity and small batch sizes. While quantization reduces\nmodel size and memory footprint, it can increase computation\ntime due to de-quantization. This highlights the need to\nevaluate trade-offs between memory savings and computation\ntime, particularly in scenarios with small batch sizes and\nsequence lengths.\nTakeaway 3. Matrix multiplication operations in the\nMoE layer contribute significantly to the end-to-end\nexecution time, making the MoE layer the costliest\ncomponent in LLM fine-tuning.\n3) Fine-Tuning Throughput: Next, we present the fine-\ntuning throughput of Mixtral and BlackMamba on the MATH\nand CS datasets separately in Fig. 8. We use a throughput\nmetric of queries/second processed, where a query includes a\nprompt and a ground-truth answer for fine-tuning. To obtain\nthese results, we extract 1000 examples from each dataset\nand fine-tuned Mixtral and BlackMamba on them using the\nsmallest batch size (batch size = 1) and the largest batch size\nthat would fill the GPU memory.\nAs illustrated in Fig. 8, sparse fine-tuning achieves higher\nthroughput than dense fine-tuning. This is because the sparse\nfine-tuning baseline consumes less memory to store interme-\ndiate values, which allows for higher batch sizes compared\nto its dense counterpart. Additionally, with the same batch\nsize, sparse fine-tuning achieves higher throughput because",
        "score": 0.7213,
        "rank": 14,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 5,
          "paragraph_index": 6,
          "section_id": "xia2024:sec5",
          "section_index": 5
        }
      },
      {
        "node_id": "xia2024:sec7:p9",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "matmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n   DRAM Bandwidth Utilization (%)\nFig. 10. GPU DRAM bandwidth utilization of different kernels in the MoE layer for different batch sizes.\nweighted by the amount of time each kernel takes. We\nuse a sequence length of 128 (\u00a7III). Sequence length will\ninfluence the choice of batch size, and we discuss the effects\nof sequence length on runtime, throughput, SM utilization,\nand memory utilization in \u00a7IV-B6. For dense fine-tuning, we\nshow the SM utilization of batch size 1 and the maximum\nbatch size that fits into memory; for sparse fine-tuning,\nwe use the two batch sizes for dense fine-tuning, and the\nmaximum batch size that fits into memory.\nFig. 9 shows the SM utilization of different kernels in the\nMoE layer, which offers the following insights. (1) For both\nsparse and dense fine-tuning, SM utilization increases with\nbatch size due to higher parallelism and GPU activity. (2)\nSparse fine-tuning has lower SM utilization than dense fine-\ntuning at the same batch size because it activates only 2\nout of 8 experts, reducing parallelism. Consequently, sparse\nfine-tuning supports a higher maximum batch size. Both\nachieve similar maximum SM utilization at their peak batch\nsizes. (3) The de-quantization kernel maintains high SM\nutilization regardless of batch size. (4) Matrix multiplication\nkernels achieve higher SM utilization with larger batch sizes,\nleveraging the GPU\u2019s parallel processing capabilities.\nMemory resource utilization study. Fig. 10 shows the\nkernel-level breakdown of GPU memory bandwidth utiliza-\ntion. We use the same experimental setup as in the evalua-\ntion of SM utilization, and find the following insights. (1)\nFor both sparse and dense fine-tuning, the time-weighted\nmemory utilization decreases with increasing batch size.\nThis is because the model parameters are loaded once and\nshared by all queries in a batch. However, a larger batch\nincreases the execution time (as discussed in \u00a7IV-B6),\nleading to a lower average memory bandwidth utilization.\nHE HE_tuned GS GS_tuned0\n25\n50\n75\n100\nExpert 0\nExpert 1\nExpert 2\nExpert 3\nExpert 4\nExpert 5\nExpert 6\nExpert 7\nHE HE_tuned GS GS_tuned0\n25\n50\n75\n100                          Avg Num. of Token Per Query\nMixtral\nMamba\nvar=55.5 var=112.3\nvar=21.2\nvar=79.2\nvar=150.7 var=93.3\nvar=186.5 var=187.9\nFig. 11. Token distribution to different experts.\n(2) For the same batch size, sparse fine-tuning achieves\nhigher memory bandwidth utilization than dense fine-tuning\ndue to shorter execution times. (3) Dequant layers\u2019 memory\nutilization is batch-size-independent, while matmul layers\u2019\nutilization decreases with larger batch sizes. To maximize\nGPU memory usage, a sufficiently large batch size should be\nused. With large batch sizes, fine-tuning becomes compute-\nbound, indicating a need for improved compute resources in\nfuture hardware to better utilize memory bandwidth.\nTakeaway 5 . As the batch size increases, LLM\nfine-tuning transitions from being memory-bound to\ncompute-bound.\n5) Effect of Load Imbalance Due to Fine-Tuning: Recent\ntrends in deploying expert parallelism in MoE models have\nhighlighted load-imbalanced computation among experts as\na significant issue impacting inference and training effi-",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 7,
          "paragraph_index": 9,
          "section_id": "xia2024:sec7",
          "section_index": 7
        }
      },
      {
        "node_id": "xia2024:sec7",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Page 7",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 7,
          "section_index": 7
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s253",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "matmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n   DRAM Bandwidth Utilization (%)\nFig.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 253
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s254",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "10.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 254
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s255",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "GPU DRAM bandwidth utilization of different kernels in the MoE layer for different batch sizes.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 255
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s256",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "weighted by the amount of time each kernel takes.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 256
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s257",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We\nuse a sequence length of 128 (\u00a7III).",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 257
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s258",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Sequence length will\ninfluence the choice of batch size, and we discuss the effects\nof sequence length on runtime, throughput, SM utilization,\nand memory utilization in \u00a7IV-B6.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 258
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s259",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "For dense fine-tuning, we\nshow the SM utilization of batch size 1 and the maximum\nbatch size that fits into memory;",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 259
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s260",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "for sparse fine-tuning,\nwe use the two batch sizes for dense fine-tuning, and the\nmaximum batch size that fits into memory.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 260
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s261",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Fig.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 261
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s262",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "9 shows the SM utilization of different kernels in the\nMoE layer, which offers the following insights.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 262
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s263",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(1) For both\nsparse and dense fine-tuning, SM utilization increases with\nbatch size due to higher parallelism and GPU activity.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 263
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s264",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(2)\nSparse fine-tuning has lower SM utilization than dense fine-\ntuning at the same batch size because it activates only 2\nout of 8 experts, reducing parallelism.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 264
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s265",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Consequently, sparse\nfine-tuning supports a higher maximum batch size.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 265
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s266",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Both\nachieve similar maximum SM utilization at their peak batch\nsizes.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 266
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s267",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(3) The de-quantization kernel maintains high SM\nutilization regardless of batch size.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 267
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s268",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(4) Matrix multiplication\nkernels achieve higher SM utilization with larger batch sizes,\nleveraging the GPU\u2019s parallel processing capabilities.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 268
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s269",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Memory resource utilization study.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 269
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s270",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Fig.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 270
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s271",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "10 shows the\nkernel-level breakdown of GPU memory bandwidth utiliza-\ntion.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 271
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s272",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We use the same experimental setup as in the evalua-\ntion of SM utilization, and find the following insights.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 272
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s273",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(1)\nFor both sparse and dense fine-tuning, the time-weighted\nmemory utilization decreases with increasing batch size.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 273
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s274",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "This is because the model parameters are loaded once and\nshared by all queries in a batch.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 274
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s275",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "However, a larger batch\nincreases the execution time (as discussed in \u00a7IV-B6),\nleading to a lower average memory bandwidth utilization.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 275
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s276",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "HE HE_tuned GS GS_tuned0\n25\n50\n75\n100\nExpert 0\nExpert 1\nExpert 2\nExpert 3\nExpert 4\nExpert 5\nExpert 6\nExpert 7\nHE HE_tuned GS GS_tuned0\n25\n50\n75\n100                          Avg Num.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 276
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s277",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "of Token Per Query\nMixtral\nMamba\nvar=55.5 var=112.3\nvar=21.2\nvar=79.2\nvar=150.7 var=93.3\nvar=186.5 var=187.9\nFig.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 277
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s278",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "11.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 278
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s279",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Token distribution to different experts.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 279
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s280",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(2) For the same batch size, sparse fine-tuning achieves\nhigher memory bandwidth utilization than dense fine-tuning\ndue to shorter execution times.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 280
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s281",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(3) Dequant layers\u2019 memory\nutilization is batch-size-independent, while matmul layers\u2019\nutilization decreases with larger batch sizes.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 281
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s282",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "To maximize\nGPU memory usage, a sufficiently large batch size should be\nused.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 282
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s283",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "With large batch sizes, fine-tuning becomes compute-\nbound, indicating a need for improved compute resources in\nfuture hardware to better utilize memory bandwidth.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 283
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s284",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Takeaway 5 .",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 284
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s285",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "As the batch size increases, LLM\nfine-tuning transitions from being memory-bound to\ncompute-bound.",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 285
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s286",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "5) Effect of Load Imbalance Due to Fine-Tuning: Recent\ntrends in deploying expert parallelism in MoE models have\nhighlighted load-imbalanced computation among experts as\na significant issue impacting inference and training effi-",
        "score": 0.7185,
        "rank": 15,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 286
        }
      },
      {
        "node_id": "xia2024:sec1:p1:s23",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "(1) Fine-tuning can be achieved in less than 10 epochs,\nand sparse MoE model that activates a subset of experts\ncan learn as well as its dense counterparts.",
        "score": 0.7112,
        "rank": 16,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec1:p1",
          "section_id": "xia2024:sec1",
          "sentence_index": 23
        }
      },
      {
        "node_id": "xia2024:sec1:p1",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Understanding the Performance and Estimating the\nCost of LLM Fine-Tuning\nYuchen Xia1 Jiho Kim 2 Yuhan Chen1 Haojie Ye1 Souvik Kundu 3\nCong (Callie) Hao 2 Nishil Talati1\n1University of Michigan 2Georgia Institute of Technology 3Intel Labs\nAbstract\u2014Due to the cost-prohibitive nature of training Large\nLanguage Models (LLMs), fine-tuning has emerged as an attrac-\ntive alternative for specializing LLMs for specific tasks using\nlimited compute resources in a cost-effective manner. In this\npaper, we characterize sparse Mixture of Experts (MoE) based\nLLM fine-tuning to understand their accuracy and runtime\nperformance on a single GPU. Our evaluation provides unique\ninsights into the training efficacy of sparse and dense versions of\nMoE models, as well as their runtime characteristics, including\nmaximum batch size, execution time breakdown, end-to-end\nthroughput, GPU hardware utilization, and load distribution.\nOur study identifies the optimization of the MoE layer as crucial\nfor further improving the performance of LLM fine-tuning.\nUsing our profiling results, we also develop and validate an\nanalytical model to estimate the cost of LLM fine-tuning on\nthe cloud. This model, based on parameters of the model and\nGPU architecture, estimates LLM throughput and the cost\nof training, aiding practitioners in industry and academia to\nbudget the cost of fine-tuning a specific model.\nI. I NTRODUCTION\nLarge Language Models (LLMs) are widely utilized in\nNatural Language Processing (NLP) [1]. Modern LLMs\ntypically possess billions to trillions of parameters, neces-\nsitating extensive time and resources for training. For in-\nstance, the estimated cost of training OpenAI\u2019s GPT-4 model\nexceeds $100 million, rendering it financially prohibitive\nfor most small-to-medium size enterprises and the academic\ncommunity [2]. Given the open-sourcing of numerous pre-\ntrained LLMs (e.g., LLAMA [3] and Mixtral [4]), fine-\ntuning has emerged as an attractive alternative for further\nspecializing these models in a cost-effective manner [5].\nGiven the learning ability of pre-trained models, it is feasible\nto use a domain-specific dataset to align the desired behav-\niors of LLMs through supervised fine-tuning on instruction-\nfollowing tasks [6]. Unlike pre-training, fine-tuning can be\nconducted in a resource-constrained environment, typically\nusing one or a few GPUs. Consequently, fine-tuning presents\na compelling case for applications such as specialized ques-\ntion answering within enterprises, legal document analysis\nand drafting, healthcare/medical research, technical and IT\nsupport, among others [7].\nThis paper characterizes LLM fine-tuning with two pri-\nmary objectives: (1) understanding the performance charac-\nteristics of LLM fine-tuning, and (2) developing an analytical\nmodel to estimate the cost of fine-tuning on the cloud. Given\nour focus on cost-efficient LLM fine-tuning, we concen-\ntrate on fine-tuning sparse Mixture-of-Expert (MoE) models.\nSpecifically, we employ an attention-based MoE model, Mix-\ntral [4], and a state-space MoE model, BlackMamba [8]. Us-\ning these models and two domain-specific datasets for math-\nematics and common-sense question-answering, we conduct\nan in-depth profiling study to understand their performance\ncharacteristics with a single GPU. We compare the dense\nand sparse counterparts of the investigated MoE models to\nevaluate their learning rates and runtime performance. Our\ninvestigation covers memory consumption, maximum batch\nsize supported within a single GPU memory budget, exe-\ncution time breakdown and bottlenecks, overall throughput,\nmicroarchitectural performance counters, and runtime load\ndistribution. The insights gained from our study are used to\ndevelop and validate an analytical model to estimate the cost.\nOur characterization uncovers the following unique in-\nsights. (1) Fine-tuning can be achieved in less than 10 epochs,\nand sparse MoE model that activates a subset of experts\ncan learn as well as its dense counterparts. (2) MoE layer\nconsumes the highest fraction of execution time in LLM\nfine-tuning; optimizing MoE layer performance is key to\nimproving the overall cost of LLM fine-tuning. (3) Sparse\nMoE model improves end-to-end throughput by supporting\na larger batch size. Given similar learning abilities of sparse\nand dense models, it is desired to use a sparse MoE model\nfor cost-effective fine-tuning. (4) The workload becomes\ncompute bound by increasing batch size; improving compute\nresources will increase performance. (5) Fine-tuning sparse\nmodel leads to more load imbalance.\nBased on these insights, we create an analytical model\nto estimate the cost of LLM fine-tuning based on model\nsize, dataset size, and GPU architecture. First, we estimate\nthe maximum batch size for a given GPU memory, then\ncompute fine-tuning throughput. We validate this throughput\nwith experimental results, showing an RMSE of less than\n0.55. Using the estimated throughput, our model calculates\nthe fine-tuning cost for different cloud providers.\nThe contributions of this paper are as follows.\n\u2022 Make a case for LLM fine-tuning for specializing pre-\ntrained models in a cost-effective manner.\narXiv:2408.04693v1  [cs.CL]  8 Aug 2024",
        "score": 0.7112,
        "rank": 16,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 1,
          "paragraph_index": 1,
          "section_id": "xia2024:sec1",
          "section_index": 1
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s244",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Sparse model significantly improves\nthroughput, reducing end-to-end cost of fine-tuning.",
        "score": 0.7065,
        "rank": 17,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 244
        }
      },
      {
        "node_id": "xia2024:sec6:p8",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Mixtral-CS0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.5 0.3 0.7\n1.7\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\nMixtral-MATH0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.3\n1.0\nDense(bsz=1)\nSparse(bsz=1)\nSparse(bsz=3)\nBlackmamba-CS0\n5\n10\n15\n20\n2.3\n7.9\n2.4\n10.5\n14.9\nDense(bsz=1)\nDense(bsz=6)\nSparse(bsz=1)\nSparse(bsz=6)\nSparse(bsz=20)\nBlackmamba-MATH0\n5\n10\n15\n20\n2.2 5.3 2.2\n6.5\n11.6\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\n      Throughput (quries/second)\nFig. 8. Query throughput of Mixtral and BlackMamba.\nmatmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n      SM Utilization (%)\nFig. 9. GPU SM utilization of different kernels in the MoE layer for different batch sizes.\nit involves fewer computational demands, resulting in lower\nlatency. This is evident when comparing the throughput of\nbatch size of 2 in Mixtral-CS for dense (0.5 qps) vs. sparse\n(0.7 qps) models.\nFig. 8 also shows that throughput does not increase linearly\nwith batch size. For instance, sparse fine-tuning of Mixtral-\nCS improves throughput by 1.9 \u00d7 when increasing the batch\nsize from 1 to 2, but only by 4.8 \u00d7 when increasing from\n1 to 8. With smaller batch sizes, the SM utilization rate\nis lower, providing enough computational resources to feed\nmore operations in parallel. However, as the batch size con-\ntinues to increase, the SMs become saturated (more details in\n\u00a7IV-B4), and we can no longer hide latency by better utilizing\ncomputational resources.\nTakeaway 4. Sparse model significantly improves\nthroughput, reducing end-to-end cost of fine-tuning.\n4) Hardware characterization: As shown in Fig. 4, the\nexecution time of LLM fine-tuning is dominated by the MoE\nlayer. To offer further insights, we use detailed microarchi-\ntecture hardware metrics on the GPU to further understand\nexecution bottlenecks in the MoE layer. The goal of this\nstudy is to identify whether various kernels in the MoE layers\nare bound by compute or memory resources, and how future\nGPU designs can further scale performance.\nCompute resource utilization study. Fig. 9 shows the\nkernel-level breakdown of GPU Streaming Multi-processor\n(SM) utilization for the MoE layer. This utilization is",
        "score": 0.7065,
        "rank": 17,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 6,
          "paragraph_index": 8,
          "section_id": "xia2024:sec6",
          "section_index": 6
        }
      },
      {
        "node_id": "xia2024:sec6:p8:s240",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "For instance, sparse fine-tuning of Mixtral-\nCS improves throughput by 1.9 \u00d7 when increasing the batch\nsize from 1 to 2, but only by 4.8 \u00d7 when increasing from\n1 to 8.",
        "score": 0.705,
        "rank": 18,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec6:p8",
          "section_id": "xia2024:sec6",
          "sentence_index": 240
        }
      },
      {
        "node_id": "xia2024:sec6:p8",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Mixtral-CS0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.5 0.3 0.7\n1.7\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\nMixtral-MATH0.0\n0.5\n1.0\n1.5\n2.0\n0.3 0.3\n1.0\nDense(bsz=1)\nSparse(bsz=1)\nSparse(bsz=3)\nBlackmamba-CS0\n5\n10\n15\n20\n2.3\n7.9\n2.4\n10.5\n14.9\nDense(bsz=1)\nDense(bsz=6)\nSparse(bsz=1)\nSparse(bsz=6)\nSparse(bsz=20)\nBlackmamba-MATH0\n5\n10\n15\n20\n2.2 5.3 2.2\n6.5\n11.6\nDense(bsz=1)\nDense(bsz=2)\nSparse(bsz=1)\nSparse(bsz=2)\nSparse(bsz=8)\n      Throughput (quries/second)\nFig. 8. Query throughput of Mixtral and BlackMamba.\nmatmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n      SM Utilization (%)\nFig. 9. GPU SM utilization of different kernels in the MoE layer for different batch sizes.\nit involves fewer computational demands, resulting in lower\nlatency. This is evident when comparing the throughput of\nbatch size of 2 in Mixtral-CS for dense (0.5 qps) vs. sparse\n(0.7 qps) models.\nFig. 8 also shows that throughput does not increase linearly\nwith batch size. For instance, sparse fine-tuning of Mixtral-\nCS improves throughput by 1.9 \u00d7 when increasing the batch\nsize from 1 to 2, but only by 4.8 \u00d7 when increasing from\n1 to 8. With smaller batch sizes, the SM utilization rate\nis lower, providing enough computational resources to feed\nmore operations in parallel. However, as the batch size con-\ntinues to increase, the SMs become saturated (more details in\n\u00a7IV-B4), and we can no longer hide latency by better utilizing\ncomputational resources.\nTakeaway 4. Sparse model significantly improves\nthroughput, reducing end-to-end cost of fine-tuning.\n4) Hardware characterization: As shown in Fig. 4, the\nexecution time of LLM fine-tuning is dominated by the MoE\nlayer. To offer further insights, we use detailed microarchi-\ntecture hardware metrics on the GPU to further understand\nexecution bottlenecks in the MoE layer. The goal of this\nstudy is to identify whether various kernels in the MoE layers\nare bound by compute or memory resources, and how future\nGPU designs can further scale performance.\nCompute resource utilization study. Fig. 9 shows the\nkernel-level breakdown of GPU Streaming Multi-processor\n(SM) utilization for the MoE layer. This utilization is",
        "score": 0.705,
        "rank": 18,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 6,
          "paragraph_index": 8,
          "section_id": "xia2024:sec6",
          "section_index": 6
        }
      },
      {
        "node_id": "xia2024:sec10:p13:s394",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nRMSE=0.03\nMixtral-CS-A100-40GB\nDense Sparse\n0 5 10 15 200.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n RMSE=0.09\nMixtral-CS-A100-80GB\n0 5 10 15 200\n1\n2\n3\n4\n5\n RMSE=0.55\nMixtral-CS-H100\nThroughput (queries/sec)\nBatch size\nFig.",
        "score": 0.7045,
        "rank": 19,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec10:p13",
          "section_id": "xia2024:sec10",
          "sentence_index": 394
        }
      },
      {
        "node_id": "xia2024:sec10:p13",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "0 2 4 6 8 100.00\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n RMSE=0.05\nMixtral-CS\nDense Sparse\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n RMSE=0.02\nMixtral-MATH\n0 5 10 15 200\n2\n4\n6\n8\n10\n12\n14\n16\n RMSE=0.79\nMamba-CS\n0 2 4 6 8 10 120\n2\n4\n6\n8\n10\n12\n RMSE=0.42\nThroughput (queries/sec)\nBatch size\nMamba-MATH\nFig. 14. Estimation and validation of LLM fine-tuning throughput for\ndifferent models, datasets for A40 GPU. Dots represent ground truth and\nlines present the estimation.\n0 1 2 3 4 50.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nRMSE=0.03\nMixtral-CS-A100-40GB\nDense Sparse\n0 5 10 15 200.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n RMSE=0.09\nMixtral-CS-A100-80GB\n0 5 10 15 200\n1\n2\n3\n4\n5\n RMSE=0.55\nMixtral-CS-H100\nThroughput (queries/sec)\nBatch size\nFig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS\nfor different GPUs: A100 and H100.\nresource renting per hour is calculated based on CUDO\ncompute [33], as other popular cloud providers do not offer\ncost/hour rates for the NVIDIA A40 GPU. However, one\ncan easily adjust the GPU renting cost per hour to estimate\nthe cost on other clouds such as Amazon AWS [34] or\nLambda [35]. Table IV estimates the cost for fine-tuning\nMixtral on the MATH dataset with a sparse setup, using\n10 epochs on different GPUs for a realistic cost estimate.\nEnterprises may use larger datasets for fine-tuning, such as,\nOpenOrca [36] and LaMini-instruction [37] containing more\nthan 2M queries. For OpenOrca, by scaling the cost by\nnumber of queries, our model predicts that the most cost-\neffective option to rent GPU resources on CUDO compute\nis NVIDIA H100 with a net cost of $3460.\nD. Generalization of the Analytical Model\nThe analytical models for estimating maximum batch size\nand throughput can be generalized to various LLM models\nand datasets. These models consider the characteristics of\nthe LLM, dataset, and GPU. Specifically, the maximum\nbatch size model combines GPU memory and LLM model\nsize to determine available memory for input data, while\ndataset sequence length and LLM sparsity determine space\nneeded per batch. In throughput estimation, based on the\nobservation we made (\u00a7IV-B4 Takeaway 5), GPU shifts from\nmemory-bound to compute-bound as batch size increases.\nThis characteristic generally applies to all GPUs due to the\nresource constraint, so the logarithmic relation between batch\nsize and throughput persists. The sparsity in (2) is model\ndependent, the influence of GPU, LLM model, and dataset\nare embedded in the coefficients C2, C3, and C4 in (2).\nThe coefficients in (1) and (2) are dependent on GPU,\nLLM model, and dataset; however, the underlying models\nare generalizable to unseen GPU, LLM model, and datasets.\nAlthough it takes some effort to sweep batch sizes and collect\nthroughput data points to fit our models, the benefits greatly\noutweigh the cost. Once the models are fit, our model can\nhelp choose the most cost-efficient GPU for fine-tuning LLM\nmodels, greatly saving resources and money.\nVI. R ELATED WORKS\nParameter-Efficient Fine-Tuning (PEFT) has been widely\nadopted to fine-tune LLM model for specialized tasks [15],\n[38]\u2013[43]. MoE additioally train specialized experts for dif-\nferent areas and the dynamic selection of experts makes\nit possible to scale the fine-tuning workload to different\nexperts in parallel. [44]\u2013[47] show that MoE models can\nimprove the ability to process knowledge for specific tasks,\nwhile maintaining the world knowledge in LLM. Kim et\nal. [48] construct an analytical model to estimate GPU\nmemory consumption for distributed fine-tuning. The model\nalso provides insights into optimizing memory usage through\ntensor, model, and pipeline parallelism.\nVII. C ONCLUSIONS\nFine-tuning LLMs is an attractive technique for tailoring\nmodern language models using domain-specific knowledge in\na cost-effective manner. This paper delved into understanding\nthe performance of fine-tuning MoE LLM models on a single\nGPU. Our profiling demonstrated that sparse MoE layers\noffer the best bang-for-buck trade-off. Using our profiling\nresults, we developed and validated an accurate analytical\nmodel to estimate the cost of LLM fine-tuning. Using this\nmodel, we showed the dollar amount that needs to be\nbudgeted for fine-tuning LLMs, which is much lower than\npre-training. For example, our model predicted that fine-\ntuning a sparse Mixtral model using a realistic data size of\n2M queries can be done with NVIDIA H100 GPU with a cost\nof $3460. A way to further reduce cost based on our study\nis to add compute resources to accelerate the MoE layers.\nWhile we showcase our study on fine-tuning LLMs using a\nsingle GPU, extending this model to multi-GPU systems is\nleft for future exploration.\nACKNOWLEDGMENTS\nThis work was supported in part by Semiconductor Re-\nsearch Corporation (SRC). We thank all the anonymous\nreviewers for their valuable comments and suggestions.",
        "score": 0.7045,
        "rank": 19,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 10,
          "paragraph_index": 13,
          "section_id": "xia2024:sec10",
          "section_index": 10
        }
      },
      {
        "node_id": "xia2024:sec5:p6:s227",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Additionally, with the same batch\nsize, sparse fine-tuning achieves higher throughput because",
        "score": 0.6987,
        "rank": 20,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec5:p6",
          "section_id": "xia2024:sec5",
          "sentence_index": 227
        }
      },
      {
        "node_id": "xia2024:sec5:p6",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0.0\n1.0\n2.0\n3.0\n4.0\n5.0\nInput normalization Attention Post attention norm. MoE\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0.0\n0.5\n1.0\n1.5\nRMS layernorm Mamba MoE\n    Execution Time\nBreakdown (seconds)\nMixtral\nMamba\nFig. 5. Execution time breakdown in terms of different model layers.\nDense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)0\n2000\n4000\n6000\nmatmul(w2)\nw2_dequant\nmatmul(w3)\nw3_dequant\nmatmul(w1)\nw1_dequant\nsoftmax\ntopk\nmatmul(router)\nrouter_dequant\nDense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0\n400\n800\n1200\n1600\n2000\nmatmul(w1)\ngelu\nmatmul(w2)\nelementwise_mult\ntop_k\nsigmoid\nmatmul(router)\n    Execution Time Breakdown (\u03bcs)\nMixtral\nMamba\nFig. 6. Execution breakdown of the MoE layer for different kernels.\nthe forward stage due to the need for gradient calculation\nand propagation, resulting in two matrix multiplication op-\nerations. In Mixtral fine-tuning, gradient calculation adds\nminimal computation as only a small portion of parameters\nneed it. However, gradient checkpointing in Mixtral saves\nmemory but increases the backward stage runtime due to the\nre-computation of intermediate values.\nWe further investigate the execution breakdown based\non various layers in two LLM models. For Mixtral, these\nlayers include input normalization, attention, post-attention\nnormalization, and MoE. In contrast, BlackMamba comprises\nthe Mamba layer, Root Mean Squared (RMS) layer nor-\nmalization, and MoE. As shown in Fig. 5, the MoE layer\nis the most time-consuming, accounting for 85% of the\noverall execution time on average. The execution time for\nthe MoE layer encompasses both the forward and backward\npasses during fine-tuning. Consequently, MoE is the costliest\nlayer and a prime target for optimization to enhance the\nperformance of LLM fine-tuning.\nTo concretely understand the opportunity for improving\nMoE layer performance, we also perform a kernel-level anal-\nysis within the MoE layer. Fig. 7 illustrates the architecture\nof the MoE layer in both Mixtral and BlackMamba models.\nEach expert in BlackMamba consists of a standard Feed-\nForward Network (FFN) layer with two serially connected\nweight matrices (W1 and W2) and a Gelu activation layer\nbetween. In contrast, experts in Mixtral are FFN layers with\nSwish-Gated Linear Units, involving an additional weight\nFig. 7. Expert architectures for Mixtral (top) and BlackMamba (bottom).\nmatrix (W3) in parallel with W1.\nFig. 6 shows the kernel-level MoE time breakdown. The\nfigure clearly shows that matrix multiplication (W1, W2,\nand W3) is the largest component of the MoE layer for\nboth BlackMamba and Mixtral. As batch size and sparsity\nincrease, so does computational demand, prolonging matrix\nmultiplication latency. The de-quantization operation in Mix-\ntral fine-tuning also becomes significant, especially with low\nsparsity and small batch sizes. While quantization reduces\nmodel size and memory footprint, it can increase computation\ntime due to de-quantization. This highlights the need to\nevaluate trade-offs between memory savings and computation\ntime, particularly in scenarios with small batch sizes and\nsequence lengths.\nTakeaway 3. Matrix multiplication operations in the\nMoE layer contribute significantly to the end-to-end\nexecution time, making the MoE layer the costliest\ncomponent in LLM fine-tuning.\n3) Fine-Tuning Throughput: Next, we present the fine-\ntuning throughput of Mixtral and BlackMamba on the MATH\nand CS datasets separately in Fig. 8. We use a throughput\nmetric of queries/second processed, where a query includes a\nprompt and a ground-truth answer for fine-tuning. To obtain\nthese results, we extract 1000 examples from each dataset\nand fine-tuned Mixtral and BlackMamba on them using the\nsmallest batch size (batch size = 1) and the largest batch size\nthat would fill the GPU memory.\nAs illustrated in Fig. 8, sparse fine-tuning achieves higher\nthroughput than dense fine-tuning. This is because the sparse\nfine-tuning baseline consumes less memory to store interme-\ndiate values, which allows for higher batch sizes compared\nto its dense counterpart. Additionally, with the same batch\nsize, sparse fine-tuning achieves higher throughput because",
        "score": 0.6987,
        "rank": 20,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 5,
          "paragraph_index": 6,
          "section_id": "xia2024:sec5",
          "section_index": 5
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s363",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "For GPU memory\ncapacities of 100GB and 120GB, our model predicts that\nthe maximum batch sizes supported for fine-tuning Mixtral\nwill be 28 and 35, respectively.",
        "score": 0.6961,
        "rank": 21,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 363
        }
      },
      {
        "node_id": "xia2024:sec9:p12",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "0 20 40 60 80 100 1200\n5\n10\n15\n20\n25\n30\n35\n40\nA100-40GB\nA100-80GB\nA40\nH100\nbsz=28\nbsz=35\nProjected GPU capacity\nGround Truth Projection\nMax batch size\nGPU DRAM capacity\nFig. 13. Projected maximum batch size of Mixtral for different GPUs.\namount of GPU memory, and need to be subtracted in the\nanalytical model. Fig. 8 supports this by showing that on the\nsame dataset, BlackMamba can support larger batch size than\nMixtral because of its smaller model size.\nMoreover, the sequence length and sparsity also affect the\nmaximum batch size. Because the sparsity only affects the\nMoE part of the LLM, we multiply its influence by C1,\nwhich we call MoE coefficient. We apply the sequence length\nand the sparsity in the denominator as they are inversely\nrelated to batch size. Then, we multiply the result by C0,\nthe scaling coefficient , which scales the batch size by a\nconstant. The scaling coefficient is different across LLM\nmodels, because different models have different architecture\n(\u00a7III), and generate different amounts of intermediate data\nfor each query. The scaling coefficient for BlackMamba is\nhigher than that of Mixtral because it is a smaller model.\nFinally, we use floor to round it to the maximum integer.\nThe MoE coefficient and scaling coefficient vary across\nmodels. These coefficients are independent of GPU microar-\nchitectural parameters. We find the maximum batch size for\nboth LLM models on NVIDIA A40 (48GB), A100 (40GB),\nA100 (80GB), and H100 (80GB), and apply our model to\nfind the optimal coefficients. For Mixtral, C0 = 82 and\nC1 = 0 .95, and for BlackMamba, C0 = 83 and C1 = 0 .88.\nWhile we showcase these parameters for the models eval-\nuated, \u00a7V-D discusses how to generalize this approach for\nother models.\nUsing our analytical model, we demonstrate the maximum\nbatch sizes for fine-tuning on four different NVIDIA GPUs:\nA40, A100-40GB, A100-80GB and H100 with memory\ncapacities of 48GB, 40GB, 80GB, and 80GB, respectively.\nFig. 13 shows our projected maximum batch size and corre-\nlate it with experimented ground truth. While the maximum\nmemory capacity available in NVIDIA GPUs today is 80GB,\nwe use our analytical model to project the maximum batch\nsize that future GPUs might support. For GPU memory\ncapacities of 100GB and 120GB, our model predicts that\nthe maximum batch sizes supported for fine-tuning Mixtral\nwill be 28 and 35, respectively. Due to space limitations, we\nonly show the projection of Mixtral model.\nTABLE IV\nESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE\nBASED ON OUR ANALYTICAL MODEL\nGPU Mem MBS Throughput Cost ($/hr) Cost ($)\nA40 48GB 4 1.01 0.79 32.7\nA100 80GB 17 2.74 1.67 25.4\nH100 80GB 17 4.90 2.1 17.9\nB. Estimating Throughput\nAs discussed in \u00a7IV-B4, when the batch size increases, the\nLLM fine-tuning gradually switches from memory bound to\ncompute bound. When the compute resources are abundant,\nthe throughput increases almost linearly with batch size.\nHowever, when compute resources become constrained, the\nthroughput improvement gradually saturates. We model this\nbehavior using a logarithmic relation between batch size and\nthroughput. Our analytical model for maximum batch size is\nshown in (2).\nT hroughput = C2 \u2217 log( batch size\nsparsity \u2217 C3 ) + C4 (2)\nIn the equation, in addition to the basic logarithmic part, we\nhave three coefficients C2, C3, and C4. C2 is the scaling\ncoefficient, which depends on the LLM model, GPU archi-\ntecture, and the dataset. The higher the compute capability a\nGPU can provide, and the lower the LLM model and dataset\ncompute requirement is, the higher the scaling coefficient will\nbe. C3 is the MoE attenuation coefficient , which tunes how\nmuch the MoE sparsity affects the throughput. MoE sparsity\nonly affects the MoE part in LLM model, and thus should\nbe attenuated to avoid over compensation. This coefficient is\nonly LLM model dependent, because once the model is fixed,\nthe influence of sparsity is determined. C4 is the intercept,\nconceptually it equals to the throughput when batch size\nequals one, because the logarithmic part in (2) is zero when\nbatch size is one. Using scipy [32] to fit the model and\ngenerate four sets (C2, C3, C4), for each model and dataset\ncombination.\nTo estimate the accuracy of this model, we correlate the\nmodel output with experimental data from our study. Fig. 14\nshows this correlation study, where discrete data points (dots)\nrepresent experimental values, and the line represents output\nof our analytical model. We use both dense and sparse\nMixtral and BlackMamba for both datasets used in our study.\nThe figure clearly shows that our model accurately predicts\nLLM fine-tuning throughput with a Root Mean Squared Error\n(RMSE) of less than 0.8. Fig. 15 shows the correlation study\nof the analytical model of three other GPUs, A100 (40GB),\nA100 (80GB), and H100. The RMSE is less than 0.6, close\nto that of A40.\nC. Estimating the Total Fine-Tuning Cost\nUsing the throughput estimation, we calculate the cost\nof fine-tuning LLMs for different GPUs. The cost of GPU",
        "score": 0.6961,
        "rank": 21,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 9,
          "paragraph_index": 12,
          "section_id": "xia2024:sec9",
          "section_index": 9
        }
      },
      {
        "node_id": "xia2024:sec7:p9:s260",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "for sparse fine-tuning,\nwe use the two batch sizes for dense fine-tuning, and the\nmaximum batch size that fits into memory.",
        "score": 0.6952,
        "rank": 22,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec7:p9",
          "section_id": "xia2024:sec7",
          "sentence_index": 260
        }
      },
      {
        "node_id": "xia2024:sec7:p9",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "matmul(w2)w2_dequantmatmul(w3)w3_dequantmatmul(w1)w1_dequant softmax topk\nmatmul(router)router_dequanttime_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=10)\nSparse(bsz=1)\nSparse(bsz=10)\nSparse(bsz=32)\nmatmul(w1)\ngelu\nmatmul(w2)elementwise_mult\ntop_k sigmoid\nmatmul_(router) time_weighted\n0\n25\n50\n75\n100\nDense(bsz=1)\nDense(bsz=30)\nSparse(bsz=1)\nSparse(bsz=30)\nSparse(bsz=84)\nMixtral\nMamba\n   DRAM Bandwidth Utilization (%)\nFig. 10. GPU DRAM bandwidth utilization of different kernels in the MoE layer for different batch sizes.\nweighted by the amount of time each kernel takes. We\nuse a sequence length of 128 (\u00a7III). Sequence length will\ninfluence the choice of batch size, and we discuss the effects\nof sequence length on runtime, throughput, SM utilization,\nand memory utilization in \u00a7IV-B6. For dense fine-tuning, we\nshow the SM utilization of batch size 1 and the maximum\nbatch size that fits into memory; for sparse fine-tuning,\nwe use the two batch sizes for dense fine-tuning, and the\nmaximum batch size that fits into memory.\nFig. 9 shows the SM utilization of different kernels in the\nMoE layer, which offers the following insights. (1) For both\nsparse and dense fine-tuning, SM utilization increases with\nbatch size due to higher parallelism and GPU activity. (2)\nSparse fine-tuning has lower SM utilization than dense fine-\ntuning at the same batch size because it activates only 2\nout of 8 experts, reducing parallelism. Consequently, sparse\nfine-tuning supports a higher maximum batch size. Both\nachieve similar maximum SM utilization at their peak batch\nsizes. (3) The de-quantization kernel maintains high SM\nutilization regardless of batch size. (4) Matrix multiplication\nkernels achieve higher SM utilization with larger batch sizes,\nleveraging the GPU\u2019s parallel processing capabilities.\nMemory resource utilization study. Fig. 10 shows the\nkernel-level breakdown of GPU memory bandwidth utiliza-\ntion. We use the same experimental setup as in the evalua-\ntion of SM utilization, and find the following insights. (1)\nFor both sparse and dense fine-tuning, the time-weighted\nmemory utilization decreases with increasing batch size.\nThis is because the model parameters are loaded once and\nshared by all queries in a batch. However, a larger batch\nincreases the execution time (as discussed in \u00a7IV-B6),\nleading to a lower average memory bandwidth utilization.\nHE HE_tuned GS GS_tuned0\n25\n50\n75\n100\nExpert 0\nExpert 1\nExpert 2\nExpert 3\nExpert 4\nExpert 5\nExpert 6\nExpert 7\nHE HE_tuned GS GS_tuned0\n25\n50\n75\n100                          Avg Num. of Token Per Query\nMixtral\nMamba\nvar=55.5 var=112.3\nvar=21.2\nvar=79.2\nvar=150.7 var=93.3\nvar=186.5 var=187.9\nFig. 11. Token distribution to different experts.\n(2) For the same batch size, sparse fine-tuning achieves\nhigher memory bandwidth utilization than dense fine-tuning\ndue to shorter execution times. (3) Dequant layers\u2019 memory\nutilization is batch-size-independent, while matmul layers\u2019\nutilization decreases with larger batch sizes. To maximize\nGPU memory usage, a sufficiently large batch size should be\nused. With large batch sizes, fine-tuning becomes compute-\nbound, indicating a need for improved compute resources in\nfuture hardware to better utilize memory bandwidth.\nTakeaway 5 . As the batch size increases, LLM\nfine-tuning transitions from being memory-bound to\ncompute-bound.\n5) Effect of Load Imbalance Due to Fine-Tuning: Recent\ntrends in deploying expert parallelism in MoE models have\nhighlighted load-imbalanced computation among experts as\na significant issue impacting inference and training effi-",
        "score": 0.6952,
        "rank": 22,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 7,
          "paragraph_index": 9,
          "section_id": "xia2024:sec7",
          "section_index": 7
        }
      },
      {
        "node_id": "xia2024:sec3:p4",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "parameters. We fine-tune the full BlackMamba model (i.e.,\noriginal weight matrices), whereas employed QLoRA [15]\nfor parameter-efficient fine-tuning (PEFT) on Mixtral due to\nGPU memory capacity budget. For QLoRA, we target the\nMoE layers, including the routers, and set the rank of the\nLoRA modules to 16. We enable FlashAttention2 [17] during\nMixtral fine-tuning for enhanced efficiency. Moreover, we use\ngradient checkpointing [18] to save memory usage.\nDatasets. Our fine-tuning process is implemented in Py-\nTorch using the LLaMA-Factory framework [19], with a\nlearning rate of 5e-5 and 10 epochs. Both models were fine-\ntuned on two datasets focused on different tasks: common-\nsense 15k (CS) and Math 14k (MATH), which address com-\nmonsense reasoning and arithmetic reasoning respectively\n(provided by LLM-adapters [20]). The details of datasets\nare used in Table II. For evaluation, we tested the models\non GSM8K [21] for arithmetic reasoning and HE [22] for\ncommonsense reasoning. Each dataset consists of thousands\nof queries. We define a query as the concatenation of a\nprompt and its ground-truth answer, which is feed to LLMs\nfor fine-tuning.\nProfiling experiments. We evaluate the fine-tuning pro-\ncess from both software and hardware perspectives. The\nsoftware evaluation includes an end-to-end assessment of\nthe fine-tuning process and measures the performance of\nthe two models on various tasks post-fine-tuning. Using\nPyTorch, we provide essential algorithm-level information\nsuch as test accuracy, training throughput, and layer-level\nlatency breakdown. The hardware evaluation offers a detailed\nanalysis of GPU performance. Utilizing NVIDIA Nsight\nCompute [23], we gather kernel-level information, including\nSM utilization, memory utilization, and kernel latency. These\nmetrics collectively offer a comprehensive overview of the\nmodels\u2019 performance, capturing both high-level algorithmic\nefficiency and detailed hardware utilization. Software evalu-\nation is dataset-dependent, and we will show the test accu-\nracy and fine-tuning throughput by utilizing both datasets.\nIn contrast, hardware evaluation is dataset-independent as\nthese workload characteristics do not depend on runtime\ndata. Because profiling is time-consuming (approximately\n10,000\u00d7 costlier compared to a native run without the profiler\nenabled), we manually set the batch size and sequence length\nto facilitate a more direct and efficient profiling process.\nWe present the sequence length distribution for the CS and\nMATH datasets in Fig. 2. The median sequence length is 79\nfor CS and 174 for MATH. Therefore, we select a sequence\nlength of 128 for the hardware evaluation section to achieve\nan approximate profiling effect. We also show a sensitivity\nstudy by varying sequence length to demonstrate its effect\non performance.\nGPU platform. Our study is focused on characterizing the\nLLM fine-tuning process on a resource-constrained environ-\nment. Therefore, we focus on fine-tuning these models on a\nsingle GPU. Specifically, we conduct our experiments using\n0 50 100 150 200 250 300 350 4000\n100\n200\n300\n400\n500\n0 50 100 150 200 250 300 350 4000\n20\n40\n60\n80\n100\n120\nCS\nMATH\nMedian=79\nMedian=174\nFrequency\nSequence Length\nFig. 2. Sequence length distribution for evaluated datasets.\n0 2 4 6 8 10\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nMixtral-dense-HE\nMixtral-sparse-HE\nMixtral-dense-GS\nMixtral-sparse-GS\n0 2 4 6 8 10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nBlackmamba -dense-HE\nBlackmamba -sparse-HE\nBlackmamba -dense-GS\nBlackmamba -sparse-GS\nEpoch\nAccuracy\nFig. 3. Testing accuracy of Mixtral and BlackMamba. Both models are\nevaluated on two datasets Hellaswag (HE) and GSM8K (GS), using dense\nand sparse fine-tuning.\nNVIDIA A40 GPU with Ampere architecture. The GPU has\n48GB memory. While our profiling study is based on this\nparticular GPU, we show the versatility of our analytical\nmodel by validating our model against three other GPU\nwith different sizes of compute and memory resources: (1)\nA100 GPU with 40GB memory, (2) A100 GPU with 80GB\nmemory, and (3) H100 GPU with 80GB memory. We use\nPython v3.8.10, PyTorch v2.1.0, and CUDA v11.8.\nIV. C HARACTERIZATION STUDY\nUsing the experimental setup discussed above, next, we\nconduct an in-depth characterization of LLM fine-tuning to\nunderstand both accuracy and runtime behaviors.\nA. Analysis of Model Trainability\nWe first evaluate if fine-tuning sparse LLM models can\nachieve the desired accuracy levels. Pre-trained models show\nlow accuracy: HE and GS have under 25% on Mixtral and",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 3,
          "paragraph_index": 4,
          "section_id": "xia2024:sec3",
          "section_index": 3
        }
      },
      {
        "node_id": "xia2024:sec3",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Page 3",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 3,
          "section_index": 3
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s86",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "parameters.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 86
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s87",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We fine-tune the full BlackMamba model (i.e.,\noriginal weight matrices), whereas employed QLoRA [15]\nfor parameter-efficient fine-tuning (PEFT) on Mixtral due to\nGPU memory capacity budget.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 87
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s88",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "For QLoRA, we target the\nMoE layers, including the routers, and set the rank of the\nLoRA modules to 16.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 88
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s89",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We enable FlashAttention2 [17] during\nMixtral fine-tuning for enhanced efficiency.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 89
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s90",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Moreover, we use\ngradient checkpointing [18] to save memory usage.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 90
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s91",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Datasets.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 91
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s92",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Our fine-tuning process is implemented in Py-\nTorch using the LLaMA-Factory framework [19], with a\nlearning rate of 5e-5 and 10 epochs.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 92
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s93",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Both models were fine-\ntuned on two datasets focused on different tasks: common-\nsense 15k (CS) and Math 14k (MATH), which address com-\nmonsense reasoning and arithmetic reasoning respectively\n(provided by LLM-adapters [20]).",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 93
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s94",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The details of datasets\nare used in Table II.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 94
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s95",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "For evaluation, we tested the models\non GSM8K [21] for arithmetic reasoning and HE [22] for\ncommonsense reasoning.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 95
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s96",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Each dataset consists of thousands\nof queries.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 96
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s97",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We define a query as the concatenation of a\nprompt and its ground-truth answer, which is feed to LLMs\nfor fine-tuning.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 97
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s98",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Profiling experiments.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 98
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s99",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We evaluate the fine-tuning pro-\ncess from both software and hardware perspectives.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 99
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s100",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The\nsoftware evaluation includes an end-to-end assessment of\nthe fine-tuning process and measures the performance of\nthe two models on various tasks post-fine-tuning.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 100
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s101",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Using\nPyTorch, we provide essential algorithm-level information\nsuch as test accuracy, training throughput, and layer-level\nlatency breakdown.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 101
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s102",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The hardware evaluation offers a detailed\nanalysis of GPU performance.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 102
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s103",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Utilizing NVIDIA Nsight\nCompute [23], we gather kernel-level information, including\nSM utilization, memory utilization, and kernel latency.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 103
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s104",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "These\nmetrics collectively offer a comprehensive overview of the\nmodels\u2019 performance, capturing both high-level algorithmic\nefficiency and detailed hardware utilization.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 104
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s105",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Software evalu-\nation is dataset-dependent, and we will show the test accu-\nracy and fine-tuning throughput by utilizing both datasets.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 105
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s106",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "In contrast, hardware evaluation is dataset-independent as\nthese workload characteristics do not depend on runtime\ndata.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 106
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s107",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Because profiling is time-consuming (approximately\n10,000\u00d7 costlier compared to a native run without the profiler\nenabled), we manually set the batch size and sequence length\nto facilitate a more direct and efficient profiling process.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 107
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s108",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We present the sequence length distribution for the CS and\nMATH datasets in Fig.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 108
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s109",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "2.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 109
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s110",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The median sequence length is 79\nfor CS and 174 for MATH.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 110
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s111",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Therefore, we select a sequence\nlength of 128 for the hardware evaluation section to achieve\nan approximate profiling effect.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 111
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s112",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We also show a sensitivity\nstudy by varying sequence length to demonstrate its effect\non performance.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 112
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s113",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "GPU platform.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 113
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s114",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Our study is focused on characterizing the\nLLM fine-tuning process on a resource-constrained environ-\nment.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 114
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s115",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Therefore, we focus on fine-tuning these models on a\nsingle GPU.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 115
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s116",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Specifically, we conduct our experiments using\n0 50 100 150 200 250 300 350 4000\n100\n200\n300\n400\n500\n0 50 100 150 200 250 300 350 4000\n20\n40\n60\n80\n100\n120\nCS\nMATH\nMedian=79\nMedian=174\nFrequency\nSequence Length\nFig.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 116
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s117",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "2.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 117
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s118",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Sequence length distribution for evaluated datasets.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 118
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s119",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "0 2 4 6 8 10\n0\n0.2\n0.4\n0.6\n0.8\n1.0\nMixtral-dense-HE\nMixtral-sparse-HE\nMixtral-dense-GS\nMixtral-sparse-GS\n0 2 4 6 8 10\n0\n0.1\n0.2\n0.3\n0.4\n0.5\nBlackmamba -dense-HE\nBlackmamba -sparse-HE\nBlackmamba -dense-GS\nBlackmamba -sparse-GS\nEpoch\nAccuracy\nFig.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 119
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s120",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "3.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 120
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s121",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Testing accuracy of Mixtral and BlackMamba.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 121
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s122",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Both models are\nevaluated on two datasets Hellaswag (HE) and GSM8K (GS), using dense\nand sparse fine-tuning.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 122
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s123",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "NVIDIA A40 GPU with Ampere architecture.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 123
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s124",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "The GPU has\n48GB memory.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 124
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s125",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "While our profiling study is based on this\nparticular GPU, we show the versatility of our analytical\nmodel by validating our model against three other GPU\nwith different sizes of compute and memory resources: (1)\nA100 GPU with 40GB memory, (2) A100 GPU with 80GB\nmemory, and (3) H100 GPU with 80GB memory.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 125
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s126",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "We use\nPython v3.8.10, PyTorch v2.1.0, and CUDA v11.8.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 126
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s127",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "IV.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 127
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s128",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "C HARACTERIZATION STUDY\nUsing the experimental setup discussed above, next, we\nconduct an in-depth characterization of LLM fine-tuning to\nunderstand both accuracy and runtime behaviors.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 128
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s129",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "A.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 129
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s130",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Analysis of Model Trainability\nWe first evaluate if fine-tuning sparse LLM models can\nachieve the desired accuracy levels.",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 130
        }
      },
      {
        "node_id": "xia2024:sec3:p4:s131",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Pre-trained models show\nlow accuracy: HE and GS have under 25% on Mixtral and",
        "score": 0.6903,
        "rank": 23,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec3:p4",
          "section_id": "xia2024:sec3",
          "sentence_index": 131
        }
      },
      {
        "node_id": "samsi2024:sec4:p5",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "responses and the total time needed to run inference over the\ninput data.\nWe monitor GPUs using the nvidia-smi [23] and\nNVIDIA DCGM [24] utilities to study GPU utilization, energy,\npower draw, etc. during our experiments. The nvidia-smi\nutility is used to capture GPU usage over time at 100ms\nintervals and the DCGM monitoring tool is used to capture\naggregate GPU energy in Joules for the rank-0 node. For a\nmulti-node, multi-GPU model, we multiply the rank-0 energy\nby the number of nodes used. Maximum power draw on GPUs\nis capped at 250 Watts unless otherwise stated. Due to limits\non resource availability, we mainly use V100 GPUs for larger-\nscale distributed experiments (i.e., for 8, 16, and 32 shards)\nand A100 GPUs for smaller scale experiments.\nInference energy metrics are calculated by combining the\ninference metrics above with the energy data collected from\nour GPUs using NVIDIA\u2019s utilities described above. Specif-\nically, energy per second is defined as the total aggregate\nGPU energy spent from a single experiment/job (across all\nshards) divided by the total run time of that experiment/job in\nseconds. A single experiment/job denotes a single run through\nall 4,096 prompts under a specified batch size. Energy per\ntoken and energy per response are similarly defined as total\nenergy divided by the number of decoded output tokens and\nthe number of responses as defined above, respectively.\nIV. R ESULTS\nA. Baselines: LLaMA 7B, 13B, & 65B\n1) Inference Performance: We begin our analysis with\na baseline comparison of LLaMA 65B with smaller-scale\nLLaMA models: LLaMA 7B and 13B. The goal is to under-\nstand the following: what do inference performance and energy\ntrade-offs look like for the different sizes of LLaMA under the\nbare-minimum set of resources required to have them running\ninference? This question can be important for researchers\nand users who have may not have limitless computational\nresources and hardware acceleration or may be constrained\nin terms of GPU memory, etc.\nGiven the sizes of the models, the size of the data, and the\nhardware memory limits, we only show results from experi-\nments that were possible for a given combination of parameters\n(i.e., for some models, certain combinations of batch size and\nnumber of shards are infeasible due to memory limits of the\nunderlying GPUs). Table II shows the bare minimum hardware\nrequirements for each LLaMA variant and the maximum batch\nsize possible for each combination, assuming no further model\ncompression, optimization, quantization, distillation etc.\nWith these limits in mind, we present the inference per-\nformance of LLaMA 7B, 13B, and 65B on the Alpaca and\nGSM8K datasets with the bare minimum hardware settings in\nFigure 2. The plots in Figure 2 show a baseline comparison of\ninference performance of the three LLaMA variants on both\nthe V100 and A100 GPUs respectively. For each model, in\nline with the spirit of the bare minimum settings, inference\nis done with a batch size of 64 and an maximum generation\nlength of 256. The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256. These limits are imposed by a combination of GPU\nmemory, model size, response length and the number of GPUs.\nWhile the 65B model can sharded across 6 V100 GPUs, we\nuse 8 since the model architecture makes it better suited for\nbalanced sharding across 8 GPUs.\nModel Size V100 32GB A100 80GB\nCount Max. Batch size Count Max. Batch size\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n13B on two GPUs in each case whereas the 65B model was\nrun on 8 V100 GPUs and 4 A100 GPUs respectively due to\nthe size of the model and available memory on the GPU(s).\nAs expected, we observe that the A100 outperforms V100\non both the Alpaca and GSM8K datasets: particularly for the\nsmaller LLaMA 7B and 13B, we see anywhere from a 2\ntimes (7B) to a 1.25 times increase (13B) in inference latency\non the A100 when compared to the V100 across words per\nsecond, tokens per second, and responses per second. Faster\nresponse rates and inference are likely due to the fact that\nthe number of computations, directly related to the number\nof parameters of said model, involved in the 7B and 13B\nmodels are significantly lower than the 65B model. We do note\nthat for LLaMA 65B, we see a much smaller improvement in\nusing the A100 over the V100; however, since the 65B model\nrequires sharding across two (A100) or four (V100) compute\nnodes at the mininum, this could result in additional latency\nto each forward pass of the model, explaining the smaller\nimprovements. We also observe that while LLaMA 7B exhibits\na considerable improvement in inference throughput on both\nAlpaca and GSM8K with the A100, the improvement is much\nlarger for Alpaca than GSM8K. This can also be attributed to\nthe different complexities of inputs from each dataset.\n2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above. For both the Alpaca and\nGSM8K datasets, we see that there is a considerable increase\nin the energy per second across all LLaMA sizes when using\nthe A100 over the V100 where the most considerable increase\nis for the smallest 7B model. Although Figure 2 shows a\nconsiderable increase in inference throughput from using the\nA100, Figure 3 shows us that this improvement does not come\nfor free: it comes at an increased energy cost per second.\nMoreover, for the largest LLaMA 65B, it is less clear whether\nthe increased inference energy per second (Figure 3) is worth\nthe small improvement in inference throughput in terms of\nwords/token/responses per second (Figure 2).",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "page": 4,
          "paragraph_index": 5,
          "section_id": "samsi2024:sec4",
          "section_index": 4
        }
      },
      {
        "node_id": "samsi2024:sec4",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Page 4",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "page": 4,
          "section_index": 4
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s112",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "responses and the total time needed to run inference over the\ninput data.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 112
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s113",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "We monitor GPUs using the nvidia-smi [23] and\nNVIDIA DCGM [24] utilities to study GPU utilization, energy,\npower draw, etc.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 113
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s114",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "during our experiments.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 114
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s115",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The nvidia-smi\nutility is used to capture GPU usage over time at 100ms\nintervals and the DCGM monitoring tool is used to capture\naggregate GPU energy in Joules for the rank-0 node.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 115
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s116",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "For a\nmulti-node, multi-GPU model, we multiply the rank-0 energy\nby the number of nodes used.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 116
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s117",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Maximum power draw on GPUs\nis capped at 250 Watts unless otherwise stated.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 117
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s118",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Due to limits\non resource availability, we mainly use V100 GPUs for larger-\nscale distributed experiments (i.e., for 8, 16, and 32 shards)\nand A100 GPUs for smaller scale experiments.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 118
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s119",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Inference energy metrics are calculated by combining the\ninference metrics above with the energy data collected from\nour GPUs using NVIDIA\u2019s utilities described above.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 119
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s120",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Specif-\nically, energy per second is defined as the total aggregate\nGPU energy spent from a single experiment/job (across all\nshards) divided by the total run time of that experiment/job in\nseconds.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 120
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s121",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "A single experiment/job denotes a single run through\nall 4,096 prompts under a specified batch size.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 121
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s122",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Energy per\ntoken and energy per response are similarly defined as total\nenergy divided by the number of decoded output tokens and\nthe number of responses as defined above, respectively.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 122
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s123",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "IV.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 123
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s124",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "R ESULTS\nA.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 124
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s125",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Baselines: LLaMA 7B, 13B, & 65B\n1) Inference Performance: We begin our analysis with\na baseline comparison of LLaMA 65B with smaller-scale\nLLaMA models: LLaMA 7B and 13B.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 125
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s126",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The goal is to under-\nstand the following: what do inference performance and energy\ntrade-offs look like for the different sizes of LLaMA under the\nbare-minimum set of resources required to have them running\ninference?",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 126
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s127",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "This question can be important for researchers\nand users who have may not have limitless computational\nresources and hardware acceleration or may be constrained\nin terms of GPU memory, etc.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 127
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s128",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Given the sizes of the models, the size of the data, and the\nhardware memory limits, we only show results from experi-\nments that were possible for a given combination of parameters\n(i.e., for some models, certain combinations of batch size and\nnumber of shards are infeasible due to memory limits of the\nunderlying GPUs).",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 128
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s129",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Table II shows the bare minimum hardware\nrequirements for each LLaMA variant and the maximum batch\nsize possible for each combination, assuming no further model\ncompression, optimization, quantization, distillation etc.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 129
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s130",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "With these limits in mind, we present the inference per-\nformance of LLaMA 7B, 13B, and 65B on the Alpaca and\nGSM8K datasets with the bare minimum hardware settings in\nFigure 2.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 130
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s131",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The plots in Figure 2 show a baseline comparison of\ninference performance of the three LLaMA variants on both\nthe V100 and A100 GPUs respectively.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 131
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s132",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "For each model, in\nline with the spirit of the bare minimum settings, inference\nis done with a batch size of 64 and an maximum generation\nlength of 256.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 132
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s133",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 133
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s134",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "These limits are imposed by a combination of GPU\nmemory, model size, response length and the number of GPUs.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 134
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s135",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "While the 65B model can sharded across 6 V100 GPUs, we\nuse 8 since the model architecture makes it better suited for\nbalanced sharding across 8 GPUs.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 135
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s136",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Model Size V100 32GB A100 80GB\nCount Max.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 136
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s137",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Batch size Count Max.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 137
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s138",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Batch size\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n13B on two GPUs in each case whereas the 65B model was\nrun on 8 V100 GPUs and 4 A100 GPUs respectively due to\nthe size of the model and available memory on the GPU(s).",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 138
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s139",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "As expected, we observe that the A100 outperforms V100\non both the Alpaca and GSM8K datasets: particularly for the\nsmaller LLaMA 7B and 13B, we see anywhere from a 2\ntimes (7B) to a 1.25 times increase (13B) in inference latency\non the A100 when compared to the V100 across words per\nsecond, tokens per second, and responses per second.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 139
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s140",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Faster\nresponse rates and inference are likely due to the fact that\nthe number of computations, directly related to the number\nof parameters of said model, involved in the 7B and 13B\nmodels are significantly lower than the 65B model.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 140
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s141",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "We do note\nthat for LLaMA 65B, we see a much smaller improvement in\nusing the A100 over the V100;",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 141
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s142",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "however, since the 65B model\nrequires sharding across two (A100) or four (V100) compute\nnodes at the mininum, this could result in additional latency\nto each forward pass of the model, explaining the smaller\nimprovements.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 142
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s143",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "We also observe that while LLaMA 7B exhibits\na considerable improvement in inference throughput on both\nAlpaca and GSM8K with the A100, the improvement is much\nlarger for Alpaca than GSM8K.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 143
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s144",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "This can also be attributed to\nthe different complexities of inputs from each dataset.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 144
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s145",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 145
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s146",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "For both the Alpaca and\nGSM8K datasets, we see that there is a considerable increase\nin the energy per second across all LLaMA sizes when using\nthe A100 over the V100 where the most considerable increase\nis for the smallest 7B model.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 146
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s147",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Although Figure 2 shows a\nconsiderable increase in inference throughput from using the\nA100, Figure 3 shows us that this improvement does not come\nfor free: it comes at an increased energy cost per second.",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 147
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s148",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Moreover, for the largest LLaMA 65B, it is less clear whether\nthe increased inference energy per second (Figure 3) is worth\nthe small improvement in inference throughput in terms of\nwords/token/responses per second (Figure 2).",
        "score": 0.6731,
        "rank": 24,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 148
        }
      },
      {
        "node_id": "kim2025:sec8:p8",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "and statistical analysis. Our system is built on top of FlexGen,\na cutting-edge framework for LLM inference that provides\nrobust KV cache offloading capabilities. A key advantage\nof InferSave is its minimal computational overhead and\nexceptional speed in determining optimal resource config-\nurations. Once user parameters and SLO requirements are\nprovided, our system quickly performs TPS predictions and\ncost-efficiency calculations, enabling rapid and precise GPU\ninstance recommendations. The complete source code of\nInferSave, along with all associated tools and algorithms,\nis publicly available for download at https://github.com/lass-\nlab/InferSave.\nVI. Evaluation\nA. Experimental setup\nFor our evaluation, we conducted two contrasting inference\ntasks representative of online and offline inference scenarios\nto comprehensively assess the impact of offloading strategies\non cost and performance across various cloud-based GPU\ninstances. The objective of the evaluation is to quantitatively\nanalyze the effects of offloading and the impacts it has\non cost and performance efficiency, as well as to pick the\noptimal instance given a SLO as input. Online inferencing\nfocuses on finding the most price-effective inference while\nmeeting the strict SLO requirement, while offline inferencing\nrelaxes the SLO requirement, allowing for strategies such as\noffloading and used lower priced instances. All experiments\nwere performed 3 times for each instance to maintain result\nintegrity, and the average of each result were used for analysis.\nWorkload Definition: For a holistic evaluation of\nInferSave\u2019s ability to select the optimal instance in a variety\nof scenarios, we perform two contrasting inference workloads.\n\u2022 Online Inference workload: To model a real-time chatbot\nsystem, we use a pattern of 128 input tokens and a 512\noutput tokens. This simulates a common AI LLM chatbot\nscenario of a user asking short questions, with the chatbot\nproviding detailed answers. The workload evaluates a total\nof 3000 requests.\n\u2022 Offline Inference workload: To model a batch processing\ntask, an input size of 1024 tokens and an output size\nof 128 tokens was used. This takes into account tasks\nsuch as document summarization and data wrangling. To\nsimulate a batch processing task, the workload evaluates\nthe performance of completing 1000 requests.\nA WS Cloud Experiment Setup : To maintain uniform\nexperimental conditions and reduce potential disruptions\ncaused by fluctuating cloud workloads, all experiments were\ncarried out on AWS in the us-east-1 (Northern Virginia)\nregion between 9:00 AM and 10:00 PM KST, spanning\nthe period from December 2024 to March 2025. To avoid\nperformance variations due to regional resource contention,\ntesting was evenly distributed across availability zones us-\neast-1a through us-east-1f. For the GPU-VMs, we utilized\ng4dn.xlarge(NVIDIA T4), g5.xlarge(NVIDIA A10G),\ng6.xlarge(NVIDIA L4) and g6e.xlarge(NVIDIA L40s)\nA detailed specification of the instances are specified in Table\nIV.\nTABLE IV\nSpecifications of VM instances, including 4 GPU-VMs based on\nA WS specifications.\nInstanceGPU-TypeOn-Demand PriceGPU MemoryFP16 PCIe B/W($/hr) (GB) (TFLOPS)(GB/s)\ng6e.xlarge L40s 2.699 48 91.61 12g6.xlarge L4 1.167 24 30.29 12g5.xlarge A10G 1.466 24 31.52 12g4dn.xlargeT4 0.71 16 8.24 6\nTo validate the effectiveness of InferSave, major\ntransformer-based LLM models such as OPT-1.3B, OPT-2.7B,\nOPT-6.7B were used for testing in an in-house benchmark\nsuite. To find the optimal performance configuration, tests\nwere conducted by varying the batch size from 1 to 64 under\ndifferent conditions for single GPU processing.\nPolicy To Select Instance : As stated in Section II-D,\nthere are no clear state of the art methodologies for GPU\ninstance selection for inferencing. Therefore, in our evaluation,\nwe compared the following two baseline approaches with\nInferSave.\n\u2022 Most powerful instance(Max-Performance) : This policy\nsimply chooses the GPU instance that offers the most per-\nformance, and aims to lower latency and raise throughput\nas much as possible. However, this methodology does not\ntake into consideration price, and therefore running costs\ncan be raised needlessly.\n\u2022 Simple performance prediction( InferSave (without\nKV Cache offloading)) : This policy uses theoretical\nperformance metrics (FLOPS, memory bandwidth) to predict\nperformance and select an instance. However, it does not\ntake into consideration the effects of KV Cache offloading,\nand may not be able to find the most optimal instance.\nB. CTCF Validation\nInferSave proposes the Compute Time Calibration Func-\ntion (CTCF) to accurately determine the optimal instance\nbased on user requirements. To validate the accuracy of\nCTCF, experiments were conducted on two GPU instances,\ng4dn.xlarge and g6.xlarge. The experiments utilized the OPT-\n2.7B model, with an input token length of 512 and an output\ntoken length of 128. The model\u2019s key computational units,\nincluding a hidden size of 2560 and an intermediate size of\n2560 \u00d7 4, were applied, and the total number of layers (32)\nwas incorporated to measure computation time. For FLOPS\nestimation, the theoretical FLOPS values provided by GPU\nmanufacturers were used: g4dn.xlarge with NVIDIA T4 (8.24\nTFLOPS) and g6.xlarge with NVIDIA L4 (30.29 TFLOPS).\nAfter applying CTCF, the corrected prediction times were\ncomputed and compared with actual measurements to analyze\nthe error rate. As shown in Figure 4, the CTCF-adjusted values\nclosely matched the actual measurements. Specifically, in the\nDecode stage of g4dn.xlarge, the corrected values exhibited\nan average error rate of 1% compared to actual measurements,\nwhile in the Prefill stage of g6.xlarge, the average error rate",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "page": 8,
          "paragraph_index": 8,
          "section_id": "kim2025:sec8",
          "section_index": 8
        }
      },
      {
        "node_id": "kim2025:sec8",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "Page 8",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "page": 8,
          "section_index": 8
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s243",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "and statistical analysis.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 243
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s244",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "Our system is built on top of FlexGen,\na cutting-edge framework for LLM inference that provides\nrobust KV cache offloading capabilities.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 244
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s245",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "A key advantage\nof InferSave is its minimal computational overhead and\nexceptional speed in determining optimal resource config-\nurations.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 245
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s246",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "Once user parameters and SLO requirements are\nprovided, our system quickly performs TPS predictions and\ncost-efficiency calculations, enabling rapid and precise GPU\ninstance recommendations.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 246
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s247",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "The complete source code of\nInferSave, along with all associated tools and algorithms,\nis publicly available for download at https://github.com/lass-\nlab/InferSave.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 247
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s248",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "VI.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 248
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s249",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "Evaluation\nA.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 249
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s250",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "Experimental setup\nFor our evaluation, we conducted two contrasting inference\ntasks representative of online and offline inference scenarios\nto comprehensively assess the impact of offloading strategies\non cost and performance across various cloud-based GPU\ninstances.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 250
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s251",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "The objective of the evaluation is to quantitatively\nanalyze the effects of offloading and the impacts it has\non cost and performance efficiency, as well as to pick the\noptimal instance given a SLO as input.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 251
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s252",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "Online inferencing\nfocuses on finding the most price-effective inference while\nmeeting the strict SLO requirement, while offline inferencing\nrelaxes the SLO requirement, allowing for strategies such as\noffloading and used lower priced instances.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 252
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s253",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "All experiments\nwere performed 3 times for each instance to maintain result\nintegrity, and the average of each result were used for analysis.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 253
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s254",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "Workload Definition: For a holistic evaluation of\nInferSave\u2019s ability to select the optimal instance in a variety\nof scenarios, we perform two contrasting inference workloads.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 254
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s255",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "\u2022 Online Inference workload: To model a real-time chatbot\nsystem, we use a pattern of 128 input tokens and a 512\noutput tokens.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 255
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s256",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "This simulates a common AI LLM chatbot\nscenario of a user asking short questions, with the chatbot\nproviding detailed answers.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 256
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s257",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "The workload evaluates a total\nof 3000 requests.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 257
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s258",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "\u2022 Offline Inference workload: To model a batch processing\ntask, an input size of 1024 tokens and an output size\nof 128 tokens was used.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 258
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s259",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "This takes into account tasks\nsuch as document summarization and data wrangling.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 259
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s260",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "To\nsimulate a batch processing task, the workload evaluates\nthe performance of completing 1000 requests.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 260
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s261",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "A WS Cloud Experiment Setup : To maintain uniform\nexperimental conditions and reduce potential disruptions\ncaused by fluctuating cloud workloads, all experiments were\ncarried out on AWS in the us-east-1 (Northern Virginia)\nregion between 9:00 AM and 10:00 PM KST, spanning\nthe period from December 2024 to March 2025.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 261
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s262",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "To avoid\nperformance variations due to regional resource contention,\ntesting was evenly distributed across availability zones us-\neast-1a through us-east-1f.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 262
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s263",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "For the GPU-VMs, we utilized\ng4dn.xlarge(NVIDIA T4), g5.xlarge(NVIDIA A10G),\ng6.xlarge(NVIDIA L4) and g6e.xlarge(NVIDIA L40s)\nA detailed specification of the instances are specified in Table\nIV.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 263
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s264",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "TABLE IV\nSpecifications of VM instances, including 4 GPU-VMs based on\nA WS specifications.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 264
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s265",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "InstanceGPU-TypeOn-Demand PriceGPU MemoryFP16 PCIe B/W($/hr) (GB) (TFLOPS)(GB/s)\ng6e.xlarge L40s 2.699 48 91.61 12g6.xlarge L4 1.167 24 30.29 12g5.xlarge A10G 1.466 24 31.52 12g4dn.xlargeT4 0.71 16 8.24 6\nTo validate the effectiveness of InferSave, major\ntransformer-based LLM models such as OPT-1.3B, OPT-2.7B,\nOPT-6.7B were used for testing in an in-house benchmark\nsuite.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 265
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s266",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "To find the optimal performance configuration, tests\nwere conducted by varying the batch size from 1 to 64 under\ndifferent conditions for single GPU processing.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 266
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s267",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "Policy To Select Instance : As stated in Section II-D,\nthere are no clear state of the art methodologies for GPU\ninstance selection for inferencing.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 267
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s268",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "Therefore, in our evaluation,\nwe compared the following two baseline approaches with\nInferSave.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 268
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s269",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "\u2022 Most powerful instance(Max-Performance) : This policy\nsimply chooses the GPU instance that offers the most per-\nformance, and aims to lower latency and raise throughput\nas much as possible.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 269
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s270",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "However, this methodology does not\ntake into consideration price, and therefore running costs\ncan be raised needlessly.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 270
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s271",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "\u2022 Simple performance prediction( InferSave (without\nKV Cache offloading)) : This policy uses theoretical\nperformance metrics (FLOPS, memory bandwidth) to predict\nperformance and select an instance.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 271
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s272",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "However, it does not\ntake into consideration the effects of KV Cache offloading,\nand may not be able to find the most optimal instance.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 272
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s273",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "B.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 273
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s274",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "CTCF Validation\nInferSave proposes the Compute Time Calibration Func-\ntion (CTCF) to accurately determine the optimal instance\nbased on user requirements.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 274
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s275",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "To validate the accuracy of\nCTCF, experiments were conducted on two GPU instances,\ng4dn.xlarge and g6.xlarge.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 275
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s276",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "The experiments utilized the OPT-\n2.7B model, with an input token length of 512 and an output\ntoken length of 128.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 276
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s277",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "The model\u2019s key computational units,\nincluding a hidden size of 2560 and an intermediate size of\n2560 \u00d7 4, were applied, and the total number of layers (32)\nwas incorporated to measure computation time.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 277
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s278",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "For FLOPS\nestimation, the theoretical FLOPS values provided by GPU\nmanufacturers were used: g4dn.xlarge with NVIDIA T4 (8.24\nTFLOPS) and g6.xlarge with NVIDIA L4 (30.29 TFLOPS).",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 278
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s279",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "After applying CTCF, the corrected prediction times were\ncomputed and compared with actual measurements to analyze\nthe error rate.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 279
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s280",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "As shown in Figure 4, the CTCF-adjusted values\nclosely matched the actual measurements.",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 280
        }
      },
      {
        "node_id": "kim2025:sec8:p8:s281",
        "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
        "text": "Specifically, in the\nDecode stage of g4dn.xlarge, the corrected values exhibited\nan average error rate of 1% compared to actual measurements,\nwhile in the Prefill stage of g6.xlarge, the average error rate",
        "score": 0.6679,
        "rank": 25,
        "metadata": {
          "document_id": "kim2025",
          "document_title": "Cost-Efficient LLM Serving in the Cloud: VM Selection with KV Cache Offloading",
          "paragraph_id": "kim2025:sec8:p8",
          "section_id": "kim2025:sec8",
          "sentence_index": 281
        }
      },
      {
        "node_id": "jegham2025:sec5:p6",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Li + Output Length\nRi\n3600\n!\n| {z }\nTotal inference time (Ti, hours)\n\u00d7\n\uf8ee\n\uf8ef\uf8f0PGPU \u00d7 UGPU,{min,max}| {z }\nGPU power (kW)\n+ Pnon-GPU \u00d7 Unon-GPU| {z }\nNon-GPU power (kW)\n\uf8f9\n\uf8fa\uf8fb \u00d7 PUE\n(1)\nWe also define an expected per-query energy as a weighted combination of both scenarios (wmax =\n0.5), and the framework aggregates all Monte Carlo draws to produce a distribution of per-query\nenergy outcomes. The final metrics are reported as the sample mean and standard deviation:\nEi,exp = wmaxEi,max + (1 \u2212 wmax)Ei,min, \u00afEquery = E[Ei,exp], \u03c3 Equery =\nq\nVar[Ei,exp] (2)\nThis stochastic formulation captures variability in runtime, hardware utilization, and data-center\nefficiency, enabling robust and reproducible estimation of per-query energy consumption across\ndiverse inference conditions.\n4.3 Hardware-Class Attribution\nWe stratify LLMs into five hardware classes based on model size: Nano (<7B), Micro (7\u201320B),\nSmall (20\u201340B), Medium (40\u201370B), and Large (>70B), assigning 1, 2, 4, or 8 GPUs accordingly.\nModels that do not disclose parameter counts, such as OpenAI and Anthropic flagship models (e.g.,\nGPT-4o, Claude-3.7 Sonnet), are classified as Large, OpenAI Mini variants (e.g., GPT-4o mini)\nas Medium, and models labeled \u201cNano\u201d such as GPT-4.1 nano asSmall based on reported model\nperformance (e.g., TPS, latency, and reasoning capabilities) [51].\nAI companies and cloud providers typically rely on dynamic batching to optimize GPU utilization\nwhile maintaining low latency [52]. Although actual batch sizes fluctuate depending on incoming\ndemand, they are generally constrained to a narrow range below 16 to preserve responsiveness.\nBenchmarks [51] show that even for large prompts, most models maintain a first-token latency below\none second. Moreover, prior studies [53, 54] show that these latency values are consistent with batch\nsizes in the range of 4 to 16. This suggests that real-world deployments prioritize small, latency-\nsensitive batches over maximal throughput. Accordingly, we adopt a batch size of 8 for all primary\ncalculations, as it represents a practical midpoint between common deployment scenarios. A detailed\nsensitivity analysis exploring the impact of alternative batch sizes is provided in Appendix A. The\nnumber of GPUs and their allocated power draw utilization rates for H100 systems are estimated from\nSplitwise [54], the Latency Processing Unit study [55], and LLM-Inference-Bench [53]. For A100\nsystems, we adopt measurements from Patel et al. and Kakolyris et al.\u2019s work [56, 57]. Per-request\nGPU and non-GPU utilization rates are calculated as:\nUGPU total = G \u00d7 DGPU\nN \u00d7 B , U non-GPU total = G \u00d7 Dnon-GPU\nN \u00d7 B (3)\nwhere G is the number of GPUs assigned per model, N = 8 is the number of GPUs per node, and\nB = 8 is the batch size. DGPU denotes the assigned GPUs\u2019 power draw, expressed as a fraction of their\nmaximum power draw, while Dnon-GPU = 0.5 represents the conservatively assigned fixed utilization\nfraction for non-GPU components (e.g., CPU, memory, storage, cooling), relative to their peak power\ndraw [32]. We exclude idle power consumption from unutilized GPUs in partially loaded nodes,\nas deployment-specific telemetry is unavailable to determine whether such capacity is reassigned,\nload-balanced, or remains idle. Table 2 summarizes GPU and non-GPU power utilization rates across\nmodel classes. Values are rounded to typical intervals observed during inference, accounting for input\nprocessing spikes, output length, decoding complexity, and a batch size of 8 parallel requests.\n5",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "page": 5,
          "paragraph_index": 6,
          "section_id": "jegham2025:sec5",
          "section_index": 5
        }
      },
      {
        "node_id": "jegham2025:sec5",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Page 5",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "page": 5,
          "section_index": 5
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s108",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Li + Output Length\nRi\n3600\n!",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 108
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s109",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "| {z }\nTotal inference time (Ti, hours)\n\u00d7\n\uf8ee\n\uf8ef\uf8f0PGPU \u00d7 UGPU,{min,max}| {z }\nGPU power (kW)\n+ Pnon-GPU \u00d7 Unon-GPU| {z }\nNon-GPU power (kW)\n\uf8f9\n\uf8fa\uf8fb \u00d7 PUE\n(1)\nWe also define an expected per-query energy as a weighted combination of both scenarios (wmax =\n0.5), and the framework aggregates all Monte Carlo draws to produce a distribution of per-query\nenergy outcomes.",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 109
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s110",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "The final metrics are reported as the sample mean and standard deviation:\nEi,exp = wmaxEi,max + (1 \u2212 wmax)Ei,min, \u00afEquery = E[Ei,exp], \u03c3 Equery =\nq\nVar[Ei,exp] (2)\nThis stochastic formulation captures variability in runtime, hardware utilization, and data-center\nefficiency, enabling robust and reproducible estimation of per-query energy consumption across\ndiverse inference conditions.",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 110
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s111",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "4.3 Hardware-Class Attribution\nWe stratify LLMs into five hardware classes based on model size: Nano (<7B), Micro (7\u201320B),\nSmall (20\u201340B), Medium (40\u201370B), and Large (>70B), assigning 1, 2, 4, or 8 GPUs accordingly.",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 111
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s112",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Models that do not disclose parameter counts, such as OpenAI and Anthropic flagship models (e.g.,\nGPT-4o, Claude-3.7 Sonnet), are classified as Large, OpenAI Mini variants (e.g., GPT-4o mini)\nas Medium, and models labeled \u201cNano\u201d such as GPT-4.1 nano asSmall based on reported model\nperformance (e.g., TPS, latency, and reasoning capabilities) [51].",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 112
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s113",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "AI companies and cloud providers typically rely on dynamic batching to optimize GPU utilization\nwhile maintaining low latency [52].",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 113
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s114",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Although actual batch sizes fluctuate depending on incoming\ndemand, they are generally constrained to a narrow range below 16 to preserve responsiveness.",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 114
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s115",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Benchmarks [51] show that even for large prompts, most models maintain a first-token latency below\none second.",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 115
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s116",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Moreover, prior studies [53, 54] show that these latency values are consistent with batch\nsizes in the range of 4 to 16.",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 116
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s117",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "This suggests that real-world deployments prioritize small, latency-\nsensitive batches over maximal throughput.",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 117
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s118",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Accordingly, we adopt a batch size of 8 for all primary\ncalculations, as it represents a practical midpoint between common deployment scenarios.",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 118
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s119",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "A detailed\nsensitivity analysis exploring the impact of alternative batch sizes is provided in Appendix A.",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 119
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s120",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "The\nnumber of GPUs and their allocated power draw utilization rates for H100 systems are estimated from\nSplitwise [54], the Latency Processing Unit study [55], and LLM-Inference-Bench [53].",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 120
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s121",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "For A100\nsystems, we adopt measurements from Patel et al.",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 121
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s122",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "and Kakolyris et al.\u2019s work [56, 57].",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 122
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s123",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Per-request\nGPU and non-GPU utilization rates are calculated as:\nUGPU total = G \u00d7 DGPU\nN \u00d7 B , U non-GPU total = G \u00d7 Dnon-GPU\nN \u00d7 B (3)\nwhere G is the number of GPUs assigned per model, N = 8 is the number of GPUs per node, and\nB = 8 is the batch size.",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 123
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s124",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "DGPU denotes the assigned GPUs\u2019 power draw, expressed as a fraction of their\nmaximum power draw, while Dnon-GPU = 0.5 represents the conservatively assigned fixed utilization\nfraction for non-GPU components (e.g., CPU, memory, storage, cooling), relative to their peak power\ndraw [32].",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 124
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s125",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "We exclude idle power consumption from unutilized GPUs in partially loaded nodes,\nas deployment-specific telemetry is unavailable to determine whether such capacity is reassigned,\nload-balanced, or remains idle.",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 125
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s126",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Table 2 summarizes GPU and non-GPU power utilization rates across\nmodel classes.",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 126
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s127",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Values are rounded to typical intervals observed during inference, accounting for input\nprocessing spikes, output length, decoding complexity, and a batch size of 8 parallel requests.",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 127
        }
      },
      {
        "node_id": "jegham2025:sec5:p6:s128",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "5",
        "score": 0.6658,
        "rank": 26,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec5:p6",
          "section_id": "jegham2025:sec5",
          "sentence_index": 128
        }
      },
      {
        "node_id": "griggs2024:sec9:p22",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "6 Evaluation\nWe assess M\u00e9lange\u2019s performance across diverse hardware, request sizes, rates, and SLOs. M\u00e9lange\nconsistently achieves significant cost savings (up to 77%) compared to single-GPU-type strategies,\nand the selected allocations successfully attain TPOT SLO for over 99.5% of requests.\n6.1 Experiment Setup\nEnvironment. We use four NVIDIA GPU types that capture a broad range of prices and specifications,\nwith details in Tab. 1. In increasing price order, we use L4, A10G, A100-80G, and H100. To\ndetermine the GPU cost, we select the lowest on-demand price available from major cloud providers\n(AWS, Azure, and GCP). Since on-demand H100 is not offered by these major providers, we defer to\nthe pricing from RunPod [39] due to its popularity and availability. To ensure fair cost comparisons,\nwe normalize RunPod\u2019s H100 pricing to match the pricing structures of major platforms. We\ncalculate this by comparing RunPod\u2019s H100 cost ($4.69) to RunPod\u2019s A100-80G cost ($2.29), then\nadjusting relative to the A100\u2019s price on major clouds ($3.67), resulting in a normalized price of\n(4.69/2.29) \u00d7 3.67 = $7 .516 for H100. In each experiment, we serve Llama2-7b [44] with vLLM\n0.2.7 [19].\nType L4 A10G (PCIe) A100-80G (SXM) H100 (SXM)\nOn-demand Price ($/h) 0.7 1.01 3.67 7.5164\nInstance Provider GCP AWS Azure RunPod\nInstance Name g2-standard-4 g5.xlarge NC24ads_A100_v4/N.A. N.A.\nMemory (GB) 24 24 80 80\nMemory Bandwidth (GB/s) 300 600 1935 3350\nFP16 (TFLOPS) 242 125 312 1979\nTable 1: Specifications of four NVIDIA GPUs: L4, A10G, A100, and H100.\nDatasets and SLOs. We evaluate across three datasets to cover a wide range of application scenarios.\nFor short-context tasks (interactive chats) we use the Chatbot Arena dataset [55], for long-context\ntasks (document summarization) we use the PubMed dataset [ 7], and for a mixed-context-length\nsetting we create a synthetic dataset by sampling 80% from Chatbot Arena and 20% from PubMed.\nThe input and output length distributions are shown in Fig. 10. We follow standard LLM inference\nbenchmarks [3] to set reasonable TPOT SLOs, and use 40ms to simulate services where swift\nresponses are essential, and 120ms where longer response times are acceptable. Both selected SLOs\nsurpass the average human reading speed, ensuring the SLOs satisfy practical user experience.\n0 2500 5000 7500 10000 12500\nInput Length (tokens)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Fraction\nDataset\nMixed (mean=1278.04)\nArena (mean=329.43)\nPubmed (mean=4174.13)\n(a) Input length distributions.\n0 250 500 750 1000\nOutput Length (tokens)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125Fraction\nDataset\nMixed (mean=219.87)\nArena (mean=195.66)\nPubmed (mean=314.1) (b) Output length distributions.\nFigure 10: Dataset input and output length distributions.\nM\u00e9lange Configuration. Bucket size ranges correspond to Figure 5, comprising of 10 input length\nranges and 6 output length ranges (60 total buckets). The slice factor is set to 8 for a total of\n60 \u00b7 8 = 480 slices.\nBaselines. We compare M\u00e9lange to allocations that use a single GPU type. To derive baseline\nallocations, we use M\u00e9lange\u2019s ILP formulation (\u00a7 5.4.3) but restrict the solver to a single GPU type.\n9",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "page": 9,
          "paragraph_index": 22,
          "section_id": "griggs2024:sec9",
          "section_index": 9
        }
      },
      {
        "node_id": "griggs2024:sec9",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "Page 9",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "page": 9,
          "section_index": 9
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s243",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "6 Evaluation\nWe assess M\u00e9lange\u2019s performance across diverse hardware, request sizes, rates, and SLOs.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 243
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s244",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "M\u00e9lange\nconsistently achieves significant cost savings (up to 77%) compared to single-GPU-type strategies,\nand the selected allocations successfully attain TPOT SLO for over 99.5% of requests.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 244
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s245",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "6.1 Experiment Setup\nEnvironment.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 245
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s246",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "We use four NVIDIA GPU types that capture a broad range of prices and specifications,\nwith details in Tab.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 246
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s247",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "1.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 247
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s248",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "In increasing price order, we use L4, A10G, A100-80G, and H100.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 248
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s249",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "To\ndetermine the GPU cost, we select the lowest on-demand price available from major cloud providers\n(AWS, Azure, and GCP).",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 249
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s250",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "Since on-demand H100 is not offered by these major providers, we defer to\nthe pricing from RunPod [39] due to its popularity and availability.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 250
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s251",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "To ensure fair cost comparisons,\nwe normalize RunPod\u2019s H100 pricing to match the pricing structures of major platforms.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 251
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s252",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "We\ncalculate this by comparing RunPod\u2019s H100 cost ($4.69) to RunPod\u2019s A100-80G cost ($2.29), then\nadjusting relative to the A100\u2019s price on major clouds ($3.67), resulting in a normalized price of\n(4.69/2.29) \u00d7 3.67 = $7 .516 for H100.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 252
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s253",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "In each experiment, we serve Llama2-7b [44] with vLLM\n0.2.7 [19].",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 253
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s254",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "Type L4 A10G (PCIe) A100-80G (SXM) H100 (SXM)\nOn-demand Price ($/h) 0.7 1.01 3.67 7.5164\nInstance Provider GCP AWS Azure RunPod\nInstance Name g2-standard-4 g5.xlarge NC24ads_A100_v4/N.A.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 254
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s255",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "N.A.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 255
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s256",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "Memory (GB) 24 24 80 80\nMemory Bandwidth (GB/s) 300 600 1935 3350\nFP16 (TFLOPS) 242 125 312 1979\nTable 1: Specifications of four NVIDIA GPUs: L4, A10G, A100, and H100.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 256
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s257",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "Datasets and SLOs.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 257
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s258",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "We evaluate across three datasets to cover a wide range of application scenarios.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 258
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s259",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "For short-context tasks (interactive chats) we use the Chatbot Arena dataset [55], for long-context\ntasks (document summarization) we use the PubMed dataset [ 7], and for a mixed-context-length\nsetting we create a synthetic dataset by sampling 80% from Chatbot Arena and 20% from PubMed.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 259
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s260",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "The input and output length distributions are shown in Fig.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 260
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s261",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "10.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 261
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s262",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "We follow standard LLM inference\nbenchmarks [3] to set reasonable TPOT SLOs, and use 40ms to simulate services where swift\nresponses are essential, and 120ms where longer response times are acceptable.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 262
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s263",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "Both selected SLOs\nsurpass the average human reading speed, ensuring the SLOs satisfy practical user experience.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 263
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s264",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "0 2500 5000 7500 10000 12500\nInput Length (tokens)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Fraction\nDataset\nMixed (mean=1278.04)\nArena (mean=329.43)\nPubmed (mean=4174.13)\n(a) Input length distributions.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 264
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s265",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "0 250 500 750 1000\nOutput Length (tokens)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125Fraction\nDataset\nMixed (mean=219.87)\nArena (mean=195.66)\nPubmed (mean=314.1) (b) Output length distributions.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 265
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s266",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "Figure 10: Dataset input and output length distributions.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 266
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s267",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "M\u00e9lange Configuration.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 267
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s268",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "Bucket size ranges correspond to Figure 5, comprising of 10 input length\nranges and 6 output length ranges (60 total buckets).",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 268
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s269",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "The slice factor is set to 8 for a total of\n60 \u00b7 8 = 480 slices.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 269
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s270",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "Baselines.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 270
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s271",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "We compare M\u00e9lange to allocations that use a single GPU type.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 271
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s272",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "To derive baseline\nallocations, we use M\u00e9lange\u2019s ILP formulation (\u00a7 5.4.3) but restrict the solver to a single GPU type.",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 272
        }
      },
      {
        "node_id": "griggs2024:sec9:p22:s273",
        "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
        "text": "9",
        "score": 0.6634,
        "rank": 27,
        "metadata": {
          "document_id": "griggs2024",
          "document_title": "M\ufffdlange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity",
          "paragraph_id": "griggs2024:sec9:p22",
          "section_id": "griggs2024:sec9",
          "sentence_index": 273
        }
      },
      {
        "node_id": "samsi2024:sec3:p4",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "III. E XPERIMENTAL SETUP\nWe conducted our experiments on the MIT Supercloud\nhigh-performance computing (HPC) system [18]. This het-\nerogeneous HPC cluster consists of 448 compute nodes with\ndual Intel Xeon Gold 6248 CPUs with 384 GB of RAM\nand two NVIDIA V olta V100 GPUs with 32 GB of memory\nper node. Each node on the system has two independent\nback-end fabrics: a 100 Gb/s Intel Omnipath as well as a\n25 Gb/s Ethernet interconnect using Mellanox ConnectX-4\nadapters with all servers connected to a single, non-blocking\nArista DCS-7516 Ethernet core switch. The GPUs, Omnipath,\nand Ethernet cards are all connected to PCIe slots that route\ndirectly to the Xeon processors without any intermediary PCIe\nswitches. All experiments in this paper exclusively used the\n25 Gb/s Ethernet interconnect. The system also includes 480\nCPU-only nodes with Intel Xeon Platinum 8260 processors.\nIn addition, four nodes with NVIDIA A100 GPUs were also\navailable for experiments described in this paper. A summary\nof the hardware is shown in Table I. All experiments described\nin this paper were run exclusively on NVIDIA GPUs.\nTABLE I: Compute node configurations: This table lists\nthe types of hardware used for inference evaluations in our\nexperiments. Each node consists of 2 CPUs and 2 GPUs in\nthe configuration listed below. All GPUs are from NVIDIA.\nCPU GPU\nType Memory TDP Type Memory TDP\n(GB) (W) (GB) (W)\nIntel Xeon\nGold 6248 384 150 V100 32 250\nIntel Xeon\nPlatinum 8358 503 240 A100 80 300\nA. Models\nExperiments were performed using open-source implemen-\ntation of the pre-trained LLaMA 65B model available via\nrequest from Meta [14] and evaluation scripts available via\nGitHub [19]. This implementation of the model uses Pytorch\nand the FairScale [20] library to enable model sharding across\nmultiple GPUs and nodes. For the models, we use a decoder\ntemperature setting \u03c4 = 0 .8 and a top- p value of 0.95 in\nattempts to align our settings with the general range of values\nthat are typically used. In future work, we aim to study\nhow varying decoding temperature, top- p, and other hyper-\nparameters may affect compute performance and energy usage\nduring inference. While our main focus is on LLaMA 65B,\nwe also examine LLaMA 7B and LLaMA 13B to characterize\ninference performance and energy under the bare minimum\nsettings/resources required to run these models.\nB. Datasets\nWe used two datasets to evaluate inference performance.\nThe first is an instruction following dataset used to fine-tune\nthe Alpaca [21] model (from here on, this dataset is referred\nto as \u201cAlpaca\u201d in our paper which is not to be confused with\nthe Alpaca model). This Alpaca dataset consists of 52,000\ninstruction-following tasks, instructions/questions where some\nhave example inputs and some do not, that the model is asked\nto answer. The second dataset is GSM8K [22], consisting of\n8,500 human crafted grade school math problems. The goal\nof using these two datasets is two-fold: (1) to evaluate the\nmodel on a diverse set of tasks in NLP and (2) evaluate\nhow different types of data and its underlying dynamics\ncan impact energy and inference performance. While natural\nlanguage is more common in LLM usage and in LLM training\ndata, increasingly new capabilities have been demonstrated\nin LLMs, including the ability to solve simple mathematical\nproblems, provide/correct examples of code, and more. Math\nquestions also differ considerably from questions posed in\nnatural language which can result in smaller context windows,\ninputs/outputs of differing lengths, number of decoded tokens,\netc. This, in turn, may impact inference performance in\neither throughput rates or energy costs. For this reason, our\nbenchmarking experiments are conducted on both datasets.\nFor both datasets, we sample 4,096 inputs for our inference\nexperiments. Using the entirety of the datasets would only\nserve to increase inference time and energy used for the ex-\nperimentation unreasonably and did not provide any significant\nbenefits to the study.\nC. Evaluation\nOur goal is to evaluate the inference performance, latency,\nand inference energy costs of LLaMA 65B as a representative\nlarge language model that requires sharding across multiple\nGPUs. We intend this to be a preliminary analysis that will\nhelp guide more in-depth experiments and benchmarking for\nour future work. Our analysis also includes limited analysis of\nsmaller LLaMA variants to illustrate inference performance\nand energy trade-offs in bare-minimum hardware settings:\nnamely, LLaMA 7B and 13B. While we do not control for\nthe correctness/quality of the outputs or the complexity of the\ninputs/outputs in studying trade-offs between inference energy\nand performance, we hope to account for this as an ablative\nstudy in future work. Similarly, we do not perform a com-\nprehensive evaluation with different optimization techniques\nor inference settings available for LLMs such as modeling\nquery arrival rates, model quantization, continuous batching,\netc. which we also leave for future work.\nInference performance is measured in terms of rates: words,\ntokens, and responses per second or, equivalently, the number\nof words, tokens, and responses generated per second. When\nrunning inference with LLaMA, the model generates a string\nof text for each input until the length of the text hits a\nmaximum generation length or a stop-word is encountered.\nThe number of words are calculated by counting the number\nof words present in the output by splitting each output string\non spaces. The number of tokens is calculated using LLaMA\u2019s\nown default tokenizer by counting the number of tokens in the\ntokenized output. Lastly, the number of responses per second\nor the response rate is calculated using the total number of",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "page": 3,
          "paragraph_index": 4,
          "section_id": "samsi2024:sec3",
          "section_index": 3
        }
      },
      {
        "node_id": "samsi2024:sec3",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Page 3",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "page": 3,
          "section_index": 3
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s69",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "III.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 69
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s70",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "E XPERIMENTAL SETUP\nWe conducted our experiments on the MIT Supercloud\nhigh-performance computing (HPC) system [18].",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 70
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s71",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "This het-\nerogeneous HPC cluster consists of 448 compute nodes with\ndual Intel Xeon Gold 6248 CPUs with 384 GB of RAM\nand two NVIDIA V olta V100 GPUs with 32 GB of memory\nper node.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 71
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s72",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Each node on the system has two independent\nback-end fabrics: a 100 Gb/s Intel Omnipath as well as a\n25 Gb/s Ethernet interconnect using Mellanox ConnectX-4\nadapters with all servers connected to a single, non-blocking\nArista DCS-7516 Ethernet core switch.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 72
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s73",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The GPUs, Omnipath,\nand Ethernet cards are all connected to PCIe slots that route\ndirectly to the Xeon processors without any intermediary PCIe\nswitches.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 73
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s74",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "All experiments in this paper exclusively used the\n25 Gb/s Ethernet interconnect.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 74
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s75",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The system also includes 480\nCPU-only nodes with Intel Xeon Platinum 8260 processors.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 75
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s76",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "In addition, four nodes with NVIDIA A100 GPUs were also\navailable for experiments described in this paper.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 76
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s77",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "A summary\nof the hardware is shown in Table I.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 77
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s78",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "All experiments described\nin this paper were run exclusively on NVIDIA GPUs.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 78
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s79",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "TABLE I: Compute node configurations: This table lists\nthe types of hardware used for inference evaluations in our\nexperiments.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 79
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s80",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Each node consists of 2 CPUs and 2 GPUs in\nthe configuration listed below.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 80
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s81",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "All GPUs are from NVIDIA.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 81
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s82",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "CPU GPU\nType Memory TDP Type Memory TDP\n(GB) (W) (GB) (W)\nIntel Xeon\nGold 6248 384 150 V100 32 250\nIntel Xeon\nPlatinum 8358 503 240 A100 80 300\nA.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 82
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s83",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Models\nExperiments were performed using open-source implemen-\ntation of the pre-trained LLaMA 65B model available via\nrequest from Meta [14] and evaluation scripts available via\nGitHub [19].",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 83
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s84",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "This implementation of the model uses Pytorch\nand the FairScale [20] library to enable model sharding across\nmultiple GPUs and nodes.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 84
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s85",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "For the models, we use a decoder\ntemperature setting \u03c4 = 0 .8 and a top- p value of 0.95 in\nattempts to align our settings with the general range of values\nthat are typically used.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 85
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s86",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "In future work, we aim to study\nhow varying decoding temperature, top- p, and other hyper-\nparameters may affect compute performance and energy usage\nduring inference.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 86
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s87",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "While our main focus is on LLaMA 65B,\nwe also examine LLaMA 7B and LLaMA 13B to characterize\ninference performance and energy under the bare minimum\nsettings/resources required to run these models.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 87
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s88",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "B.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 88
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s89",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Datasets\nWe used two datasets to evaluate inference performance.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 89
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s90",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The first is an instruction following dataset used to fine-tune\nthe Alpaca [21] model (from here on, this dataset is referred\nto as \u201cAlpaca\u201d in our paper which is not to be confused with\nthe Alpaca model).",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 90
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s91",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "This Alpaca dataset consists of 52,000\ninstruction-following tasks, instructions/questions where some\nhave example inputs and some do not, that the model is asked\nto answer.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 91
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s92",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The second dataset is GSM8K [22], consisting of\n8,500 human crafted grade school math problems.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 92
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s93",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The goal\nof using these two datasets is two-fold: (1) to evaluate the\nmodel on a diverse set of tasks in NLP and (2) evaluate\nhow different types of data and its underlying dynamics\ncan impact energy and inference performance.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 93
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s94",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "While natural\nlanguage is more common in LLM usage and in LLM training\ndata, increasingly new capabilities have been demonstrated\nin LLMs, including the ability to solve simple mathematical\nproblems, provide/correct examples of code, and more.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 94
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s95",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Math\nquestions also differ considerably from questions posed in\nnatural language which can result in smaller context windows,\ninputs/outputs of differing lengths, number of decoded tokens,\netc.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 95
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s96",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "This, in turn, may impact inference performance in\neither throughput rates or energy costs.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 96
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s97",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "For this reason, our\nbenchmarking experiments are conducted on both datasets.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 97
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s98",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "For both datasets, we sample 4,096 inputs for our inference\nexperiments.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 98
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s99",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Using the entirety of the datasets would only\nserve to increase inference time and energy used for the ex-\nperimentation unreasonably and did not provide any significant\nbenefits to the study.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 99
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s100",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "C.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 100
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s101",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Evaluation\nOur goal is to evaluate the inference performance, latency,\nand inference energy costs of LLaMA 65B as a representative\nlarge language model that requires sharding across multiple\nGPUs.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 101
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s102",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "We intend this to be a preliminary analysis that will\nhelp guide more in-depth experiments and benchmarking for\nour future work.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 102
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s103",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Our analysis also includes limited analysis of\nsmaller LLaMA variants to illustrate inference performance\nand energy trade-offs in bare-minimum hardware settings:\nnamely, LLaMA 7B and 13B.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 103
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s104",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "While we do not control for\nthe correctness/quality of the outputs or the complexity of the\ninputs/outputs in studying trade-offs between inference energy\nand performance, we hope to account for this as an ablative\nstudy in future work.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 104
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s105",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Similarly, we do not perform a com-\nprehensive evaluation with different optimization techniques\nor inference settings available for LLMs such as modeling\nquery arrival rates, model quantization, continuous batching,\netc.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 105
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s106",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "which we also leave for future work.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 106
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s107",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Inference performance is measured in terms of rates: words,\ntokens, and responses per second or, equivalently, the number\nof words, tokens, and responses generated per second.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 107
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s108",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "When\nrunning inference with LLaMA, the model generates a string\nof text for each input until the length of the text hits a\nmaximum generation length or a stop-word is encountered.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 108
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s109",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The number of words are calculated by counting the number\nof words present in the output by splitting each output string\non spaces.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 109
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s110",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The number of tokens is calculated using LLaMA\u2019s\nown default tokenizer by counting the number of tokens in the\ntokenized output.",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 110
        }
      },
      {
        "node_id": "samsi2024:sec3:p4:s111",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Lastly, the number of responses per second\nor the response rate is calculated using the total number of",
        "score": 0.6629,
        "rank": 28,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 111
        }
      },
      {
        "node_id": "xia2024:sec9:p12:s343",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "Projected maximum batch size of Mixtral for different GPUs.",
        "score": 0.6594,
        "rank": 29,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "paragraph_id": "xia2024:sec9:p12",
          "section_id": "xia2024:sec9",
          "sentence_index": 343
        }
      },
      {
        "node_id": "xia2024:sec9:p12",
        "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
        "text": "0 20 40 60 80 100 1200\n5\n10\n15\n20\n25\n30\n35\n40\nA100-40GB\nA100-80GB\nA40\nH100\nbsz=28\nbsz=35\nProjected GPU capacity\nGround Truth Projection\nMax batch size\nGPU DRAM capacity\nFig. 13. Projected maximum batch size of Mixtral for different GPUs.\namount of GPU memory, and need to be subtracted in the\nanalytical model. Fig. 8 supports this by showing that on the\nsame dataset, BlackMamba can support larger batch size than\nMixtral because of its smaller model size.\nMoreover, the sequence length and sparsity also affect the\nmaximum batch size. Because the sparsity only affects the\nMoE part of the LLM, we multiply its influence by C1,\nwhich we call MoE coefficient. We apply the sequence length\nand the sparsity in the denominator as they are inversely\nrelated to batch size. Then, we multiply the result by C0,\nthe scaling coefficient , which scales the batch size by a\nconstant. The scaling coefficient is different across LLM\nmodels, because different models have different architecture\n(\u00a7III), and generate different amounts of intermediate data\nfor each query. The scaling coefficient for BlackMamba is\nhigher than that of Mixtral because it is a smaller model.\nFinally, we use floor to round it to the maximum integer.\nThe MoE coefficient and scaling coefficient vary across\nmodels. These coefficients are independent of GPU microar-\nchitectural parameters. We find the maximum batch size for\nboth LLM models on NVIDIA A40 (48GB), A100 (40GB),\nA100 (80GB), and H100 (80GB), and apply our model to\nfind the optimal coefficients. For Mixtral, C0 = 82 and\nC1 = 0 .95, and for BlackMamba, C0 = 83 and C1 = 0 .88.\nWhile we showcase these parameters for the models eval-\nuated, \u00a7V-D discusses how to generalize this approach for\nother models.\nUsing our analytical model, we demonstrate the maximum\nbatch sizes for fine-tuning on four different NVIDIA GPUs:\nA40, A100-40GB, A100-80GB and H100 with memory\ncapacities of 48GB, 40GB, 80GB, and 80GB, respectively.\nFig. 13 shows our projected maximum batch size and corre-\nlate it with experimented ground truth. While the maximum\nmemory capacity available in NVIDIA GPUs today is 80GB,\nwe use our analytical model to project the maximum batch\nsize that future GPUs might support. For GPU memory\ncapacities of 100GB and 120GB, our model predicts that\nthe maximum batch sizes supported for fine-tuning Mixtral\nwill be 28 and 35, respectively. Due to space limitations, we\nonly show the projection of Mixtral model.\nTABLE IV\nESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE\nBASED ON OUR ANALYTICAL MODEL\nGPU Mem MBS Throughput Cost ($/hr) Cost ($)\nA40 48GB 4 1.01 0.79 32.7\nA100 80GB 17 2.74 1.67 25.4\nH100 80GB 17 4.90 2.1 17.9\nB. Estimating Throughput\nAs discussed in \u00a7IV-B4, when the batch size increases, the\nLLM fine-tuning gradually switches from memory bound to\ncompute bound. When the compute resources are abundant,\nthe throughput increases almost linearly with batch size.\nHowever, when compute resources become constrained, the\nthroughput improvement gradually saturates. We model this\nbehavior using a logarithmic relation between batch size and\nthroughput. Our analytical model for maximum batch size is\nshown in (2).\nT hroughput = C2 \u2217 log( batch size\nsparsity \u2217 C3 ) + C4 (2)\nIn the equation, in addition to the basic logarithmic part, we\nhave three coefficients C2, C3, and C4. C2 is the scaling\ncoefficient, which depends on the LLM model, GPU archi-\ntecture, and the dataset. The higher the compute capability a\nGPU can provide, and the lower the LLM model and dataset\ncompute requirement is, the higher the scaling coefficient will\nbe. C3 is the MoE attenuation coefficient , which tunes how\nmuch the MoE sparsity affects the throughput. MoE sparsity\nonly affects the MoE part in LLM model, and thus should\nbe attenuated to avoid over compensation. This coefficient is\nonly LLM model dependent, because once the model is fixed,\nthe influence of sparsity is determined. C4 is the intercept,\nconceptually it equals to the throughput when batch size\nequals one, because the logarithmic part in (2) is zero when\nbatch size is one. Using scipy [32] to fit the model and\ngenerate four sets (C2, C3, C4), for each model and dataset\ncombination.\nTo estimate the accuracy of this model, we correlate the\nmodel output with experimental data from our study. Fig. 14\nshows this correlation study, where discrete data points (dots)\nrepresent experimental values, and the line represents output\nof our analytical model. We use both dense and sparse\nMixtral and BlackMamba for both datasets used in our study.\nThe figure clearly shows that our model accurately predicts\nLLM fine-tuning throughput with a Root Mean Squared Error\n(RMSE) of less than 0.8. Fig. 15 shows the correlation study\nof the analytical model of three other GPUs, A100 (40GB),\nA100 (80GB), and H100. The RMSE is less than 0.6, close\nto that of A40.\nC. Estimating the Total Fine-Tuning Cost\nUsing the throughput estimation, we calculate the cost\nof fine-tuning LLMs for different GPUs. The cost of GPU",
        "score": 0.6594,
        "rank": 29,
        "metadata": {
          "document_id": "xia2024",
          "document_title": "Understanding the Performance and Estimating the Cost of LLM Fine-Tuning",
          "page": 9,
          "paragraph_index": 12,
          "section_id": "xia2024:sec9",
          "section_index": 9
        }
      }
    ],
    "num_snippets": 556,
    "retry_count": 2
  },
  {
    "id": "q318",
    "question": "True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.",
    "gt_value": "0",
    "gt_unit": "is_blank",
    "gt_ref": "['ebert2024']",
    "pred_value": "0",
    "pred_unit": "is_blank",
    "pred_ref": "[\"ebert2024\"]",
    "pred_explanation": "The context explicitly states that GPU-level power consumption monitoring substantially under-represents actual energy consumption because it measures only a single component, and advocates against using GPU-level or other component-based tracking for overall energy measurements. Therefore, it is not recommended as the preferred method.",
    "raw_response": "{\n  \"explanation\": \"The context explicitly states that GPU-level power consumption monitoring substantially under-represents actual energy consumption because it measures only a single component, and advocates against using GPU-level or other component-based tracking for overall energy measurements. Therefore, it is not recommended as the preferred method.\",\n  \"answer\": \"False\",\n  \"answer_value\": \"0\",\n  \"ref_id\": [\"ebert2024\"],\n  \"ref_url\": [],\n  \"supporting_materials\": \"We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.\"\n}",
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true,
    "weighted_score": 1.0,
    "latency_seconds": 48.02803993225098,
    "retrieval_seconds": 12.841233253479004,
    "generation_seconds": 35.186545610427856,
    "error": null,
    "pred_ref_url": [],
    "pred_supporting_materials": "We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.",
    "rendered_prompt": "You will answer a question using ONLY the provided context snippets.\nIf the context does not clearly support an answer, use \"is_blank\".\n\nContext snippets from documents:\n[ref_id=ebert2024] Nvidia GPUs and certain\nCPUs already provide straightforward power consumption moni-\ntoring, therefore estimations are usually not necessary.\n---\n[ref_id=ebert2024] AI, Climate, and Regulation: From Data Centers to the AI Act Conference\u201917, July 2017, Washington, DC, USA\nimpacted by energy consumption reporting. We assume that they\npossess privileged access or considerable influence over the utilized\ninfrastructure given their financial power or strategic value. One\ncan distinguish between companies that develop closed models (e.g.,\nOpenAI) and those that create open-source models (e.g., Meta).\nStartups and smaller companies are expected to either directly\ndeploy such models, use available API services of leading model\nproviders, or fine-tune existing models. Although these companies\ndo not typically train large models themselves, they must report\nenergy consumption for fine-tuning. Under our proposed inter-\npretation, however, only if the fine-tuning of a model leads to a\nsubstantial modification.\n6.2 Levels for Measuring and Estimating Energy\nConsumption\nThere are several levels within a data center based on which energy\nconsumption may be measured or estimated [4]. These include (1)\nthe data center level, (2) the cumulative server level, (3) the GPU-\nLevel and other hardware within a server and (4) various other\nlevels. In this section, we outline each level along with their bene-\nfits, drawbacks, and estimation methods for when measurement is\nunavailable.\n6.2.1 Data Center Level. On the data center level, the power re-\nquired to operate the entire data center is measured, including both\nthe direct power consumption of computing equipment and the\nadditional overhead for cooling and maintaining the data center.\nThis approach provides the most extensive and complete figures\nsince it represents the actual energy usage, but also assumes that a\ndata center is exclusively utilized for the pre-training by the model\nprovider. It encourages the selection of an efficient data center. Ad-\nditionally, data centers have average PUE values of 1.58, so this\noverhead makes up a significant portion of the energy consump-\ntion. On the other hand, the power usage resulting solely from the\nmodel\u2019s architecture, the quantity of training data, the efficiency of\nthe implementation, and experimental settings is very important\nbut is somewhat skewed by the efficiency of the data center.\nIf data-center level power consumption measurement is not avail-\nable, using the PUE factor for estimation is deemed appropriate. To\ncalculate total energy usage, the PUE factor is multiplied by the raw\ncomputational power consumption measured or estimated at the cu-\nmulative server or rack level (see below). This might be reasonable,\nif only parts of a data center are utilized and only measurements\ncloser to the ICT equipment are available.\n6.2.2 Cumulative Server Level. A large-scale model is trained across\nmany servers in a distributed manner. Each server includes GPUs\nresponsible for the primary computation. To accurately monitor\nthe power consumption over time, a local power distribution unit\n(PDU), capable of measuring the provided power, is attached to each\nserver. Aggregating these measurements yields a highly precise\nfigure of the total energy consumption attributable to the model\u2019s\ncomputations. Instead of aggregating local PDUs, the usage of pri-\nmary PDUs or uninterruptible power supply (UPS) systems already\nmeasuring at the rack level or even many racks is also suitable\n(See also Appendix, Figure 1), as long as the measurements pre-\ncisely match fully the utilized hardware resources by the model\nproviders. The goal is to include all ICT-related power consumption\nbut exclude data center specific efficiency properties.\nThe upside of this method is its high accuracy, highly correlating\nwith model size and structure, data quantity, and hardware-aware\nsoftware implementation. It is widely recognized in the industry\nfor assessing power consumption in data centers. According to a\n2023 Green Grid industry survey [77], 66% of data centers can track\npower demand at least on rack-level. Roughly one third of over-\nall data centers are already able to collect average utilization and\npower demand data for individual servers and storage equipment\nand match this data to their IT equipment inventory. We assume\nthat the data centers that are used for training by large model\nproviders are already able to track this information given its high\ncost relevance. However, a significant number of data centers do not\ntrack power demand yet. The surveyed data center professionals\nestimate to require between 3-6 months (Europe: 15%, Global: 19%),\n1 year (Europe: 29%, Global: 28%), 2 years (Europe: 12%, Global:\n10%), 3 years (Europe: 4%, Global: 4%) or more than 4 years (Europe:\n11%, Global: 8%) to implement adequate power collection abilities.\nFor European data centers these numbers stand in (surprising) con-\ntrast to the obligation under the EED and Delegated Regulation to\nprovide energy consumption data by Sep 15, 2024 (Art. 3(1) and\nAnnex II(1)(e) Delegated Regulation). Either a substantial number\nof the participants was unaware of the obligations; or thereby ac-\nknowledged their inability to comply in time; or the question was\ntoo narrow, being pointed at individual servers, while the EED also\nallows measurement at the higher-level UPS.\nAn estimation is possible when GPU hours and the hardware\nused are known. By multiplying a GPU-hour with the peak utiliza-\ntion power consumption specified by the manufacturer, one can\nestimate the upper limit of energy consumption at the server level.\nThis upper bound is typically higher than the actual consumption,\nas GPU utilization rarely reaches its theoretical peak due to other\nresource constraints. Higher GPU efficiency means that the same\noperations are completed in less time, reducing the overhead from\nnon-utilization-dependent hardware power consumption.\n6.2.3 GPU-Level and other hardware within a server. The mea-\nsurement may be based on the energy usage of particular compo-\nnents as determined by on-chip sensors. Nvidia GPUs and certain\nCPUs already provide straightforward power consumption moni-\ntoring, therefore estimations are usually not necessary. However,\ndespite GPU power consumption being a significant factor and its\nusage correlating with the total power usage, it substantially under-\nrepresents the actual energy consumption since it measures just a\nsingle component. CPU power usage is a relatively minor factor in\nconsumption. Most other server components cannot be measured.\nWe advocate against using GPU-level or other component-based\npower consumption tracking for overall energy measurements.\n6.2.4 Other levels. Other measurement levels, such as Workload,\nCloud Instance, or Virtual Machine, involve high complexity and\nnumerous assumptions, resulting in a lack of standardized mea-\nsurement or estimation methods with considerable uncertainty. We\nadvise against using these levels for power consumption tracking.\n6.2.5 Interim Conclusion on the Level of Measurement. In our anal-\nysis, we argue that energy consumption should be measured and\n---\n[ref_id=ebert2024] This approach\ncaptures the total computation-related power usage and is better\nsuited to help providers optimize their AI models and algorithms for\nenergy efficiency.\n---\n[ref_id=ebert2024] Conference\u201917, July 2017, Washington, DC, USA Kai Ebert, Nicolas Alder, Ralf Herbrich, and Philipp Hacker\nreported at the cumulative server level (see also [4]). This approach\ncaptures the total computation-related power usage and is better\nsuited to help providers optimize their AI models and algorithms for\nenergy efficiency. Additionally, the PUE factor of each data center,\nwhich is reported and published by the data center operator under\nthe Energy Efficiency Directive (EU) 2023/1791 and Delegated Reg-\nulation (EU) 2024/1364, provides a useful estimate of overall energy\nconsumption [4]. With these two figures, it is possible to distinguish\nbetween model-specific power usage (server-level computation)\nand the data center\u2019s efficiency, offering a clearer picture of the\ntotal energy investment [4]. Although the EED mandates that PUE\nfactors must be available for data centers situated within the EU,\nthe responsibility of reporting these factors should also fall on the\nmodel provider. Specifically, model providers utilizing data center\nfacilities outside the EU should not be granted an exemption.\nEstimates of server-level power consumption should be based\non peak utilization figures provided by the hardware manufacturer\n(e.g., Nvidia) [4]. Still, it is important to consider advancements in\nresearch. Interpretation of the legal requirements could accommo-\ndate justifiable alternative assumptions that may not require peak\nutilization figures. For instance, tracking GPU utilization through\ninterfaces like those provided by Nvidia and referencing hardware\nbenchmarks based on specific GPU utilization rates could serve as\na basis for such assumptions.\n6.3 Measurement or Estimation\nAlthough actual measurement is more onerous, it also yields more\nprecise results for energy consumption reporting. Major model\nproviders are likely to already measure power consumption as it\nis a primary cost factor and highly linked to computational power.\nDespite the availability of power consumption data, companies\nmay be tempted to use estimated values to protect sensitive infor-\nmation. This practice should be restricted by legally prioritizing\nmeasurements over estimations in Annex XI Section 1(2)(e).\nThe same reasoning applies to smaller entities relying on cloud\ncomputing services for fine-tuning, for example such offered by\nAmazon Web Services or Microsoft Azure. Fine-tuning is crucial\nfor employing foundational models in task-specific applications\nand tailoring them to specific datasets. The higher the expense\nof initial model trainings, the stronger the incentive to perform\nfine-tuning rather than retraining the model. Therefore, this is the\ntype of adaptation that most businesses will focus on to effectively\nintegrate large AI models into practical products.\nCloud platforms still lack client-oriented power consumption\nreporting as part of their products. It is essential that ordinary\ncompanies with limited resources do not face obstacles in fulfill-\ning their reporting obligations only because their cloud computing\nproviders do not offer this data. The access rights enshrined in Art.\n25(2) AI Act can help if the deploying entity becomes a provider\u2014\nbut providers may invoke trade secrets and IP rights to dilute this\nobligation (Art. 25(5) AI Act). In our view, the interests of down-\nstream actors in reporting correct figures should generally trump\nthe secrecy interests of upstream providers, in this case. Under the\ncurrent setup, companies could always resort to computation-based\nestimations for their reporting. However, in order to gain the most\naccurate data, cloud platforms should be incentivized to provide\nenergy consumption data to their clients. Complementing Art. 25(2)\nAI Act, the obligation established in Sec. 15 of the German Energy\nEfficiency Act, requiring data centers to inform customers on their\nattributable annual energy consumption, could serve as a blueprint.\nNotably, such a law could also apply to cloud service providers\nbased outside the EU, with the caveat, however, that such an ex-\ntraterritorial application oftentimes lacks enforcement capabilities.\n6.4 Sustainability Impact Assessments\nThe operationalization of sustainability impact assessments (SIAs)\nwithin the risk assessments required under the AI Act involves\nintegrating environmental considerations into the existing risk\nmanagement frameworks that high-risk AI model providers and\nGPAI providers must follow. Much like data protection or algo-\nrithmic impact assessments, SIAs would serve as a practical tool\nfor embedding climate considerations into the development and\ndeployment of AI systems. Importantly, these assessments should\nnot be limited to high-risk AI models but should also apply to all AI\nsystems, regardless of the associated risk to health or safety. This\nis because the carbon footprint of AI models is often unrelated to\ntheir classification as high or low risk under the Act. Therefore, an\nSIA could ensure that environmental impacts are considered across\nthe entire AI landscape.\nThe SIA should involve evaluating various models and design\nchoices during the development process, comparing them not only\non their performance but also on their estimated environmental\nimpact. For instance, developers would need to assess whether a\nsimpler model, like linear regression or even a non-AI model, could\nachieve similar results with a smaller carbon footprint compared\nto more complex models like deep learning [33, 37]. Similarly, the\ndecision to use large, pre-trained models or training new, narrow\nmodels (almost) from scratch should factor in the potential climate\nbenefits. By using existing tools to measure the carbon impact of AI\nmodels, developers would be required to opt for the more environ-\nmentally sustainable option when performance is comparable. To\neffectively implement sustainability impact assessments, providers\nwould need to establish standardized methodologies for measuring\nthe environmental effects of AI models, particularly energy and\nwater usage during training and inference phases.\nTaking one step back, the concept of \"data protection by design\"\nshould be expanded to include \u201csustainability by design, \u201d under\nwhich developers should actively seek to reduce the contribution\nof ICT, including AI, to climate change [33]. At both the technical\nand organizational levels, this would involve adopting all feasible\nmeasures to limit environmental impact, a shift that has already\nbeen applied in other industries through consumption practices\nand product design. These approaches are also gaining traction in\nsupply chain management and other sectors in pursuit of corpo-\nrate Environmental, Social, and Governance (ESG) objectives. By\ndrawing on these existing practices, sustainability by design could\nbecome a core principle guiding the regulation of the ICT and AI\nsectors.\n---\n[ref_id=ebert2024] We advocate against using GPU-level or other component-based\npower consumption tracking for overall energy measurements.\n---\n[ref_id=ebert2024] AI, Climate, and Regulation: From Data Centers to the AI Act Conference\u201917, July 2017, Washington, DC, USA\nimpacted by energy consumption reporting. We assume that they\npossess privileged access or considerable influence over the utilized\ninfrastructure given their financial power or strategic value. One\ncan distinguish between companies that develop closed models (e.g.,\nOpenAI) and those that create open-source models (e.g., Meta).\nStartups and smaller companies are expected to either directly\ndeploy such models, use available API services of leading model\nproviders, or fine-tune existing models. Although these companies\ndo not typically train large models themselves, they must report\nenergy consumption for fine-tuning. Under our proposed inter-\npretation, however, only if the fine-tuning of a model leads to a\nsubstantial modification.\n6.2 Levels for Measuring and Estimating Energy\nConsumption\nThere are several levels within a data center based on which energy\nconsumption may be measured or estimated [4]. These include (1)\nthe data center level, (2) the cumulative server level, (3) the GPU-\nLevel and other hardware within a server and (4) various other\nlevels. In this section, we outline each level along with their bene-\nfits, drawbacks, and estimation methods for when measurement is\nunavailable.\n6.2.1 Data Center Level. On the data center level, the power re-\nquired to operate the entire data center is measured, including both\nthe direct power consumption of computing equipment and the\nadditional overhead for cooling and maintaining the data center.\nThis approach provides the most extensive and complete figures\nsince it represents the actual energy usage, but also assumes that a\ndata center is exclusively utilized for the pre-training by the model\nprovider. It encourages the selection of an efficient data center. Ad-\nditionally, data centers have average PUE values of 1.58, so this\noverhead makes up a significant portion of the energy consump-\ntion. On the other hand, the power usage resulting solely from the\nmodel\u2019s architecture, the quantity of training data, the efficiency of\nthe implementation, and experimental settings is very important\nbut is somewhat skewed by the efficiency of the data center.\nIf data-center level power consumption measurement is not avail-\nable, using the PUE factor for estimation is deemed appropriate. To\ncalculate total energy usage, the PUE factor is multiplied by the raw\ncomputational power consumption measured or estimated at the cu-\nmulative server or rack level (see below). This might be reasonable,\nif only parts of a data center are utilized and only measurements\ncloser to the ICT equipment are available.\n6.2.2 Cumulative Server Level. A large-scale model is trained across\nmany servers in a distributed manner. Each server includes GPUs\nresponsible for the primary computation. To accurately monitor\nthe power consumption over time, a local power distribution unit\n(PDU), capable of measuring the provided power, is attached to each\nserver. Aggregating these measurements yields a highly precise\nfigure of the total energy consumption attributable to the model\u2019s\ncomputations. Instead of aggregating local PDUs, the usage of pri-\nmary PDUs or uninterruptible power supply (UPS) systems already\nmeasuring at the rack level or even many racks is also suitable\n(See also Appendix, Figure 1), as long as the measurements pre-\ncisely match fully the utilized hardware resources by the model\nproviders. The goal is to include all ICT-related power consumption\nbut exclude data center specific efficiency properties.\nThe upside of this method is its high accuracy, highly correlating\nwith model size and structure, data quantity, and hardware-aware\nsoftware implementation. It is widely recognized in the industry\nfor assessing power consumption in data centers. According to a\n2023 Green Grid industry survey [77], 66% of data centers can track\npower demand at least on rack-level. Roughly one third of over-\nall data centers are already able to collect average utilization and\npower demand data for individual servers and storage equipment\nand match this data to their IT equipment inventory. We assume\nthat the data centers that are used for training by large model\nproviders are already able to track this information given its high\ncost relevance. However, a significant number of data centers do not\ntrack power demand yet. The surveyed data center professionals\nestimate to require between 3-6 months (Europe: 15%, Global: 19%),\n1 year (Europe: 29%, Global: 28%), 2 years (Europe: 12%, Global:\n10%), 3 years (Europe: 4%, Global: 4%) or more than 4 years (Europe:\n11%, Global: 8%) to implement adequate power collection abilities.\nFor European data centers these numbers stand in (surprising) con-\ntrast to the obligation under the EED and Delegated Regulation to\nprovide energy consumption data by Sep 15, 2024 (Art. 3(1) and\nAnnex II(1)(e) Delegated Regulation). Either a substantial number\nof the participants was unaware of the obligations; or thereby ac-\nknowledged their inability to comply in time; or the question was\ntoo narrow, being pointed at individual servers, while the EED also\nallows measurement at the higher-level UPS.\nAn estimation is possible when GPU hours and the hardware\nused are known. By multiplying a GPU-hour with the peak utiliza-\ntion power consumption specified by the manufacturer, one can\nestimate the upper limit of energy consumption at the server level.\nThis upper bound is typically higher than the actual consumption,\nas GPU utilization rarely reaches its theoretical peak due to other\nresource constraints. Higher GPU efficiency means that the same\noperations are completed in less time, reducing the overhead from\nnon-utilization-dependent hardware power consumption.\n6.2.3 GPU-Level and other hardware within a server. The mea-\nsurement may be based on the energy usage of particular compo-\nnents as determined by on-chip sensors. Nvidia GPUs and certain\nCPUs already provide straightforward power consumption moni-\ntoring, therefore estimations are usually not necessary. However,\ndespite GPU power consumption being a significant factor and its\nusage correlating with the total power usage, it substantially under-\nrepresents the actual energy consumption since it measures just a\nsingle component. CPU power usage is a relatively minor factor in\nconsumption. Most other server components cannot be measured.\nWe advocate against using GPU-level or other component-based\npower consumption tracking for overall energy measurements.\n6.2.4 Other levels. Other measurement levels, such as Workload,\nCloud Instance, or Virtual Machine, involve high complexity and\nnumerous assumptions, resulting in a lack of standardized mea-\nsurement or estimation methods with considerable uncertainty. We\nadvise against using these levels for power consumption tracking.\n6.2.5 Interim Conclusion on the Level of Measurement. In our anal-\nysis, we argue that energy consumption should be measured and\n---\n[ref_id=dodge2022] Measuring the Carbon Intensity of AI in Cloud Instances FAccT \u201922, June 21\u201324, 2022, Seoul, Republic of Korea\nwhere \ud835\udc36 = \ud835\udc42 + \ud835\udc40 is the software carbon intensity for a given cloud instance. In this paper, we focus on measuring\noperational emissions\ud835\udc42, and leave measurement and accounting for embodied emissions due to specialized ML hardware\nsuch as GPUs to future work (see \u00a78).\nThe objective of the Green Software Foundation\u2019s Software Carbon Intensity (SCI) specification is to calculate and\nreduce a SCI score, based on carbon emissions reductions, rather than the currently-used market-based neutralization.\nSpecifically, the SCI uses a \"consequential\" carbon accounting approach, which aims to quantify the marginal change in\nemissions caused by decisions or interventions. This differs from the commonly used \"attributional\" carbon accounting\napproach, which uses average carbon intensity data, meaning it does not provide the most actionable information to\nhelp reduce carbon emissions. Due to the myriad potential pitfalls of relying on market-based measures in place of\nactual reduction in emissions [36], it is not possible to reduce the SCI through carbon neutralization or carbon offsets.\nWe assert that cloud providers should provide the SCI to developers and data scientists to help them make choices that\nreduce the carbon footprint of their ML workloads.\n3.2 The Scope of our Tool: GPU Computation of a Single Cloud Instance\nData centers typically comprise many computer systems and hardware components, including storage, GPUs, CPUs,\nand networking components. We can break down the electricity usage for data centers into: 1) electricity that is used\nfor a single cloud instance, and 2) electricity that is used for the benefit of the whole data center. In this work we focus\non the former, a single cloud instance; because of this, a reader should understand that our estimates of the electricity\nconsumption and emissions are underestimates.1\nElectricity Consumption from a Single Cloud Instance. The most accurate and popular AI models today are typically\n(deep) neural networks, which are most performant on specialized, highly parallelized, and often energy-intensive\nhardware [43]. The most common scenario is for AI workloads to run on graphics processing units (GPUs), which provide\nsignificant acceleration compared to CPUs (central processing units) but are more power-hungry (often consuming\n250W-350W, compared to CPU consumption of 10-150W). Due to specialization to the matrix multiply operations at\nthe core of neural network computations and a high rate of parallelization, GPUs can perform many more of these\ntypes of computations in the same amount of time as a CPU, but this increased computation throughput comes at an\nincreased energy cost. Thus in ML applications based on deep learning, the majority of the electricity consumption\nis due to the GPU [ 5, 45]. While this result is fairly uncontroversial, we ran an experiment to confirm it. To do so,\nwe trained a BERT-base model [8] on a single NVIDIA TITAN X GPU (12 GB) in a commodity server with two Intel\nXeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs) to measure the relative electricity consumption\nof different components. We trained the model using the original code provided by Devlin et al. [8] on the language\nmodel pre-training task for 12 hours on one GPU, sampling the instantaneous energy use of the GPU, CPU and DRAM\nfor each socket throughout that period, then averaging to get average power draw per component in watts. GPU\nenergy draw was measured using nvidia-smi and CPU and DRAM power draw were obtained using Intel\u2019s RAPL.\nOur measurements, in watts, are presented in Table 1. As expected the GPU accounts for almost 3/4 of electricity\nconsumption.\nFocus on GPUs. In cloud datacenters, the CPUs, RAM, storage, and motherboards are often shared across multiple\ninstances; while this provides the flexibility that makes the cloud so useful, it leads to technical limitations that make it\n1There is related work on estimating and reducing the electricity of data centers in general, e.g., [10, 24].\n5\n---\n[ref_id=dodge2022] Page 5\n---\n[ref_id=dodge2022] Measuring the Carbon Intensity of AI in Cloud Instances FAccT \u201922, June 21\u201324, 2022, Seoul, Republic of Korea\nwhere \ud835\udc36 = \ud835\udc42 + \ud835\udc40 is the software carbon intensity for a given cloud instance.\n---\n[ref_id=dodge2022] In this paper, we focus on measuring\noperational emissions\ud835\udc42, and leave measurement and accounting for embodied emissions due to specialized ML hardware\nsuch as GPUs to future work (see \u00a78).\n---\n[ref_id=dodge2022] The objective of the Green Software Foundation\u2019s Software Carbon Intensity (SCI) specification is to calculate and\nreduce a SCI score, based on carbon emissions reductions, rather than the currently-used market-based neutralization.\n---\n[ref_id=dodge2022] Specifically, the SCI uses a \"consequential\" carbon accounting approach, which aims to quantify the marginal change in\nemissions caused by decisions or interventions.\n---\n[ref_id=dodge2022] This differs from the commonly used \"attributional\" carbon accounting\napproach, which uses average carbon intensity data, meaning it does not provide the most actionable information to\nhelp reduce carbon emissions.\n---\n[ref_id=dodge2022] Due to the myriad potential pitfalls of relying on market-based measures in place of\nactual reduction in emissions [36], it is not possible to reduce the SCI through carbon neutralization or carbon offsets.\n---\n[ref_id=dodge2022] We assert that cloud providers should provide the SCI to developers and data scientists to help them make choices that\nreduce the carbon footprint of their ML workloads.\n---\n[ref_id=dodge2022] 3.2 The Scope of our Tool: GPU Computation of a Single Cloud Instance\nData centers typically comprise many computer systems and hardware components, including storage, GPUs, CPUs,\nand networking components.\n---\n[ref_id=dodge2022] We can break down the electricity usage for data centers into: 1) electricity that is used\nfor a single cloud instance, and 2) electricity that is used for the benefit of the whole data center.\n---\n[ref_id=dodge2022] In this work we focus\non the former, a single cloud instance;\n---\n[ref_id=dodge2022] because of this, a reader should understand that our estimates of the electricity\nconsumption and emissions are underestimates.1\nElectricity Consumption from a Single Cloud Instance.\n---\n[ref_id=dodge2022] The most accurate and popular AI models today are typically\n(deep) neural networks, which are most performant on specialized, highly parallelized, and often energy-intensive\nhardware [43].\n---\n[ref_id=dodge2022] The most common scenario is for AI workloads to run on graphics processing units (GPUs), which provide\nsignificant acceleration compared to CPUs (central processing units) but are more power-hungry (often consuming\n250W-350W, compared to CPU consumption of 10-150W).\n---\n[ref_id=dodge2022] Due to specialization to the matrix multiply operations at\nthe core of neural network computations and a high rate of parallelization, GPUs can perform many more of these\ntypes of computations in the same amount of time as a CPU, but this increased computation throughput comes at an\nincreased energy cost.\n---\n[ref_id=dodge2022] Thus in ML applications based on deep learning, the majority of the electricity consumption\nis due to the GPU [ 5, 45].\n---\n[ref_id=dodge2022] While this result is fairly uncontroversial, we ran an experiment to confirm it.\n---\n[ref_id=dodge2022] To do so,\nwe trained a BERT-base model [8] on a single NVIDIA TITAN X GPU (12 GB) in a commodity server with two Intel\nXeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs) to measure the relative electricity consumption\nof different components.\n---\n[ref_id=dodge2022] We trained the model using the original code provided by Devlin et al.\n---\n[ref_id=dodge2022] [8] on the language\nmodel pre-training task for 12 hours on one GPU, sampling the instantaneous energy use of the GPU, CPU and DRAM\nfor each socket throughout that period, then averaging to get average power draw per component in watts.\n---\n[ref_id=dodge2022] GPU\nenergy draw was measured using nvidia-smi and CPU and DRAM power draw were obtained using Intel\u2019s RAPL.\n---\n[ref_id=dodge2022] Our measurements, in watts, are presented in Table 1.\n---\n[ref_id=dodge2022] As expected the GPU accounts for almost 3/4 of electricity\nconsumption.\n---\n[ref_id=dodge2022] Focus on GPUs.\n---\n[ref_id=dodge2022] In cloud datacenters, the CPUs, RAM, storage, and motherboards are often shared across multiple\ninstances;\n---\n[ref_id=dodge2022] while this provides the flexibility that makes the cloud so useful, it leads to technical limitations that make it\n1There is related work on estimating and reducing the electricity of data centers in general, e.g., [10, 24].\n---\n[ref_id=dodge2022] 5\n---\n[ref_id=ebert2024] However,\ndespite GPU power consumption being a significant factor and its\nusage correlating with the total power usage, it substantially under-\nrepresents the actual energy consumption since it measures just a\nsingle component.\n---\n[ref_id=ebert2024] AI, Climate, and Regulation: From Data Centers to the AI Act Conference\u201917, July 2017, Washington, DC, USA\nimpacted by energy consumption reporting. We assume that they\npossess privileged access or considerable influence over the utilized\ninfrastructure given their financial power or strategic value. One\ncan distinguish between companies that develop closed models (e.g.,\nOpenAI) and those that create open-source models (e.g., Meta).\nStartups and smaller companies are expected to either directly\ndeploy such models, use available API services of leading model\nproviders, or fine-tune existing models. Although these companies\ndo not typically train large models themselves, they must report\nenergy consumption for fine-tuning. Under our proposed inter-\npretation, however, only if the fine-tuning of a model leads to a\nsubstantial modification.\n6.2 Levels for Measuring and Estimating Energy\nConsumption\nThere are several levels within a data center based on which energy\nconsumption may be measured or estimated [4]. These include (1)\nthe data center level, (2) the cumulative server level, (3) the GPU-\nLevel and other hardware within a server and (4) various other\nlevels. In this section, we outline each level along with their bene-\nfits, drawbacks, and estimation methods for when measurement is\nunavailable.\n6.2.1 Data Center Level. On the data center level, the power re-\nquired to operate the entire data center is measured, including both\nthe direct power consumption of computing equipment and the\nadditional overhead for cooling and maintaining the data center.\nThis approach provides the most extensive and complete figures\nsince it represents the actual energy usage, but also assumes that a\ndata center is exclusively utilized for the pre-training by the model\nprovider. It encourages the selection of an efficient data center. Ad-\nditionally, data centers have average PUE values of 1.58, so this\noverhead makes up a significant portion of the energy consump-\ntion. On the other hand, the power usage resulting solely from the\nmodel\u2019s architecture, the quantity of training data, the efficiency of\nthe implementation, and experimental settings is very important\nbut is somewhat skewed by the efficiency of the data center.\nIf data-center level power consumption measurement is not avail-\nable, using the PUE factor for estimation is deemed appropriate. To\ncalculate total energy usage, the PUE factor is multiplied by the raw\ncomputational power consumption measured or estimated at the cu-\nmulative server or rack level (see below). This might be reasonable,\nif only parts of a data center are utilized and only measurements\ncloser to the ICT equipment are available.\n6.2.2 Cumulative Server Level. A large-scale model is trained across\nmany servers in a distributed manner. Each server includes GPUs\nresponsible for the primary computation. To accurately monitor\nthe power consumption over time, a local power distribution unit\n(PDU), capable of measuring the provided power, is attached to each\nserver. Aggregating these measurements yields a highly precise\nfigure of the total energy consumption attributable to the model\u2019s\ncomputations. Instead of aggregating local PDUs, the usage of pri-\nmary PDUs or uninterruptible power supply (UPS) systems already\nmeasuring at the rack level or even many racks is also suitable\n(See also Appendix, Figure 1), as long as the measurements pre-\ncisely match fully the utilized hardware resources by the model\nproviders. The goal is to include all ICT-related power consumption\nbut exclude data center specific efficiency properties.\nThe upside of this method is its high accuracy, highly correlating\nwith model size and structure, data quantity, and hardware-aware\nsoftware implementation. It is widely recognized in the industry\nfor assessing power consumption in data centers. According to a\n2023 Green Grid industry survey [77], 66% of data centers can track\npower demand at least on rack-level. Roughly one third of over-\nall data centers are already able to collect average utilization and\npower demand data for individual servers and storage equipment\nand match this data to their IT equipment inventory. We assume\nthat the data centers that are used for training by large model\nproviders are already able to track this information given its high\ncost relevance. However, a significant number of data centers do not\ntrack power demand yet. The surveyed data center professionals\nestimate to require between 3-6 months (Europe: 15%, Global: 19%),\n1 year (Europe: 29%, Global: 28%), 2 years (Europe: 12%, Global:\n10%), 3 years (Europe: 4%, Global: 4%) or more than 4 years (Europe:\n11%, Global: 8%) to implement adequate power collection abilities.\nFor European data centers these numbers stand in (surprising) con-\ntrast to the obligation under the EED and Delegated Regulation to\nprovide energy consumption data by Sep 15, 2024 (Art. 3(1) and\nAnnex II(1)(e) Delegated Regulation). Either a substantial number\nof the participants was unaware of the obligations; or thereby ac-\nknowledged their inability to comply in time; or the question was\ntoo narrow, being pointed at individual servers, while the EED also\nallows measurement at the higher-level UPS.\nAn estimation is possible when GPU hours and the hardware\nused are known. By multiplying a GPU-hour with the peak utiliza-\ntion power consumption specified by the manufacturer, one can\nestimate the upper limit of energy consumption at the server level.\nThis upper bound is typically higher than the actual consumption,\nas GPU utilization rarely reaches its theoretical peak due to other\nresource constraints. Higher GPU efficiency means that the same\noperations are completed in less time, reducing the overhead from\nnon-utilization-dependent hardware power consumption.\n6.2.3 GPU-Level and other hardware within a server. The mea-\nsurement may be based on the energy usage of particular compo-\nnents as determined by on-chip sensors. Nvidia GPUs and certain\nCPUs already provide straightforward power consumption moni-\ntoring, therefore estimations are usually not necessary. However,\ndespite GPU power consumption being a significant factor and its\nusage correlating with the total power usage, it substantially under-\nrepresents the actual energy consumption since it measures just a\nsingle component. CPU power usage is a relatively minor factor in\nconsumption. Most other server components cannot be measured.\nWe advocate against using GPU-level or other component-based\npower consumption tracking for overall energy measurements.\n6.2.4 Other levels. Other measurement levels, such as Workload,\nCloud Instance, or Virtual Machine, involve high complexity and\nnumerous assumptions, resulting in a lack of standardized mea-\nsurement or estimation methods with considerable uncertainty. We\nadvise against using these levels for power consumption tracking.\n6.2.5 Interim Conclusion on the Level of Measurement. In our anal-\nysis, we argue that energy consumption should be measured and\n---\n[ref_id=jegham2025] [ 26] measure GPU\npower draw across prompt lengths but exclude proprietary systems and broader environmental factors,\nlacking a standardized scaling method for production-level inference.\n---\n[ref_id=jegham2025] Despite the growing environmental footprint of large-scale model deployment, a standard method\nto quantify the cost of inference at the prompt level remains absent. A core obstacle to developing\nmore accurate assessments is the lack of model-specific inference data for commercial AI models.\nExisting environmental reports tend to aggregate emissions across entire cloud infrastructures without\ndisaggregating by model or workload [16, 17]. This lack of public information hinders independent\nverification and undermines both scientific benchmarking and policy efforts aimed at regulating AI\u2019s\ntrue environmental cost.\nTo address these issues, we introduce a novel infrastructure-aware benchmarking framework to\nquantify the operational environmental footprint of LLM inference at the per-prompt level as deployed\nin data centers. Unlike existing studies [ 13, 15, 18], our method adopts a more comprehensive\nstrategy by integrating performance metrics such as latency and throughput from public APIs with\npublished GPU and system power specifications. Furthermore, we scale these combined data\npoints using company-specific multipliers, including Power Usage Effectiveness (PUE) [ 19, 20],\nWater Usage Effectiveness (WUE) [19, 20], and Carbon Intensity Factors (CIF) [21, 22] to account\nfor infrastructural overhead. This method enables us to evaluate the energy, water, and carbon\neffects of both open-source and proprietary models, a gap that, to our knowledge, has not been\ncomprehensively explored in prior research. Additionally, we employ statistical analysis, including\nANOV A and Tukey HSD, to estimate underlying hardware configurations. To enhance transparency\nand reproducibility, we also developed an automated and interactive Power BI dashboard that\nvisualizes the daily fluctuations in the energy, water, and carbon footprint of an extended list of models\nacross multiple data centers. This novel dashboard incorporates new models as they get released.\nMoreover, to contextualize resource use relative to model capability, we apply cross-efficiency Data\nEnvelopment Analysis (DEA) to assess how effectively each model converts environmental inputs\ninto performance. As a key application of this framework, we perform a case study to estimate the\nfootprint of GPT-4o text generation based on scaled usage data. We further extend our analysis to\nGPT-5, focusing on the disparities in energy consumption between queries that involve different levels\nof reasoning. Our framework enables infrastructure-aware decision-making, empowers accountability,\nand provides a foundational step toward sustainability standards in AI deployment.\nThe remainder of the paper is organized as follows. Section 2 reviews existing studies on the\nenvironmental impact of LLMs. Section 3 introduces key concepts, including hardware configurations\nand environmental multipliers. Section 4 details our framework for estimating inference-phase cost.\nSection 5 presents findings across 30 models. Section 6 provides a focused analysis of GPT-4o\u2019s\nannual environmental footprint and section 7 analyzes the impact of GPT-5\u2019s adapative model routing.\nSection 8 outlines key insights and implications. Section 9 summarizes the main takeaways and\nlimitations and directions for future work.\n2 Related Work\nThe environmental impact of AI systems has garnered increasing attention in recent years, with a\ngrowing body of work attempting to quantify the energy, carbon, and water costs associated with\ntraining and deploying LLMs.\nLi et al. [13] analyzed GPT-3\u2019s freshwater consumption, estimating over 5 million liters used during\ntraining and projecting that AI-related withdrawals could reach 6.6 trillion liters annually by 2027.\nAlthough their spatiotemporal methodology is a significant early contribution, it overlooks carbon\nemissions, depends on an outdated model, and requires previous knowledge of energy usage, which\nrestricts its scalability. In parallel, Strubell et al. [ 23] estimated carbon emissions from training\nBERT and GPT-2 by accounting for GPU, CPU, and DRAM power draw alongside PUE adjustments.\nHowever, their analysis excludes inference and infrastructural overhead. Similar limitations appear\nin Meta\u2019s LLaMA reports [7, 24, 25], which provide carbon footprints based on GPUs\u2019 TDPs but\ndisregard water use, system-wide energy consumption, and the inference phase entirely.\nRegarding inference, Husom et al. [18] (MELODI) measure real-time energy consumption of GPUs\nand CPUs at the prompt level, but they neglect carbon emissions, water usage, and infrastructure\noverhead, only concentrating on small-scale open-source models. Samsi et al. [ 26] measure GPU\npower draw across prompt lengths but exclude proprietary systems and broader environmental factors,\nlacking a standardized scaling method for production-level inference. Yang et al. [ 27] evaluate\nover 1,200 vision models and introduce an energy-efficiency score. However, their analysis does\n2\n---\n[ref_id=dodge2022] For this reason, and because they typically consume the majority\nof electricity in AI workloads, in this work we only measure GPU electricity consumption.\n---\n[ref_id=dodge2022] FAccT \u201922, June 21\u201324, 2022, Seoul, Republic of Korea Dodge et al.\nHardwa. GPU CPU0 CPU1 DRAM0 DRAM1 Total\nWatts 187.1 22.9 9.3 23.0 9.3 251.6\nFraction 74% 9% 4% 9% 4% 100%\nTable 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB),\nin a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is\naveraged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU\nalone accounts for 74% of the total energy consumption due to these components.\ndifficult (and in some cases impossible) to properly estimate electricity consumption from these sources for a single\ninstance. However, GPUs are typically not shared across instances, and in fact for large AI workloads it\u2019s often the\ncase that multiple GPUs are attached to a single instance, leading to an even greater proportion of the total energy\nconsumption being used by the GPUs. Thus, it is relatively easy to measure the GPU electricity consumption for a\nsingle instance, while it is not for other components. For this reason, and because they typically consume the majority\nof electricity in AI workloads, in this work we only measure GPU electricity consumption. We recognize this is a first\nstep towards a more complete measurement, and provide further discussion in the next section.2\nOther sources of CO 2. Data centers have a number of electricity uses that are important, but will not be covered by\nour tool. According to the U.S. Department of Energy: \u201cThe electricity consumed in these data centers is mainly by the\nequipment (50%) and HVAC (25%\u201340%)\u201d [47]. Such other sources of emissions can be accounted for using methods\nsuch as Power Usage Effectiveness (PUE), which can be used to describe the proportion of electricity consumption\nby the computing equipment vs. other sources. For a given datacenter, this can be turned into a factor which can be\nmultiplied against the electricity consumption of computing equipment to get an estimate of the total consumption.\nSome companies have highlighted particularly low PUEs, such as Google claiming a PUE of 1.10 across its fleet of data\ncenters for the 12 months ending in Q1 2021,3 compared to an average global PUE of 1.59 [2].\nOther factors, such as the emissions produced by maintenance workers driving to and from the data center, emissions\nfrom manufacturing the computer systems, and emissions from building the structure in which the data center is\nhoused4 are non-negligible but beyond the scope of this paper. Finally, for workloads that do not use GPUs (e.g., storage\nor web hosting) we recommend users choose low emissions regions and times of day, as they will not have access to\nsingle-instance emissions calculations. We leave it open for future research to address how to appropriately allocate\nCO2 emissions from such data center-wide processes to individual reserved cloud instances.\n4 ELECTRICITY CONSUMPTION FOR AI WORKLOADS\nAs outlined in \u00a73.1, calculating software carbon intensity begins with recording the electricity consumption, which\ncan then be mapped to emissions based on the emissions of the grid being used. In this section, we present data on\nelectricity consumption for experiments training 11 different models, covering natural language processing (NLP) and\ncomputer vision applications, ranging from less than an hour on a single GPU up to more than 8 days on 256 GPUs. We\noutline both the experiments themselves and their electricity consumption, and in the following section we use the\nelectricity consumption and carbon intensity tool described in the previous section to calculate their software carbon\nintensity.\n2We note that our conclusions drawn from experiments and analyses on time-shifting and location-shifting are still applicable with tools that measure\nmore electricity than just the GPU.\n3https://www.google.com/about/datacenters/efficiency/\n4One of the largest single source of CO2 emissions, contributing to 7%-8% of global emissions, is the production of cement [20].\n6\n---\n[ref_id=ebert2024] AI, Climate, and Regulation: From Data Centers to the AI Act Conference\u201917, July 2017, Washington, DC, USA\nimpacted by energy consumption reporting. We assume that they\npossess privileged access or considerable influence over the utilized\ninfrastructure given their financial power or strategic value. One\ncan distinguish between companies that develop closed models (e.g.,\nOpenAI) and those that create open-source models (e.g., Meta).\nStartups and smaller companies are expected to either directly\ndeploy such models, use available API services of leading model\nproviders, or fine-tune existing models. Although these companies\ndo not typically train large models themselves, they must report\nenergy consumption for fine-tuning. Under our proposed inter-\npretation, however, only if the fine-tuning of a model leads to a\nsubstantial modification.\n6.2 Levels for Measuring and Estimating Energy\nConsumption\nThere are several levels within a data center based on which energy\nconsumption may be measured or estimated [4]. These include (1)\nthe data center level, (2) the cumulative server level, (3) the GPU-\nLevel and other hardware within a server and (4) various other\nlevels. In this section, we outline each level along with their bene-\nfits, drawbacks, and estimation methods for when measurement is\nunavailable.\n6.2.1 Data Center Level. On the data center level, the power re-\nquired to operate the entire data center is measured, including both\nthe direct power consumption of computing equipment and the\nadditional overhead for cooling and maintaining the data center.\nThis approach provides the most extensive and complete figures\nsince it represents the actual energy usage, but also assumes that a\ndata center is exclusively utilized for the pre-training by the model\nprovider. It encourages the selection of an efficient data center. Ad-\nditionally, data centers have average PUE values of 1.58, so this\noverhead makes up a significant portion of the energy consump-\ntion. On the other hand, the power usage resulting solely from the\nmodel\u2019s architecture, the quantity of training data, the efficiency of\nthe implementation, and experimental settings is very important\nbut is somewhat skewed by the efficiency of the data center.\nIf data-center level power consumption measurement is not avail-\nable, using the PUE factor for estimation is deemed appropriate. To\ncalculate total energy usage, the PUE factor is multiplied by the raw\ncomputational power consumption measured or estimated at the cu-\nmulative server or rack level (see below). This might be reasonable,\nif only parts of a data center are utilized and only measurements\ncloser to the ICT equipment are available.\n6.2.2 Cumulative Server Level. A large-scale model is trained across\nmany servers in a distributed manner. Each server includes GPUs\nresponsible for the primary computation. To accurately monitor\nthe power consumption over time, a local power distribution unit\n(PDU), capable of measuring the provided power, is attached to each\nserver. Aggregating these measurements yields a highly precise\nfigure of the total energy consumption attributable to the model\u2019s\ncomputations. Instead of aggregating local PDUs, the usage of pri-\nmary PDUs or uninterruptible power supply (UPS) systems already\nmeasuring at the rack level or even many racks is also suitable\n(See also Appendix, Figure 1), as long as the measurements pre-\ncisely match fully the utilized hardware resources by the model\nproviders. The goal is to include all ICT-related power consumption\nbut exclude data center specific efficiency properties.\nThe upside of this method is its high accuracy, highly correlating\nwith model size and structure, data quantity, and hardware-aware\nsoftware implementation. It is widely recognized in the industry\nfor assessing power consumption in data centers. According to a\n2023 Green Grid industry survey [77], 66% of data centers can track\npower demand at least on rack-level. Roughly one third of over-\nall data centers are already able to collect average utilization and\npower demand data for individual servers and storage equipment\nand match this data to their IT equipment inventory. We assume\nthat the data centers that are used for training by large model\nproviders are already able to track this information given its high\ncost relevance. However, a significant number of data centers do not\ntrack power demand yet. The surveyed data center professionals\nestimate to require between 3-6 months (Europe: 15%, Global: 19%),\n1 year (Europe: 29%, Global: 28%), 2 years (Europe: 12%, Global:\n10%), 3 years (Europe: 4%, Global: 4%) or more than 4 years (Europe:\n11%, Global: 8%) to implement adequate power collection abilities.\nFor European data centers these numbers stand in (surprising) con-\ntrast to the obligation under the EED and Delegated Regulation to\nprovide energy consumption data by Sep 15, 2024 (Art. 3(1) and\nAnnex II(1)(e) Delegated Regulation). Either a substantial number\nof the participants was unaware of the obligations; or thereby ac-\nknowledged their inability to comply in time; or the question was\ntoo narrow, being pointed at individual servers, while the EED also\nallows measurement at the higher-level UPS.\nAn estimation is possible when GPU hours and the hardware\nused are known. By multiplying a GPU-hour with the peak utiliza-\ntion power consumption specified by the manufacturer, one can\nestimate the upper limit of energy consumption at the server level.\nThis upper bound is typically higher than the actual consumption,\nas GPU utilization rarely reaches its theoretical peak due to other\nresource constraints. Higher GPU efficiency means that the same\noperations are completed in less time, reducing the overhead from\nnon-utilization-dependent hardware power consumption.\n6.2.3 GPU-Level and other hardware within a server. The mea-\nsurement may be based on the energy usage of particular compo-\nnents as determined by on-chip sensors. Nvidia GPUs and certain\nCPUs already provide straightforward power consumption moni-\ntoring, therefore estimations are usually not necessary. However,\ndespite GPU power consumption being a significant factor and its\nusage correlating with the total power usage, it substantially under-\nrepresents the actual energy consumption since it measures just a\nsingle component. CPU power usage is a relatively minor factor in\nconsumption. Most other server components cannot be measured.\nWe advocate against using GPU-level or other component-based\npower consumption tracking for overall energy measurements.\n6.2.4 Other levels. Other measurement levels, such as Workload,\nCloud Instance, or Virtual Machine, involve high complexity and\nnumerous assumptions, resulting in a lack of standardized mea-\nsurement or estimation methods with considerable uncertainty. We\nadvise against using these levels for power consumption tracking.\n6.2.5 Interim Conclusion on the Level of Measurement. In our anal-\nysis, we argue that energy consumption should be measured and\n---\n[ref_id=ebert2024] Page 7\n---\n[ref_id=ebert2024] AI, Climate, and Regulation: From Data Centers to the AI Act Conference\u201917, July 2017, Washington, DC, USA\nimpacted by energy consumption reporting.\n---\n[ref_id=ebert2024] We assume that they\npossess privileged access or considerable influence over the utilized\ninfrastructure given their financial power or strategic value.\n---\n[ref_id=ebert2024] One\ncan distinguish between companies that develop closed models (e.g.,\nOpenAI) and those that create open-source models (e.g., Meta).\n---\n[ref_id=ebert2024] Startups and smaller companies are expected to either directly\ndeploy such models, use available API services of leading model\nproviders, or fine-tune existing models.\n---\n[ref_id=ebert2024] Although these companies\ndo not typically train large models themselves, they must report\nenergy consumption for fine-tuning.\n---\n[ref_id=ebert2024] Under our proposed inter-\npretation, however, only if the fine-tuning of a model leads to a\nsubstantial modification.\n---\n[ref_id=ebert2024] 6.2 Levels for Measuring and Estimating Energy\nConsumption\nThere are several levels within a data center based on which energy\nconsumption may be measured or estimated [4].\n---\n[ref_id=ebert2024] These include (1)\nthe data center level, (2) the cumulative server level, (3) the GPU-\nLevel and other hardware within a server and (4) various other\nlevels.\n---\n[ref_id=ebert2024] In this section, we outline each level along with their bene-\nfits, drawbacks, and estimation methods for when measurement is\nunavailable.\n---\n[ref_id=ebert2024] 6.2.1 Data Center Level.\n---\n[ref_id=ebert2024] On the data center level, the power re-\nquired to operate the entire data center is measured, including both\nthe direct power consumption of computing equipment and the\nadditional overhead for cooling and maintaining the data center.\n---\n[ref_id=ebert2024] This approach provides the most extensive and complete figures\nsince it represents the actual energy usage, but also assumes that a\ndata center is exclusively utilized for the pre-training by the model\nprovider.\n---\n[ref_id=ebert2024] It encourages the selection of an efficient data center.\n---\n[ref_id=ebert2024] Ad-\nditionally, data centers have average PUE values of 1.58, so this\noverhead makes up a significant portion of the energy consump-\ntion.\n---\n[ref_id=ebert2024] On the other hand, the power usage resulting solely from the\nmodel\u2019s architecture, the quantity of training data, the efficiency of\nthe implementation, and experimental settings is very important\nbut is somewhat skewed by the efficiency of the data center.\n---\n[ref_id=ebert2024] If data-center level power consumption measurement is not avail-\nable, using the PUE factor for estimation is deemed appropriate.\n---\n[ref_id=ebert2024] To\ncalculate total energy usage, the PUE factor is multiplied by the raw\ncomputational power consumption measured or estimated at the cu-\nmulative server or rack level (see below).\n---\n[ref_id=ebert2024] This might be reasonable,\nif only parts of a data center are utilized and only measurements\ncloser to the ICT equipment are available.\n---\n[ref_id=ebert2024] 6.2.2 Cumulative Server Level.\n---\n[ref_id=ebert2024] A large-scale model is trained across\nmany servers in a distributed manner.\n---\n[ref_id=ebert2024] Each server includes GPUs\nresponsible for the primary computation.\n---\n[ref_id=ebert2024] To accurately monitor\nthe power consumption over time, a local power distribution unit\n(PDU), capable of measuring the provided power, is attached to each\nserver.\n---\n[ref_id=ebert2024] Aggregating these measurements yields a highly precise\nfigure of the total energy consumption attributable to the model\u2019s\ncomputations.\n---\n[ref_id=ebert2024] Instead of aggregating local PDUs, the usage of pri-\nmary PDUs or uninterruptible power supply (UPS) systems already\nmeasuring at the rack level or even many racks is also suitable\n(See also Appendix, Figure 1), as long as the measurements pre-\ncisely match fully the utilized hardware resources by the model\nproviders.\n---\n[ref_id=ebert2024] The goal is to include all ICT-related power consumption\nbut exclude data center specific efficiency properties.\n---\n[ref_id=ebert2024] The upside of this method is its high accuracy, highly correlating\nwith model size and structure, data quantity, and hardware-aware\nsoftware implementation.\n---\n[ref_id=ebert2024] It is widely recognized in the industry\nfor assessing power consumption in data centers.\n---\n[ref_id=ebert2024] According to a\n2023 Green Grid industry survey [77], 66% of data centers can track\npower demand at least on rack-level.\n---\n[ref_id=ebert2024] Roughly one third of over-\nall data centers are already able to collect average utilization and\npower demand data for individual servers and storage equipment\nand match this data to their IT equipment inventory.\n---\n[ref_id=ebert2024] We assume\nthat the data centers that are used for training by large model\nproviders are already able to track this information given its high\ncost relevance.\n---\n[ref_id=ebert2024] However, a significant number of data centers do not\ntrack power demand yet.\n---\n[ref_id=ebert2024] The surveyed data center professionals\nestimate to require between 3-6 months (Europe: 15%, Global: 19%),\n1 year (Europe: 29%, Global: 28%), 2 years (Europe: 12%, Global:\n10%), 3 years (Europe: 4%, Global: 4%) or more than 4 years (Europe:\n11%, Global: 8%) to implement adequate power collection abilities.\n---\n[ref_id=ebert2024] For European data centers these numbers stand in (surprising) con-\ntrast to the obligation under the EED and Delegated Regulation to\nprovide energy consumption data by Sep 15, 2024 (Art.\n---\n[ref_id=ebert2024] 3(1) and\nAnnex II(1)(e) Delegated Regulation).\n---\n[ref_id=ebert2024] Either a substantial number\nof the participants was unaware of the obligations;\n---\n[ref_id=ebert2024] or thereby ac-\nknowledged their inability to comply in time;\n---\n[ref_id=ebert2024] or the question was\ntoo narrow, being pointed at individual servers, while the EED also\nallows measurement at the higher-level UPS.\n---\n[ref_id=ebert2024] An estimation is possible when GPU hours and the hardware\nused are known.\n---\n[ref_id=ebert2024] By multiplying a GPU-hour with the peak utiliza-\ntion power consumption specified by the manufacturer, one can\nestimate the upper limit of energy consumption at the server level.\n---\n[ref_id=ebert2024] This upper bound is typically higher than the actual consumption,\nas GPU utilization rarely reaches its theoretical peak due to other\nresource constraints.\n---\n[ref_id=ebert2024] Higher GPU efficiency means that the same\noperations are completed in less time, reducing the overhead from\nnon-utilization-dependent hardware power consumption.\n---\n[ref_id=ebert2024] 6.2.3 GPU-Level and other hardware within a server.\n---\n[ref_id=ebert2024] The mea-\nsurement may be based on the energy usage of particular compo-\nnents as determined by on-chip sensors.\n---\n[ref_id=ebert2024] Nvidia GPUs and certain\nCPUs already provide straightforward power consumption moni-\ntoring, therefore estimations are usually not necessary.\n---\n[ref_id=ebert2024] However,\ndespite GPU power consumption being a significant factor and its\nusage correlating with the total power usage, it substantially under-\nrepresents the actual energy consumption since it measures just a\nsingle component.\n---\n[ref_id=ebert2024] CPU power usage is a relatively minor factor in\nconsumption.\n---\n[ref_id=ebert2024] Most other server components cannot be measured.\n---\n[ref_id=ebert2024] We advocate against using GPU-level or other component-based\npower consumption tracking for overall energy measurements.\n---\n[ref_id=ebert2024] 6.2.4 Other levels.\n---\n[ref_id=ebert2024] Other measurement levels, such as Workload,\nCloud Instance, or Virtual Machine, involve high complexity and\nnumerous assumptions, resulting in a lack of standardized mea-\nsurement or estimation methods with considerable uncertainty.\n---\n[ref_id=ebert2024] We\nadvise against using these levels for power consumption tracking.\n---\n[ref_id=ebert2024] 6.2.5 Interim Conclusion on the Level of Measurement.\n---\n[ref_id=ebert2024] In our anal-\nysis, we argue that energy consumption should be measured and\n---\n[ref_id=morrison2025] For example, unlike previous works\nthat assume GPUs operate at 100% of their theoretical maximum power draw (Dubey et al., 2024)\nand report only the cost to train a small set of released models, we measure power consumption\nat sub-second intervals throughout training.\n---\n[ref_id=morrison2025] Published as a conference paper at ICLR 2025\nincreasing numbers of stakeholders become involved in the development and use of AI systems, it is\nimperative to carefully characterize the true cost of building and deploying state-of-the-art models,\nto inform effective strategies for mitigating potential harms and planning for future demand.\n100 101 102\nCarbon Emissions (tCO eq)\n100\n101\n102\n103\nWater Consumption (kL)OLMo 20M\nOLMo 150M\nOLMo 700M\nOLMo 1B\nOLMo 2 7B\nOLMo 2 13B\nOLMoE 0924\nFigure 1: Environmental impact for a selection of the\nfinal training runs described in Section 4.1, where we\nrank each model by both its total water consumption\nand its CO2 emissions. Our small models (<1B param-\neters) were trained on 1.7 trillion tokens, OLMo 1B was\ntrained on 3 trillion, OLMo 2 7B was trained on 4 tril-\nlion, OLMoE was trained on 5 trillion, and OLMo 2\n13B was trained on 5.6 trillion. We see that the total\nenvironmental impact for larger training runs is quite\nhigh, and increases quickly with model and dataset size.\nIn this paper, we estimate the energy use and\nenvironmental impacts caused by training the\nOLMo series of transformer language models\n(Groeneveld et al., 2024; OLMo et al., 2025),\nranging in size from 20 million to 13 billion\nactive parameters, trained on 1.7 to 5.6 trillion\ntokens. To do this, we calculate Scope 2 CO 2\nemissions in accordance with the Greenhouse\nGas Protocol\u2019s definitions,3 and Scope 1 and 2\nwater consumption following Li et al. (2023);\nin addition, we calculate \u201cupstream\u201d embod-\nied carbon and water consumption, and provide\n\u201cdownstream\u201d estimates from use of our mod-\nels (which are part, but not all, of Scope 3).\nImportantly, we calculate (i) electricity con-\nsumption, (ii) carbon emissions, and (iii) wa-\nter consumption at three points in the machine\nlearning pipeline: early model development\n(e.g., hyperparameter tuning and experiments\nbefore the final training run), training of the\nmain model, and inference. To the best of our\nknowledge, we are the first to report this in-\nformation for model development of large lan-\nguage models, and we find the environmental\nimpact of developing even our relatively small\nmodels (only up to 13B parameters) is equivalent to burning 2.1 gasoline tanker trucks of fuel, or\nthe amount of water consumed by one average person in the United States in about 7.5 years. We\nencourage the reader to consider larger models released by other organizations to have equivalently\nlarger environmental impacts.\nOur methodology draws upon best practices from recent publications, aiming to provide the most\nthorough reporting yet of the environmental impact of LLMs. For example, unlike previous works\nthat assume GPUs operate at 100% of their theoretical maximum power draw (Dubey et al., 2024)\nand report only the cost to train a small set of released models, we measure power consumption\nat sub-second intervals throughout training. We focus our efforts on a wide range of model sizes,\noptimized for widespread deployment (Dubey et al., 2024; Mehta et al., 2024; Gemma Team et al.,\n2024), and estimate what the environmental impact would be if our models were deployed in a va-\nriety of different scenarios. We find that in some scenarios, our models would need to run inference\non a few billion instances to match the electricity consumed, carbon emitted, and water consumed\nof the entire training process, a figure that can be reached by production systems in weeks to months\nbased on current usage trends.4\nWe conclude that more transparency is needed across the industry in reporting the environmental\nimpact of AI systems. Systems orders of magnitude larger than those in this paper are being built,\nand deployed at a global scale, leading to emissions 10s or 100s of times larger than what we\nreport. This work is a step in the right direction, but responsibility of reporting and reducing the\nenvironmental impact must fall on those training the largest models, as they have the largest impact.\n2 R ELATED WORK\nWhile most publicly available models do not report any climate impact, including CO 2 emissions,\nwater usage, or embodied carbon, a few reports recently have included some estimates. For example,\n3https://ghgprotocol.org/sites/default/files/standards/ghg-protocol-revised.pdf\n4https://www.cnbc.com/2025/02/20/openai-tops-400-million-users-despite-deepseeks-emergence.\nhtml\n2\n---\n[ref_id=jegham2025] Despite the growing environmental footprint of large-scale model deployment, a standard method\nto quantify the cost of inference at the prompt level remains absent. A core obstacle to developing\nmore accurate assessments is the lack of model-specific inference data for commercial AI models.\nExisting environmental reports tend to aggregate emissions across entire cloud infrastructures without\ndisaggregating by model or workload [16, 17]. This lack of public information hinders independent\nverification and undermines both scientific benchmarking and policy efforts aimed at regulating AI\u2019s\ntrue environmental cost.\nTo address these issues, we introduce a novel infrastructure-aware benchmarking framework to\nquantify the operational environmental footprint of LLM inference at the per-prompt level as deployed\nin data centers. Unlike existing studies [ 13, 15, 18], our method adopts a more comprehensive\nstrategy by integrating performance metrics such as latency and throughput from public APIs with\npublished GPU and system power specifications. Furthermore, we scale these combined data\npoints using company-specific multipliers, including Power Usage Effectiveness (PUE) [ 19, 20],\nWater Usage Effectiveness (WUE) [19, 20], and Carbon Intensity Factors (CIF) [21, 22] to account\nfor infrastructural overhead. This method enables us to evaluate the energy, water, and carbon\neffects of both open-source and proprietary models, a gap that, to our knowledge, has not been\ncomprehensively explored in prior research. Additionally, we employ statistical analysis, including\nANOV A and Tukey HSD, to estimate underlying hardware configurations. To enhance transparency\nand reproducibility, we also developed an automated and interactive Power BI dashboard that\nvisualizes the daily fluctuations in the energy, water, and carbon footprint of an extended list of models\nacross multiple data centers. This novel dashboard incorporates new models as they get released.\nMoreover, to contextualize resource use relative to model capability, we apply cross-efficiency Data\nEnvelopment Analysis (DEA) to assess how effectively each model converts environmental inputs\ninto performance. As a key application of this framework, we perform a case study to estimate the\nfootprint of GPT-4o text generation based on scaled usage data. We further extend our analysis to\nGPT-5, focusing on the disparities in energy consumption between queries that involve different levels\nof reasoning. Our framework enables infrastructure-aware decision-making, empowers accountability,\nand provides a foundational step toward sustainability standards in AI deployment.\nThe remainder of the paper is organized as follows. Section 2 reviews existing studies on the\nenvironmental impact of LLMs. Section 3 introduces key concepts, including hardware configurations\nand environmental multipliers. Section 4 details our framework for estimating inference-phase cost.\nSection 5 presents findings across 30 models. Section 6 provides a focused analysis of GPT-4o\u2019s\nannual environmental footprint and section 7 analyzes the impact of GPT-5\u2019s adapative model routing.\nSection 8 outlines key insights and implications. Section 9 summarizes the main takeaways and\nlimitations and directions for future work.\n2 Related Work\nThe environmental impact of AI systems has garnered increasing attention in recent years, with a\ngrowing body of work attempting to quantify the energy, carbon, and water costs associated with\ntraining and deploying LLMs.\nLi et al. [13] analyzed GPT-3\u2019s freshwater consumption, estimating over 5 million liters used during\ntraining and projecting that AI-related withdrawals could reach 6.6 trillion liters annually by 2027.\nAlthough their spatiotemporal methodology is a significant early contribution, it overlooks carbon\nemissions, depends on an outdated model, and requires previous knowledge of energy usage, which\nrestricts its scalability. In parallel, Strubell et al. [ 23] estimated carbon emissions from training\nBERT and GPT-2 by accounting for GPU, CPU, and DRAM power draw alongside PUE adjustments.\nHowever, their analysis excludes inference and infrastructural overhead. Similar limitations appear\nin Meta\u2019s LLaMA reports [7, 24, 25], which provide carbon footprints based on GPUs\u2019 TDPs but\ndisregard water use, system-wide energy consumption, and the inference phase entirely.\nRegarding inference, Husom et al. [18] (MELODI) measure real-time energy consumption of GPUs\nand CPUs at the prompt level, but they neglect carbon emissions, water usage, and infrastructure\noverhead, only concentrating on small-scale open-source models. Samsi et al. [ 26] measure GPU\npower draw across prompt lengths but exclude proprietary systems and broader environmental factors,\nlacking a standardized scaling method for production-level inference. Yang et al. [ 27] evaluate\nover 1,200 vision models and introduce an energy-efficiency score. However, their analysis does\n2\n---\n[ref_id=jegham2025] Page 2\n---\n[ref_id=jegham2025] Despite the growing environmental footprint of large-scale model deployment, a standard method\nto quantify the cost of inference at the prompt level remains absent.\n---\n[ref_id=jegham2025] A core obstacle to developing\nmore accurate assessments is the lack of model-specific inference data for commercial AI models.\n---\n[ref_id=jegham2025] Existing environmental reports tend to aggregate emissions across entire cloud infrastructures without\ndisaggregating by model or workload [16, 17].\n---\n[ref_id=jegham2025] This lack of public information hinders independent\nverification and undermines both scientific benchmarking and policy efforts aimed at regulating AI\u2019s\ntrue environmental cost.\n---\n[ref_id=jegham2025] To address these issues, we introduce a novel infrastructure-aware benchmarking framework to\nquantify the operational environmental footprint of LLM inference at the per-prompt level as deployed\nin data centers.\n---\n[ref_id=jegham2025] Unlike existing studies [ 13, 15, 18], our method adopts a more comprehensive\nstrategy by integrating performance metrics such as latency and throughput from public APIs with\npublished GPU and system power specifications.\n---\n[ref_id=jegham2025] Furthermore, we scale these combined data\npoints using company-specific multipliers, including Power Usage Effectiveness (PUE) [ 19, 20],\nWater Usage Effectiveness (WUE) [19, 20], and Carbon Intensity Factors (CIF) [21, 22] to account\nfor infrastructural overhead.\n---\n[ref_id=jegham2025] This method enables us to evaluate the energy, water, and carbon\neffects of both open-source and proprietary models, a gap that, to our knowledge, has not been\ncomprehensively explored in prior research.\n---\n[ref_id=jegham2025] Additionally, we employ statistical analysis, including\nANOV A and Tukey HSD, to estimate underlying hardware configurations.\n---\n[ref_id=jegham2025] To enhance transparency\nand reproducibility, we also developed an automated and interactive Power BI dashboard that\nvisualizes the daily fluctuations in the energy, water, and carbon footprint of an extended list of models\nacross multiple data centers.\n---\n[ref_id=jegham2025] This novel dashboard incorporates new models as they get released.\n---\n[ref_id=jegham2025] Moreover, to contextualize resource use relative to model capability, we apply cross-efficiency Data\nEnvelopment Analysis (DEA) to assess how effectively each model converts environmental inputs\ninto performance.\n---\n[ref_id=jegham2025] As a key application of this framework, we perform a case study to estimate the\nfootprint of GPT-4o text generation based on scaled usage data.\n---\n[ref_id=jegham2025] We further extend our analysis to\nGPT-5, focusing on the disparities in energy consumption between queries that involve different levels\nof reasoning.\n---\n[ref_id=jegham2025] Our framework enables infrastructure-aware decision-making, empowers accountability,\nand provides a foundational step toward sustainability standards in AI deployment.\n---\n[ref_id=jegham2025] The remainder of the paper is organized as follows.\n---\n[ref_id=jegham2025] Section 2 reviews existing studies on the\nenvironmental impact of LLMs.\n---\n[ref_id=jegham2025] Section 3 introduces key concepts, including hardware configurations\nand environmental multipliers.\n---\n[ref_id=jegham2025] Section 4 details our framework for estimating inference-phase cost.\n---\n[ref_id=jegham2025] Section 5 presents findings across 30 models.\n---\n[ref_id=jegham2025] Section 6 provides a focused analysis of GPT-4o\u2019s\nannual environmental footprint and section 7 analyzes the impact of GPT-5\u2019s adapative model routing.\n---\n[ref_id=jegham2025] Section 8 outlines key insights and implications.\n---\n[ref_id=jegham2025] Section 9 summarizes the main takeaways and\nlimitations and directions for future work.\n---\n[ref_id=jegham2025] 2 Related Work\nThe environmental impact of AI systems has garnered increasing attention in recent years, with a\ngrowing body of work attempting to quantify the energy, carbon, and water costs associated with\ntraining and deploying LLMs.\n---\n[ref_id=jegham2025] Li et al.\n---\n[ref_id=jegham2025] [13] analyzed GPT-3\u2019s freshwater consumption, estimating over 5 million liters used during\ntraining and projecting that AI-related withdrawals could reach 6.6 trillion liters annually by 2027.\n---\n[ref_id=jegham2025] Although their spatiotemporal methodology is a significant early contribution, it overlooks carbon\nemissions, depends on an outdated model, and requires previous knowledge of energy usage, which\nrestricts its scalability.\n---\n[ref_id=jegham2025] In parallel, Strubell et al.\n---\n[ref_id=jegham2025] [ 23] estimated carbon emissions from training\nBERT and GPT-2 by accounting for GPU, CPU, and DRAM power draw alongside PUE adjustments.\n---\n[ref_id=jegham2025] However, their analysis excludes inference and infrastructural overhead.\n---\n[ref_id=jegham2025] Similar limitations appear\nin Meta\u2019s LLaMA reports [7, 24, 25], which provide carbon footprints based on GPUs\u2019 TDPs but\ndisregard water use, system-wide energy consumption, and the inference phase entirely.\n---\n[ref_id=jegham2025] Regarding inference, Husom et al.\n---\n[ref_id=jegham2025] [18] (MELODI) measure real-time energy consumption of GPUs\nand CPUs at the prompt level, but they neglect carbon emissions, water usage, and infrastructure\noverhead, only concentrating on small-scale open-source models.\n---\n[ref_id=jegham2025] Samsi et al.\n---\n[ref_id=jegham2025] [ 26] measure GPU\npower draw across prompt lengths but exclude proprietary systems and broader environmental factors,\nlacking a standardized scaling method for production-level inference.\n---\n[ref_id=jegham2025] Yang et al.\n---\n[ref_id=jegham2025] [ 27] evaluate\nover 1,200 vision models and introduce an energy-efficiency score.\n---\n[ref_id=jegham2025] However, their analysis does\n2\n\n---\n\nNow answer the following question based ONLY on the context above.\n\nQuestion: True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.\n\nAdditional info (JSON): {\"answer_unit\": \"is_blank\", \"question_id\": \"q318\"}\n\nIMPORTANT: The \"answer_unit\" field specifies the expected format/unit for answer_value.\n- If answer_unit is a unit (e.g., \"kW\", \"USD\"), express answer_value as a number in that unit (no unit name).\n- If answer_unit is \"is_blank\", answer_value should be the exact identifier/name from context that answers the question.\n- If the answer is a numeric range, format as [lower,upper].\n\nReturn STRICT JSON with these keys in order:\n- explanation          (1-3 sentences explaining how context supports the answer and how you applied answer_unit; or \"is_blank\")\n- answer               (short natural-language response)\n- answer_value         (the value matching the expected format; or \"is_blank\")\n- ref_id               (list of document ids from context used as evidence; or \"is_blank\")\n- ref_url              (list of URLs for the cited documents; or \"is_blank\")\n- supporting_materials (verbatim quote, table reference, or figure reference from the cited document; or \"is_blank\")\n\nJSON Answer:",
    "retrieved_snippets": [
      {
        "node_id": "ebert2024:sec7:p7:s330",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "Nvidia GPUs and certain\nCPUs already provide straightforward power consumption moni-\ntoring, therefore estimations are usually not necessary.",
        "score": 0.786,
        "rank": 1,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 330
        }
      },
      {
        "node_id": "ebert2024:sec7:p7",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "AI, Climate, and Regulation: From Data Centers to the AI Act Conference\u201917, July 2017, Washington, DC, USA\nimpacted by energy consumption reporting. We assume that they\npossess privileged access or considerable influence over the utilized\ninfrastructure given their financial power or strategic value. One\ncan distinguish between companies that develop closed models (e.g.,\nOpenAI) and those that create open-source models (e.g., Meta).\nStartups and smaller companies are expected to either directly\ndeploy such models, use available API services of leading model\nproviders, or fine-tune existing models. Although these companies\ndo not typically train large models themselves, they must report\nenergy consumption for fine-tuning. Under our proposed inter-\npretation, however, only if the fine-tuning of a model leads to a\nsubstantial modification.\n6.2 Levels for Measuring and Estimating Energy\nConsumption\nThere are several levels within a data center based on which energy\nconsumption may be measured or estimated [4]. These include (1)\nthe data center level, (2) the cumulative server level, (3) the GPU-\nLevel and other hardware within a server and (4) various other\nlevels. In this section, we outline each level along with their bene-\nfits, drawbacks, and estimation methods for when measurement is\nunavailable.\n6.2.1 Data Center Level. On the data center level, the power re-\nquired to operate the entire data center is measured, including both\nthe direct power consumption of computing equipment and the\nadditional overhead for cooling and maintaining the data center.\nThis approach provides the most extensive and complete figures\nsince it represents the actual energy usage, but also assumes that a\ndata center is exclusively utilized for the pre-training by the model\nprovider. It encourages the selection of an efficient data center. Ad-\nditionally, data centers have average PUE values of 1.58, so this\noverhead makes up a significant portion of the energy consump-\ntion. On the other hand, the power usage resulting solely from the\nmodel\u2019s architecture, the quantity of training data, the efficiency of\nthe implementation, and experimental settings is very important\nbut is somewhat skewed by the efficiency of the data center.\nIf data-center level power consumption measurement is not avail-\nable, using the PUE factor for estimation is deemed appropriate. To\ncalculate total energy usage, the PUE factor is multiplied by the raw\ncomputational power consumption measured or estimated at the cu-\nmulative server or rack level (see below). This might be reasonable,\nif only parts of a data center are utilized and only measurements\ncloser to the ICT equipment are available.\n6.2.2 Cumulative Server Level. A large-scale model is trained across\nmany servers in a distributed manner. Each server includes GPUs\nresponsible for the primary computation. To accurately monitor\nthe power consumption over time, a local power distribution unit\n(PDU), capable of measuring the provided power, is attached to each\nserver. Aggregating these measurements yields a highly precise\nfigure of the total energy consumption attributable to the model\u2019s\ncomputations. Instead of aggregating local PDUs, the usage of pri-\nmary PDUs or uninterruptible power supply (UPS) systems already\nmeasuring at the rack level or even many racks is also suitable\n(See also Appendix, Figure 1), as long as the measurements pre-\ncisely match fully the utilized hardware resources by the model\nproviders. The goal is to include all ICT-related power consumption\nbut exclude data center specific efficiency properties.\nThe upside of this method is its high accuracy, highly correlating\nwith model size and structure, data quantity, and hardware-aware\nsoftware implementation. It is widely recognized in the industry\nfor assessing power consumption in data centers. According to a\n2023 Green Grid industry survey [77], 66% of data centers can track\npower demand at least on rack-level. Roughly one third of over-\nall data centers are already able to collect average utilization and\npower demand data for individual servers and storage equipment\nand match this data to their IT equipment inventory. We assume\nthat the data centers that are used for training by large model\nproviders are already able to track this information given its high\ncost relevance. However, a significant number of data centers do not\ntrack power demand yet. The surveyed data center professionals\nestimate to require between 3-6 months (Europe: 15%, Global: 19%),\n1 year (Europe: 29%, Global: 28%), 2 years (Europe: 12%, Global:\n10%), 3 years (Europe: 4%, Global: 4%) or more than 4 years (Europe:\n11%, Global: 8%) to implement adequate power collection abilities.\nFor European data centers these numbers stand in (surprising) con-\ntrast to the obligation under the EED and Delegated Regulation to\nprovide energy consumption data by Sep 15, 2024 (Art. 3(1) and\nAnnex II(1)(e) Delegated Regulation). Either a substantial number\nof the participants was unaware of the obligations; or thereby ac-\nknowledged their inability to comply in time; or the question was\ntoo narrow, being pointed at individual servers, while the EED also\nallows measurement at the higher-level UPS.\nAn estimation is possible when GPU hours and the hardware\nused are known. By multiplying a GPU-hour with the peak utiliza-\ntion power consumption specified by the manufacturer, one can\nestimate the upper limit of energy consumption at the server level.\nThis upper bound is typically higher than the actual consumption,\nas GPU utilization rarely reaches its theoretical peak due to other\nresource constraints. Higher GPU efficiency means that the same\noperations are completed in less time, reducing the overhead from\nnon-utilization-dependent hardware power consumption.\n6.2.3 GPU-Level and other hardware within a server. The mea-\nsurement may be based on the energy usage of particular compo-\nnents as determined by on-chip sensors. Nvidia GPUs and certain\nCPUs already provide straightforward power consumption moni-\ntoring, therefore estimations are usually not necessary. However,\ndespite GPU power consumption being a significant factor and its\nusage correlating with the total power usage, it substantially under-\nrepresents the actual energy consumption since it measures just a\nsingle component. CPU power usage is a relatively minor factor in\nconsumption. Most other server components cannot be measured.\nWe advocate against using GPU-level or other component-based\npower consumption tracking for overall energy measurements.\n6.2.4 Other levels. Other measurement levels, such as Workload,\nCloud Instance, or Virtual Machine, involve high complexity and\nnumerous assumptions, resulting in a lack of standardized mea-\nsurement or estimation methods with considerable uncertainty. We\nadvise against using these levels for power consumption tracking.\n6.2.5 Interim Conclusion on the Level of Measurement. In our anal-\nysis, we argue that energy consumption should be measured and",
        "score": 0.786,
        "rank": 1,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "page": 7,
          "paragraph_index": 7,
          "section_id": "ebert2024:sec7",
          "section_index": 7
        }
      },
      {
        "node_id": "ebert2024:sec8:p8:s341",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "This approach\ncaptures the total computation-related power usage and is better\nsuited to help providers optimize their AI models and algorithms for\nenergy efficiency.",
        "score": 0.767,
        "rank": 2,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec8:p8",
          "section_id": "ebert2024:sec8",
          "sentence_index": 341
        }
      },
      {
        "node_id": "ebert2024:sec8:p8",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "Conference\u201917, July 2017, Washington, DC, USA Kai Ebert, Nicolas Alder, Ralf Herbrich, and Philipp Hacker\nreported at the cumulative server level (see also [4]). This approach\ncaptures the total computation-related power usage and is better\nsuited to help providers optimize their AI models and algorithms for\nenergy efficiency. Additionally, the PUE factor of each data center,\nwhich is reported and published by the data center operator under\nthe Energy Efficiency Directive (EU) 2023/1791 and Delegated Reg-\nulation (EU) 2024/1364, provides a useful estimate of overall energy\nconsumption [4]. With these two figures, it is possible to distinguish\nbetween model-specific power usage (server-level computation)\nand the data center\u2019s efficiency, offering a clearer picture of the\ntotal energy investment [4]. Although the EED mandates that PUE\nfactors must be available for data centers situated within the EU,\nthe responsibility of reporting these factors should also fall on the\nmodel provider. Specifically, model providers utilizing data center\nfacilities outside the EU should not be granted an exemption.\nEstimates of server-level power consumption should be based\non peak utilization figures provided by the hardware manufacturer\n(e.g., Nvidia) [4]. Still, it is important to consider advancements in\nresearch. Interpretation of the legal requirements could accommo-\ndate justifiable alternative assumptions that may not require peak\nutilization figures. For instance, tracking GPU utilization through\ninterfaces like those provided by Nvidia and referencing hardware\nbenchmarks based on specific GPU utilization rates could serve as\na basis for such assumptions.\n6.3 Measurement or Estimation\nAlthough actual measurement is more onerous, it also yields more\nprecise results for energy consumption reporting. Major model\nproviders are likely to already measure power consumption as it\nis a primary cost factor and highly linked to computational power.\nDespite the availability of power consumption data, companies\nmay be tempted to use estimated values to protect sensitive infor-\nmation. This practice should be restricted by legally prioritizing\nmeasurements over estimations in Annex XI Section 1(2)(e).\nThe same reasoning applies to smaller entities relying on cloud\ncomputing services for fine-tuning, for example such offered by\nAmazon Web Services or Microsoft Azure. Fine-tuning is crucial\nfor employing foundational models in task-specific applications\nand tailoring them to specific datasets. The higher the expense\nof initial model trainings, the stronger the incentive to perform\nfine-tuning rather than retraining the model. Therefore, this is the\ntype of adaptation that most businesses will focus on to effectively\nintegrate large AI models into practical products.\nCloud platforms still lack client-oriented power consumption\nreporting as part of their products. It is essential that ordinary\ncompanies with limited resources do not face obstacles in fulfill-\ning their reporting obligations only because their cloud computing\nproviders do not offer this data. The access rights enshrined in Art.\n25(2) AI Act can help if the deploying entity becomes a provider\u2014\nbut providers may invoke trade secrets and IP rights to dilute this\nobligation (Art. 25(5) AI Act). In our view, the interests of down-\nstream actors in reporting correct figures should generally trump\nthe secrecy interests of upstream providers, in this case. Under the\ncurrent setup, companies could always resort to computation-based\nestimations for their reporting. However, in order to gain the most\naccurate data, cloud platforms should be incentivized to provide\nenergy consumption data to their clients. Complementing Art. 25(2)\nAI Act, the obligation established in Sec. 15 of the German Energy\nEfficiency Act, requiring data centers to inform customers on their\nattributable annual energy consumption, could serve as a blueprint.\nNotably, such a law could also apply to cloud service providers\nbased outside the EU, with the caveat, however, that such an ex-\ntraterritorial application oftentimes lacks enforcement capabilities.\n6.4 Sustainability Impact Assessments\nThe operationalization of sustainability impact assessments (SIAs)\nwithin the risk assessments required under the AI Act involves\nintegrating environmental considerations into the existing risk\nmanagement frameworks that high-risk AI model providers and\nGPAI providers must follow. Much like data protection or algo-\nrithmic impact assessments, SIAs would serve as a practical tool\nfor embedding climate considerations into the development and\ndeployment of AI systems. Importantly, these assessments should\nnot be limited to high-risk AI models but should also apply to all AI\nsystems, regardless of the associated risk to health or safety. This\nis because the carbon footprint of AI models is often unrelated to\ntheir classification as high or low risk under the Act. Therefore, an\nSIA could ensure that environmental impacts are considered across\nthe entire AI landscape.\nThe SIA should involve evaluating various models and design\nchoices during the development process, comparing them not only\non their performance but also on their estimated environmental\nimpact. For instance, developers would need to assess whether a\nsimpler model, like linear regression or even a non-AI model, could\nachieve similar results with a smaller carbon footprint compared\nto more complex models like deep learning [33, 37]. Similarly, the\ndecision to use large, pre-trained models or training new, narrow\nmodels (almost) from scratch should factor in the potential climate\nbenefits. By using existing tools to measure the carbon impact of AI\nmodels, developers would be required to opt for the more environ-\nmentally sustainable option when performance is comparable. To\neffectively implement sustainability impact assessments, providers\nwould need to establish standardized methodologies for measuring\nthe environmental effects of AI models, particularly energy and\nwater usage during training and inference phases.\nTaking one step back, the concept of \"data protection by design\"\nshould be expanded to include \u201csustainability by design, \u201d under\nwhich developers should actively seek to reduce the contribution\nof ICT, including AI, to climate change [33]. At both the technical\nand organizational levels, this would involve adopting all feasible\nmeasures to limit environmental impact, a shift that has already\nbeen applied in other industries through consumption practices\nand product design. These approaches are also gaining traction in\nsupply chain management and other sectors in pursuit of corpo-\nrate Environmental, Social, and Governance (ESG) objectives. By\ndrawing on these existing practices, sustainability by design could\nbecome a core principle guiding the regulation of the ICT and AI\nsectors.",
        "score": 0.767,
        "rank": 2,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "page": 8,
          "paragraph_index": 8,
          "section_id": "ebert2024:sec8",
          "section_index": 8
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s334",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "We advocate against using GPU-level or other component-based\npower consumption tracking for overall energy measurements.",
        "score": 0.7572,
        "rank": 3,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 334
        }
      },
      {
        "node_id": "ebert2024:sec7:p7",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "AI, Climate, and Regulation: From Data Centers to the AI Act Conference\u201917, July 2017, Washington, DC, USA\nimpacted by energy consumption reporting. We assume that they\npossess privileged access or considerable influence over the utilized\ninfrastructure given their financial power or strategic value. One\ncan distinguish between companies that develop closed models (e.g.,\nOpenAI) and those that create open-source models (e.g., Meta).\nStartups and smaller companies are expected to either directly\ndeploy such models, use available API services of leading model\nproviders, or fine-tune existing models. Although these companies\ndo not typically train large models themselves, they must report\nenergy consumption for fine-tuning. Under our proposed inter-\npretation, however, only if the fine-tuning of a model leads to a\nsubstantial modification.\n6.2 Levels for Measuring and Estimating Energy\nConsumption\nThere are several levels within a data center based on which energy\nconsumption may be measured or estimated [4]. These include (1)\nthe data center level, (2) the cumulative server level, (3) the GPU-\nLevel and other hardware within a server and (4) various other\nlevels. In this section, we outline each level along with their bene-\nfits, drawbacks, and estimation methods for when measurement is\nunavailable.\n6.2.1 Data Center Level. On the data center level, the power re-\nquired to operate the entire data center is measured, including both\nthe direct power consumption of computing equipment and the\nadditional overhead for cooling and maintaining the data center.\nThis approach provides the most extensive and complete figures\nsince it represents the actual energy usage, but also assumes that a\ndata center is exclusively utilized for the pre-training by the model\nprovider. It encourages the selection of an efficient data center. Ad-\nditionally, data centers have average PUE values of 1.58, so this\noverhead makes up a significant portion of the energy consump-\ntion. On the other hand, the power usage resulting solely from the\nmodel\u2019s architecture, the quantity of training data, the efficiency of\nthe implementation, and experimental settings is very important\nbut is somewhat skewed by the efficiency of the data center.\nIf data-center level power consumption measurement is not avail-\nable, using the PUE factor for estimation is deemed appropriate. To\ncalculate total energy usage, the PUE factor is multiplied by the raw\ncomputational power consumption measured or estimated at the cu-\nmulative server or rack level (see below). This might be reasonable,\nif only parts of a data center are utilized and only measurements\ncloser to the ICT equipment are available.\n6.2.2 Cumulative Server Level. A large-scale model is trained across\nmany servers in a distributed manner. Each server includes GPUs\nresponsible for the primary computation. To accurately monitor\nthe power consumption over time, a local power distribution unit\n(PDU), capable of measuring the provided power, is attached to each\nserver. Aggregating these measurements yields a highly precise\nfigure of the total energy consumption attributable to the model\u2019s\ncomputations. Instead of aggregating local PDUs, the usage of pri-\nmary PDUs or uninterruptible power supply (UPS) systems already\nmeasuring at the rack level or even many racks is also suitable\n(See also Appendix, Figure 1), as long as the measurements pre-\ncisely match fully the utilized hardware resources by the model\nproviders. The goal is to include all ICT-related power consumption\nbut exclude data center specific efficiency properties.\nThe upside of this method is its high accuracy, highly correlating\nwith model size and structure, data quantity, and hardware-aware\nsoftware implementation. It is widely recognized in the industry\nfor assessing power consumption in data centers. According to a\n2023 Green Grid industry survey [77], 66% of data centers can track\npower demand at least on rack-level. Roughly one third of over-\nall data centers are already able to collect average utilization and\npower demand data for individual servers and storage equipment\nand match this data to their IT equipment inventory. We assume\nthat the data centers that are used for training by large model\nproviders are already able to track this information given its high\ncost relevance. However, a significant number of data centers do not\ntrack power demand yet. The surveyed data center professionals\nestimate to require between 3-6 months (Europe: 15%, Global: 19%),\n1 year (Europe: 29%, Global: 28%), 2 years (Europe: 12%, Global:\n10%), 3 years (Europe: 4%, Global: 4%) or more than 4 years (Europe:\n11%, Global: 8%) to implement adequate power collection abilities.\nFor European data centers these numbers stand in (surprising) con-\ntrast to the obligation under the EED and Delegated Regulation to\nprovide energy consumption data by Sep 15, 2024 (Art. 3(1) and\nAnnex II(1)(e) Delegated Regulation). Either a substantial number\nof the participants was unaware of the obligations; or thereby ac-\nknowledged their inability to comply in time; or the question was\ntoo narrow, being pointed at individual servers, while the EED also\nallows measurement at the higher-level UPS.\nAn estimation is possible when GPU hours and the hardware\nused are known. By multiplying a GPU-hour with the peak utiliza-\ntion power consumption specified by the manufacturer, one can\nestimate the upper limit of energy consumption at the server level.\nThis upper bound is typically higher than the actual consumption,\nas GPU utilization rarely reaches its theoretical peak due to other\nresource constraints. Higher GPU efficiency means that the same\noperations are completed in less time, reducing the overhead from\nnon-utilization-dependent hardware power consumption.\n6.2.3 GPU-Level and other hardware within a server. The mea-\nsurement may be based on the energy usage of particular compo-\nnents as determined by on-chip sensors. Nvidia GPUs and certain\nCPUs already provide straightforward power consumption moni-\ntoring, therefore estimations are usually not necessary. However,\ndespite GPU power consumption being a significant factor and its\nusage correlating with the total power usage, it substantially under-\nrepresents the actual energy consumption since it measures just a\nsingle component. CPU power usage is a relatively minor factor in\nconsumption. Most other server components cannot be measured.\nWe advocate against using GPU-level or other component-based\npower consumption tracking for overall energy measurements.\n6.2.4 Other levels. Other measurement levels, such as Workload,\nCloud Instance, or Virtual Machine, involve high complexity and\nnumerous assumptions, resulting in a lack of standardized mea-\nsurement or estimation methods with considerable uncertainty. We\nadvise against using these levels for power consumption tracking.\n6.2.5 Interim Conclusion on the Level of Measurement. In our anal-\nysis, we argue that energy consumption should be measured and",
        "score": 0.7572,
        "rank": 3,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "page": 7,
          "paragraph_index": 7,
          "section_id": "ebert2024:sec7",
          "section_index": 7
        }
      },
      {
        "node_id": "dodge2022:sec5:p5",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "Measuring the Carbon Intensity of AI in Cloud Instances FAccT \u201922, June 21\u201324, 2022, Seoul, Republic of Korea\nwhere \ud835\udc36 = \ud835\udc42 + \ud835\udc40 is the software carbon intensity for a given cloud instance. In this paper, we focus on measuring\noperational emissions\ud835\udc42, and leave measurement and accounting for embodied emissions due to specialized ML hardware\nsuch as GPUs to future work (see \u00a78).\nThe objective of the Green Software Foundation\u2019s Software Carbon Intensity (SCI) specification is to calculate and\nreduce a SCI score, based on carbon emissions reductions, rather than the currently-used market-based neutralization.\nSpecifically, the SCI uses a \"consequential\" carbon accounting approach, which aims to quantify the marginal change in\nemissions caused by decisions or interventions. This differs from the commonly used \"attributional\" carbon accounting\napproach, which uses average carbon intensity data, meaning it does not provide the most actionable information to\nhelp reduce carbon emissions. Due to the myriad potential pitfalls of relying on market-based measures in place of\nactual reduction in emissions [36], it is not possible to reduce the SCI through carbon neutralization or carbon offsets.\nWe assert that cloud providers should provide the SCI to developers and data scientists to help them make choices that\nreduce the carbon footprint of their ML workloads.\n3.2 The Scope of our Tool: GPU Computation of a Single Cloud Instance\nData centers typically comprise many computer systems and hardware components, including storage, GPUs, CPUs,\nand networking components. We can break down the electricity usage for data centers into: 1) electricity that is used\nfor a single cloud instance, and 2) electricity that is used for the benefit of the whole data center. In this work we focus\non the former, a single cloud instance; because of this, a reader should understand that our estimates of the electricity\nconsumption and emissions are underestimates.1\nElectricity Consumption from a Single Cloud Instance. The most accurate and popular AI models today are typically\n(deep) neural networks, which are most performant on specialized, highly parallelized, and often energy-intensive\nhardware [43]. The most common scenario is for AI workloads to run on graphics processing units (GPUs), which provide\nsignificant acceleration compared to CPUs (central processing units) but are more power-hungry (often consuming\n250W-350W, compared to CPU consumption of 10-150W). Due to specialization to the matrix multiply operations at\nthe core of neural network computations and a high rate of parallelization, GPUs can perform many more of these\ntypes of computations in the same amount of time as a CPU, but this increased computation throughput comes at an\nincreased energy cost. Thus in ML applications based on deep learning, the majority of the electricity consumption\nis due to the GPU [ 5, 45]. While this result is fairly uncontroversial, we ran an experiment to confirm it. To do so,\nwe trained a BERT-base model [8] on a single NVIDIA TITAN X GPU (12 GB) in a commodity server with two Intel\nXeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs) to measure the relative electricity consumption\nof different components. We trained the model using the original code provided by Devlin et al. [8] on the language\nmodel pre-training task for 12 hours on one GPU, sampling the instantaneous energy use of the GPU, CPU and DRAM\nfor each socket throughout that period, then averaging to get average power draw per component in watts. GPU\nenergy draw was measured using nvidia-smi and CPU and DRAM power draw were obtained using Intel\u2019s RAPL.\nOur measurements, in watts, are presented in Table 1. As expected the GPU accounts for almost 3/4 of electricity\nconsumption.\nFocus on GPUs. In cloud datacenters, the CPUs, RAM, storage, and motherboards are often shared across multiple\ninstances; while this provides the flexibility that makes the cloud so useful, it leads to technical limitations that make it\n1There is related work on estimating and reducing the electricity of data centers in general, e.g., [10, 24].\n5",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "page": 5,
          "paragraph_index": 5,
          "section_id": "dodge2022:sec5",
          "section_index": 5
        }
      },
      {
        "node_id": "dodge2022:sec5",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "Page 5",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "page": 5,
          "section_index": 5
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s84",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "Measuring the Carbon Intensity of AI in Cloud Instances FAccT \u201922, June 21\u201324, 2022, Seoul, Republic of Korea\nwhere \ud835\udc36 = \ud835\udc42 + \ud835\udc40 is the software carbon intensity for a given cloud instance.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 84
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s85",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "In this paper, we focus on measuring\noperational emissions\ud835\udc42, and leave measurement and accounting for embodied emissions due to specialized ML hardware\nsuch as GPUs to future work (see \u00a78).",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 85
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s86",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "The objective of the Green Software Foundation\u2019s Software Carbon Intensity (SCI) specification is to calculate and\nreduce a SCI score, based on carbon emissions reductions, rather than the currently-used market-based neutralization.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 86
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s87",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "Specifically, the SCI uses a \"consequential\" carbon accounting approach, which aims to quantify the marginal change in\nemissions caused by decisions or interventions.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 87
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s88",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "This differs from the commonly used \"attributional\" carbon accounting\napproach, which uses average carbon intensity data, meaning it does not provide the most actionable information to\nhelp reduce carbon emissions.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 88
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s89",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "Due to the myriad potential pitfalls of relying on market-based measures in place of\nactual reduction in emissions [36], it is not possible to reduce the SCI through carbon neutralization or carbon offsets.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 89
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s90",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "We assert that cloud providers should provide the SCI to developers and data scientists to help them make choices that\nreduce the carbon footprint of their ML workloads.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 90
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s91",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "3.2 The Scope of our Tool: GPU Computation of a Single Cloud Instance\nData centers typically comprise many computer systems and hardware components, including storage, GPUs, CPUs,\nand networking components.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 91
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s92",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "We can break down the electricity usage for data centers into: 1) electricity that is used\nfor a single cloud instance, and 2) electricity that is used for the benefit of the whole data center.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 92
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s93",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "In this work we focus\non the former, a single cloud instance;",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 93
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s94",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "because of this, a reader should understand that our estimates of the electricity\nconsumption and emissions are underestimates.1\nElectricity Consumption from a Single Cloud Instance.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 94
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s95",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "The most accurate and popular AI models today are typically\n(deep) neural networks, which are most performant on specialized, highly parallelized, and often energy-intensive\nhardware [43].",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 95
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s96",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "The most common scenario is for AI workloads to run on graphics processing units (GPUs), which provide\nsignificant acceleration compared to CPUs (central processing units) but are more power-hungry (often consuming\n250W-350W, compared to CPU consumption of 10-150W).",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 96
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s97",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "Due to specialization to the matrix multiply operations at\nthe core of neural network computations and a high rate of parallelization, GPUs can perform many more of these\ntypes of computations in the same amount of time as a CPU, but this increased computation throughput comes at an\nincreased energy cost.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 97
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s98",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "Thus in ML applications based on deep learning, the majority of the electricity consumption\nis due to the GPU [ 5, 45].",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 98
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s99",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "While this result is fairly uncontroversial, we ran an experiment to confirm it.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 99
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s100",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "To do so,\nwe trained a BERT-base model [8] on a single NVIDIA TITAN X GPU (12 GB) in a commodity server with two Intel\nXeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs) to measure the relative electricity consumption\nof different components.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 100
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s101",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "We trained the model using the original code provided by Devlin et al.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 101
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s102",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "[8] on the language\nmodel pre-training task for 12 hours on one GPU, sampling the instantaneous energy use of the GPU, CPU and DRAM\nfor each socket throughout that period, then averaging to get average power draw per component in watts.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 102
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s103",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "GPU\nenergy draw was measured using nvidia-smi and CPU and DRAM power draw were obtained using Intel\u2019s RAPL.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 103
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s104",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "Our measurements, in watts, are presented in Table 1.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 104
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s105",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "As expected the GPU accounts for almost 3/4 of electricity\nconsumption.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 105
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s106",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "Focus on GPUs.",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 106
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s107",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "In cloud datacenters, the CPUs, RAM, storage, and motherboards are often shared across multiple\ninstances;",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 107
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s108",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "while this provides the flexibility that makes the cloud so useful, it leads to technical limitations that make it\n1There is related work on estimating and reducing the electricity of data centers in general, e.g., [10, 24].",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 108
        }
      },
      {
        "node_id": "dodge2022:sec5:p5:s109",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "5",
        "score": 0.7471,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec5:p5",
          "section_id": "dodge2022:sec5",
          "sentence_index": 109
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s331",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "However,\ndespite GPU power consumption being a significant factor and its\nusage correlating with the total power usage, it substantially under-\nrepresents the actual energy consumption since it measures just a\nsingle component.",
        "score": 0.7117,
        "rank": 5,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 331
        }
      },
      {
        "node_id": "ebert2024:sec7:p7",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "AI, Climate, and Regulation: From Data Centers to the AI Act Conference\u201917, July 2017, Washington, DC, USA\nimpacted by energy consumption reporting. We assume that they\npossess privileged access or considerable influence over the utilized\ninfrastructure given their financial power or strategic value. One\ncan distinguish between companies that develop closed models (e.g.,\nOpenAI) and those that create open-source models (e.g., Meta).\nStartups and smaller companies are expected to either directly\ndeploy such models, use available API services of leading model\nproviders, or fine-tune existing models. Although these companies\ndo not typically train large models themselves, they must report\nenergy consumption for fine-tuning. Under our proposed inter-\npretation, however, only if the fine-tuning of a model leads to a\nsubstantial modification.\n6.2 Levels for Measuring and Estimating Energy\nConsumption\nThere are several levels within a data center based on which energy\nconsumption may be measured or estimated [4]. These include (1)\nthe data center level, (2) the cumulative server level, (3) the GPU-\nLevel and other hardware within a server and (4) various other\nlevels. In this section, we outline each level along with their bene-\nfits, drawbacks, and estimation methods for when measurement is\nunavailable.\n6.2.1 Data Center Level. On the data center level, the power re-\nquired to operate the entire data center is measured, including both\nthe direct power consumption of computing equipment and the\nadditional overhead for cooling and maintaining the data center.\nThis approach provides the most extensive and complete figures\nsince it represents the actual energy usage, but also assumes that a\ndata center is exclusively utilized for the pre-training by the model\nprovider. It encourages the selection of an efficient data center. Ad-\nditionally, data centers have average PUE values of 1.58, so this\noverhead makes up a significant portion of the energy consump-\ntion. On the other hand, the power usage resulting solely from the\nmodel\u2019s architecture, the quantity of training data, the efficiency of\nthe implementation, and experimental settings is very important\nbut is somewhat skewed by the efficiency of the data center.\nIf data-center level power consumption measurement is not avail-\nable, using the PUE factor for estimation is deemed appropriate. To\ncalculate total energy usage, the PUE factor is multiplied by the raw\ncomputational power consumption measured or estimated at the cu-\nmulative server or rack level (see below). This might be reasonable,\nif only parts of a data center are utilized and only measurements\ncloser to the ICT equipment are available.\n6.2.2 Cumulative Server Level. A large-scale model is trained across\nmany servers in a distributed manner. Each server includes GPUs\nresponsible for the primary computation. To accurately monitor\nthe power consumption over time, a local power distribution unit\n(PDU), capable of measuring the provided power, is attached to each\nserver. Aggregating these measurements yields a highly precise\nfigure of the total energy consumption attributable to the model\u2019s\ncomputations. Instead of aggregating local PDUs, the usage of pri-\nmary PDUs or uninterruptible power supply (UPS) systems already\nmeasuring at the rack level or even many racks is also suitable\n(See also Appendix, Figure 1), as long as the measurements pre-\ncisely match fully the utilized hardware resources by the model\nproviders. The goal is to include all ICT-related power consumption\nbut exclude data center specific efficiency properties.\nThe upside of this method is its high accuracy, highly correlating\nwith model size and structure, data quantity, and hardware-aware\nsoftware implementation. It is widely recognized in the industry\nfor assessing power consumption in data centers. According to a\n2023 Green Grid industry survey [77], 66% of data centers can track\npower demand at least on rack-level. Roughly one third of over-\nall data centers are already able to collect average utilization and\npower demand data for individual servers and storage equipment\nand match this data to their IT equipment inventory. We assume\nthat the data centers that are used for training by large model\nproviders are already able to track this information given its high\ncost relevance. However, a significant number of data centers do not\ntrack power demand yet. The surveyed data center professionals\nestimate to require between 3-6 months (Europe: 15%, Global: 19%),\n1 year (Europe: 29%, Global: 28%), 2 years (Europe: 12%, Global:\n10%), 3 years (Europe: 4%, Global: 4%) or more than 4 years (Europe:\n11%, Global: 8%) to implement adequate power collection abilities.\nFor European data centers these numbers stand in (surprising) con-\ntrast to the obligation under the EED and Delegated Regulation to\nprovide energy consumption data by Sep 15, 2024 (Art. 3(1) and\nAnnex II(1)(e) Delegated Regulation). Either a substantial number\nof the participants was unaware of the obligations; or thereby ac-\nknowledged their inability to comply in time; or the question was\ntoo narrow, being pointed at individual servers, while the EED also\nallows measurement at the higher-level UPS.\nAn estimation is possible when GPU hours and the hardware\nused are known. By multiplying a GPU-hour with the peak utiliza-\ntion power consumption specified by the manufacturer, one can\nestimate the upper limit of energy consumption at the server level.\nThis upper bound is typically higher than the actual consumption,\nas GPU utilization rarely reaches its theoretical peak due to other\nresource constraints. Higher GPU efficiency means that the same\noperations are completed in less time, reducing the overhead from\nnon-utilization-dependent hardware power consumption.\n6.2.3 GPU-Level and other hardware within a server. The mea-\nsurement may be based on the energy usage of particular compo-\nnents as determined by on-chip sensors. Nvidia GPUs and certain\nCPUs already provide straightforward power consumption moni-\ntoring, therefore estimations are usually not necessary. However,\ndespite GPU power consumption being a significant factor and its\nusage correlating with the total power usage, it substantially under-\nrepresents the actual energy consumption since it measures just a\nsingle component. CPU power usage is a relatively minor factor in\nconsumption. Most other server components cannot be measured.\nWe advocate against using GPU-level or other component-based\npower consumption tracking for overall energy measurements.\n6.2.4 Other levels. Other measurement levels, such as Workload,\nCloud Instance, or Virtual Machine, involve high complexity and\nnumerous assumptions, resulting in a lack of standardized mea-\nsurement or estimation methods with considerable uncertainty. We\nadvise against using these levels for power consumption tracking.\n6.2.5 Interim Conclusion on the Level of Measurement. In our anal-\nysis, we argue that energy consumption should be measured and",
        "score": 0.7117,
        "rank": 5,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "page": 7,
          "paragraph_index": 7,
          "section_id": "ebert2024:sec7",
          "section_index": 7
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s54",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "[ 26] measure GPU\npower draw across prompt lengths but exclude proprietary systems and broader environmental factors,\nlacking a standardized scaling method for production-level inference.",
        "score": 0.6948,
        "rank": 6,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 54
        }
      },
      {
        "node_id": "jegham2025:sec2:p2",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Despite the growing environmental footprint of large-scale model deployment, a standard method\nto quantify the cost of inference at the prompt level remains absent. A core obstacle to developing\nmore accurate assessments is the lack of model-specific inference data for commercial AI models.\nExisting environmental reports tend to aggregate emissions across entire cloud infrastructures without\ndisaggregating by model or workload [16, 17]. This lack of public information hinders independent\nverification and undermines both scientific benchmarking and policy efforts aimed at regulating AI\u2019s\ntrue environmental cost.\nTo address these issues, we introduce a novel infrastructure-aware benchmarking framework to\nquantify the operational environmental footprint of LLM inference at the per-prompt level as deployed\nin data centers. Unlike existing studies [ 13, 15, 18], our method adopts a more comprehensive\nstrategy by integrating performance metrics such as latency and throughput from public APIs with\npublished GPU and system power specifications. Furthermore, we scale these combined data\npoints using company-specific multipliers, including Power Usage Effectiveness (PUE) [ 19, 20],\nWater Usage Effectiveness (WUE) [19, 20], and Carbon Intensity Factors (CIF) [21, 22] to account\nfor infrastructural overhead. This method enables us to evaluate the energy, water, and carbon\neffects of both open-source and proprietary models, a gap that, to our knowledge, has not been\ncomprehensively explored in prior research. Additionally, we employ statistical analysis, including\nANOV A and Tukey HSD, to estimate underlying hardware configurations. To enhance transparency\nand reproducibility, we also developed an automated and interactive Power BI dashboard that\nvisualizes the daily fluctuations in the energy, water, and carbon footprint of an extended list of models\nacross multiple data centers. This novel dashboard incorporates new models as they get released.\nMoreover, to contextualize resource use relative to model capability, we apply cross-efficiency Data\nEnvelopment Analysis (DEA) to assess how effectively each model converts environmental inputs\ninto performance. As a key application of this framework, we perform a case study to estimate the\nfootprint of GPT-4o text generation based on scaled usage data. We further extend our analysis to\nGPT-5, focusing on the disparities in energy consumption between queries that involve different levels\nof reasoning. Our framework enables infrastructure-aware decision-making, empowers accountability,\nand provides a foundational step toward sustainability standards in AI deployment.\nThe remainder of the paper is organized as follows. Section 2 reviews existing studies on the\nenvironmental impact of LLMs. Section 3 introduces key concepts, including hardware configurations\nand environmental multipliers. Section 4 details our framework for estimating inference-phase cost.\nSection 5 presents findings across 30 models. Section 6 provides a focused analysis of GPT-4o\u2019s\nannual environmental footprint and section 7 analyzes the impact of GPT-5\u2019s adapative model routing.\nSection 8 outlines key insights and implications. Section 9 summarizes the main takeaways and\nlimitations and directions for future work.\n2 Related Work\nThe environmental impact of AI systems has garnered increasing attention in recent years, with a\ngrowing body of work attempting to quantify the energy, carbon, and water costs associated with\ntraining and deploying LLMs.\nLi et al. [13] analyzed GPT-3\u2019s freshwater consumption, estimating over 5 million liters used during\ntraining and projecting that AI-related withdrawals could reach 6.6 trillion liters annually by 2027.\nAlthough their spatiotemporal methodology is a significant early contribution, it overlooks carbon\nemissions, depends on an outdated model, and requires previous knowledge of energy usage, which\nrestricts its scalability. In parallel, Strubell et al. [ 23] estimated carbon emissions from training\nBERT and GPT-2 by accounting for GPU, CPU, and DRAM power draw alongside PUE adjustments.\nHowever, their analysis excludes inference and infrastructural overhead. Similar limitations appear\nin Meta\u2019s LLaMA reports [7, 24, 25], which provide carbon footprints based on GPUs\u2019 TDPs but\ndisregard water use, system-wide energy consumption, and the inference phase entirely.\nRegarding inference, Husom et al. [18] (MELODI) measure real-time energy consumption of GPUs\nand CPUs at the prompt level, but they neglect carbon emissions, water usage, and infrastructure\noverhead, only concentrating on small-scale open-source models. Samsi et al. [ 26] measure GPU\npower draw across prompt lengths but exclude proprietary systems and broader environmental factors,\nlacking a standardized scaling method for production-level inference. Yang et al. [ 27] evaluate\nover 1,200 vision models and introduce an energy-efficiency score. However, their analysis does\n2",
        "score": 0.6948,
        "rank": 6,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "page": 2,
          "paragraph_index": 2,
          "section_id": "jegham2025:sec2",
          "section_index": 2
        }
      },
      {
        "node_id": "dodge2022:sec6:p6:s119",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "For this reason, and because they typically consume the majority\nof electricity in AI workloads, in this work we only measure GPU electricity consumption.",
        "score": 0.6909,
        "rank": 7,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec6:p6",
          "section_id": "dodge2022:sec6",
          "sentence_index": 119
        }
      },
      {
        "node_id": "dodge2022:sec6:p6",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "FAccT \u201922, June 21\u201324, 2022, Seoul, Republic of Korea Dodge et al.\nHardwa. GPU CPU0 CPU1 DRAM0 DRAM1 Total\nWatts 187.1 22.9 9.3 23.0 9.3 251.6\nFraction 74% 9% 4% 9% 4% 100%\nTable 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB),\nin a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is\naveraged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU\nalone accounts for 74% of the total energy consumption due to these components.\ndifficult (and in some cases impossible) to properly estimate electricity consumption from these sources for a single\ninstance. However, GPUs are typically not shared across instances, and in fact for large AI workloads it\u2019s often the\ncase that multiple GPUs are attached to a single instance, leading to an even greater proportion of the total energy\nconsumption being used by the GPUs. Thus, it is relatively easy to measure the GPU electricity consumption for a\nsingle instance, while it is not for other components. For this reason, and because they typically consume the majority\nof electricity in AI workloads, in this work we only measure GPU electricity consumption. We recognize this is a first\nstep towards a more complete measurement, and provide further discussion in the next section.2\nOther sources of CO 2. Data centers have a number of electricity uses that are important, but will not be covered by\nour tool. According to the U.S. Department of Energy: \u201cThe electricity consumed in these data centers is mainly by the\nequipment (50%) and HVAC (25%\u201340%)\u201d [47]. Such other sources of emissions can be accounted for using methods\nsuch as Power Usage Effectiveness (PUE), which can be used to describe the proportion of electricity consumption\nby the computing equipment vs. other sources. For a given datacenter, this can be turned into a factor which can be\nmultiplied against the electricity consumption of computing equipment to get an estimate of the total consumption.\nSome companies have highlighted particularly low PUEs, such as Google claiming a PUE of 1.10 across its fleet of data\ncenters for the 12 months ending in Q1 2021,3 compared to an average global PUE of 1.59 [2].\nOther factors, such as the emissions produced by maintenance workers driving to and from the data center, emissions\nfrom manufacturing the computer systems, and emissions from building the structure in which the data center is\nhoused4 are non-negligible but beyond the scope of this paper. Finally, for workloads that do not use GPUs (e.g., storage\nor web hosting) we recommend users choose low emissions regions and times of day, as they will not have access to\nsingle-instance emissions calculations. We leave it open for future research to address how to appropriately allocate\nCO2 emissions from such data center-wide processes to individual reserved cloud instances.\n4 ELECTRICITY CONSUMPTION FOR AI WORKLOADS\nAs outlined in \u00a73.1, calculating software carbon intensity begins with recording the electricity consumption, which\ncan then be mapped to emissions based on the emissions of the grid being used. In this section, we present data on\nelectricity consumption for experiments training 11 different models, covering natural language processing (NLP) and\ncomputer vision applications, ranging from less than an hour on a single GPU up to more than 8 days on 256 GPUs. We\noutline both the experiments themselves and their electricity consumption, and in the following section we use the\nelectricity consumption and carbon intensity tool described in the previous section to calculate their software carbon\nintensity.\n2We note that our conclusions drawn from experiments and analyses on time-shifting and location-shifting are still applicable with tools that measure\nmore electricity than just the GPU.\n3https://www.google.com/about/datacenters/efficiency/\n4One of the largest single source of CO2 emissions, contributing to 7%-8% of global emissions, is the production of cement [20].\n6",
        "score": 0.6909,
        "rank": 7,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "page": 6,
          "paragraph_index": 6,
          "section_id": "dodge2022:sec6",
          "section_index": 6
        }
      },
      {
        "node_id": "ebert2024:sec7:p7",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "AI, Climate, and Regulation: From Data Centers to the AI Act Conference\u201917, July 2017, Washington, DC, USA\nimpacted by energy consumption reporting. We assume that they\npossess privileged access or considerable influence over the utilized\ninfrastructure given their financial power or strategic value. One\ncan distinguish between companies that develop closed models (e.g.,\nOpenAI) and those that create open-source models (e.g., Meta).\nStartups and smaller companies are expected to either directly\ndeploy such models, use available API services of leading model\nproviders, or fine-tune existing models. Although these companies\ndo not typically train large models themselves, they must report\nenergy consumption for fine-tuning. Under our proposed inter-\npretation, however, only if the fine-tuning of a model leads to a\nsubstantial modification.\n6.2 Levels for Measuring and Estimating Energy\nConsumption\nThere are several levels within a data center based on which energy\nconsumption may be measured or estimated [4]. These include (1)\nthe data center level, (2) the cumulative server level, (3) the GPU-\nLevel and other hardware within a server and (4) various other\nlevels. In this section, we outline each level along with their bene-\nfits, drawbacks, and estimation methods for when measurement is\nunavailable.\n6.2.1 Data Center Level. On the data center level, the power re-\nquired to operate the entire data center is measured, including both\nthe direct power consumption of computing equipment and the\nadditional overhead for cooling and maintaining the data center.\nThis approach provides the most extensive and complete figures\nsince it represents the actual energy usage, but also assumes that a\ndata center is exclusively utilized for the pre-training by the model\nprovider. It encourages the selection of an efficient data center. Ad-\nditionally, data centers have average PUE values of 1.58, so this\noverhead makes up a significant portion of the energy consump-\ntion. On the other hand, the power usage resulting solely from the\nmodel\u2019s architecture, the quantity of training data, the efficiency of\nthe implementation, and experimental settings is very important\nbut is somewhat skewed by the efficiency of the data center.\nIf data-center level power consumption measurement is not avail-\nable, using the PUE factor for estimation is deemed appropriate. To\ncalculate total energy usage, the PUE factor is multiplied by the raw\ncomputational power consumption measured or estimated at the cu-\nmulative server or rack level (see below). This might be reasonable,\nif only parts of a data center are utilized and only measurements\ncloser to the ICT equipment are available.\n6.2.2 Cumulative Server Level. A large-scale model is trained across\nmany servers in a distributed manner. Each server includes GPUs\nresponsible for the primary computation. To accurately monitor\nthe power consumption over time, a local power distribution unit\n(PDU), capable of measuring the provided power, is attached to each\nserver. Aggregating these measurements yields a highly precise\nfigure of the total energy consumption attributable to the model\u2019s\ncomputations. Instead of aggregating local PDUs, the usage of pri-\nmary PDUs or uninterruptible power supply (UPS) systems already\nmeasuring at the rack level or even many racks is also suitable\n(See also Appendix, Figure 1), as long as the measurements pre-\ncisely match fully the utilized hardware resources by the model\nproviders. The goal is to include all ICT-related power consumption\nbut exclude data center specific efficiency properties.\nThe upside of this method is its high accuracy, highly correlating\nwith model size and structure, data quantity, and hardware-aware\nsoftware implementation. It is widely recognized in the industry\nfor assessing power consumption in data centers. According to a\n2023 Green Grid industry survey [77], 66% of data centers can track\npower demand at least on rack-level. Roughly one third of over-\nall data centers are already able to collect average utilization and\npower demand data for individual servers and storage equipment\nand match this data to their IT equipment inventory. We assume\nthat the data centers that are used for training by large model\nproviders are already able to track this information given its high\ncost relevance. However, a significant number of data centers do not\ntrack power demand yet. The surveyed data center professionals\nestimate to require between 3-6 months (Europe: 15%, Global: 19%),\n1 year (Europe: 29%, Global: 28%), 2 years (Europe: 12%, Global:\n10%), 3 years (Europe: 4%, Global: 4%) or more than 4 years (Europe:\n11%, Global: 8%) to implement adequate power collection abilities.\nFor European data centers these numbers stand in (surprising) con-\ntrast to the obligation under the EED and Delegated Regulation to\nprovide energy consumption data by Sep 15, 2024 (Art. 3(1) and\nAnnex II(1)(e) Delegated Regulation). Either a substantial number\nof the participants was unaware of the obligations; or thereby ac-\nknowledged their inability to comply in time; or the question was\ntoo narrow, being pointed at individual servers, while the EED also\nallows measurement at the higher-level UPS.\nAn estimation is possible when GPU hours and the hardware\nused are known. By multiplying a GPU-hour with the peak utiliza-\ntion power consumption specified by the manufacturer, one can\nestimate the upper limit of energy consumption at the server level.\nThis upper bound is typically higher than the actual consumption,\nas GPU utilization rarely reaches its theoretical peak due to other\nresource constraints. Higher GPU efficiency means that the same\noperations are completed in less time, reducing the overhead from\nnon-utilization-dependent hardware power consumption.\n6.2.3 GPU-Level and other hardware within a server. The mea-\nsurement may be based on the energy usage of particular compo-\nnents as determined by on-chip sensors. Nvidia GPUs and certain\nCPUs already provide straightforward power consumption moni-\ntoring, therefore estimations are usually not necessary. However,\ndespite GPU power consumption being a significant factor and its\nusage correlating with the total power usage, it substantially under-\nrepresents the actual energy consumption since it measures just a\nsingle component. CPU power usage is a relatively minor factor in\nconsumption. Most other server components cannot be measured.\nWe advocate against using GPU-level or other component-based\npower consumption tracking for overall energy measurements.\n6.2.4 Other levels. Other measurement levels, such as Workload,\nCloud Instance, or Virtual Machine, involve high complexity and\nnumerous assumptions, resulting in a lack of standardized mea-\nsurement or estimation methods with considerable uncertainty. We\nadvise against using these levels for power consumption tracking.\n6.2.5 Interim Conclusion on the Level of Measurement. In our anal-\nysis, we argue that energy consumption should be measured and",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "page": 7,
          "paragraph_index": 7,
          "section_id": "ebert2024:sec7",
          "section_index": 7
        }
      },
      {
        "node_id": "ebert2024:sec7",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "Page 7",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "page": 7,
          "section_index": 7
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s287",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "AI, Climate, and Regulation: From Data Centers to the AI Act Conference\u201917, July 2017, Washington, DC, USA\nimpacted by energy consumption reporting.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 287
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s288",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "We assume that they\npossess privileged access or considerable influence over the utilized\ninfrastructure given their financial power or strategic value.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 288
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s289",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "One\ncan distinguish between companies that develop closed models (e.g.,\nOpenAI) and those that create open-source models (e.g., Meta).",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 289
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s290",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "Startups and smaller companies are expected to either directly\ndeploy such models, use available API services of leading model\nproviders, or fine-tune existing models.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 290
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s291",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "Although these companies\ndo not typically train large models themselves, they must report\nenergy consumption for fine-tuning.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 291
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s292",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "Under our proposed inter-\npretation, however, only if the fine-tuning of a model leads to a\nsubstantial modification.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 292
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s293",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "6.2 Levels for Measuring and Estimating Energy\nConsumption\nThere are several levels within a data center based on which energy\nconsumption may be measured or estimated [4].",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 293
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s294",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "These include (1)\nthe data center level, (2) the cumulative server level, (3) the GPU-\nLevel and other hardware within a server and (4) various other\nlevels.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 294
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s295",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "In this section, we outline each level along with their bene-\nfits, drawbacks, and estimation methods for when measurement is\nunavailable.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 295
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s296",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "6.2.1 Data Center Level.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 296
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s297",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "On the data center level, the power re-\nquired to operate the entire data center is measured, including both\nthe direct power consumption of computing equipment and the\nadditional overhead for cooling and maintaining the data center.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 297
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s298",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "This approach provides the most extensive and complete figures\nsince it represents the actual energy usage, but also assumes that a\ndata center is exclusively utilized for the pre-training by the model\nprovider.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 298
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s299",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "It encourages the selection of an efficient data center.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 299
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s300",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "Ad-\nditionally, data centers have average PUE values of 1.58, so this\noverhead makes up a significant portion of the energy consump-\ntion.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 300
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s301",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "On the other hand, the power usage resulting solely from the\nmodel\u2019s architecture, the quantity of training data, the efficiency of\nthe implementation, and experimental settings is very important\nbut is somewhat skewed by the efficiency of the data center.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 301
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s302",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "If data-center level power consumption measurement is not avail-\nable, using the PUE factor for estimation is deemed appropriate.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 302
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s303",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "To\ncalculate total energy usage, the PUE factor is multiplied by the raw\ncomputational power consumption measured or estimated at the cu-\nmulative server or rack level (see below).",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 303
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s304",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "This might be reasonable,\nif only parts of a data center are utilized and only measurements\ncloser to the ICT equipment are available.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 304
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s305",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "6.2.2 Cumulative Server Level.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 305
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s306",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "A large-scale model is trained across\nmany servers in a distributed manner.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 306
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s307",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "Each server includes GPUs\nresponsible for the primary computation.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 307
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s308",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "To accurately monitor\nthe power consumption over time, a local power distribution unit\n(PDU), capable of measuring the provided power, is attached to each\nserver.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 308
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s309",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "Aggregating these measurements yields a highly precise\nfigure of the total energy consumption attributable to the model\u2019s\ncomputations.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 309
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s310",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "Instead of aggregating local PDUs, the usage of pri-\nmary PDUs or uninterruptible power supply (UPS) systems already\nmeasuring at the rack level or even many racks is also suitable\n(See also Appendix, Figure 1), as long as the measurements pre-\ncisely match fully the utilized hardware resources by the model\nproviders.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 310
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s311",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "The goal is to include all ICT-related power consumption\nbut exclude data center specific efficiency properties.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 311
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s312",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "The upside of this method is its high accuracy, highly correlating\nwith model size and structure, data quantity, and hardware-aware\nsoftware implementation.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 312
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s313",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "It is widely recognized in the industry\nfor assessing power consumption in data centers.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 313
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s314",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "According to a\n2023 Green Grid industry survey [77], 66% of data centers can track\npower demand at least on rack-level.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 314
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s315",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "Roughly one third of over-\nall data centers are already able to collect average utilization and\npower demand data for individual servers and storage equipment\nand match this data to their IT equipment inventory.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 315
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s316",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "We assume\nthat the data centers that are used for training by large model\nproviders are already able to track this information given its high\ncost relevance.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 316
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s317",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "However, a significant number of data centers do not\ntrack power demand yet.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 317
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s318",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "The surveyed data center professionals\nestimate to require between 3-6 months (Europe: 15%, Global: 19%),\n1 year (Europe: 29%, Global: 28%), 2 years (Europe: 12%, Global:\n10%), 3 years (Europe: 4%, Global: 4%) or more than 4 years (Europe:\n11%, Global: 8%) to implement adequate power collection abilities.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 318
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s319",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "For European data centers these numbers stand in (surprising) con-\ntrast to the obligation under the EED and Delegated Regulation to\nprovide energy consumption data by Sep 15, 2024 (Art.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 319
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s320",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "3(1) and\nAnnex II(1)(e) Delegated Regulation).",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 320
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s321",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "Either a substantial number\nof the participants was unaware of the obligations;",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 321
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s322",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "or thereby ac-\nknowledged their inability to comply in time;",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 322
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s323",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "or the question was\ntoo narrow, being pointed at individual servers, while the EED also\nallows measurement at the higher-level UPS.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 323
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s324",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "An estimation is possible when GPU hours and the hardware\nused are known.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 324
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s325",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "By multiplying a GPU-hour with the peak utiliza-\ntion power consumption specified by the manufacturer, one can\nestimate the upper limit of energy consumption at the server level.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 325
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s326",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "This upper bound is typically higher than the actual consumption,\nas GPU utilization rarely reaches its theoretical peak due to other\nresource constraints.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 326
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s327",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "Higher GPU efficiency means that the same\noperations are completed in less time, reducing the overhead from\nnon-utilization-dependent hardware power consumption.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 327
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s328",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "6.2.3 GPU-Level and other hardware within a server.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 328
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s329",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "The mea-\nsurement may be based on the energy usage of particular compo-\nnents as determined by on-chip sensors.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 329
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s330",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "Nvidia GPUs and certain\nCPUs already provide straightforward power consumption moni-\ntoring, therefore estimations are usually not necessary.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 330
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s331",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "However,\ndespite GPU power consumption being a significant factor and its\nusage correlating with the total power usage, it substantially under-\nrepresents the actual energy consumption since it measures just a\nsingle component.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 331
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s332",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "CPU power usage is a relatively minor factor in\nconsumption.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 332
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s333",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "Most other server components cannot be measured.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 333
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s334",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "We advocate against using GPU-level or other component-based\npower consumption tracking for overall energy measurements.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 334
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s335",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "6.2.4 Other levels.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 335
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s336",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "Other measurement levels, such as Workload,\nCloud Instance, or Virtual Machine, involve high complexity and\nnumerous assumptions, resulting in a lack of standardized mea-\nsurement or estimation methods with considerable uncertainty.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 336
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s337",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "We\nadvise against using these levels for power consumption tracking.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 337
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s338",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "6.2.5 Interim Conclusion on the Level of Measurement.",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 338
        }
      },
      {
        "node_id": "ebert2024:sec7:p7:s339",
        "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
        "text": "In our anal-\nysis, we argue that energy consumption should be measured and",
        "score": 0.6811,
        "rank": 8,
        "metadata": {
          "document_id": "ebert2024",
          "document_title": "AI, Climate, and Regulation: From Data Centers to the AI Act",
          "paragraph_id": "ebert2024:sec7:p7",
          "section_id": "ebert2024:sec7",
          "sentence_index": 339
        }
      },
      {
        "node_id": "morrison2025:sec2:p2:s34",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "For example, unlike previous works\nthat assume GPUs operate at 100% of their theoretical maximum power draw (Dubey et al., 2024)\nand report only the cost to train a small set of released models, we measure power consumption\nat sub-second intervals throughout training.",
        "score": 0.6793,
        "rank": 9,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec2:p2",
          "section_id": "morrison2025:sec2",
          "sentence_index": 34
        }
      },
      {
        "node_id": "morrison2025:sec2:p2",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Published as a conference paper at ICLR 2025\nincreasing numbers of stakeholders become involved in the development and use of AI systems, it is\nimperative to carefully characterize the true cost of building and deploying state-of-the-art models,\nto inform effective strategies for mitigating potential harms and planning for future demand.\n100 101 102\nCarbon Emissions (tCO eq)\n100\n101\n102\n103\nWater Consumption (kL)OLMo 20M\nOLMo 150M\nOLMo 700M\nOLMo 1B\nOLMo 2 7B\nOLMo 2 13B\nOLMoE 0924\nFigure 1: Environmental impact for a selection of the\nfinal training runs described in Section 4.1, where we\nrank each model by both its total water consumption\nand its CO2 emissions. Our small models (<1B param-\neters) were trained on 1.7 trillion tokens, OLMo 1B was\ntrained on 3 trillion, OLMo 2 7B was trained on 4 tril-\nlion, OLMoE was trained on 5 trillion, and OLMo 2\n13B was trained on 5.6 trillion. We see that the total\nenvironmental impact for larger training runs is quite\nhigh, and increases quickly with model and dataset size.\nIn this paper, we estimate the energy use and\nenvironmental impacts caused by training the\nOLMo series of transformer language models\n(Groeneveld et al., 2024; OLMo et al., 2025),\nranging in size from 20 million to 13 billion\nactive parameters, trained on 1.7 to 5.6 trillion\ntokens. To do this, we calculate Scope 2 CO 2\nemissions in accordance with the Greenhouse\nGas Protocol\u2019s definitions,3 and Scope 1 and 2\nwater consumption following Li et al. (2023);\nin addition, we calculate \u201cupstream\u201d embod-\nied carbon and water consumption, and provide\n\u201cdownstream\u201d estimates from use of our mod-\nels (which are part, but not all, of Scope 3).\nImportantly, we calculate (i) electricity con-\nsumption, (ii) carbon emissions, and (iii) wa-\nter consumption at three points in the machine\nlearning pipeline: early model development\n(e.g., hyperparameter tuning and experiments\nbefore the final training run), training of the\nmain model, and inference. To the best of our\nknowledge, we are the first to report this in-\nformation for model development of large lan-\nguage models, and we find the environmental\nimpact of developing even our relatively small\nmodels (only up to 13B parameters) is equivalent to burning 2.1 gasoline tanker trucks of fuel, or\nthe amount of water consumed by one average person in the United States in about 7.5 years. We\nencourage the reader to consider larger models released by other organizations to have equivalently\nlarger environmental impacts.\nOur methodology draws upon best practices from recent publications, aiming to provide the most\nthorough reporting yet of the environmental impact of LLMs. For example, unlike previous works\nthat assume GPUs operate at 100% of their theoretical maximum power draw (Dubey et al., 2024)\nand report only the cost to train a small set of released models, we measure power consumption\nat sub-second intervals throughout training. We focus our efforts on a wide range of model sizes,\noptimized for widespread deployment (Dubey et al., 2024; Mehta et al., 2024; Gemma Team et al.,\n2024), and estimate what the environmental impact would be if our models were deployed in a va-\nriety of different scenarios. We find that in some scenarios, our models would need to run inference\non a few billion instances to match the electricity consumed, carbon emitted, and water consumed\nof the entire training process, a figure that can be reached by production systems in weeks to months\nbased on current usage trends.4\nWe conclude that more transparency is needed across the industry in reporting the environmental\nimpact of AI systems. Systems orders of magnitude larger than those in this paper are being built,\nand deployed at a global scale, leading to emissions 10s or 100s of times larger than what we\nreport. This work is a step in the right direction, but responsibility of reporting and reducing the\nenvironmental impact must fall on those training the largest models, as they have the largest impact.\n2 R ELATED WORK\nWhile most publicly available models do not report any climate impact, including CO 2 emissions,\nwater usage, or embodied carbon, a few reports recently have included some estimates. For example,\n3https://ghgprotocol.org/sites/default/files/standards/ghg-protocol-revised.pdf\n4https://www.cnbc.com/2025/02/20/openai-tops-400-million-users-despite-deepseeks-emergence.\nhtml\n2",
        "score": 0.6793,
        "rank": 9,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "page": 2,
          "paragraph_index": 2,
          "section_id": "morrison2025:sec2",
          "section_index": 2
        }
      },
      {
        "node_id": "jegham2025:sec2:p2",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Despite the growing environmental footprint of large-scale model deployment, a standard method\nto quantify the cost of inference at the prompt level remains absent. A core obstacle to developing\nmore accurate assessments is the lack of model-specific inference data for commercial AI models.\nExisting environmental reports tend to aggregate emissions across entire cloud infrastructures without\ndisaggregating by model or workload [16, 17]. This lack of public information hinders independent\nverification and undermines both scientific benchmarking and policy efforts aimed at regulating AI\u2019s\ntrue environmental cost.\nTo address these issues, we introduce a novel infrastructure-aware benchmarking framework to\nquantify the operational environmental footprint of LLM inference at the per-prompt level as deployed\nin data centers. Unlike existing studies [ 13, 15, 18], our method adopts a more comprehensive\nstrategy by integrating performance metrics such as latency and throughput from public APIs with\npublished GPU and system power specifications. Furthermore, we scale these combined data\npoints using company-specific multipliers, including Power Usage Effectiveness (PUE) [ 19, 20],\nWater Usage Effectiveness (WUE) [19, 20], and Carbon Intensity Factors (CIF) [21, 22] to account\nfor infrastructural overhead. This method enables us to evaluate the energy, water, and carbon\neffects of both open-source and proprietary models, a gap that, to our knowledge, has not been\ncomprehensively explored in prior research. Additionally, we employ statistical analysis, including\nANOV A and Tukey HSD, to estimate underlying hardware configurations. To enhance transparency\nand reproducibility, we also developed an automated and interactive Power BI dashboard that\nvisualizes the daily fluctuations in the energy, water, and carbon footprint of an extended list of models\nacross multiple data centers. This novel dashboard incorporates new models as they get released.\nMoreover, to contextualize resource use relative to model capability, we apply cross-efficiency Data\nEnvelopment Analysis (DEA) to assess how effectively each model converts environmental inputs\ninto performance. As a key application of this framework, we perform a case study to estimate the\nfootprint of GPT-4o text generation based on scaled usage data. We further extend our analysis to\nGPT-5, focusing on the disparities in energy consumption between queries that involve different levels\nof reasoning. Our framework enables infrastructure-aware decision-making, empowers accountability,\nand provides a foundational step toward sustainability standards in AI deployment.\nThe remainder of the paper is organized as follows. Section 2 reviews existing studies on the\nenvironmental impact of LLMs. Section 3 introduces key concepts, including hardware configurations\nand environmental multipliers. Section 4 details our framework for estimating inference-phase cost.\nSection 5 presents findings across 30 models. Section 6 provides a focused analysis of GPT-4o\u2019s\nannual environmental footprint and section 7 analyzes the impact of GPT-5\u2019s adapative model routing.\nSection 8 outlines key insights and implications. Section 9 summarizes the main takeaways and\nlimitations and directions for future work.\n2 Related Work\nThe environmental impact of AI systems has garnered increasing attention in recent years, with a\ngrowing body of work attempting to quantify the energy, carbon, and water costs associated with\ntraining and deploying LLMs.\nLi et al. [13] analyzed GPT-3\u2019s freshwater consumption, estimating over 5 million liters used during\ntraining and projecting that AI-related withdrawals could reach 6.6 trillion liters annually by 2027.\nAlthough their spatiotemporal methodology is a significant early contribution, it overlooks carbon\nemissions, depends on an outdated model, and requires previous knowledge of energy usage, which\nrestricts its scalability. In parallel, Strubell et al. [ 23] estimated carbon emissions from training\nBERT and GPT-2 by accounting for GPU, CPU, and DRAM power draw alongside PUE adjustments.\nHowever, their analysis excludes inference and infrastructural overhead. Similar limitations appear\nin Meta\u2019s LLaMA reports [7, 24, 25], which provide carbon footprints based on GPUs\u2019 TDPs but\ndisregard water use, system-wide energy consumption, and the inference phase entirely.\nRegarding inference, Husom et al. [18] (MELODI) measure real-time energy consumption of GPUs\nand CPUs at the prompt level, but they neglect carbon emissions, water usage, and infrastructure\noverhead, only concentrating on small-scale open-source models. Samsi et al. [ 26] measure GPU\npower draw across prompt lengths but exclude proprietary systems and broader environmental factors,\nlacking a standardized scaling method for production-level inference. Yang et al. [ 27] evaluate\nover 1,200 vision models and introduce an energy-efficiency score. However, their analysis does\n2",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "page": 2,
          "paragraph_index": 2,
          "section_id": "jegham2025:sec2",
          "section_index": 2
        }
      },
      {
        "node_id": "jegham2025:sec2",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Page 2",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "page": 2,
          "section_index": 2
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s20",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Despite the growing environmental footprint of large-scale model deployment, a standard method\nto quantify the cost of inference at the prompt level remains absent.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 20
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s21",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "A core obstacle to developing\nmore accurate assessments is the lack of model-specific inference data for commercial AI models.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 21
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s22",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Existing environmental reports tend to aggregate emissions across entire cloud infrastructures without\ndisaggregating by model or workload [16, 17].",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 22
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s23",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "This lack of public information hinders independent\nverification and undermines both scientific benchmarking and policy efforts aimed at regulating AI\u2019s\ntrue environmental cost.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 23
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s24",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "To address these issues, we introduce a novel infrastructure-aware benchmarking framework to\nquantify the operational environmental footprint of LLM inference at the per-prompt level as deployed\nin data centers.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 24
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s25",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Unlike existing studies [ 13, 15, 18], our method adopts a more comprehensive\nstrategy by integrating performance metrics such as latency and throughput from public APIs with\npublished GPU and system power specifications.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 25
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s26",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Furthermore, we scale these combined data\npoints using company-specific multipliers, including Power Usage Effectiveness (PUE) [ 19, 20],\nWater Usage Effectiveness (WUE) [19, 20], and Carbon Intensity Factors (CIF) [21, 22] to account\nfor infrastructural overhead.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 26
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s27",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "This method enables us to evaluate the energy, water, and carbon\neffects of both open-source and proprietary models, a gap that, to our knowledge, has not been\ncomprehensively explored in prior research.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 27
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s28",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Additionally, we employ statistical analysis, including\nANOV A and Tukey HSD, to estimate underlying hardware configurations.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 28
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s29",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "To enhance transparency\nand reproducibility, we also developed an automated and interactive Power BI dashboard that\nvisualizes the daily fluctuations in the energy, water, and carbon footprint of an extended list of models\nacross multiple data centers.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 29
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s30",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "This novel dashboard incorporates new models as they get released.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 30
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s31",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Moreover, to contextualize resource use relative to model capability, we apply cross-efficiency Data\nEnvelopment Analysis (DEA) to assess how effectively each model converts environmental inputs\ninto performance.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 31
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s32",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "As a key application of this framework, we perform a case study to estimate the\nfootprint of GPT-4o text generation based on scaled usage data.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 32
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s33",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "We further extend our analysis to\nGPT-5, focusing on the disparities in energy consumption between queries that involve different levels\nof reasoning.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 33
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s34",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Our framework enables infrastructure-aware decision-making, empowers accountability,\nand provides a foundational step toward sustainability standards in AI deployment.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 34
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s35",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "The remainder of the paper is organized as follows.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 35
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s36",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Section 2 reviews existing studies on the\nenvironmental impact of LLMs.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 36
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s37",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Section 3 introduces key concepts, including hardware configurations\nand environmental multipliers.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 37
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s38",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Section 4 details our framework for estimating inference-phase cost.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 38
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s39",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Section 5 presents findings across 30 models.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 39
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s40",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Section 6 provides a focused analysis of GPT-4o\u2019s\nannual environmental footprint and section 7 analyzes the impact of GPT-5\u2019s adapative model routing.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 40
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s41",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Section 8 outlines key insights and implications.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 41
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s42",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Section 9 summarizes the main takeaways and\nlimitations and directions for future work.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 42
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s43",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "2 Related Work\nThe environmental impact of AI systems has garnered increasing attention in recent years, with a\ngrowing body of work attempting to quantify the energy, carbon, and water costs associated with\ntraining and deploying LLMs.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 43
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s44",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Li et al.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 44
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s45",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "[13] analyzed GPT-3\u2019s freshwater consumption, estimating over 5 million liters used during\ntraining and projecting that AI-related withdrawals could reach 6.6 trillion liters annually by 2027.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 45
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s46",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Although their spatiotemporal methodology is a significant early contribution, it overlooks carbon\nemissions, depends on an outdated model, and requires previous knowledge of energy usage, which\nrestricts its scalability.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 46
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s47",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "In parallel, Strubell et al.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 47
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s48",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "[ 23] estimated carbon emissions from training\nBERT and GPT-2 by accounting for GPU, CPU, and DRAM power draw alongside PUE adjustments.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 48
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s49",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "However, their analysis excludes inference and infrastructural overhead.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 49
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s50",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Similar limitations appear\nin Meta\u2019s LLaMA reports [7, 24, 25], which provide carbon footprints based on GPUs\u2019 TDPs but\ndisregard water use, system-wide energy consumption, and the inference phase entirely.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 50
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s51",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Regarding inference, Husom et al.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 51
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s52",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "[18] (MELODI) measure real-time energy consumption of GPUs\nand CPUs at the prompt level, but they neglect carbon emissions, water usage, and infrastructure\noverhead, only concentrating on small-scale open-source models.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 52
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s53",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Samsi et al.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 53
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s54",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "[ 26] measure GPU\npower draw across prompt lengths but exclude proprietary systems and broader environmental factors,\nlacking a standardized scaling method for production-level inference.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 54
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s55",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Yang et al.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 55
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s56",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "[ 27] evaluate\nover 1,200 vision models and introduce an energy-efficiency score.",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 56
        }
      },
      {
        "node_id": "jegham2025:sec2:p2:s57",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "However, their analysis does\n2",
        "score": 0.6735,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec2:p2",
          "section_id": "jegham2025:sec2",
          "sentence_index": 57
        }
      }
    ],
    "num_snippets": 137,
    "retry_count": 0
  },
  {
    "id": "q319",
    "question": "In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?",
    "gt_value": "50",
    "gt_unit": "percent",
    "gt_ref": "luccioni2025b",
    "pred_value": "50",
    "pred_unit": "percent",
    "pred_ref": "[\"luccioni2025b\"]",
    "pred_explanation": "The context from ref_id=luccioni2025b states that in a 2023 article estimating the carbon footprint of BLOOM, training accounted for only half of the model\u2019s overall emissions. This directly provides the percentage as 50%.",
    "raw_response": "{\n  \"explanation\": \"The context from ref_id=luccioni2025b states that in a 2023 article estimating the carbon footprint of BLOOM, training accounted for only half of the model\u2019s overall emissions. This directly provides the percentage as 50%.\",\n  \"answer\": \"Training accounted for 50% of the model's overall emissions.\",\n  \"answer_value\": 50,\n  \"ref_id\": [\"luccioni2025b\"],\n  \"ref_url\": [],\n  \"supporting_materials\": \"In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, since it takes into account different stages of the model life cycle including the manufacturing of computing hardware, idle energy usage, and model deployment, finding that training accounted for only half of the model\u2019s overall emissions [121]\"\n}",
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true,
    "weighted_score": 1.0,
    "latency_seconds": 61.085614919662476,
    "retrieval_seconds": 16.092579126358032,
    "generation_seconds": 44.99285435676575,
    "error": null,
    "pred_ref_url": [],
    "pred_supporting_materials": "In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, since it takes into account different stages of the model life cycle including the manufacturing of computing hardware, idle energy usage, and model deployment, finding that training accounted for only half of the model\u2019s overall emissions [121]",
    "rendered_prompt": "You will answer a question using ONLY the provided context snippets.\nIf the context does not clearly support an answer, use \"is_blank\".\n\nContext snippets from documents:\n[ref_id=luccioni2025b] In a 2023 article estimating the carbon\nfootprint of BLOOM, a 176 billion parameter LLM, Luccioni et al.\n---\n[ref_id=luccioni2025b] 8 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefal a, and Nyalleng Moorosi\nSimilarly, evaluating the environmental impacts of AI syst ems is far from straightforward, and we are still missing\nmany pieces of the puzzle needed in order to meaningfully est imate these impacts. For instance, most of the carbon\nfootprint assessments only focus on the training stage of AI models, which is easier to quantify and report [153, 192],\nbut which only represents a portion of models\u2019 total environ mental impacts. In a 2023 article estimating the carbon\nfootprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach\nfor this evaluation, since it takes into account di\ufb00erent st ages of the model life cycle including the manufacturing of\ncomputing hardware, idle energy usage, and model deploymen t, \ufb01nding that training accounted for only half of the\nmodel\u2019s overall emissions [121], meaning that similar stud ies that only took training into account were potentially\nunderestimating their emissions by half. Also, while comme ndable in terms of its granularity, this kind of carbon\naccounting fails to recognize the societal and economic aspects of sustainability, such as the contribution of LLMs such\nas BLOOM towards amplifying the existing inequalities in th e \ufb01eld of AI due to the increased amount of computing\nresources that they require, which are unattainable to many members of the AI community, as well as the propagation\nof biases via their usage. Furthermore, the authors themsel ves note that there is currently no information available\nabout the embodied emissions linked to manufacturing GPUs, so it is impossible to estimate what portion of the\noverall carbon footprint this represents. This highlights that the emphasis on environmental sustainability often fa ils\nto account for other aspects of AI\u2019s global impacts \u2013 and any k ind of evaluation hinges upon transparency, which is\nsorely lacking in the \ufb01eld of AI \u2013 we discuss this in more lengt h in the following section.\n4.3 Transparency\nTransparency is widely recognized as a fundamental princip le in science in general and AI in particular [63, 107, 208,\n212] but actualizing it in practice can be challenging. This is, in part, due to the fact that machine learning-based\nsystems are not inherently transparent or interpretable, given the complexity of the neural network architectures the y\nespouse and the number of parameters they contain [105, 113, 140]. E\ufb00orts such as interpretability approaches are\nuseful and can help interpret the predictions of models post hoc [104, 164], whereas artifacts such as data sheets and\nmodel cards [69, 137] can contribute towards making AI syste ms more understandable for users, providing essential\ninformation about AI models in a user-friendly format. Thes e artifacts allow users to understand not just how an\nAI system functions, but also its limitations, potential bi ases, implications, and environmental impacts. However, even\nthough model cards are increasingly used in practice (for instance by AI model-sharing platforms such as Hugging Face)\nand provide important information about models, they are no t su\ufb03cient to guarantee, for instance, the reproducibility\nof reported results, which is a core tenet of scienti\ufb01c pract ice [146].\nIndeed, several studies of transparency found that an overw helming amount of results published at technical AI\nconferences do not document all of the variables necessary to reproduce the results they report [78, 156]. This situation\nhighlights the need for an approach to transparency that would involve reporting but also ensuring the reproducibility\nof AI models and their \ufb01ndings. Such an approach would acknow ledge the connection between transparency and\nreproducibility: transparent research practices enable r eproducibility, which in turn facilitates independent scr utiny,\nvalidation, further development of research \ufb01ndings by other scientists [80]. The absence of transparency, especially in\nsharing essential materials such as model weights, code and data, impedes the ability to reproduce results, diminishing\nAI models\u2019 scienti\ufb01c impact and impeding their adoption wit hin the wider scienti\ufb01c community [80].\nIn terms of sustainability, the AI community has historically been even less transparent regarding the environmental\nimpacts of AI models and systems, with most work in this \ufb01eld b eing done post-hoc by researchers who did not do the\ninitial model training and deployment (e.g. [116, 192]). Th e most common environmental sustainability metric for AI\n---\n[ref_id=morrison2025] Following the more comprehensive analysis provided for the BLOOM model (Luccioni et al., 2023),\nwe expand our measurement to include both operational GHG emissions arising from the energy\nrequired for the development, training, and inference phases of the ML model lifecycle, as well as\nembodied emissions attributed to manufacturing of the hardware supporting those operations.\n---\n[ref_id=morrison2025] Published as a conference paper at ICLR 2025\nLuccioni et al. (2023) reported estimates for emissions from the manufacturing process (embodied\nemissions), from electricity consumption during training, and from electricity consumption of the\ncluster while it was idle (see their Table 2). Dodge et al. (2022) measured electricity consump-\ntion and carbon emissions for training language models and computer vision models with granular\ntimesteps with region-specific carbon intensity, but did not measure development costs, water con-\nsumption, or inference. Similarly, developers of the Llama models (Touvron et al., 2023a;b; Dubey\net al., 2024) reported electricity consumption and carbon emissions estimates of training their final\nmodels; they did not estimate development cost or water consumption, and their approach to carbon\nintensity varied.5 Gemma developers (Gemma Team et al., 2024) only report a single number: the\ntotal emissions from pretraining their models, not broken down by model or by different stages of\ntraining, or by electricity consumption and carbon intensity. The OLMo report (Groeneveld et al.,\n2024) documents electricity consumption per model, and uses region-specific carbon intensity to\nestimate emissions for two regions, but does not estimate other environmental impacts. The OLMo\n2 report (OLMo et al., 2025) again documents electricity consumption per model and uses region-\nand datacenter-specific intensity factors to estimate emissions and also water consumption, but does\nnot measure development costs or potential inference costs. Energy use and environmental impacts\nare not typically documented for proprietary models.\nComparably little transparency has been provided on the water consumption of AI systems. Li et al.\n(2023) estimate the water consumption of some closed models like GPT-3, but these estimates are\nbased on speculation about location of training, energy consumption, etc., as there is very little\npublic information about GPT-3\u2019s training. Similarly, there are few estimates of embodied carbon\nfor AI systems, as the manufacturing process is notoriously opaque. In addition, almost all reporting\nof environmental impact is based on training of the final model that is released. Instead of only\nfocusing on training, Luccioni et al. (2024) estimate the impact of inference of deployed AI systems.\nTo the best of our knowledge our work provides the first public estimates of environmental impact\nof development of an LLM, i.e. hyperparameter tuning and ablations before the main training run.\n3 M ETHODOLOGY\nOur goal in this work is to characterize the holistic environmental impacts of large language models\nin as much detail as possible, enabling assessment of key challenges and future directions towards\nreducing those impacts. Typically, studies documenting language model training and development\nmethodology will address this concern by reporting the cost to train the final, deployed model mea-\nsured in GPU hours, kWh energy, and/or CO 2 emissions. However, this calculation provides an\nincomplete characterization of the factors leading to environmental degradation due to LLMs that\nunder-estimates impacts and provides insufficient information to inform strategies for developing\nand deploying LLMs in a more environmentally conscious way.\nFollowing the more comprehensive analysis provided for the BLOOM model (Luccioni et al., 2023),\nwe expand our measurement to include both operational GHG emissions arising from the energy\nrequired for the development, training, and inference phases of the ML model lifecycle, as well as\nembodied emissions attributed to manufacturing of the hardware supporting those operations. We\nalso go beyond previous work to report non-GHG externalities such as water use, and finer-grained\ndata such as variance in energy use throughout training. We describe our methodology for measuring\nand estimating these impacts in more detail below.\n3.1 O PERATIONAL IMPACTS\nOperational environmental impacts of LLMs are those that arise directly from the development\nand use of models, and include the GHG emissions arising from energy sources used to power\nmodel training and deployment, including servers and data center cooling. We base our analysis of\noperational emissions around the following equation introduced by Schwartz et al. (2020) to describe\nthe amount of computation required to produce a machine learning artifact, such as an LLM:\nCost(R) \u221d E \u00b7 D \u00b7 H (1)\n5Llama 1 did not use the data center location\u2019s carbon intensity, instead using US national average carbon\nintensity; Llama 2 did not specify the carbon intensity; Llama 3 used a region-specific carbon intensity. All 3\nassumed 100% GPU power draw throughout training.\n3\n---\n[ref_id=luccioni2024] ACM FAccT \u201924, June 3\u20136, 2024, Rio de Janeiro, Brazil Luccioni et al\nchoices to deploy these models for different tasks and applying them in different scenarios. We further discuss our\nresults and further avenues of research in the next and final section.\n4.3 Comparing model training and inference costs\nAn important trade-off for many AI practitioners and policy-makers is determining when exactly model inference costs\nreach parity with model training (and fine-tuning) - i.e. when does the deployment of models use as much energy as\ntheir initial training? This comparison is often hard to make because it requires the total energy cost of all steps of the\nML model life cycle, which is very rarely available. Of the models that we examined in our study, neither the BLOOMz\nnor the Flan-T5 families of models reported the total energy used nor carbon emitted during their training in the papers\ndescribing the models. However, given that the BLOOMz models are fine-tuned versions of the original BLOOM family\nof models [56], we can base ourselves on the logs provided by the authors of the BLOOM carbon footprint estimation\npaper [31]. We can add to these numbers the energy cost of fine-tuning each model, which we were able to estimate\nbased on the training logs provided by the authors of the BLOOMz paper [34], although we were lacking the necessary\ninformation to infer the carbon footprint 6. We present these numbers, alongside the average energy consumption\nper inference, in Table 5. We can see that the amount of energy required per inference varies from 5.4\u00d7 10\u22125 for the\nsmallest model, BLOOMz-560M to 1.0 \u00d7 10\u22124 kWh for the biggest one, BLOOMz-7B. This is coherent to the numbers\nreported by Luccioni et al. for BLOOM-176B, which required, on average, 0.004 kWh of energy per query, or 40 times\nmore than BLOOMz-7B, being roughly 25 times bigger [ 31] - although this included API deployment of the model,\nwhich is not the case for the models in our study.\nBLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M\nTraining energy (kWh) 51,686 25,634 17,052 10,505\nFinetuning energy (kWh) 7,571 3,242 1,081 543\nInference energy (kWh) 1.0 \u00d7 10\u22124 7.3 \u00d7 10\u22125 6.2 \u00d7 10\u22125 5.4 \u00d7 10\u22125\nCost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592\nTable 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference\ncost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.\nIf we compare the amount of energy used per inference for each of the models with the total amount of energy\nused for both training and fine-tuning them, we can estimate how many inferences would be needed to be carried\nout with a given model in order for the cost of inference to reach the cost of training. As can be seen in Table 5, this\nvaries depending on model size: from around 200 million inferences for the smallest model, BLOOMz-560M, to over\n590 million inferences for the biggest model, BLOOMz-7B. This may seem like a lot if a single instance of a model is\ndeployed, but can add up quickly if there are multiple instances of models deployed in parallel. For instance, it has been\nestimated that, at its peak, ChatGPT had upward of 10 million users per day [36]; the most recent statistics indicate that\nthe ChatGPT login page received 1.7B visits in October 2023 7. Even assuming a single query per user, which is rarely\nthe case, the energy costs of deploying it would surpass its training costs after a few weeks or months of deployment.\nWhile the BLOOMz models are not deployed in real-time in the same manner as ChatGPT, they have been downloaded\nhundreds of thousands of times from the Hugging Face Hub, which would indicate that they have been extensively used\n6The energy consumption can be based on the Thermal Design Power (TDP) of the GPUs used \u2013 while it assumes 100% GPU utilization, it is the most\naccurate estimate possible without energy usage tracking during training.\n7According to SimilarWeb: https://www.similarweb.com/website/chat.openai.com/.\n12\n---\n[ref_id=luccioni2024] Page 12\n---\n[ref_id=luccioni2024] ACM FAccT \u201924, June 3\u20136, 2024, Rio de Janeiro, Brazil Luccioni et al\nchoices to deploy these models for different tasks and applying them in different scenarios.\n---\n[ref_id=luccioni2024] We further discuss our\nresults and further avenues of research in the next and final section.\n---\n[ref_id=luccioni2024] 4.3 Comparing model training and inference costs\nAn important trade-off for many AI practitioners and policy-makers is determining when exactly model inference costs\nreach parity with model training (and fine-tuning) - i.e.\n---\n[ref_id=luccioni2024] when does the deployment of models use as much energy as\ntheir initial training?\n---\n[ref_id=luccioni2024] This comparison is often hard to make because it requires the total energy cost of all steps of the\nML model life cycle, which is very rarely available.\n---\n[ref_id=luccioni2024] Of the models that we examined in our study, neither the BLOOMz\nnor the Flan-T5 families of models reported the total energy used nor carbon emitted during their training in the papers\ndescribing the models.\n---\n[ref_id=luccioni2024] However, given that the BLOOMz models are fine-tuned versions of the original BLOOM family\nof models [56], we can base ourselves on the logs provided by the authors of the BLOOM carbon footprint estimation\npaper [31].\n---\n[ref_id=luccioni2024] We can add to these numbers the energy cost of fine-tuning each model, which we were able to estimate\nbased on the training logs provided by the authors of the BLOOMz paper [34], although we were lacking the necessary\ninformation to infer the carbon footprint 6.\n---\n[ref_id=luccioni2024] We present these numbers, alongside the average energy consumption\nper inference, in Table 5.\n---\n[ref_id=luccioni2024] We can see that the amount of energy required per inference varies from 5.4\u00d7 10\u22125 for the\nsmallest model, BLOOMz-560M to 1.0 \u00d7 10\u22124 kWh for the biggest one, BLOOMz-7B.\n---\n[ref_id=luccioni2024] This is coherent to the numbers\nreported by Luccioni et al.\n---\n[ref_id=luccioni2024] for BLOOM-176B, which required, on average, 0.004 kWh of energy per query, or 40 times\nmore than BLOOMz-7B, being roughly 25 times bigger [ 31] - although this included API deployment of the model,\nwhich is not the case for the models in our study.\n---\n[ref_id=luccioni2024] BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M\nTraining energy (kWh) 51,686 25,634 17,052 10,505\nFinetuning energy (kWh) 7,571 3,242 1,081 543\nInference energy (kWh) 1.0 \u00d7 10\u22124 7.3 \u00d7 10\u22125 6.2 \u00d7 10\u22125 5.4 \u00d7 10\u22125\nCost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592\nTable 5.\n---\n[ref_id=luccioni2024] The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference\ncost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.\n---\n[ref_id=luccioni2024] If we compare the amount of energy used per inference for each of the models with the total amount of energy\nused for both training and fine-tuning them, we can estimate how many inferences would be needed to be carried\nout with a given model in order for the cost of inference to reach the cost of training.\n---\n[ref_id=luccioni2024] As can be seen in Table 5, this\nvaries depending on model size: from around 200 million inferences for the smallest model, BLOOMz-560M, to over\n590 million inferences for the biggest model, BLOOMz-7B.\n---\n[ref_id=luccioni2024] This may seem like a lot if a single instance of a model is\ndeployed, but can add up quickly if there are multiple instances of models deployed in parallel.\n---\n[ref_id=luccioni2024] For instance, it has been\nestimated that, at its peak, ChatGPT had upward of 10 million users per day [36];\n---\n[ref_id=luccioni2024] the most recent statistics indicate that\nthe ChatGPT login page received 1.7B visits in October 2023 7.\n---\n[ref_id=luccioni2024] Even assuming a single query per user, which is rarely\nthe case, the energy costs of deploying it would surpass its training costs after a few weeks or months of deployment.\n---\n[ref_id=luccioni2024] While the BLOOMz models are not deployed in real-time in the same manner as ChatGPT, they have been downloaded\nhundreds of thousands of times from the Hugging Face Hub, which would indicate that they have been extensively used\n6The energy consumption can be based on the Thermal Design Power (TDP) of the GPUs used \u2013 while it assumes 100% GPU utilization, it is the most\naccurate estimate possible without energy usage tracking during training.\n---\n[ref_id=luccioni2024] 7According to SimilarWeb: https://www.similarweb.com/website/chat.openai.com/.\n---\n[ref_id=luccioni2024] 12\n---\n[ref_id=luccioni2025a] Es-\ntimating the carbon footprint of BLOOM, a 176B parameter language model.\n---\n[ref_id=luccioni2025a] FAccT \u201925, June 23\u201326, 2025, Athens, Greece Luccioni et al.\n[43] C\u00e9dric Gossart. 2015. Rebound effects and ICT: a review of the literature. ICT\ninnovations for sustainability (2015), 435\u2013448.\n[44] GreenPeace. 2020. Oil in the Cloud: How Tech Companies are Helping Big Oil\nProfit from Climate Destructions. https://www.greenpeace.org/usa/reports/oil-\nin-the-cloud/\n[45] Gianluca Guidi, Francesca Dominici, Jonathan Gilmour, Kevin Butler, Eric Bell,\nScott Delaney, and Falco J Bargagli-Stoffi. 2024. Environmental Burden of\nUnited States Data Centers in the Artificial Intelligence Era. arXiv preprint\narXiv:2411.09786 (2024).\n[46] Robert Hahn and Kenneth Richards. 2013. Understanding the effectiveness\nof environmental offset policies. Journal of Regulatory Economics 44 (2013),\n103\u2013119.\n[47] Patrick Hartmann, Aitor Marcos, Juana Castro, and Vanessa Apaolaza. 2023.\nPerspectives: Advertising and climate change\u2013Part of the problem or part of\nthe solution? International Journal of Advertising 42, 2 (2023), 430\u2013457.\n[48] Chris T Hendrickson, Lester B Lave, and H Scott Matthews. 2010.Environmental\nlife cycle assessment of goods and services: an input-output approach . Routledge.\n[49] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,\nHassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. 2017.\nDeep learning scaling is predictable, empirically.arXiv preprint arXiv:1712.00409\n(2017).\n[50] Lorenz M Hilty, Andreas K\u00f6hler, Fabian Von Sch\u00e9ele, Rainer Zah, and Thomas\nRuddy. 2006. Rebound effects of progress in information technology. Poiesis &\nPraxis 4, 1 (2006), 19\u201338.\n[51] Torsten Hoefler, Ariel Hendel, and Duncan Roweth. 2022. The convergence of\nhyperscale data center and high-performance computing networks. Computer\n55, 7 (2022), 29\u201337.\n[52] M\u00e9l Hogan. 2015. Data flows and water woes: The Utah Data Center.Big Data &\nSociety 2, 2 (2015), 2053951715592429. https://doi.org/10.1177/2053951715592429\narXiv:https://doi.org/10.1177/2053951715592429\n[53] Nathaniel C Horner, Arman Shehabi, and In\u00eas L Azevedo. 2016. Known un-\nknowns: indirect energy effects of information and communication technology.\nEnvironmental Research Letters 11, 10 (2016), 103001.\n[54] Laura Hostettler Macias, Emmanuel Ravalet, and Patrick R\u00e9rat. 2022. Potential\nrebound effects of teleworking on residential and daily mobility. Geography\nCompass 16, 9 (2022), e12657.\n[55] Kerstin H\u00f6tte, Melline Somers, and Angelos Theodorakopoulos. 2023. Tech-\nnology and jobs: A systematic literature review. Technological Forecasting and\nSocial Change 194 (2023), 122750.\n[56] International Energy Authority. 2023. Data Centres and Data Transmission\nNetworks. https://www.iea.org/energy-system/buildings/data-centres-and-\ndata-transmission-networks\n[57] International Energy Authority. 2024. World Energy Outlook 2024. https:\n//www.iea.org/reports/world-energy-outlook-2024\n[58] Ren\u00e9 Itten, Roland Hischier, Anders SG Andrae, Jan CT Bieser, Livia Cabernard,\nAnnemarie Falke, Hugues Ferreboeuf, Lorenz M Hilty, Regula L Keller, Etienne\nLees-Perasso, et al. 2020. Digital transformation\u2014life cycle assessment of digital\nservices, multifunctional devices and cloud computing.The International Journal\nof Life Cycle Assessment 25 (2020), 2093\u20132098.\n[59] Sami Jaghouar, Jack Min Ong, Manveer Basra, Fares Obeid, Jannik Straube,\nMichael Keiblinger, Elie Bakouch, Lucas Atkins, Maziyar Panahi, Charles God-\ndard, et al. 2024. INTELLECT-1 Technical Report.arXiv preprint arXiv:2412.01152\n(2024).\n[60] W Stanley Jevons. 1866. The coal question. In The Economics of Population .\nRoutledge, 193\u2013204.\n[61] Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele Paolo Scarpazza. 2019. Dis-\nsecting the graphcore ipu architecture via microbenchmarking. arXiv preprint\narXiv:1912.03413 (2019).\n[62] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,\nRaminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017.\nIn-datacenter performance analysis of a tensor processing unit. In Proceedings\nof the 44th annual international symposium on computer architecture . 1\u201312.\n[63] Lynn H Kaack, Priya L Donti, Emma Strubell, George Kamiya, Felix Creutzig,\nand David Rolnick. 2022. Aligning artificial intelligence with climate change\nmitigation. Nature Climate Change 12, 6 (2022), 518\u2013527.\n[64] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\n[65] Jonathan Koomey and Eric Masanet. 2021. Does not compute: Avoiding pitfalls\nassessing the Internet\u2019s energy and carbon impacts.Joule 5, 7 (2021), 1625\u20131628.\nhttps://doi.org/10.1016/j.joule.2021.05.007\n[66] Adam Krechowicz, Maria Krechowicz, and Katarzyna Poczeta. 2022. Machine\nlearning approaches to predict electricity production from renewable energy\nsources. Energies 15, 23 (2022), 9146.\n[67] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.\n2019. Quantifying the carbon emissions of machine learning. arXiv preprint\narXiv:1910.09700 (2019).\n[68] Pengfei Li, Jianyi Yang, Mohammad A Islam, and Shaolei Ren. 2023. Making\nAI Less\" Thirsty\": Uncovering and Addressing the Secret Water Footprint of AI\nModels. arXiv preprint arXiv:2304.03271 (2023).\n[69] Alexandra Sasha Luccioni, Yacine Jernite, and Emma Strubell. 2023.\nPower Hungry Processing: Watts Driving the Cost of AI Deployment?\narXiv:2311.16863 [cs.LG]\n[70] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Es-\ntimating the carbon footprint of BLOOM, a 176B parameter language model.\narXiv preprint arXiv:2211.02001 (2022).\n[71] Sasha Luccioni, Boris Gamazaychikov, Sara Hooker, R\u00e9gis Pierrard, Emma\nStrubell, Yacine Jernite, and Carole-Jean Wu. 2024. Light bulbs have energy\nratings\u2014so why can\u2019t AI chatbots? Nature 632, 8026 (2024), 736\u2013738.\n[72] Amy Luers, Jonathan Koomey, Eric Masanet, Owen Gaffney, Felix Creutzig,\nJuan Lavista Ferres, and Eric Horvitz. 2024. Will AI accelerate or delay the\nrace to net-zero emissions? Nature Comments 628 (2024), 718\u2013720. https:\n//doi.org/10.1038/d41586-024-01137-x\n[73] Ian MacKenzie. 2013. How Retailers Can Keep Up with Consumers. McKinsey\n& Company (2013).\n[74] Eric Masanet, Arman Shehabi, Nuoa Lei, Sarah Smith, and Jonathan\nKoomey. 2020. Recalibrating global data center energy-use estimates. Sci-\nence 367, 6481 (2020), 984\u2013986. https://doi.org/10.1126/science.aba3758\narXiv:https://www.science.org/doi/pdf/10.1126/science.aba3758\n[75] Eric Masanet, Arman Shehabi, Nuoa Lei, Sarah Smith, and Jonathan Koomey.\n2020. Recalibrating global data center energy-use estimates. Science 367, 6481\n(2020), 984\u2013986.\n[76] Amil Merchant, Simon Batzner, Samuel S Schoenholz, Muratahan Aykol,\nGowoon Cheon, and Ekin Dogus Cubuk. 2023. Scaling deep learning for mate-\nrials discovery. Nature 624, 7990 (2023), 80\u201385.\n[77] Meta. 2021. Meta 2021 Sustainability Report. https://sustainability.fb.com/2021-\nsustainability-report/\n[78] Microsoft. 2024. Microsoft Environmental Report 2024. https://cdn-dynmedia-\n1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/\npresentations/CSR/Microsoft-2024-Environmental-Sustainability-Report.pdf\n[79] Elizabeth Minton, Christopher Lee, Ulrich Orth, Chung-Hyun Kim, and Lynn\nKahle. 2012. Sustainable marketing and social media: A cross-country analysis\nof motives for sustainable behaviors. Journal of advertising 41, 4 (2012), 69\u201384.\n[80] David Mytton. 2021. Data centre water consumption. npj Clean Water 4, 1\n(2021), 11.\n[81] Kirsi Niinim\u00e4ki, Greg Peters, Helena Dahlbo, Patsy Perry, Timo Rissanen, and\nAlison Gwilt. 2020. The environmental price of fast fashion. Nature Reviews\nEarth & Environment 1, 4 (2020), 189\u2013200.\n[82] NVIDIA. 2024. Power Efficiency. https://www.nvidia.com/en-us/glossary/\npower-efficiency/\n[83] OECD. 2023. Emissions Measurement in Supply Chains: Business Realities and\nChallenges. https://www.weforum.org/publications/emissions-measurement-\nin-supply-chains-business-realities-and-challenges/\n[84] Michael I Ojovan and Hans J Steinmetz. 2022. Approaches to disposal of nuclear\nwaste. Energies 15, 20 (2022), 7804.\n[85] Otto, Melissa. 2024. Global Digital Advertising Revenues \u2013 A Look at the Big\nThree: Alphabet (GOOGL), Meta Platforms (META), Amazon.com (AMZN).\nhttps://visiblealpha.com/blog/global-digital-advertising-revenues-a-look-at-\nthe-big-three-alphabet-googl-meta-platforms-meta-amazon-com-amzn/\n[86] John R Owen, Deanna Kemp, Alex M Lechner, Jill Harris, Ruilian Zhang, and\n\u00c9l\u00e9onore L\u00e8bre. 2023. Energy transition minerals and their intersection with\nland-connected peoples. Nature Sustainability 6, 2 (2023), 203\u2013211.\n[87] Tufan \u00d6zsoy. 2024. The \u201cenergy rebound effect\u201d within the framework of\nenvironmental sustainability. Wiley Interdisciplinary Reviews: Energy and Envi-\nronment 13, 2 (2024), e517.\n[88] Isabel O\u2019Brien. 2024. Data center emissions probably 662% higher than big tech\nclaims. Can it keep up the ruse? The Guardian 15 (2024).\n[89] David Patterson, Joseph Gonzalez, Urs H\u00f6lzle, Quoc Le, Chen Liang, Lluis-\nMiquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2022.\nThe Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink.\nhttps://doi.org/10.48550/ARXIV.2204.05149\n[90] David Patterson, Joseph Gonzalez, Urs H\u00f6lzle, Quoc Le, Chen Liang, Lluis-\nMiquel Munguia, Daniel Rothchild, David R. So, Maud Texier, and Jeff Dean.\n2022. The Carbon Footprint of Machine Learning Training Will Plateau, Then\nShrink. Computer 55, 7 (2022), 18\u201328. https://doi.org/10.1109/MC.2022.3148714\n[91] PlanetTracker. 2024. From ADversity to ADvantage. https://planet-tracker.\norg/wp-content/uploads/2024/02/From-Adversity-to-Advantage.pdf\n[92] Remus Pr\u0103v\u0103lie and Georgeta Bandoc. 2018. Nuclear energy: Between global\nelectricity demand, worldwide decarbonisation imperativeness, and planetary\nenvironmental implications. Journal of environmental management 209 (2018),\n81\u201392.\n[93] Guillem Ram\u00edrez, Matthias Lindemann, Alexandra Birch, and Ivan Titov. 2023.\nCache & distil: Optimising API calls to large language models. arXiv preprint\narXiv:2310.13561 (2023).\n---\n[ref_id=luccioni2024] We can add to these numbers the energy cost of fine-tuning each model, which we were able to estimate\nbased on the training logs provided by the authors of the BLOOMz paper [34], although we were lacking the necessary\ninformation to infer the carbon footprint 6.\n---\n[ref_id=luccioni2024] ACM FAccT \u201924, June 3\u20136, 2024, Rio de Janeiro, Brazil Luccioni et al\nchoices to deploy these models for different tasks and applying them in different scenarios. We further discuss our\nresults and further avenues of research in the next and final section.\n4.3 Comparing model training and inference costs\nAn important trade-off for many AI practitioners and policy-makers is determining when exactly model inference costs\nreach parity with model training (and fine-tuning) - i.e. when does the deployment of models use as much energy as\ntheir initial training? This comparison is often hard to make because it requires the total energy cost of all steps of the\nML model life cycle, which is very rarely available. Of the models that we examined in our study, neither the BLOOMz\nnor the Flan-T5 families of models reported the total energy used nor carbon emitted during their training in the papers\ndescribing the models. However, given that the BLOOMz models are fine-tuned versions of the original BLOOM family\nof models [56], we can base ourselves on the logs provided by the authors of the BLOOM carbon footprint estimation\npaper [31]. We can add to these numbers the energy cost of fine-tuning each model, which we were able to estimate\nbased on the training logs provided by the authors of the BLOOMz paper [34], although we were lacking the necessary\ninformation to infer the carbon footprint 6. We present these numbers, alongside the average energy consumption\nper inference, in Table 5. We can see that the amount of energy required per inference varies from 5.4\u00d7 10\u22125 for the\nsmallest model, BLOOMz-560M to 1.0 \u00d7 10\u22124 kWh for the biggest one, BLOOMz-7B. This is coherent to the numbers\nreported by Luccioni et al. for BLOOM-176B, which required, on average, 0.004 kWh of energy per query, or 40 times\nmore than BLOOMz-7B, being roughly 25 times bigger [ 31] - although this included API deployment of the model,\nwhich is not the case for the models in our study.\nBLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M\nTraining energy (kWh) 51,686 25,634 17,052 10,505\nFinetuning energy (kWh) 7,571 3,242 1,081 543\nInference energy (kWh) 1.0 \u00d7 10\u22124 7.3 \u00d7 10\u22125 6.2 \u00d7 10\u22125 5.4 \u00d7 10\u22125\nCost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592\nTable 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference\ncost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.\nIf we compare the amount of energy used per inference for each of the models with the total amount of energy\nused for both training and fine-tuning them, we can estimate how many inferences would be needed to be carried\nout with a given model in order for the cost of inference to reach the cost of training. As can be seen in Table 5, this\nvaries depending on model size: from around 200 million inferences for the smallest model, BLOOMz-560M, to over\n590 million inferences for the biggest model, BLOOMz-7B. This may seem like a lot if a single instance of a model is\ndeployed, but can add up quickly if there are multiple instances of models deployed in parallel. For instance, it has been\nestimated that, at its peak, ChatGPT had upward of 10 million users per day [36]; the most recent statistics indicate that\nthe ChatGPT login page received 1.7B visits in October 2023 7. Even assuming a single query per user, which is rarely\nthe case, the energy costs of deploying it would surpass its training costs after a few weeks or months of deployment.\nWhile the BLOOMz models are not deployed in real-time in the same manner as ChatGPT, they have been downloaded\nhundreds of thousands of times from the Hugging Face Hub, which would indicate that they have been extensively used\n6The energy consumption can be based on the Thermal Design Power (TDP) of the GPUs used \u2013 while it assumes 100% GPU utilization, it is the most\naccurate estimate possible without energy usage tracking during training.\n7According to SimilarWeb: https://www.similarweb.com/website/chat.openai.com/.\n12\n---\n[ref_id=cottier2024] Estimating the Carbon Footprint of BLOOM,\na 176B Parameter Language Model.\n---\n[ref_id=cottier2024] [20] Tae Kim. Raymond James estimates it costs Nvidia $3,320. https://x.com/firstadopter/status/\n1691877797487165443, 2024. Accessed: 2024-05-30.\n[21] TechPowerUp. NVIDIA Tesla K80 Specs, 2024.\n[22] TechPowerUp. NVIDIA Tesla P100 PCIe Specs, 2024.\n[23] TechPowerUp. NVIDIA Tesla V100 SXM2 32 GB Specs , 2024.\n[24] TechPowerUp. NVIDIA A100 SXM4 40GB Specs , 2024.\n[25] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow,\nRoman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, et al. BLOOM: A 176B-Parameter Open-Access\nMultilingual Language Model. arXiv preprint arXiv:2211.05100, 2022.\n[26] Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas B. Jablin, George Kurian, James\nLaudon, Sheng Li, Peter Ma, Xiaoyu Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei\nZhou, and David Patterson. Ten Lessons From Three Generations Shaped Google\u2019s TPUv4i : Industrial Product.\nIn 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) , pages 1\u201314, 2021.\n[27] NVIDIA Corporation. NVIDIA DGX H100 Datasheet , 2023.\n[28] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David\nSo, Maud Texier, and Jeff Dean. Carbon Emissions and Large Neural Network Training. arXiv preprint\narXiv:2104.10350, 2021.\n[29] NVIDIA Corporation. NVIDIA DGX SuperPOD Data Center Design (for NVIDIA DGX H100 Systems) , 4 2023.\nVersion 01.\n[30] Luiz Andre Barroso, Urs Holzle, Parthasarathy Ranganathan, and Margaret Martonosi. The Datacenter As a\nComputer: Designing Warehouse-scale Machines, 2018.\n[31] Meta. Data centers - Meta sustainability. https://sustainability.fb.com/data-centers/, 2024. Ac-\ncessed: 2024-05-30.\n[32] Dylan Patel, Daniel Nishball, and Jeremie Eliahou Ontiveros. AI Datacenter Energy Dilemma - Race for AI\nDatacenter Space. https://www.semianalysis.com/p/ai-datacenter-energy-dilemma-race , 2024.\nAccessed: 2024-05-30.\n[33] Electric Power Monthly. https://www.eia.gov/electricity/monthly/epm_table_grapher.php?t=\nepmt_5_6_a, 2024. Accessed: 2024-01-15.\n[34] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint\narXiv:2205.01068, 2022.\n[35] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang,\nFiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable AI: Environmental implications, challenges and\nopportunities. Proceedings of Machine Learning and Systems , 4:795\u2013813, 2022.\n[36] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the Carbon Footprint of BLOOM,\na 176B Parameter Language Model. arXiv preprint arXiv:2211.02001, 2022.\n[37] NVIDIA Corporation. Why GPUs are great for AI. https://blogs.nvidia.com/blog/\nwhy-gpus-are-great-for-ai/ , 2023. Accessed: 2024-05-30.\n[38] Electricity generation, capacity, and sales in the United States. https://web.archive.\norg/web/20240407085026/https://www.eia.gov/energyexplained/electricity/\nelectricity-in-the-us-top-10.php , 2022. Accessed: 2024-05-29.\n12\n---\n[ref_id=luccioni2024] However, given that the BLOOMz models are fine-tuned versions of the original BLOOM family\nof models [56], we can base ourselves on the logs provided by the authors of the BLOOM carbon footprint estimation\npaper [31].\n---\n[ref_id=luccioni2024] ACM FAccT \u201924, June 3\u20136, 2024, Rio de Janeiro, Brazil Luccioni et al\nchoices to deploy these models for different tasks and applying them in different scenarios. We further discuss our\nresults and further avenues of research in the next and final section.\n4.3 Comparing model training and inference costs\nAn important trade-off for many AI practitioners and policy-makers is determining when exactly model inference costs\nreach parity with model training (and fine-tuning) - i.e. when does the deployment of models use as much energy as\ntheir initial training? This comparison is often hard to make because it requires the total energy cost of all steps of the\nML model life cycle, which is very rarely available. Of the models that we examined in our study, neither the BLOOMz\nnor the Flan-T5 families of models reported the total energy used nor carbon emitted during their training in the papers\ndescribing the models. However, given that the BLOOMz models are fine-tuned versions of the original BLOOM family\nof models [56], we can base ourselves on the logs provided by the authors of the BLOOM carbon footprint estimation\npaper [31]. We can add to these numbers the energy cost of fine-tuning each model, which we were able to estimate\nbased on the training logs provided by the authors of the BLOOMz paper [34], although we were lacking the necessary\ninformation to infer the carbon footprint 6. We present these numbers, alongside the average energy consumption\nper inference, in Table 5. We can see that the amount of energy required per inference varies from 5.4\u00d7 10\u22125 for the\nsmallest model, BLOOMz-560M to 1.0 \u00d7 10\u22124 kWh for the biggest one, BLOOMz-7B. This is coherent to the numbers\nreported by Luccioni et al. for BLOOM-176B, which required, on average, 0.004 kWh of energy per query, or 40 times\nmore than BLOOMz-7B, being roughly 25 times bigger [ 31] - although this included API deployment of the model,\nwhich is not the case for the models in our study.\nBLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M\nTraining energy (kWh) 51,686 25,634 17,052 10,505\nFinetuning energy (kWh) 7,571 3,242 1,081 543\nInference energy (kWh) 1.0 \u00d7 10\u22124 7.3 \u00d7 10\u22125 6.2 \u00d7 10\u22125 5.4 \u00d7 10\u22125\nCost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592\nTable 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference\ncost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.\nIf we compare the amount of energy used per inference for each of the models with the total amount of energy\nused for both training and fine-tuning them, we can estimate how many inferences would be needed to be carried\nout with a given model in order for the cost of inference to reach the cost of training. As can be seen in Table 5, this\nvaries depending on model size: from around 200 million inferences for the smallest model, BLOOMz-560M, to over\n590 million inferences for the biggest model, BLOOMz-7B. This may seem like a lot if a single instance of a model is\ndeployed, but can add up quickly if there are multiple instances of models deployed in parallel. For instance, it has been\nestimated that, at its peak, ChatGPT had upward of 10 million users per day [36]; the most recent statistics indicate that\nthe ChatGPT login page received 1.7B visits in October 2023 7. Even assuming a single query per user, which is rarely\nthe case, the energy costs of deploying it would surpass its training costs after a few weeks or months of deployment.\nWhile the BLOOMz models are not deployed in real-time in the same manner as ChatGPT, they have been downloaded\nhundreds of thousands of times from the Hugging Face Hub, which would indicate that they have been extensively used\n6The energy consumption can be based on the Thermal Design Power (TDP) of the GPUs used \u2013 while it assumes 100% GPU utilization, it is the most\naccurate estimate possible without energy usage tracking during training.\n7According to SimilarWeb: https://www.similarweb.com/website/chat.openai.com/.\n12\n---\n[ref_id=cottier2024] According to BLOOM\u2019s model page on Hugging Face, the \u201cEstimated\ncost of training\u201d is the \u201cEquivalent of $2\u20135M in cloud computing (including preliminary experiments)\u201d.\n---\n[ref_id=cottier2024] sensitive to the GPU and TPU unit cost (accurate within factors of 2 and 4 respectively) and the training chip-hours\n(factor of 3).\nCost quantity 90% CI\nHardware acquisition (GPUs) 0.5x to 2x\nHardware acquisition (TPUs) 0.2x to 4x\nAmortized hardware CapEx + energy (GPUs) 0.3x to 4x\nAmortized hardware CapEx + energy (TPUs) 0.2x to 5x\nTable 5: Estimated relative uncertainty in individual cost estimates, for different methods. TPU estimates have larger\nuncertainty due to the additional uncertainty in estimating their equivalent costs.\nA.8 Ground truth cost comparison\nIn order to verify that our results are reasonable, we sought to compare our cost estimates with true costs reported by\ndevelopers and other sources. However, there are very few models where the developers report both the computing\nresource usage and the total cost. Training costs and compute resources are independently known for BLOOM-176B\nand OPT-175B, so we compare our estimates with these.\nBLOOM-176B was trained on 1,161,261 A100-hours at a throughput of 150 TFLOP/GPU/s at 48% model FLOPs\nutilization and a cost of $3 million (including experiments) [25]. We estimated a cloud compute cost of $1.99M or an\namortized cost of $0.8M for BLOOM-176B. The accuracy of this estimate depends on how much of the grant was spent\non experiments versus the final training run. According to BLOOM\u2019s model page on Hugging Face, the \u201cEstimated\ncost of training\u201d is the \u201cEquivalent of $2\u20135M in cloud computing (including preliminary experiments)\u201d. Preliminary\nexperiments included training smaller BLOOM models. The final training run for the 176B model used 37.24% of the\nenergy of the BLOOM project [36]; if the total cost of the project was C3M as in the grant description, this implies that\nBLOOM-176B had a cost of $1.2M, which is between our two estimates and aligns more closely with the amortized\ncost approach ($900K) than the cloud cost approach ($2M).\nOPT-175B was trained for 793.5 hours, at a cost of $2500/hour as reported in the training logbook [ 34], for a total\ncost of $1.98 million. We estimated a cloud compute cost of $1.5M for the final training run of OPT-175B, which is\noff by 25%, and an amortized hardware and energy cost of $700K, off by 65%. OPT\u2019s cluster cost rate per hour was\nlikely greater than what we estimate from the quantity of GPUs, or less than the approximate figure mentioned by the\ndevelopers in the training log.\nB Sensitivity analysis\nB.1 Selection of historic frontier models\nIn order to analyse trends in frontier ML models, we must define what counts as a frontier model at any point in time.7\nOur preferred approach is to select models from the database that were in the top 10 most compute-intensive models as\nof their release date, although we considered others as shown in Figure 7.\nFor the most part, different selection approaches gave similar results. The exception was selecting frontier models based\non distance from the compute trend. This approach imposes an artificially flat floor on the eligible models. Due to this,\nit leaves out many earlier models, and produces a flatter cost trend than the other methods.\nOur preferred approach has an advantage over alternatives: the selection is more robust to the sampling of our dataset.\nApproaches based on quantiles, or distance from the historic trend, are influenced by data collected on models outside\nthe frontier. Selecting the top-ranked models, in comparison, is merely influenced by whether the dataset contains those\nfrontier models.\n7Models in the database meet one or more of the following criteria: (i) advanced the state of the art on a qualifying benchmark,\n(ii) at least 1000 citations, (iii) at least one million monthly active users, or (iv) equivalent historical significance [7]. However, this\nmeans the database includes many models that were far from the frontier of compute.\n17\n---\n[ref_id=luccioni2024] We have provided initial\nnumbers for comparing the relative energy costs of model training, finetuning and inference for different sizes of\nmodels from the BLOOMz family, and found that the parity between training/finetuning and inference grows with\nmodel size.\n---\n[ref_id=luccioni2024] Power Hungry Processing ACM FAccT \u201924, June 3\u20136, 2024, Rio de Janeiro, Brazil\nby the open-source community: at the time of writing this article (November 2023), BLOOMz-7B has been downloaded\n606,096 times, BLOOMz-3B has been downloaded 357,368 times, BLOOMz-1B has been downloaded 61,757 times and\nBLOOMz-560m has been downloaded 498,601 times. They have also been finetuned for a number of downstream\ntasks, such as chat, and deployed in HuggingFace Spaces, interactive interfaces for model interaction. While this\nanalysis represents a relatively small sample of models, analyses such as this are vital for estimating the relative energy\nconsumption (and ensuing emissions) of different stages of the ML training and deployment cycle, understanding\ntrade-offs between training and inference emissions patterns, and characterizing the lifetime emissions of ML models,\nand we hope that others will be possible in the future, which would require more transparency from model creators\nregarding both the up front (i.e. training) and downstream (i.e. inference) costs of ML models. We discuss the importance\nof transparency and other important actions that members of the community can take in the next, and final, section.\n5 DISCUSSION\nThere have been limited studies regarding the energy consumption and carbon emissions of LLM inference, largely due\nto its distributed nature \u2014 compared to the relatively time- and location-constrained nature of training \u2014 making it\ndifficult to make meaningful comparisons between different models and tasks. In this work, we have endeavored to\nkeep as many parameters stable as possible, including the code, hardware, datasets, batch size and Python library. We\nprovide all of the code that we used for our analysis as well as an interactive tool to allow users to more deeply explore\nthe results we present here. We also highlight the main high-level takeaways of our study below:\nGenerative tasks are more energy- and carbon-intensive compared to discriminative tasks. As shown in Figure 1, the\nmost energy- and carbon-intensive tasks are those that generate new content: text generation, summarization, image\ncaptioning, and image generation.\nTasks involving images are more energy- and carbon-intensive compared to those involving text alone. More specifically,\ntasks involving predicting categories (text-to-category, image-to-category) are less energy-intensive than those involving\ngenerating images (e.g. text-to-image), with those involving text between the two (see Figure 2).\nDecoder-only models are slightly more energy- and carbon- intensive than sequence-to-sequence models for models of a\nsimilar size and applied to the same tasks. The findings we present in Table 3, Figure 3, and Figure 6 would indicate that\nmore computation (i.e. energy) is required for decoder-only tasks, and that this phenomenon is particularly marked for\ntasks with longer outputs. This observation is worth verifying for other architectures from both categories, and well as\nother tasks and datasets.\nTraining remains orders of magnitude more energy- and carbon- intensive than inference. We have provided initial\nnumbers for comparing the relative energy costs of model training, finetuning and inference for different sizes of\nmodels from the BLOOMz family, and found that the parity between training/finetuning and inference grows with\nmodel size. While the ratio is hundreds of millions of inferences for a single training, given the ubiquity of ML model\ndeployment, this parity can be reached quickly for many popular models.\nUsing multi-purpose models for discriminative tasks is more energy-intensive compared to task-specific models for these\nsame tasks. This is especially the case for text classification (on IMDB, SST 2 and Rotten Tomatoes) and question\nanswering (on SciQ, SQuAD v1 and v2), where the gap between task-specific and zero-shot models is particularly large,\nand less so for summarization (for CNN-Daily Mail, SamSUM and XSum). As can be seen in Table 4, the difference\n13\n---\n[ref_id=morrison2025] Published as a conference paper at ICLR 2025\nLuccioni et al. (2023) reported estimates for emissions from the manufacturing process (embodied\nemissions), from electricity consumption during training, and from electricity consumption of the\ncluster while it was idle (see their Table 2). Dodge et al. (2022) measured electricity consump-\ntion and carbon emissions for training language models and computer vision models with granular\ntimesteps with region-specific carbon intensity, but did not measure development costs, water con-\nsumption, or inference. Similarly, developers of the Llama models (Touvron et al., 2023a;b; Dubey\net al., 2024) reported electricity consumption and carbon emissions estimates of training their final\nmodels; they did not estimate development cost or water consumption, and their approach to carbon\nintensity varied.5 Gemma developers (Gemma Team et al., 2024) only report a single number: the\ntotal emissions from pretraining their models, not broken down by model or by different stages of\ntraining, or by electricity consumption and carbon intensity. The OLMo report (Groeneveld et al.,\n2024) documents electricity consumption per model, and uses region-specific carbon intensity to\nestimate emissions for two regions, but does not estimate other environmental impacts. The OLMo\n2 report (OLMo et al., 2025) again documents electricity consumption per model and uses region-\nand datacenter-specific intensity factors to estimate emissions and also water consumption, but does\nnot measure development costs or potential inference costs. Energy use and environmental impacts\nare not typically documented for proprietary models.\nComparably little transparency has been provided on the water consumption of AI systems. Li et al.\n(2023) estimate the water consumption of some closed models like GPT-3, but these estimates are\nbased on speculation about location of training, energy consumption, etc., as there is very little\npublic information about GPT-3\u2019s training. Similarly, there are few estimates of embodied carbon\nfor AI systems, as the manufacturing process is notoriously opaque. In addition, almost all reporting\nof environmental impact is based on training of the final model that is released. Instead of only\nfocusing on training, Luccioni et al. (2024) estimate the impact of inference of deployed AI systems.\nTo the best of our knowledge our work provides the first public estimates of environmental impact\nof development of an LLM, i.e. hyperparameter tuning and ablations before the main training run.\n3 M ETHODOLOGY\nOur goal in this work is to characterize the holistic environmental impacts of large language models\nin as much detail as possible, enabling assessment of key challenges and future directions towards\nreducing those impacts. Typically, studies documenting language model training and development\nmethodology will address this concern by reporting the cost to train the final, deployed model mea-\nsured in GPU hours, kWh energy, and/or CO 2 emissions. However, this calculation provides an\nincomplete characterization of the factors leading to environmental degradation due to LLMs that\nunder-estimates impacts and provides insufficient information to inform strategies for developing\nand deploying LLMs in a more environmentally conscious way.\nFollowing the more comprehensive analysis provided for the BLOOM model (Luccioni et al., 2023),\nwe expand our measurement to include both operational GHG emissions arising from the energy\nrequired for the development, training, and inference phases of the ML model lifecycle, as well as\nembodied emissions attributed to manufacturing of the hardware supporting those operations. We\nalso go beyond previous work to report non-GHG externalities such as water use, and finer-grained\ndata such as variance in energy use throughout training. We describe our methodology for measuring\nand estimating these impacts in more detail below.\n3.1 O PERATIONAL IMPACTS\nOperational environmental impacts of LLMs are those that arise directly from the development\nand use of models, and include the GHG emissions arising from energy sources used to power\nmodel training and deployment, including servers and data center cooling. We base our analysis of\noperational emissions around the following equation introduced by Schwartz et al. (2020) to describe\nthe amount of computation required to produce a machine learning artifact, such as an LLM:\nCost(R) \u221d E \u00b7 D \u00b7 H (1)\n5Llama 1 did not use the data center location\u2019s carbon intensity, instead using US national average carbon\nintensity; Llama 2 did not specify the carbon intensity; Llama 3 used a region-specific carbon intensity. All 3\nassumed 100% GPU power draw throughout training.\n3\n---\n[ref_id=morrison2025] Page 3\n---\n[ref_id=morrison2025] Published as a conference paper at ICLR 2025\nLuccioni et al.\n---\n[ref_id=morrison2025] (2023) reported estimates for emissions from the manufacturing process (embodied\nemissions), from electricity consumption during training, and from electricity consumption of the\ncluster while it was idle (see their Table 2).\n---\n[ref_id=morrison2025] Dodge et al.\n---\n[ref_id=morrison2025] (2022) measured electricity consump-\ntion and carbon emissions for training language models and computer vision models with granular\ntimesteps with region-specific carbon intensity, but did not measure development costs, water con-\nsumption, or inference.\n---\n[ref_id=morrison2025] Similarly, developers of the Llama models (Touvron et al., 2023a;b;\n---\n[ref_id=morrison2025] Dubey\net al., 2024) reported electricity consumption and carbon emissions estimates of training their final\nmodels;\n---\n[ref_id=morrison2025] they did not estimate development cost or water consumption, and their approach to carbon\nintensity varied.5 Gemma developers (Gemma Team et al., 2024) only report a single number: the\ntotal emissions from pretraining their models, not broken down by model or by different stages of\ntraining, or by electricity consumption and carbon intensity.\n---\n[ref_id=morrison2025] The OLMo report (Groeneveld et al.,\n2024) documents electricity consumption per model, and uses region-specific carbon intensity to\nestimate emissions for two regions, but does not estimate other environmental impacts.\n---\n[ref_id=morrison2025] The OLMo\n2 report (OLMo et al., 2025) again documents electricity consumption per model and uses region-\nand datacenter-specific intensity factors to estimate emissions and also water consumption, but does\nnot measure development costs or potential inference costs.\n---\n[ref_id=morrison2025] Energy use and environmental impacts\nare not typically documented for proprietary models.\n---\n[ref_id=morrison2025] Comparably little transparency has been provided on the water consumption of AI systems.\n---\n[ref_id=morrison2025] Li et al.\n---\n[ref_id=morrison2025] (2023) estimate the water consumption of some closed models like GPT-3, but these estimates are\nbased on speculation about location of training, energy consumption, etc., as there is very little\npublic information about GPT-3\u2019s training.\n---\n[ref_id=morrison2025] Similarly, there are few estimates of embodied carbon\nfor AI systems, as the manufacturing process is notoriously opaque.\n---\n[ref_id=morrison2025] In addition, almost all reporting\nof environmental impact is based on training of the final model that is released.\n---\n[ref_id=morrison2025] Instead of only\nfocusing on training, Luccioni et al.\n---\n[ref_id=morrison2025] (2024) estimate the impact of inference of deployed AI systems.\n---\n[ref_id=morrison2025] To the best of our knowledge our work provides the first public estimates of environmental impact\nof development of an LLM, i.e.\n---\n[ref_id=morrison2025] hyperparameter tuning and ablations before the main training run.\n---\n[ref_id=morrison2025] 3 M ETHODOLOGY\nOur goal in this work is to characterize the holistic environmental impacts of large language models\nin as much detail as possible, enabling assessment of key challenges and future directions towards\nreducing those impacts.\n---\n[ref_id=morrison2025] Typically, studies documenting language model training and development\nmethodology will address this concern by reporting the cost to train the final, deployed model mea-\nsured in GPU hours, kWh energy, and/or CO 2 emissions.\n---\n[ref_id=morrison2025] However, this calculation provides an\nincomplete characterization of the factors leading to environmental degradation due to LLMs that\nunder-estimates impacts and provides insufficient information to inform strategies for developing\nand deploying LLMs in a more environmentally conscious way.\n---\n[ref_id=morrison2025] Following the more comprehensive analysis provided for the BLOOM model (Luccioni et al., 2023),\nwe expand our measurement to include both operational GHG emissions arising from the energy\nrequired for the development, training, and inference phases of the ML model lifecycle, as well as\nembodied emissions attributed to manufacturing of the hardware supporting those operations.\n---\n[ref_id=morrison2025] We\nalso go beyond previous work to report non-GHG externalities such as water use, and finer-grained\ndata such as variance in energy use throughout training.\n---\n[ref_id=morrison2025] We describe our methodology for measuring\nand estimating these impacts in more detail below.\n---\n[ref_id=morrison2025] 3.1 O PERATIONAL IMPACTS\nOperational environmental impacts of LLMs are those that arise directly from the development\nand use of models, and include the GHG emissions arising from energy sources used to power\nmodel training and deployment, including servers and data center cooling.\n---\n[ref_id=morrison2025] We base our analysis of\noperational emissions around the following equation introduced by Schwartz et al.\n---\n[ref_id=morrison2025] (2020) to describe\nthe amount of computation required to produce a machine learning artifact, such as an LLM:\nCost(R) \u221d E \u00b7 D \u00b7 H (1)\n5Llama 1 did not use the data center location\u2019s carbon intensity, instead using US national average carbon\nintensity;\n---\n[ref_id=morrison2025] Llama 2 did not specify the carbon intensity;\n---\n[ref_id=morrison2025] Llama 3 used a region-specific carbon intensity.\n---\n[ref_id=morrison2025] All 3\nassumed 100% GPU power draw throughout training.\n---\n[ref_id=morrison2025] 3\n\n---\n\nNow answer the following question based ONLY on the context above.\n\nQuestion: In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?\n\nAdditional info (JSON): {\"answer_unit\": \"percent\", \"question_id\": \"q319\"}\n\nIMPORTANT: The \"answer_unit\" field specifies the expected format/unit for answer_value.\n- If answer_unit is a unit (e.g., \"kW\", \"USD\"), express answer_value as a number in that unit (no unit name).\n- If answer_unit is \"is_blank\", answer_value should be the exact identifier/name from context that answers the question.\n- If the answer is a numeric range, format as [lower,upper].\n\nReturn STRICT JSON with these keys in order:\n- explanation          (1-3 sentences explaining how context supports the answer and how you applied answer_unit; or \"is_blank\")\n- answer               (short natural-language response)\n- answer_value         (the value matching the expected format; or \"is_blank\")\n- ref_id               (list of document ids from context used as evidence; or \"is_blank\")\n- ref_url              (list of URLs for the cited documents; or \"is_blank\")\n- supporting_materials (verbatim quote, table reference, or figure reference from the cited document; or \"is_blank\")\n\nJSON Answer:",
    "retrieved_snippets": [
      {
        "node_id": "luccioni2025b:sec8:p12:s159",
        "document_title": "Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice",
        "text": "In a 2023 article estimating the carbon\nfootprint of BLOOM, a 176 billion parameter LLM, Luccioni et al.",
        "score": 0.8046,
        "rank": 1,
        "metadata": {
          "document_id": "luccioni2025b",
          "document_title": "Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice",
          "paragraph_id": "luccioni2025b:sec8:p12",
          "section_id": "luccioni2025b:sec8",
          "sentence_index": 159
        }
      },
      {
        "node_id": "luccioni2025b:sec8:p12",
        "document_title": "Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice",
        "text": "8 Alexandra Sasha Luccioni, Giada Pistilli, Raesetje Sefal a, and Nyalleng Moorosi\nSimilarly, evaluating the environmental impacts of AI syst ems is far from straightforward, and we are still missing\nmany pieces of the puzzle needed in order to meaningfully est imate these impacts. For instance, most of the carbon\nfootprint assessments only focus on the training stage of AI models, which is easier to quantify and report [153, 192],\nbut which only represents a portion of models\u2019 total environ mental impacts. In a 2023 article estimating the carbon\nfootprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach\nfor this evaluation, since it takes into account di\ufb00erent st ages of the model life cycle including the manufacturing of\ncomputing hardware, idle energy usage, and model deploymen t, \ufb01nding that training accounted for only half of the\nmodel\u2019s overall emissions [121], meaning that similar stud ies that only took training into account were potentially\nunderestimating their emissions by half. Also, while comme ndable in terms of its granularity, this kind of carbon\naccounting fails to recognize the societal and economic aspects of sustainability, such as the contribution of LLMs such\nas BLOOM towards amplifying the existing inequalities in th e \ufb01eld of AI due to the increased amount of computing\nresources that they require, which are unattainable to many members of the AI community, as well as the propagation\nof biases via their usage. Furthermore, the authors themsel ves note that there is currently no information available\nabout the embodied emissions linked to manufacturing GPUs, so it is impossible to estimate what portion of the\noverall carbon footprint this represents. This highlights that the emphasis on environmental sustainability often fa ils\nto account for other aspects of AI\u2019s global impacts \u2013 and any k ind of evaluation hinges upon transparency, which is\nsorely lacking in the \ufb01eld of AI \u2013 we discuss this in more lengt h in the following section.\n4.3 Transparency\nTransparency is widely recognized as a fundamental princip le in science in general and AI in particular [63, 107, 208,\n212] but actualizing it in practice can be challenging. This is, in part, due to the fact that machine learning-based\nsystems are not inherently transparent or interpretable, given the complexity of the neural network architectures the y\nespouse and the number of parameters they contain [105, 113, 140]. E\ufb00orts such as interpretability approaches are\nuseful and can help interpret the predictions of models post hoc [104, 164], whereas artifacts such as data sheets and\nmodel cards [69, 137] can contribute towards making AI syste ms more understandable for users, providing essential\ninformation about AI models in a user-friendly format. Thes e artifacts allow users to understand not just how an\nAI system functions, but also its limitations, potential bi ases, implications, and environmental impacts. However, even\nthough model cards are increasingly used in practice (for instance by AI model-sharing platforms such as Hugging Face)\nand provide important information about models, they are no t su\ufb03cient to guarantee, for instance, the reproducibility\nof reported results, which is a core tenet of scienti\ufb01c pract ice [146].\nIndeed, several studies of transparency found that an overw helming amount of results published at technical AI\nconferences do not document all of the variables necessary to reproduce the results they report [78, 156]. This situation\nhighlights the need for an approach to transparency that would involve reporting but also ensuring the reproducibility\nof AI models and their \ufb01ndings. Such an approach would acknow ledge the connection between transparency and\nreproducibility: transparent research practices enable r eproducibility, which in turn facilitates independent scr utiny,\nvalidation, further development of research \ufb01ndings by other scientists [80]. The absence of transparency, especially in\nsharing essential materials such as model weights, code and data, impedes the ability to reproduce results, diminishing\nAI models\u2019 scienti\ufb01c impact and impeding their adoption wit hin the wider scienti\ufb01c community [80].\nIn terms of sustainability, the AI community has historically been even less transparent regarding the environmental\nimpacts of AI models and systems, with most work in this \ufb01eld b eing done post-hoc by researchers who did not do the\ninitial model training and deployment (e.g. [116, 192]). Th e most common environmental sustainability metric for AI",
        "score": 0.8046,
        "rank": 1,
        "metadata": {
          "document_id": "luccioni2025b",
          "document_title": "Bridging the Gap: Integrating Ethics and Environmental Sustainability in AI Research and Practice",
          "page": 8,
          "paragraph_index": 12,
          "section_id": "luccioni2025b:sec8",
          "section_index": 8
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s66",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Following the more comprehensive analysis provided for the BLOOM model (Luccioni et al., 2023),\nwe expand our measurement to include both operational GHG emissions arising from the energy\nrequired for the development, training, and inference phases of the ML model lifecycle, as well as\nembodied emissions attributed to manufacturing of the hardware supporting those operations.",
        "score": 0.7846,
        "rank": 2,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 66
        }
      },
      {
        "node_id": "morrison2025:sec3:p3",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Published as a conference paper at ICLR 2025\nLuccioni et al. (2023) reported estimates for emissions from the manufacturing process (embodied\nemissions), from electricity consumption during training, and from electricity consumption of the\ncluster while it was idle (see their Table 2). Dodge et al. (2022) measured electricity consump-\ntion and carbon emissions for training language models and computer vision models with granular\ntimesteps with region-specific carbon intensity, but did not measure development costs, water con-\nsumption, or inference. Similarly, developers of the Llama models (Touvron et al., 2023a;b; Dubey\net al., 2024) reported electricity consumption and carbon emissions estimates of training their final\nmodels; they did not estimate development cost or water consumption, and their approach to carbon\nintensity varied.5 Gemma developers (Gemma Team et al., 2024) only report a single number: the\ntotal emissions from pretraining their models, not broken down by model or by different stages of\ntraining, or by electricity consumption and carbon intensity. The OLMo report (Groeneveld et al.,\n2024) documents electricity consumption per model, and uses region-specific carbon intensity to\nestimate emissions for two regions, but does not estimate other environmental impacts. The OLMo\n2 report (OLMo et al., 2025) again documents electricity consumption per model and uses region-\nand datacenter-specific intensity factors to estimate emissions and also water consumption, but does\nnot measure development costs or potential inference costs. Energy use and environmental impacts\nare not typically documented for proprietary models.\nComparably little transparency has been provided on the water consumption of AI systems. Li et al.\n(2023) estimate the water consumption of some closed models like GPT-3, but these estimates are\nbased on speculation about location of training, energy consumption, etc., as there is very little\npublic information about GPT-3\u2019s training. Similarly, there are few estimates of embodied carbon\nfor AI systems, as the manufacturing process is notoriously opaque. In addition, almost all reporting\nof environmental impact is based on training of the final model that is released. Instead of only\nfocusing on training, Luccioni et al. (2024) estimate the impact of inference of deployed AI systems.\nTo the best of our knowledge our work provides the first public estimates of environmental impact\nof development of an LLM, i.e. hyperparameter tuning and ablations before the main training run.\n3 M ETHODOLOGY\nOur goal in this work is to characterize the holistic environmental impacts of large language models\nin as much detail as possible, enabling assessment of key challenges and future directions towards\nreducing those impacts. Typically, studies documenting language model training and development\nmethodology will address this concern by reporting the cost to train the final, deployed model mea-\nsured in GPU hours, kWh energy, and/or CO 2 emissions. However, this calculation provides an\nincomplete characterization of the factors leading to environmental degradation due to LLMs that\nunder-estimates impacts and provides insufficient information to inform strategies for developing\nand deploying LLMs in a more environmentally conscious way.\nFollowing the more comprehensive analysis provided for the BLOOM model (Luccioni et al., 2023),\nwe expand our measurement to include both operational GHG emissions arising from the energy\nrequired for the development, training, and inference phases of the ML model lifecycle, as well as\nembodied emissions attributed to manufacturing of the hardware supporting those operations. We\nalso go beyond previous work to report non-GHG externalities such as water use, and finer-grained\ndata such as variance in energy use throughout training. We describe our methodology for measuring\nand estimating these impacts in more detail below.\n3.1 O PERATIONAL IMPACTS\nOperational environmental impacts of LLMs are those that arise directly from the development\nand use of models, and include the GHG emissions arising from energy sources used to power\nmodel training and deployment, including servers and data center cooling. We base our analysis of\noperational emissions around the following equation introduced by Schwartz et al. (2020) to describe\nthe amount of computation required to produce a machine learning artifact, such as an LLM:\nCost(R) \u221d E \u00b7 D \u00b7 H (1)\n5Llama 1 did not use the data center location\u2019s carbon intensity, instead using US national average carbon\nintensity; Llama 2 did not specify the carbon intensity; Llama 3 used a region-specific carbon intensity. All 3\nassumed 100% GPU power draw throughout training.\n3",
        "score": 0.7846,
        "rank": 2,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "page": 3,
          "paragraph_index": 3,
          "section_id": "morrison2025:sec3",
          "section_index": 3
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "ACM FAccT \u201924, June 3\u20136, 2024, Rio de Janeiro, Brazil Luccioni et al\nchoices to deploy these models for different tasks and applying them in different scenarios. We further discuss our\nresults and further avenues of research in the next and final section.\n4.3 Comparing model training and inference costs\nAn important trade-off for many AI practitioners and policy-makers is determining when exactly model inference costs\nreach parity with model training (and fine-tuning) - i.e. when does the deployment of models use as much energy as\ntheir initial training? This comparison is often hard to make because it requires the total energy cost of all steps of the\nML model life cycle, which is very rarely available. Of the models that we examined in our study, neither the BLOOMz\nnor the Flan-T5 families of models reported the total energy used nor carbon emitted during their training in the papers\ndescribing the models. However, given that the BLOOMz models are fine-tuned versions of the original BLOOM family\nof models [56], we can base ourselves on the logs provided by the authors of the BLOOM carbon footprint estimation\npaper [31]. We can add to these numbers the energy cost of fine-tuning each model, which we were able to estimate\nbased on the training logs provided by the authors of the BLOOMz paper [34], although we were lacking the necessary\ninformation to infer the carbon footprint 6. We present these numbers, alongside the average energy consumption\nper inference, in Table 5. We can see that the amount of energy required per inference varies from 5.4\u00d7 10\u22125 for the\nsmallest model, BLOOMz-560M to 1.0 \u00d7 10\u22124 kWh for the biggest one, BLOOMz-7B. This is coherent to the numbers\nreported by Luccioni et al. for BLOOM-176B, which required, on average, 0.004 kWh of energy per query, or 40 times\nmore than BLOOMz-7B, being roughly 25 times bigger [ 31] - although this included API deployment of the model,\nwhich is not the case for the models in our study.\nBLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M\nTraining energy (kWh) 51,686 25,634 17,052 10,505\nFinetuning energy (kWh) 7,571 3,242 1,081 543\nInference energy (kWh) 1.0 \u00d7 10\u22124 7.3 \u00d7 10\u22125 6.2 \u00d7 10\u22125 5.4 \u00d7 10\u22125\nCost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592\nTable 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference\ncost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.\nIf we compare the amount of energy used per inference for each of the models with the total amount of energy\nused for both training and fine-tuning them, we can estimate how many inferences would be needed to be carried\nout with a given model in order for the cost of inference to reach the cost of training. As can be seen in Table 5, this\nvaries depending on model size: from around 200 million inferences for the smallest model, BLOOMz-560M, to over\n590 million inferences for the biggest model, BLOOMz-7B. This may seem like a lot if a single instance of a model is\ndeployed, but can add up quickly if there are multiple instances of models deployed in parallel. For instance, it has been\nestimated that, at its peak, ChatGPT had upward of 10 million users per day [36]; the most recent statistics indicate that\nthe ChatGPT login page received 1.7B visits in October 2023 7. Even assuming a single query per user, which is rarely\nthe case, the energy costs of deploying it would surpass its training costs after a few weeks or months of deployment.\nWhile the BLOOMz models are not deployed in real-time in the same manner as ChatGPT, they have been downloaded\nhundreds of thousands of times from the Hugging Face Hub, which would indicate that they have been extensively used\n6The energy consumption can be based on the Thermal Design Power (TDP) of the GPUs used \u2013 while it assumes 100% GPU utilization, it is the most\naccurate estimate possible without energy usage tracking during training.\n7According to SimilarWeb: https://www.similarweb.com/website/chat.openai.com/.\n12",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "page": 12,
          "paragraph_index": 18,
          "section_id": "luccioni2024:sec12",
          "section_index": 12
        }
      },
      {
        "node_id": "luccioni2024:sec12",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "Page 12",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "page": 12,
          "section_index": 12
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s217",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "ACM FAccT \u201924, June 3\u20136, 2024, Rio de Janeiro, Brazil Luccioni et al\nchoices to deploy these models for different tasks and applying them in different scenarios.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 217
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s218",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "We further discuss our\nresults and further avenues of research in the next and final section.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 218
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s219",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "4.3 Comparing model training and inference costs\nAn important trade-off for many AI practitioners and policy-makers is determining when exactly model inference costs\nreach parity with model training (and fine-tuning) - i.e.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 219
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s220",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "when does the deployment of models use as much energy as\ntheir initial training?",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 220
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s221",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "This comparison is often hard to make because it requires the total energy cost of all steps of the\nML model life cycle, which is very rarely available.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 221
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s222",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "Of the models that we examined in our study, neither the BLOOMz\nnor the Flan-T5 families of models reported the total energy used nor carbon emitted during their training in the papers\ndescribing the models.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 222
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s223",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "However, given that the BLOOMz models are fine-tuned versions of the original BLOOM family\nof models [56], we can base ourselves on the logs provided by the authors of the BLOOM carbon footprint estimation\npaper [31].",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 223
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s224",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "We can add to these numbers the energy cost of fine-tuning each model, which we were able to estimate\nbased on the training logs provided by the authors of the BLOOMz paper [34], although we were lacking the necessary\ninformation to infer the carbon footprint 6.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 224
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s225",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "We present these numbers, alongside the average energy consumption\nper inference, in Table 5.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 225
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s226",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "We can see that the amount of energy required per inference varies from 5.4\u00d7 10\u22125 for the\nsmallest model, BLOOMz-560M to 1.0 \u00d7 10\u22124 kWh for the biggest one, BLOOMz-7B.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 226
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s227",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "This is coherent to the numbers\nreported by Luccioni et al.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 227
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s228",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "for BLOOM-176B, which required, on average, 0.004 kWh of energy per query, or 40 times\nmore than BLOOMz-7B, being roughly 25 times bigger [ 31] - although this included API deployment of the model,\nwhich is not the case for the models in our study.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 228
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s229",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M\nTraining energy (kWh) 51,686 25,634 17,052 10,505\nFinetuning energy (kWh) 7,571 3,242 1,081 543\nInference energy (kWh) 1.0 \u00d7 10\u22124 7.3 \u00d7 10\u22125 6.2 \u00d7 10\u22125 5.4 \u00d7 10\u22125\nCost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592\nTable 5.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 229
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s230",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference\ncost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 230
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s231",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "If we compare the amount of energy used per inference for each of the models with the total amount of energy\nused for both training and fine-tuning them, we can estimate how many inferences would be needed to be carried\nout with a given model in order for the cost of inference to reach the cost of training.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 231
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s232",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "As can be seen in Table 5, this\nvaries depending on model size: from around 200 million inferences for the smallest model, BLOOMz-560M, to over\n590 million inferences for the biggest model, BLOOMz-7B.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 232
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s233",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "This may seem like a lot if a single instance of a model is\ndeployed, but can add up quickly if there are multiple instances of models deployed in parallel.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 233
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s234",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "For instance, it has been\nestimated that, at its peak, ChatGPT had upward of 10 million users per day [36];",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 234
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s235",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "the most recent statistics indicate that\nthe ChatGPT login page received 1.7B visits in October 2023 7.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 235
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s236",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "Even assuming a single query per user, which is rarely\nthe case, the energy costs of deploying it would surpass its training costs after a few weeks or months of deployment.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 236
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s237",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "While the BLOOMz models are not deployed in real-time in the same manner as ChatGPT, they have been downloaded\nhundreds of thousands of times from the Hugging Face Hub, which would indicate that they have been extensively used\n6The energy consumption can be based on the Thermal Design Power (TDP) of the GPUs used \u2013 while it assumes 100% GPU utilization, it is the most\naccurate estimate possible without energy usage tracking during training.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 237
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s238",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "7According to SimilarWeb: https://www.similarweb.com/website/chat.openai.com/.",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 238
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s239",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "12",
        "score": 0.7744,
        "rank": 3,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 239
        }
      },
      {
        "node_id": "luccioni2025a:sec12:p12:s653",
        "document_title": "From Efficiency Gains to Rebound Effects: The Problem of Jevons' Paradox in AI's Polarized Environmental Debate",
        "text": "Es-\ntimating the carbon footprint of BLOOM, a 176B parameter language model.",
        "score": 0.7731,
        "rank": 4,
        "metadata": {
          "document_id": "luccioni2025a",
          "document_title": "From Efficiency Gains to Rebound Effects: The Problem of Jevons' Paradox in AI's Polarized Environmental Debate",
          "paragraph_id": "luccioni2025a:sec12:p12",
          "section_id": "luccioni2025a:sec12",
          "sentence_index": 653
        }
      },
      {
        "node_id": "luccioni2025a:sec12:p12",
        "document_title": "From Efficiency Gains to Rebound Effects: The Problem of Jevons' Paradox in AI's Polarized Environmental Debate",
        "text": "FAccT \u201925, June 23\u201326, 2025, Athens, Greece Luccioni et al.\n[43] C\u00e9dric Gossart. 2015. Rebound effects and ICT: a review of the literature. ICT\ninnovations for sustainability (2015), 435\u2013448.\n[44] GreenPeace. 2020. Oil in the Cloud: How Tech Companies are Helping Big Oil\nProfit from Climate Destructions. https://www.greenpeace.org/usa/reports/oil-\nin-the-cloud/\n[45] Gianluca Guidi, Francesca Dominici, Jonathan Gilmour, Kevin Butler, Eric Bell,\nScott Delaney, and Falco J Bargagli-Stoffi. 2024. Environmental Burden of\nUnited States Data Centers in the Artificial Intelligence Era. arXiv preprint\narXiv:2411.09786 (2024).\n[46] Robert Hahn and Kenneth Richards. 2013. Understanding the effectiveness\nof environmental offset policies. Journal of Regulatory Economics 44 (2013),\n103\u2013119.\n[47] Patrick Hartmann, Aitor Marcos, Juana Castro, and Vanessa Apaolaza. 2023.\nPerspectives: Advertising and climate change\u2013Part of the problem or part of\nthe solution? International Journal of Advertising 42, 2 (2023), 430\u2013457.\n[48] Chris T Hendrickson, Lester B Lave, and H Scott Matthews. 2010.Environmental\nlife cycle assessment of goods and services: an input-output approach . Routledge.\n[49] Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,\nHassan Kianinejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. 2017.\nDeep learning scaling is predictable, empirically.arXiv preprint arXiv:1712.00409\n(2017).\n[50] Lorenz M Hilty, Andreas K\u00f6hler, Fabian Von Sch\u00e9ele, Rainer Zah, and Thomas\nRuddy. 2006. Rebound effects of progress in information technology. Poiesis &\nPraxis 4, 1 (2006), 19\u201338.\n[51] Torsten Hoefler, Ariel Hendel, and Duncan Roweth. 2022. The convergence of\nhyperscale data center and high-performance computing networks. Computer\n55, 7 (2022), 29\u201337.\n[52] M\u00e9l Hogan. 2015. Data flows and water woes: The Utah Data Center.Big Data &\nSociety 2, 2 (2015), 2053951715592429. https://doi.org/10.1177/2053951715592429\narXiv:https://doi.org/10.1177/2053951715592429\n[53] Nathaniel C Horner, Arman Shehabi, and In\u00eas L Azevedo. 2016. Known un-\nknowns: indirect energy effects of information and communication technology.\nEnvironmental Research Letters 11, 10 (2016), 103001.\n[54] Laura Hostettler Macias, Emmanuel Ravalet, and Patrick R\u00e9rat. 2022. Potential\nrebound effects of teleworking on residential and daily mobility. Geography\nCompass 16, 9 (2022), e12657.\n[55] Kerstin H\u00f6tte, Melline Somers, and Angelos Theodorakopoulos. 2023. Tech-\nnology and jobs: A systematic literature review. Technological Forecasting and\nSocial Change 194 (2023), 122750.\n[56] International Energy Authority. 2023. Data Centres and Data Transmission\nNetworks. https://www.iea.org/energy-system/buildings/data-centres-and-\ndata-transmission-networks\n[57] International Energy Authority. 2024. World Energy Outlook 2024. https:\n//www.iea.org/reports/world-energy-outlook-2024\n[58] Ren\u00e9 Itten, Roland Hischier, Anders SG Andrae, Jan CT Bieser, Livia Cabernard,\nAnnemarie Falke, Hugues Ferreboeuf, Lorenz M Hilty, Regula L Keller, Etienne\nLees-Perasso, et al. 2020. Digital transformation\u2014life cycle assessment of digital\nservices, multifunctional devices and cloud computing.The International Journal\nof Life Cycle Assessment 25 (2020), 2093\u20132098.\n[59] Sami Jaghouar, Jack Min Ong, Manveer Basra, Fares Obeid, Jannik Straube,\nMichael Keiblinger, Elie Bakouch, Lucas Atkins, Maziyar Panahi, Charles God-\ndard, et al. 2024. INTELLECT-1 Technical Report.arXiv preprint arXiv:2412.01152\n(2024).\n[60] W Stanley Jevons. 1866. The coal question. In The Economics of Population .\nRoutledge, 193\u2013204.\n[61] Zhe Jia, Blake Tillman, Marco Maggioni, and Daniele Paolo Scarpazza. 2019. Dis-\nsecting the graphcore ipu architecture via microbenchmarking. arXiv preprint\narXiv:1912.03413 (2019).\n[62] Norman P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,\nRaminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al Borchers, et al. 2017.\nIn-datacenter performance analysis of a tensor processing unit. In Proceedings\nof the 44th annual international symposium on computer architecture . 1\u201312.\n[63] Lynn H Kaack, Priya L Donti, Emma Strubell, George Kamiya, Felix Creutzig,\nand David Rolnick. 2022. Aligning artificial intelligence with climate change\nmitigation. Nature Climate Change 12, 6 (2022), 518\u2013527.\n[64] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).\n[65] Jonathan Koomey and Eric Masanet. 2021. Does not compute: Avoiding pitfalls\nassessing the Internet\u2019s energy and carbon impacts.Joule 5, 7 (2021), 1625\u20131628.\nhttps://doi.org/10.1016/j.joule.2021.05.007\n[66] Adam Krechowicz, Maria Krechowicz, and Katarzyna Poczeta. 2022. Machine\nlearning approaches to predict electricity production from renewable energy\nsources. Energies 15, 23 (2022), 9146.\n[67] Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres.\n2019. Quantifying the carbon emissions of machine learning. arXiv preprint\narXiv:1910.09700 (2019).\n[68] Pengfei Li, Jianyi Yang, Mohammad A Islam, and Shaolei Ren. 2023. Making\nAI Less\" Thirsty\": Uncovering and Addressing the Secret Water Footprint of AI\nModels. arXiv preprint arXiv:2304.03271 (2023).\n[69] Alexandra Sasha Luccioni, Yacine Jernite, and Emma Strubell. 2023.\nPower Hungry Processing: Watts Driving the Cost of AI Deployment?\narXiv:2311.16863 [cs.LG]\n[70] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. 2022. Es-\ntimating the carbon footprint of BLOOM, a 176B parameter language model.\narXiv preprint arXiv:2211.02001 (2022).\n[71] Sasha Luccioni, Boris Gamazaychikov, Sara Hooker, R\u00e9gis Pierrard, Emma\nStrubell, Yacine Jernite, and Carole-Jean Wu. 2024. Light bulbs have energy\nratings\u2014so why can\u2019t AI chatbots? Nature 632, 8026 (2024), 736\u2013738.\n[72] Amy Luers, Jonathan Koomey, Eric Masanet, Owen Gaffney, Felix Creutzig,\nJuan Lavista Ferres, and Eric Horvitz. 2024. Will AI accelerate or delay the\nrace to net-zero emissions? Nature Comments 628 (2024), 718\u2013720. https:\n//doi.org/10.1038/d41586-024-01137-x\n[73] Ian MacKenzie. 2013. How Retailers Can Keep Up with Consumers. McKinsey\n& Company (2013).\n[74] Eric Masanet, Arman Shehabi, Nuoa Lei, Sarah Smith, and Jonathan\nKoomey. 2020. Recalibrating global data center energy-use estimates. Sci-\nence 367, 6481 (2020), 984\u2013986. https://doi.org/10.1126/science.aba3758\narXiv:https://www.science.org/doi/pdf/10.1126/science.aba3758\n[75] Eric Masanet, Arman Shehabi, Nuoa Lei, Sarah Smith, and Jonathan Koomey.\n2020. Recalibrating global data center energy-use estimates. Science 367, 6481\n(2020), 984\u2013986.\n[76] Amil Merchant, Simon Batzner, Samuel S Schoenholz, Muratahan Aykol,\nGowoon Cheon, and Ekin Dogus Cubuk. 2023. Scaling deep learning for mate-\nrials discovery. Nature 624, 7990 (2023), 80\u201385.\n[77] Meta. 2021. Meta 2021 Sustainability Report. https://sustainability.fb.com/2021-\nsustainability-report/\n[78] Microsoft. 2024. Microsoft Environmental Report 2024. https://cdn-dynmedia-\n1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/\npresentations/CSR/Microsoft-2024-Environmental-Sustainability-Report.pdf\n[79] Elizabeth Minton, Christopher Lee, Ulrich Orth, Chung-Hyun Kim, and Lynn\nKahle. 2012. Sustainable marketing and social media: A cross-country analysis\nof motives for sustainable behaviors. Journal of advertising 41, 4 (2012), 69\u201384.\n[80] David Mytton. 2021. Data centre water consumption. npj Clean Water 4, 1\n(2021), 11.\n[81] Kirsi Niinim\u00e4ki, Greg Peters, Helena Dahlbo, Patsy Perry, Timo Rissanen, and\nAlison Gwilt. 2020. The environmental price of fast fashion. Nature Reviews\nEarth & Environment 1, 4 (2020), 189\u2013200.\n[82] NVIDIA. 2024. Power Efficiency. https://www.nvidia.com/en-us/glossary/\npower-efficiency/\n[83] OECD. 2023. Emissions Measurement in Supply Chains: Business Realities and\nChallenges. https://www.weforum.org/publications/emissions-measurement-\nin-supply-chains-business-realities-and-challenges/\n[84] Michael I Ojovan and Hans J Steinmetz. 2022. Approaches to disposal of nuclear\nwaste. Energies 15, 20 (2022), 7804.\n[85] Otto, Melissa. 2024. Global Digital Advertising Revenues \u2013 A Look at the Big\nThree: Alphabet (GOOGL), Meta Platforms (META), Amazon.com (AMZN).\nhttps://visiblealpha.com/blog/global-digital-advertising-revenues-a-look-at-\nthe-big-three-alphabet-googl-meta-platforms-meta-amazon-com-amzn/\n[86] John R Owen, Deanna Kemp, Alex M Lechner, Jill Harris, Ruilian Zhang, and\n\u00c9l\u00e9onore L\u00e8bre. 2023. Energy transition minerals and their intersection with\nland-connected peoples. Nature Sustainability 6, 2 (2023), 203\u2013211.\n[87] Tufan \u00d6zsoy. 2024. The \u201cenergy rebound effect\u201d within the framework of\nenvironmental sustainability. Wiley Interdisciplinary Reviews: Energy and Envi-\nronment 13, 2 (2024), e517.\n[88] Isabel O\u2019Brien. 2024. Data center emissions probably 662% higher than big tech\nclaims. Can it keep up the ruse? The Guardian 15 (2024).\n[89] David Patterson, Joseph Gonzalez, Urs H\u00f6lzle, Quoc Le, Chen Liang, Lluis-\nMiquel Munguia, Daniel Rothchild, David So, Maud Texier, and Jeff Dean. 2022.\nThe Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink.\nhttps://doi.org/10.48550/ARXIV.2204.05149\n[90] David Patterson, Joseph Gonzalez, Urs H\u00f6lzle, Quoc Le, Chen Liang, Lluis-\nMiquel Munguia, Daniel Rothchild, David R. So, Maud Texier, and Jeff Dean.\n2022. The Carbon Footprint of Machine Learning Training Will Plateau, Then\nShrink. Computer 55, 7 (2022), 18\u201328. https://doi.org/10.1109/MC.2022.3148714\n[91] PlanetTracker. 2024. From ADversity to ADvantage. https://planet-tracker.\norg/wp-content/uploads/2024/02/From-Adversity-to-Advantage.pdf\n[92] Remus Pr\u0103v\u0103lie and Georgeta Bandoc. 2018. Nuclear energy: Between global\nelectricity demand, worldwide decarbonisation imperativeness, and planetary\nenvironmental implications. Journal of environmental management 209 (2018),\n81\u201392.\n[93] Guillem Ram\u00edrez, Matthias Lindemann, Alexandra Birch, and Ivan Titov. 2023.\nCache & distil: Optimising API calls to large language models. arXiv preprint\narXiv:2310.13561 (2023).",
        "score": 0.7731,
        "rank": 4,
        "metadata": {
          "document_id": "luccioni2025a",
          "document_title": "From Efficiency Gains to Rebound Effects: The Problem of Jevons' Paradox in AI's Polarized Environmental Debate",
          "page": 12,
          "paragraph_index": 12,
          "section_id": "luccioni2025a:sec12",
          "section_index": 12
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s224",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "We can add to these numbers the energy cost of fine-tuning each model, which we were able to estimate\nbased on the training logs provided by the authors of the BLOOMz paper [34], although we were lacking the necessary\ninformation to infer the carbon footprint 6.",
        "score": 0.7654,
        "rank": 5,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 224
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "ACM FAccT \u201924, June 3\u20136, 2024, Rio de Janeiro, Brazil Luccioni et al\nchoices to deploy these models for different tasks and applying them in different scenarios. We further discuss our\nresults and further avenues of research in the next and final section.\n4.3 Comparing model training and inference costs\nAn important trade-off for many AI practitioners and policy-makers is determining when exactly model inference costs\nreach parity with model training (and fine-tuning) - i.e. when does the deployment of models use as much energy as\ntheir initial training? This comparison is often hard to make because it requires the total energy cost of all steps of the\nML model life cycle, which is very rarely available. Of the models that we examined in our study, neither the BLOOMz\nnor the Flan-T5 families of models reported the total energy used nor carbon emitted during their training in the papers\ndescribing the models. However, given that the BLOOMz models are fine-tuned versions of the original BLOOM family\nof models [56], we can base ourselves on the logs provided by the authors of the BLOOM carbon footprint estimation\npaper [31]. We can add to these numbers the energy cost of fine-tuning each model, which we were able to estimate\nbased on the training logs provided by the authors of the BLOOMz paper [34], although we were lacking the necessary\ninformation to infer the carbon footprint 6. We present these numbers, alongside the average energy consumption\nper inference, in Table 5. We can see that the amount of energy required per inference varies from 5.4\u00d7 10\u22125 for the\nsmallest model, BLOOMz-560M to 1.0 \u00d7 10\u22124 kWh for the biggest one, BLOOMz-7B. This is coherent to the numbers\nreported by Luccioni et al. for BLOOM-176B, which required, on average, 0.004 kWh of energy per query, or 40 times\nmore than BLOOMz-7B, being roughly 25 times bigger [ 31] - although this included API deployment of the model,\nwhich is not the case for the models in our study.\nBLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M\nTraining energy (kWh) 51,686 25,634 17,052 10,505\nFinetuning energy (kWh) 7,571 3,242 1,081 543\nInference energy (kWh) 1.0 \u00d7 10\u22124 7.3 \u00d7 10\u22125 6.2 \u00d7 10\u22125 5.4 \u00d7 10\u22125\nCost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592\nTable 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference\ncost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.\nIf we compare the amount of energy used per inference for each of the models with the total amount of energy\nused for both training and fine-tuning them, we can estimate how many inferences would be needed to be carried\nout with a given model in order for the cost of inference to reach the cost of training. As can be seen in Table 5, this\nvaries depending on model size: from around 200 million inferences for the smallest model, BLOOMz-560M, to over\n590 million inferences for the biggest model, BLOOMz-7B. This may seem like a lot if a single instance of a model is\ndeployed, but can add up quickly if there are multiple instances of models deployed in parallel. For instance, it has been\nestimated that, at its peak, ChatGPT had upward of 10 million users per day [36]; the most recent statistics indicate that\nthe ChatGPT login page received 1.7B visits in October 2023 7. Even assuming a single query per user, which is rarely\nthe case, the energy costs of deploying it would surpass its training costs after a few weeks or months of deployment.\nWhile the BLOOMz models are not deployed in real-time in the same manner as ChatGPT, they have been downloaded\nhundreds of thousands of times from the Hugging Face Hub, which would indicate that they have been extensively used\n6The energy consumption can be based on the Thermal Design Power (TDP) of the GPUs used \u2013 while it assumes 100% GPU utilization, it is the most\naccurate estimate possible without energy usage tracking during training.\n7According to SimilarWeb: https://www.similarweb.com/website/chat.openai.com/.\n12",
        "score": 0.7654,
        "rank": 5,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "page": 12,
          "paragraph_index": 18,
          "section_id": "luccioni2024:sec12",
          "section_index": 12
        }
      },
      {
        "node_id": "cottier2024:sec12:p12:s361",
        "document_title": "The Rising Costs of Training Frontier AI Models",
        "text": "Estimating the Carbon Footprint of BLOOM,\na 176B Parameter Language Model.",
        "score": 0.765,
        "rank": 6,
        "metadata": {
          "document_id": "cottier2024",
          "document_title": "The Rising Costs of Training Frontier AI Models",
          "paragraph_id": "cottier2024:sec12:p12",
          "section_id": "cottier2024:sec12",
          "sentence_index": 361
        }
      },
      {
        "node_id": "cottier2024:sec12:p12",
        "document_title": "The Rising Costs of Training Frontier AI Models",
        "text": "[20] Tae Kim. Raymond James estimates it costs Nvidia $3,320. https://x.com/firstadopter/status/\n1691877797487165443, 2024. Accessed: 2024-05-30.\n[21] TechPowerUp. NVIDIA Tesla K80 Specs, 2024.\n[22] TechPowerUp. NVIDIA Tesla P100 PCIe Specs, 2024.\n[23] TechPowerUp. NVIDIA Tesla V100 SXM2 32 GB Specs , 2024.\n[24] TechPowerUp. NVIDIA A100 SXM4 40GB Specs , 2024.\n[25] BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow,\nRoman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, et al. BLOOM: A 176B-Parameter Open-Access\nMultilingual Language Model. arXiv preprint arXiv:2211.05100, 2022.\n[26] Norman P. Jouppi, Doe Hyun Yoon, Matthew Ashcraft, Mark Gottscho, Thomas B. Jablin, George Kurian, James\nLaudon, Sheng Li, Peter Ma, Xiaoyu Ma, Thomas Norrie, Nishant Patil, Sushma Prasad, Cliff Young, Zongwei\nZhou, and David Patterson. Ten Lessons From Three Generations Shaped Google\u2019s TPUv4i : Industrial Product.\nIn 2021 ACM/IEEE 48th Annual International Symposium on Computer Architecture (ISCA) , pages 1\u201314, 2021.\n[27] NVIDIA Corporation. NVIDIA DGX H100 Datasheet , 2023.\n[28] David Patterson, Joseph Gonzalez, Quoc Le, Chen Liang, Lluis-Miquel Munguia, Daniel Rothchild, David\nSo, Maud Texier, and Jeff Dean. Carbon Emissions and Large Neural Network Training. arXiv preprint\narXiv:2104.10350, 2021.\n[29] NVIDIA Corporation. NVIDIA DGX SuperPOD Data Center Design (for NVIDIA DGX H100 Systems) , 4 2023.\nVersion 01.\n[30] Luiz Andre Barroso, Urs Holzle, Parthasarathy Ranganathan, and Margaret Martonosi. The Datacenter As a\nComputer: Designing Warehouse-scale Machines, 2018.\n[31] Meta. Data centers - Meta sustainability. https://sustainability.fb.com/data-centers/, 2024. Ac-\ncessed: 2024-05-30.\n[32] Dylan Patel, Daniel Nishball, and Jeremie Eliahou Ontiveros. AI Datacenter Energy Dilemma - Race for AI\nDatacenter Space. https://www.semianalysis.com/p/ai-datacenter-energy-dilemma-race , 2024.\nAccessed: 2024-05-30.\n[33] Electric Power Monthly. https://www.eia.gov/electricity/monthly/epm_table_grapher.php?t=\nepmt_5_6_a, 2024. Accessed: 2024-01-15.\n[34] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open pre-trained transformer language models. arXiv preprint\narXiv:2205.01068, 2022.\n[35] Carole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang,\nFiona Aga, Jinshi Huang, Charles Bai, et al. Sustainable AI: Environmental implications, challenges and\nopportunities. Proceedings of Machine Learning and Systems , 4:795\u2013813, 2022.\n[36] Alexandra Sasha Luccioni, Sylvain Viguier, and Anne-Laure Ligozat. Estimating the Carbon Footprint of BLOOM,\na 176B Parameter Language Model. arXiv preprint arXiv:2211.02001, 2022.\n[37] NVIDIA Corporation. Why GPUs are great for AI. https://blogs.nvidia.com/blog/\nwhy-gpus-are-great-for-ai/ , 2023. Accessed: 2024-05-30.\n[38] Electricity generation, capacity, and sales in the United States. https://web.archive.\norg/web/20240407085026/https://www.eia.gov/energyexplained/electricity/\nelectricity-in-the-us-top-10.php , 2022. Accessed: 2024-05-29.\n12",
        "score": 0.765,
        "rank": 6,
        "metadata": {
          "document_id": "cottier2024",
          "document_title": "The Rising Costs of Training Frontier AI Models",
          "page": 12,
          "paragraph_index": 12,
          "section_id": "cottier2024:sec12",
          "section_index": 12
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18:s223",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "However, given that the BLOOMz models are fine-tuned versions of the original BLOOM family\nof models [56], we can base ourselves on the logs provided by the authors of the BLOOM carbon footprint estimation\npaper [31].",
        "score": 0.7642,
        "rank": 7,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec12:p18",
          "section_id": "luccioni2024:sec12",
          "sentence_index": 223
        }
      },
      {
        "node_id": "luccioni2024:sec12:p18",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "ACM FAccT \u201924, June 3\u20136, 2024, Rio de Janeiro, Brazil Luccioni et al\nchoices to deploy these models for different tasks and applying them in different scenarios. We further discuss our\nresults and further avenues of research in the next and final section.\n4.3 Comparing model training and inference costs\nAn important trade-off for many AI practitioners and policy-makers is determining when exactly model inference costs\nreach parity with model training (and fine-tuning) - i.e. when does the deployment of models use as much energy as\ntheir initial training? This comparison is often hard to make because it requires the total energy cost of all steps of the\nML model life cycle, which is very rarely available. Of the models that we examined in our study, neither the BLOOMz\nnor the Flan-T5 families of models reported the total energy used nor carbon emitted during their training in the papers\ndescribing the models. However, given that the BLOOMz models are fine-tuned versions of the original BLOOM family\nof models [56], we can base ourselves on the logs provided by the authors of the BLOOM carbon footprint estimation\npaper [31]. We can add to these numbers the energy cost of fine-tuning each model, which we were able to estimate\nbased on the training logs provided by the authors of the BLOOMz paper [34], although we were lacking the necessary\ninformation to infer the carbon footprint 6. We present these numbers, alongside the average energy consumption\nper inference, in Table 5. We can see that the amount of energy required per inference varies from 5.4\u00d7 10\u22125 for the\nsmallest model, BLOOMz-560M to 1.0 \u00d7 10\u22124 kWh for the biggest one, BLOOMz-7B. This is coherent to the numbers\nreported by Luccioni et al. for BLOOM-176B, which required, on average, 0.004 kWh of energy per query, or 40 times\nmore than BLOOMz-7B, being roughly 25 times bigger [ 31] - although this included API deployment of the model,\nwhich is not the case for the models in our study.\nBLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M\nTraining energy (kWh) 51,686 25,634 17,052 10,505\nFinetuning energy (kWh) 7,571 3,242 1,081 543\nInference energy (kWh) 1.0 \u00d7 10\u22124 7.3 \u00d7 10\u22125 6.2 \u00d7 10\u22125 5.4 \u00d7 10\u22125\nCost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592\nTable 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference\ncost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.\nIf we compare the amount of energy used per inference for each of the models with the total amount of energy\nused for both training and fine-tuning them, we can estimate how many inferences would be needed to be carried\nout with a given model in order for the cost of inference to reach the cost of training. As can be seen in Table 5, this\nvaries depending on model size: from around 200 million inferences for the smallest model, BLOOMz-560M, to over\n590 million inferences for the biggest model, BLOOMz-7B. This may seem like a lot if a single instance of a model is\ndeployed, but can add up quickly if there are multiple instances of models deployed in parallel. For instance, it has been\nestimated that, at its peak, ChatGPT had upward of 10 million users per day [36]; the most recent statistics indicate that\nthe ChatGPT login page received 1.7B visits in October 2023 7. Even assuming a single query per user, which is rarely\nthe case, the energy costs of deploying it would surpass its training costs after a few weeks or months of deployment.\nWhile the BLOOMz models are not deployed in real-time in the same manner as ChatGPT, they have been downloaded\nhundreds of thousands of times from the Hugging Face Hub, which would indicate that they have been extensively used\n6The energy consumption can be based on the Thermal Design Power (TDP) of the GPUs used \u2013 while it assumes 100% GPU utilization, it is the most\naccurate estimate possible without energy usage tracking during training.\n7According to SimilarWeb: https://www.similarweb.com/website/chat.openai.com/.\n12",
        "score": 0.7642,
        "rank": 7,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "page": 12,
          "paragraph_index": 18,
          "section_id": "luccioni2024:sec12",
          "section_index": 12
        }
      },
      {
        "node_id": "cottier2024:sec17:p17:s503",
        "document_title": "The Rising Costs of Training Frontier AI Models",
        "text": "According to BLOOM\u2019s model page on Hugging Face, the \u201cEstimated\ncost of training\u201d is the \u201cEquivalent of $2\u20135M in cloud computing (including preliminary experiments)\u201d.",
        "score": 0.7409,
        "rank": 8,
        "metadata": {
          "document_id": "cottier2024",
          "document_title": "The Rising Costs of Training Frontier AI Models",
          "paragraph_id": "cottier2024:sec17:p17",
          "section_id": "cottier2024:sec17",
          "sentence_index": 503
        }
      },
      {
        "node_id": "cottier2024:sec17:p17",
        "document_title": "The Rising Costs of Training Frontier AI Models",
        "text": "sensitive to the GPU and TPU unit cost (accurate within factors of 2 and 4 respectively) and the training chip-hours\n(factor of 3).\nCost quantity 90% CI\nHardware acquisition (GPUs) 0.5x to 2x\nHardware acquisition (TPUs) 0.2x to 4x\nAmortized hardware CapEx + energy (GPUs) 0.3x to 4x\nAmortized hardware CapEx + energy (TPUs) 0.2x to 5x\nTable 5: Estimated relative uncertainty in individual cost estimates, for different methods. TPU estimates have larger\nuncertainty due to the additional uncertainty in estimating their equivalent costs.\nA.8 Ground truth cost comparison\nIn order to verify that our results are reasonable, we sought to compare our cost estimates with true costs reported by\ndevelopers and other sources. However, there are very few models where the developers report both the computing\nresource usage and the total cost. Training costs and compute resources are independently known for BLOOM-176B\nand OPT-175B, so we compare our estimates with these.\nBLOOM-176B was trained on 1,161,261 A100-hours at a throughput of 150 TFLOP/GPU/s at 48% model FLOPs\nutilization and a cost of $3 million (including experiments) [25]. We estimated a cloud compute cost of $1.99M or an\namortized cost of $0.8M for BLOOM-176B. The accuracy of this estimate depends on how much of the grant was spent\non experiments versus the final training run. According to BLOOM\u2019s model page on Hugging Face, the \u201cEstimated\ncost of training\u201d is the \u201cEquivalent of $2\u20135M in cloud computing (including preliminary experiments)\u201d. Preliminary\nexperiments included training smaller BLOOM models. The final training run for the 176B model used 37.24% of the\nenergy of the BLOOM project [36]; if the total cost of the project was C3M as in the grant description, this implies that\nBLOOM-176B had a cost of $1.2M, which is between our two estimates and aligns more closely with the amortized\ncost approach ($900K) than the cloud cost approach ($2M).\nOPT-175B was trained for 793.5 hours, at a cost of $2500/hour as reported in the training logbook [ 34], for a total\ncost of $1.98 million. We estimated a cloud compute cost of $1.5M for the final training run of OPT-175B, which is\noff by 25%, and an amortized hardware and energy cost of $700K, off by 65%. OPT\u2019s cluster cost rate per hour was\nlikely greater than what we estimate from the quantity of GPUs, or less than the approximate figure mentioned by the\ndevelopers in the training log.\nB Sensitivity analysis\nB.1 Selection of historic frontier models\nIn order to analyse trends in frontier ML models, we must define what counts as a frontier model at any point in time.7\nOur preferred approach is to select models from the database that were in the top 10 most compute-intensive models as\nof their release date, although we considered others as shown in Figure 7.\nFor the most part, different selection approaches gave similar results. The exception was selecting frontier models based\non distance from the compute trend. This approach imposes an artificially flat floor on the eligible models. Due to this,\nit leaves out many earlier models, and produces a flatter cost trend than the other methods.\nOur preferred approach has an advantage over alternatives: the selection is more robust to the sampling of our dataset.\nApproaches based on quantiles, or distance from the historic trend, are influenced by data collected on models outside\nthe frontier. Selecting the top-ranked models, in comparison, is merely influenced by whether the dataset contains those\nfrontier models.\n7Models in the database meet one or more of the following criteria: (i) advanced the state of the art on a qualifying benchmark,\n(ii) at least 1000 citations, (iii) at least one million monthly active users, or (iv) equivalent historical significance [7]. However, this\nmeans the database includes many models that were far from the frontier of compute.\n17",
        "score": 0.7409,
        "rank": 8,
        "metadata": {
          "document_id": "cottier2024",
          "document_title": "The Rising Costs of Training Frontier AI Models",
          "page": 17,
          "paragraph_index": 17,
          "section_id": "cottier2024:sec17",
          "section_index": 17
        }
      },
      {
        "node_id": "luccioni2024:sec13:p19:s259",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "We have provided initial\nnumbers for comparing the relative energy costs of model training, finetuning and inference for different sizes of\nmodels from the BLOOMz family, and found that the parity between training/finetuning and inference grows with\nmodel size.",
        "score": 0.7168,
        "rank": 9,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "paragraph_id": "luccioni2024:sec13:p19",
          "section_id": "luccioni2024:sec13",
          "sentence_index": 259
        }
      },
      {
        "node_id": "luccioni2024:sec13:p19",
        "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
        "text": "Power Hungry Processing ACM FAccT \u201924, June 3\u20136, 2024, Rio de Janeiro, Brazil\nby the open-source community: at the time of writing this article (November 2023), BLOOMz-7B has been downloaded\n606,096 times, BLOOMz-3B has been downloaded 357,368 times, BLOOMz-1B has been downloaded 61,757 times and\nBLOOMz-560m has been downloaded 498,601 times. They have also been finetuned for a number of downstream\ntasks, such as chat, and deployed in HuggingFace Spaces, interactive interfaces for model interaction. While this\nanalysis represents a relatively small sample of models, analyses such as this are vital for estimating the relative energy\nconsumption (and ensuing emissions) of different stages of the ML training and deployment cycle, understanding\ntrade-offs between training and inference emissions patterns, and characterizing the lifetime emissions of ML models,\nand we hope that others will be possible in the future, which would require more transparency from model creators\nregarding both the up front (i.e. training) and downstream (i.e. inference) costs of ML models. We discuss the importance\nof transparency and other important actions that members of the community can take in the next, and final, section.\n5 DISCUSSION\nThere have been limited studies regarding the energy consumption and carbon emissions of LLM inference, largely due\nto its distributed nature \u2014 compared to the relatively time- and location-constrained nature of training \u2014 making it\ndifficult to make meaningful comparisons between different models and tasks. In this work, we have endeavored to\nkeep as many parameters stable as possible, including the code, hardware, datasets, batch size and Python library. We\nprovide all of the code that we used for our analysis as well as an interactive tool to allow users to more deeply explore\nthe results we present here. We also highlight the main high-level takeaways of our study below:\nGenerative tasks are more energy- and carbon-intensive compared to discriminative tasks. As shown in Figure 1, the\nmost energy- and carbon-intensive tasks are those that generate new content: text generation, summarization, image\ncaptioning, and image generation.\nTasks involving images are more energy- and carbon-intensive compared to those involving text alone. More specifically,\ntasks involving predicting categories (text-to-category, image-to-category) are less energy-intensive than those involving\ngenerating images (e.g. text-to-image), with those involving text between the two (see Figure 2).\nDecoder-only models are slightly more energy- and carbon- intensive than sequence-to-sequence models for models of a\nsimilar size and applied to the same tasks. The findings we present in Table 3, Figure 3, and Figure 6 would indicate that\nmore computation (i.e. energy) is required for decoder-only tasks, and that this phenomenon is particularly marked for\ntasks with longer outputs. This observation is worth verifying for other architectures from both categories, and well as\nother tasks and datasets.\nTraining remains orders of magnitude more energy- and carbon- intensive than inference. We have provided initial\nnumbers for comparing the relative energy costs of model training, finetuning and inference for different sizes of\nmodels from the BLOOMz family, and found that the parity between training/finetuning and inference grows with\nmodel size. While the ratio is hundreds of millions of inferences for a single training, given the ubiquity of ML model\ndeployment, this parity can be reached quickly for many popular models.\nUsing multi-purpose models for discriminative tasks is more energy-intensive compared to task-specific models for these\nsame tasks. This is especially the case for text classification (on IMDB, SST 2 and Rotten Tomatoes) and question\nanswering (on SciQ, SQuAD v1 and v2), where the gap between task-specific and zero-shot models is particularly large,\nand less so for summarization (for CNN-Daily Mail, SamSUM and XSum). As can be seen in Table 4, the difference\n13",
        "score": 0.7168,
        "rank": 9,
        "metadata": {
          "document_id": "luccioni2024",
          "document_title": "Power Hungry Processing: Watts Driving the Cost of AI Deployment?",
          "page": 13,
          "paragraph_index": 19,
          "section_id": "luccioni2024:sec13",
          "section_index": 13
        }
      },
      {
        "node_id": "morrison2025:sec3:p3",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Published as a conference paper at ICLR 2025\nLuccioni et al. (2023) reported estimates for emissions from the manufacturing process (embodied\nemissions), from electricity consumption during training, and from electricity consumption of the\ncluster while it was idle (see their Table 2). Dodge et al. (2022) measured electricity consump-\ntion and carbon emissions for training language models and computer vision models with granular\ntimesteps with region-specific carbon intensity, but did not measure development costs, water con-\nsumption, or inference. Similarly, developers of the Llama models (Touvron et al., 2023a;b; Dubey\net al., 2024) reported electricity consumption and carbon emissions estimates of training their final\nmodels; they did not estimate development cost or water consumption, and their approach to carbon\nintensity varied.5 Gemma developers (Gemma Team et al., 2024) only report a single number: the\ntotal emissions from pretraining their models, not broken down by model or by different stages of\ntraining, or by electricity consumption and carbon intensity. The OLMo report (Groeneveld et al.,\n2024) documents electricity consumption per model, and uses region-specific carbon intensity to\nestimate emissions for two regions, but does not estimate other environmental impacts. The OLMo\n2 report (OLMo et al., 2025) again documents electricity consumption per model and uses region-\nand datacenter-specific intensity factors to estimate emissions and also water consumption, but does\nnot measure development costs or potential inference costs. Energy use and environmental impacts\nare not typically documented for proprietary models.\nComparably little transparency has been provided on the water consumption of AI systems. Li et al.\n(2023) estimate the water consumption of some closed models like GPT-3, but these estimates are\nbased on speculation about location of training, energy consumption, etc., as there is very little\npublic information about GPT-3\u2019s training. Similarly, there are few estimates of embodied carbon\nfor AI systems, as the manufacturing process is notoriously opaque. In addition, almost all reporting\nof environmental impact is based on training of the final model that is released. Instead of only\nfocusing on training, Luccioni et al. (2024) estimate the impact of inference of deployed AI systems.\nTo the best of our knowledge our work provides the first public estimates of environmental impact\nof development of an LLM, i.e. hyperparameter tuning and ablations before the main training run.\n3 M ETHODOLOGY\nOur goal in this work is to characterize the holistic environmental impacts of large language models\nin as much detail as possible, enabling assessment of key challenges and future directions towards\nreducing those impacts. Typically, studies documenting language model training and development\nmethodology will address this concern by reporting the cost to train the final, deployed model mea-\nsured in GPU hours, kWh energy, and/or CO 2 emissions. However, this calculation provides an\nincomplete characterization of the factors leading to environmental degradation due to LLMs that\nunder-estimates impacts and provides insufficient information to inform strategies for developing\nand deploying LLMs in a more environmentally conscious way.\nFollowing the more comprehensive analysis provided for the BLOOM model (Luccioni et al., 2023),\nwe expand our measurement to include both operational GHG emissions arising from the energy\nrequired for the development, training, and inference phases of the ML model lifecycle, as well as\nembodied emissions attributed to manufacturing of the hardware supporting those operations. We\nalso go beyond previous work to report non-GHG externalities such as water use, and finer-grained\ndata such as variance in energy use throughout training. We describe our methodology for measuring\nand estimating these impacts in more detail below.\n3.1 O PERATIONAL IMPACTS\nOperational environmental impacts of LLMs are those that arise directly from the development\nand use of models, and include the GHG emissions arising from energy sources used to power\nmodel training and deployment, including servers and data center cooling. We base our analysis of\noperational emissions around the following equation introduced by Schwartz et al. (2020) to describe\nthe amount of computation required to produce a machine learning artifact, such as an LLM:\nCost(R) \u221d E \u00b7 D \u00b7 H (1)\n5Llama 1 did not use the data center location\u2019s carbon intensity, instead using US national average carbon\nintensity; Llama 2 did not specify the carbon intensity; Llama 3 used a region-specific carbon intensity. All 3\nassumed 100% GPU power draw throughout training.\n3",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "page": 3,
          "paragraph_index": 3,
          "section_id": "morrison2025:sec3",
          "section_index": 3
        }
      },
      {
        "node_id": "morrison2025:sec3",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Page 3",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "page": 3,
          "section_index": 3
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s44",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Published as a conference paper at ICLR 2025\nLuccioni et al.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 44
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s45",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "(2023) reported estimates for emissions from the manufacturing process (embodied\nemissions), from electricity consumption during training, and from electricity consumption of the\ncluster while it was idle (see their Table 2).",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 45
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s46",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Dodge et al.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 46
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s47",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "(2022) measured electricity consump-\ntion and carbon emissions for training language models and computer vision models with granular\ntimesteps with region-specific carbon intensity, but did not measure development costs, water con-\nsumption, or inference.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 47
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s48",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Similarly, developers of the Llama models (Touvron et al., 2023a;b;",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 48
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s49",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Dubey\net al., 2024) reported electricity consumption and carbon emissions estimates of training their final\nmodels;",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 49
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s50",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "they did not estimate development cost or water consumption, and their approach to carbon\nintensity varied.5 Gemma developers (Gemma Team et al., 2024) only report a single number: the\ntotal emissions from pretraining their models, not broken down by model or by different stages of\ntraining, or by electricity consumption and carbon intensity.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 50
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s51",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "The OLMo report (Groeneveld et al.,\n2024) documents electricity consumption per model, and uses region-specific carbon intensity to\nestimate emissions for two regions, but does not estimate other environmental impacts.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 51
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s52",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "The OLMo\n2 report (OLMo et al., 2025) again documents electricity consumption per model and uses region-\nand datacenter-specific intensity factors to estimate emissions and also water consumption, but does\nnot measure development costs or potential inference costs.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 52
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s53",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Energy use and environmental impacts\nare not typically documented for proprietary models.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 53
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s54",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Comparably little transparency has been provided on the water consumption of AI systems.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 54
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s55",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Li et al.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 55
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s56",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "(2023) estimate the water consumption of some closed models like GPT-3, but these estimates are\nbased on speculation about location of training, energy consumption, etc., as there is very little\npublic information about GPT-3\u2019s training.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 56
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s57",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Similarly, there are few estimates of embodied carbon\nfor AI systems, as the manufacturing process is notoriously opaque.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 57
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s58",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "In addition, almost all reporting\nof environmental impact is based on training of the final model that is released.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 58
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s59",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Instead of only\nfocusing on training, Luccioni et al.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 59
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s60",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "(2024) estimate the impact of inference of deployed AI systems.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 60
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s61",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "To the best of our knowledge our work provides the first public estimates of environmental impact\nof development of an LLM, i.e.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 61
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s62",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "hyperparameter tuning and ablations before the main training run.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 62
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s63",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "3 M ETHODOLOGY\nOur goal in this work is to characterize the holistic environmental impacts of large language models\nin as much detail as possible, enabling assessment of key challenges and future directions towards\nreducing those impacts.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 63
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s64",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Typically, studies documenting language model training and development\nmethodology will address this concern by reporting the cost to train the final, deployed model mea-\nsured in GPU hours, kWh energy, and/or CO 2 emissions.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 64
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s65",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "However, this calculation provides an\nincomplete characterization of the factors leading to environmental degradation due to LLMs that\nunder-estimates impacts and provides insufficient information to inform strategies for developing\nand deploying LLMs in a more environmentally conscious way.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 65
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s66",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Following the more comprehensive analysis provided for the BLOOM model (Luccioni et al., 2023),\nwe expand our measurement to include both operational GHG emissions arising from the energy\nrequired for the development, training, and inference phases of the ML model lifecycle, as well as\nembodied emissions attributed to manufacturing of the hardware supporting those operations.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 66
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s67",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "We\nalso go beyond previous work to report non-GHG externalities such as water use, and finer-grained\ndata such as variance in energy use throughout training.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 67
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s68",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "We describe our methodology for measuring\nand estimating these impacts in more detail below.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 68
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s69",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "3.1 O PERATIONAL IMPACTS\nOperational environmental impacts of LLMs are those that arise directly from the development\nand use of models, and include the GHG emissions arising from energy sources used to power\nmodel training and deployment, including servers and data center cooling.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 69
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s70",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "We base our analysis of\noperational emissions around the following equation introduced by Schwartz et al.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 70
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s71",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "(2020) to describe\nthe amount of computation required to produce a machine learning artifact, such as an LLM:\nCost(R) \u221d E \u00b7 D \u00b7 H (1)\n5Llama 1 did not use the data center location\u2019s carbon intensity, instead using US national average carbon\nintensity;",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 71
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s72",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Llama 2 did not specify the carbon intensity;",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 72
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s73",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Llama 3 used a region-specific carbon intensity.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 73
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s74",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "All 3\nassumed 100% GPU power draw throughout training.",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 74
        }
      },
      {
        "node_id": "morrison2025:sec3:p3:s75",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "3",
        "score": 0.712,
        "rank": 10,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec3:p3",
          "section_id": "morrison2025:sec3",
          "sentence_index": 75
        }
      }
    ],
    "num_snippets": 75,
    "retry_count": 0
  },
  {
    "id": "q320",
    "question": "What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?",
    "gt_value": "1",
    "gt_unit": "V100_32GB_GPU",
    "gt_ref": "['samsi2024']",
    "pred_value": "1",
    "pred_unit": "V100_32GB_GPU",
    "pred_ref": "[\"samsi2024\"]",
    "pred_explanation": "The context from [ref_id=samsi2024] Table II explicitly states that the bare minimum hardware requirement for LLaMA-7B inference on V100 32GB GPUs is 1 GPU, with a maximum batch size of 64 and a maximum generation length of 256. No compression or quantization is assumed, matching the question's conditions.",
    "raw_response": "{\n  \"explanation\": \"The context from [ref_id=samsi2024] Table II explicitly states that the bare minimum hardware requirement for LLaMA-7B inference on V100 32GB GPUs is 1 GPU, with a maximum batch size of 64 and a maximum generation length of 256. No compression or quantization is assumed, matching the question's conditions.\",\n  \"answer\": \"One NVIDIA V100 32GB GPU is required.\",\n  \"answer_value\": 1,\n  \"ref_id\": [\"samsi2024\"],\n  \"ref_url\": [],\n  \"supporting_materials\": \"TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64\"\n}",
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true,
    "weighted_score": 1.0,
    "latency_seconds": 69.69905018806458,
    "retrieval_seconds": 19.305443286895752,
    "generation_seconds": 50.39339256286621,
    "error": null,
    "pred_ref_url": [],
    "pred_supporting_materials": "TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64",
    "rendered_prompt": "You will answer a question using ONLY the provided context snippets.\nIf the context does not clearly support an answer, use \"is_blank\".\n\nContext snippets from documents:\n[ref_id=rubei2025] Since the GPU provided\nonly 8GB of RAM, we used the quantized version of the\nLlama model i.e., we used 16-bit float rather than the default\n32-bit.\n---\n[ref_id=rubei2025] TABLE I: Summary of the Experimental Settings\nModel Llama3 8B - Instruct\nSnippets 1,000\nPETs 3\nCustom Prompts 5\nRepetitions 5\nPause 10 seconds\nMetrics (Performance) Energy Consumption, Execution Time\nMetrics (Accuracy) Exact Match, Edit Distance\nincomplete Java snippets retrieved from the code-completion\ndataset of CodeXGLUE. As discussed in Section IV the overall\nexecution requires more than 250 hours. We calculated an\naverage test time per snippet of about 900 seconds. Therefore,\nwe limit ourselves to 1,000 snippets. with the abovementioned\nPETs\nAs discussed in Section III-B, we defined five distinct\nconfigurations for each query. Consequently, we tested every\ncombination of prompting techniques and the use of custom\ntags. To ensure experimental reliability, each test is repeated\nfive times [33], [34], with a ten-second pause between each test\nto mitigate potential tail effects [34], [35]. We use two metrics\nto evaluate energy consumption and execution time, and two\nprimary metrics (exact match and edit distance) to assess the\nimpact of different configurations on accuracy. These metrics\nalign with those used in the original evaluation of the code\ncompletion benchmark suite by the authors of CodeXGLUE.\nAll the experiments have been conducted on an isolated\ndesktop equipped with an AMD Ryzen 7 5800X 3.8GHz CPU\nand an Nvidia Geforce RTX 4060 TI (8 GB VRAM). 5 The\noperating system is Xubuntu 23.04. Since the GPU provided\nonly 8GB of RAM, we used the quantized version of the\nLlama model i.e., we used 16-bit float rather than the default\n32-bit.\nIV. E XPERIMENTAL RESULTS\nAnswering RQ1: Figure 3a shows the energy consumption\nof the three prompt techniques applied to the five different\nconfigurations. In particular, with the default configuration C0,\nzero-shot is the most energy-efficient, with an average cost of\nabout 0.000016 kWh. one-shot and a few-shots consumed an\naverage of 0.000035 kWh and 0.000054 kWh, respectively.\nCustom tags can contribute to reducing the energy consump-\ntion of the video card. As shown in Fig. 3a, the best config-\nuration is the C2 (explanation in prompts). While the zero-\nshot technique passed from 0.0000157 (of C0) to 0.0000146\n(-7%), one-shot and few-shots reduced the consumption from\n0.0000347 to 0.0000174 (-99%) and from 0.0000537 to\n0.0000293 (-83%) comparing with the default configuration\nC0, respectively. It is also interesting to see the results of C4,\nin which we do not specify any role in the system token. The\nconsumption increased from 0.0000157 to 0.000189 kWh for\nzero-shot and from 0.0000347 to 0.000181 kWh for one-shot.\n5https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4060-\n4060ti/\nThe reason is that the model started to generate completely\nnew code snippets when asked to finalize the code given as\ninput. The few-shots technique seems to be less affected by\nthis problem. The sequence of example questions and answers\ninstructed the model on the behaviour despite the lack of the\nsystem role specification.\nConcerning the execution time, Figure 3b reports the results\nobtained for all the prompt configurations. Similar to energy\nconsumption, the usage of custom tags provides a general\nimprovement in performance. In particular, the one-shot and\nfew-shots reduced the average time from 1.54 seconds of\nconfiguration C0 to 0.74 (-52%) and from 2.1 to 1.09 (-48%),\nrespectively, using configuration C2. The zero-shot technique\nperformed better using C1, reporting an improvement from\n0.74 seconds to 0.63 (-14.8%). Similarly, for the energy\nconsumption, in the case of C4, we can notice a remarkable\nincrease in execution time for zero-shot and one-shot.\nAnswer to RQ 1: Our study reveals that custom tags\ncan reduce the energy consumption of LLMs across the\nthree prompt engineering techniques tested for source\ncode completion tasks.\nAnswering RQ2: Figure 4 depicts the obtained results in\nterms of accuracy metrics. In particular, Figure 4a shows the\neffects of custom tags on exact match performance across\ndifferent prompt engineering techniques. Overall, we observe\nan increase in exact matches for configuration C1-C3 in com-\nparison with the default configuration C0. Notably, zero-shot\nshows the greatest improvement with C1, where exact matches\nrise from 63 to 82, reflecting a 23% increase. Both one-\nshot and few-shots see substantial gains with C3, achieving\napproximately a 44% improvement. Interestingly, with C4,\nzero-shot fails to achieve any exact matches.\nFigure 4b shows the impact of custom tags on edit distance\nmetrics, where an edit distance of 0 indicates a perfect result.\nOverall, custom tags contributed to a reduction in edit distance,\nwith C2 emerging as the most effective configuration across all\nprompt engineering techniques. Specifically, zero-shot showed\na 24% improvement, one-shot achieved a 64% reduction, and\nfew-shots improved by 70%. Results for zero-shot and one-\nshot are omitted for C4 because, with this configuration,\nthe LLM produced uncontrolled responses. As a result, it\nwas impossible to calculate edit distance accurately, as the\noutputs included both code and explanatory text. Despite\nlacking explicit role definitions, few-shots continued to yield\nsatisfactory results.\nAnswer to RQ 2: Prompt customizations enhanced\nthe accuracy of the tested PETs, showing a positive\ntrend with increased exact matches and reduced edit\ndistances.\n---\n[ref_id=samsi2024] The LLaMA model is available in a\nnumber of sizes but, in most cases, its larger variants typically\nrequire multiple high-end GPUs for both training and in-\nference (assuming no further compression/distillation).\n---\n[ref_id=samsi2024] From Words to Watts: Benchmarking the Energy\nCosts of Large Language Model Inference\nSiddharth Samsi \u2217\u00a7, Dan Zhao \u2020, Joseph McDonald \u2217, Baolin Li \u2021, Adam Michaleas \u2217,\nMichael Jones \u2217, William Bergeron \u2217, Jeremy Kepner \u2217, Devesh Tiwari \u2021, Vijay Gadepally \u2217\n\u2217 MIT, \u2020 NYU, \u2021 Northeastern University\nAbstract\u2014Large language models (LLMs) have exploded in\npopularity due to their new generative capabilities that go far\nbeyond prior state-of-the-art. These technologies are increasingly\nbeing leveraged in various domains such as law, finance, and\nmedicine. However, these models carry significant computational\nchallenges, especially the compute and energy costs required for\ninference. Inference energy costs already receive less attention\nthan the energy costs of training LLMs\u2014despite how often these\nlarge models are called on to conduct inference in reality (e.g.,\nChatGPT). As these state-of-the-art LLMs see increasing usage\nand deployment in various domains, a better understanding\nof their resource utilization is crucial for cost-savings, scaling\nperformance, efficient hardware usage, and optimal inference\nstrategies.\nIn this paper, we describe experiments conducted to study the\ncomputational and energy utilization of inference with LLMs. We\nbenchmark and conduct a preliminary analysis of the inference\nperformance and inference energy costs of different sizes of\nLLaMA\u2014a recent state-of-the-art LLM\u2014developed by Meta AI\non two generations of popular GPUs (NVIDIA V100 & A100)\nand two datasets (Alpaca and GSM8K) to reflect the diverse\nset of tasks/benchmarks for LLMs in research and practice.\nWe present the results of multi-node, multi-GPU inference using\nmodel sharding across up to 32 GPUs. To our knowledge, our\nwork is the one of the first to study LLM inference performance\nfrom the perspective of computational and energy resources at\nthis scale.\nIndex Terms \u2014Large Language Models, Natural Language\nProcessing, Inference, Green AI, LLM, NLP, Deep Learning,\nDistributed Computing, Energy, Sustainability\nI. I NTRODUCTION\nGenerative models (GenAI) are able to produce new content\nfrom synthesizing text, images, and audio from which it\u2019s\ntrained on. While GenAI is not entirely new, the recent\napplication and broad availability of this technology via tools\nsuch as Stable Diffusion [1], OpenAI\u2019s ChatGPT, Google\u2019s\nBard and integration into the Microsoft Bing search engine\nhas captured the imagination of the world and led to a massive\nsurge in interest in deploying these types of models across a\nDISTRIBUTION STATEMENT A. Approved for public release. Distribu-\ntion is unlimited. This material is based upon work supported by the Assistant\nSecretary of Defense for Research and Engineering under Air Force Contract\nNo. FA8702-15-D-0001, and United States Air Force Research Laboratory\nCooperative Agreement Number FA8750-19-2-1000. Any opinions, findings,\nconclusions or recommendations expressed in this material are those of the\nauthor(s) and do not necessarily reflect the views of the Assistant Secretary\nof Defense for Research and Engineering, or the United States Air Force.\nThe U.S. Government is authorized to reproduce and distribute reprints for\nGovernment purposes notwithstanding any copyright notation herein.\nvariety of domains ranging such as education, government,\nengineering, law, finance and many more.\nThe popularity of these models has also put a spotlight on\nmany societal concerns stemming from their usage. From ethi-\ncal concerns ranging from violations of copyright laws [2], [3]\nto safety concerns arising from the fact that these models are\ncapable of hallucinating or fabricating information, concerns\nabout these models in the educational and medical domain [4],\n[5], their carbon footprint, and many more.\nIn this paper, we focus primarily on understanding the\nsignificant amount of resources\u2014time, computation, and\nenergy\u2014required for using and deploying some of the large\nlanguage models (LLM) like those that underlie ChatGPT,\nBard, etc. Several prior works have estimated the compute\nand energy costs of training language models. Works like [6]\ndiscuss the carbon footprint of language models such as BERT,\nELMo, and precursors to larger models such as GPT-3 and\nGPT-4 that power some of the popular AI chatbots today. Oth-\ners have also looked to larger language models; for instance,\nthe largest NVIDIA Megatron-LM model required 3,072 A100\nGPUs [7]\u2013[9] for its training. While the complete details (time\nand resources used) of compute required for training GPT-\n3/4 are not available, several estimates for training [10], [11]\nand inference are publicly available. As industry attempts to\nshore up competitive moats and restrict information regarding\ntheir underlying LLM technologies, these details can become\nless reliable and available. Compounding this issue, estimates\nfor inference are even less readily available [12] despite their\nsignificant share of energy costs and their likely larger impact\non the environment [13]\u2014especially since model inference\ncalls can occur more frequently than training/fine-tuning for\nreal-world deployments and applications.\nWe present the results of our inference experiments on\nLLaMA [14]: an open sourced pre-trained large language\nmodels by Meta AI. The LLaMA model is available in a\nnumber of sizes but, in most cases, its larger variants typically\nrequire multiple high-end GPUs for both training and in-\nference (assuming no further compression/distillation). While\nour emphasis is on characterizing the compute performance\nand energy used for multi-node, multi-GPU inference, we\nalso include results from single node instances using smaller\nvariants of the model as a baseline comparison. We hope our\nwork will help illustrate some of the compute performance\nand energy utilization characteristics of LLM inference. We\nalso hope that our experiments, analysis, and data on real-\narXiv:2310.03003v1  [cs.CL]  4 Oct 2023\n---\n[ref_id=samsi2024] of 256. For this configuration, we ran on four A100 GPUs\nand 8, 16, 32 V100 GPUs. These results are summarized in\nTables IV and V. In all cases, the streaming multiprocessors\n(SM) utilization as reported by the DCGM utility was observed\nto be in the 94%-95% range. For the A100 GPUs, the average\nSM utilization rises to 98% when the maximum generated\noutput length is increased to 2048. Given that the model is\nsharded in a manner that enables us to load it fully in GPU\nmemory and run inference on a non-trivial amount of data,\nwe expect memory utilization to be low depending on the\nspecific model parameters and input sizes used. Thus, on\nthe four 80GB A100 nodes, the memory utilization varies\nbetween 23%-27% depending the maximum generated output\nlength. This under-utilization of memory implies that it may\nbe possible to co-locate multiple models on the same set\nof GPUs to increase aggregate throughput and potentially\nreduce cloud compute costs or improve system utilization at\na supercomputer center. With new GPU sharing capabilities\nsuch as Multi-Process Service (MPS) [27] and Multi-Instance\nGPU (MIG) [28], a single GPU may be shared by diverse\nworkloads for an overall improvement in system throughput\nas shown in recent work [29]. The optimal GPU configuration\nfor sharing LLMs and other workloads is a part of our future\nwork in this area.\nModel Shards Output Length Max. Memory Util. Avg. SM Util.\n4 256 23.36 95.00\n4 512 24.54 98.81\n4 1024 24.85 98.85\n4 2048 27.00 98.00\nTABLE IV: A100 Utilization: This table shows GPU utiliza-\ntion for 80GB A100 GPUs and LLaMA 65B with 4 shards,\nbatch size of 64 averaged across both datasets used in this\npaper.\nModel Shards Output Length Max. Memory Util. Avg. SM Util.\n8 256 24.25 94.75\n16 256 13.33 95.00\n32 256 6.66 95.66\nTABLE V: V100 Utilization: This table shows GPU utiliza-\ntion for 32GB V100 GPUs and LLaMA 65B with 8, 16, 32\nshards, a batch size of 64 and maximum generated output\nlength of 256 averaged across both datasets used in this paper.\nWe limit this result to an ouptut length of 256 because longer\noutputs on 8 V100 GPUs are not possible given memory limits\nof the GPU.\nV. D ISCUSSION\nIn this paper, we show the results of benchmarking a\nrepresentative large language model on NVIDIA GPUs. We\nshow baseline results from smaller models (7B, 13B) and\ncompare them against the largest available version (65B) of\nLLaMA. We also examine the inference performance and en-\nergy across distributed settings and different configurations by\nvarying model parameters, input data, and hardware configu-\nrations. By comparing a natural language instruction following\ndataset (Alpaca) and a mathematical question-answer dataset\n(GSM8K), we also find that the complexity of the input dataset\ncan affect the model performance for a given set of hyper-\nparameters and hardware configuration.\nGiven the size of LLMs and the limits imposed by current\nhardware, inference with large models can impose onerous\nrequirements. For example, we find that, at a minimum, 8\nV100 GPUs each with 32 GB of RAM or 4 A100 GPUs\neach with 80GB of memory are required for any meaningful\ninferences with the 65B LLaMA model. In each case among\nour experiments, we shard the model evenly across all GPUs in\norder to fit the model/data; however, this results in only 20%-\n25% of the GPU memory being utilized at any given time. This\nover-provisioning of resources represents new opportunities\nfor resource sharing across multiple workloads in the latest\nNVIDIA GPUs. The Multi-Process Service (MPS) [27] and\nMulti-Instance GPU (MIG) [28] are new capabilities that\nenable GPU sharing across different workloads. Although\nidentifying the optimal MPS or MIG configuration for a\ngiven set of workloads is challenging, recent work [29] has\ndeveloped new techniques to exploit these capabilities in order\nto dynamically partition GPU resources. This opens up the\npotential to optimally partition high-end GPUs such as the\nA100s or H100s to co-locate multiple LLMs for inference\u2014\nwith the potential of only minimal degradation to computa-\ntional performance.\nFinally, as AI compute requirements have increased, there\nis an increasing focus on approaches to reduce the carbon\nand energy footprints of datacenters by making larger models\nleaner or more efficient. Approaches such as model quan-\ntization, distillation, sparsification, etc. are being developed\nto reduce the compute required for AI along with the de-\nvelopment of custom, energy-efficient hardware for inference\nand training. However, simple interventions like GPU power\ncapping is available to be deployed today\u2014our preliminary\nanalysis with LLM inference in this paper suggests that power\ncapping can be an effective tool for reducing inference energy.\nIf applied at the datacenter-scale, this intervention has the\npotential to reduce overall energy usage in the long-run as new\napproaches are developed to address the energy consumption\nof AI compute.\nAs part of our future plans, we aim to conduct similar\nexperiments on other open-source, large language models\nalong with more in-depth characterization of compute and\nenergy for not just inference, but also for the training/fine-\ntuning of these models. It is our hope that this paper provides\na baseline for inference with LLMs and fosters a broader\ndiscussion of the challenges and opportunities in this field.\nACKNOWLEDGEMENTS\nThe authors acknowledge the MIT SuperCloud [18] and\nLincoln Laboratory Supercomputing Center for providing HPC\nand consultation resources that have contributed to the research\nresults reported within this paper. The authors acknowledge the\n---\n[ref_id=samsi2024] Page 8\n---\n[ref_id=samsi2024] of 256.\n---\n[ref_id=samsi2024] For this configuration, we ran on four A100 GPUs\nand 8, 16, 32 V100 GPUs.\n---\n[ref_id=samsi2024] These results are summarized in\nTables IV and V.\n---\n[ref_id=samsi2024] In all cases, the streaming multiprocessors\n(SM) utilization as reported by the DCGM utility was observed\nto be in the 94%-95% range.\n---\n[ref_id=samsi2024] For the A100 GPUs, the average\nSM utilization rises to 98% when the maximum generated\noutput length is increased to 2048.\n---\n[ref_id=samsi2024] Given that the model is\nsharded in a manner that enables us to load it fully in GPU\nmemory and run inference on a non-trivial amount of data,\nwe expect memory utilization to be low depending on the\nspecific model parameters and input sizes used.\n---\n[ref_id=samsi2024] Thus, on\nthe four 80GB A100 nodes, the memory utilization varies\nbetween 23%-27% depending the maximum generated output\nlength.\n---\n[ref_id=samsi2024] This under-utilization of memory implies that it may\nbe possible to co-locate multiple models on the same set\nof GPUs to increase aggregate throughput and potentially\nreduce cloud compute costs or improve system utilization at\na supercomputer center.\n---\n[ref_id=samsi2024] With new GPU sharing capabilities\nsuch as Multi-Process Service (MPS) [27] and Multi-Instance\nGPU (MIG) [28], a single GPU may be shared by diverse\nworkloads for an overall improvement in system throughput\nas shown in recent work [29].\n---\n[ref_id=samsi2024] The optimal GPU configuration\nfor sharing LLMs and other workloads is a part of our future\nwork in this area.\n---\n[ref_id=samsi2024] Model Shards Output Length Max.\n---\n[ref_id=samsi2024] Memory Util.\n---\n[ref_id=samsi2024] Avg.\n---\n[ref_id=samsi2024] SM Util.\n---\n[ref_id=samsi2024] 4 256 23.36 95.00\n4 512 24.54 98.81\n4 1024 24.85 98.85\n4 2048 27.00 98.00\nTABLE IV: A100 Utilization: This table shows GPU utiliza-\ntion for 80GB A100 GPUs and LLaMA 65B with 4 shards,\nbatch size of 64 averaged across both datasets used in this\npaper.\n---\n[ref_id=samsi2024] Model Shards Output Length Max.\n---\n[ref_id=samsi2024] Memory Util.\n---\n[ref_id=samsi2024] Avg.\n---\n[ref_id=samsi2024] SM Util.\n---\n[ref_id=samsi2024] 8 256 24.25 94.75\n16 256 13.33 95.00\n32 256 6.66 95.66\nTABLE V: V100 Utilization: This table shows GPU utiliza-\ntion for 32GB V100 GPUs and LLaMA 65B with 8, 16, 32\nshards, a batch size of 64 and maximum generated output\nlength of 256 averaged across both datasets used in this paper.\n---\n[ref_id=samsi2024] We limit this result to an ouptut length of 256 because longer\noutputs on 8 V100 GPUs are not possible given memory limits\nof the GPU.\n---\n[ref_id=samsi2024] V.\n---\n[ref_id=samsi2024] D ISCUSSION\nIn this paper, we show the results of benchmarking a\nrepresentative large language model on NVIDIA GPUs.\n---\n[ref_id=samsi2024] We\nshow baseline results from smaller models (7B, 13B) and\ncompare them against the largest available version (65B) of\nLLaMA.\n---\n[ref_id=samsi2024] We also examine the inference performance and en-\nergy across distributed settings and different configurations by\nvarying model parameters, input data, and hardware configu-\nrations.\n---\n[ref_id=samsi2024] By comparing a natural language instruction following\ndataset (Alpaca) and a mathematical question-answer dataset\n(GSM8K), we also find that the complexity of the input dataset\ncan affect the model performance for a given set of hyper-\nparameters and hardware configuration.\n---\n[ref_id=samsi2024] Given the size of LLMs and the limits imposed by current\nhardware, inference with large models can impose onerous\nrequirements.\n---\n[ref_id=samsi2024] For example, we find that, at a minimum, 8\nV100 GPUs each with 32 GB of RAM or 4 A100 GPUs\neach with 80GB of memory are required for any meaningful\ninferences with the 65B LLaMA model.\n---\n[ref_id=samsi2024] In each case among\nour experiments, we shard the model evenly across all GPUs in\norder to fit the model/data;\n---\n[ref_id=samsi2024] however, this results in only 20%-\n25% of the GPU memory being utilized at any given time.\n---\n[ref_id=samsi2024] This\nover-provisioning of resources represents new opportunities\nfor resource sharing across multiple workloads in the latest\nNVIDIA GPUs.\n---\n[ref_id=samsi2024] The Multi-Process Service (MPS) [27] and\nMulti-Instance GPU (MIG) [28] are new capabilities that\nenable GPU sharing across different workloads.\n---\n[ref_id=samsi2024] Although\nidentifying the optimal MPS or MIG configuration for a\ngiven set of workloads is challenging, recent work [29] has\ndeveloped new techniques to exploit these capabilities in order\nto dynamically partition GPU resources.\n---\n[ref_id=samsi2024] This opens up the\npotential to optimally partition high-end GPUs such as the\nA100s or H100s to co-locate multiple LLMs for inference\u2014\nwith the potential of only minimal degradation to computa-\ntional performance.\n---\n[ref_id=samsi2024] Finally, as AI compute requirements have increased, there\nis an increasing focus on approaches to reduce the carbon\nand energy footprints of datacenters by making larger models\nleaner or more efficient.\n---\n[ref_id=samsi2024] Approaches such as model quan-\ntization, distillation, sparsification, etc.\n---\n[ref_id=samsi2024] are being developed\nto reduce the compute required for AI along with the de-\nvelopment of custom, energy-efficient hardware for inference\nand training.\n---\n[ref_id=samsi2024] However, simple interventions like GPU power\ncapping is available to be deployed today\u2014our preliminary\nanalysis with LLM inference in this paper suggests that power\ncapping can be an effective tool for reducing inference energy.\n---\n[ref_id=samsi2024] If applied at the datacenter-scale, this intervention has the\npotential to reduce overall energy usage in the long-run as new\napproaches are developed to address the energy consumption\nof AI compute.\n---\n[ref_id=samsi2024] As part of our future plans, we aim to conduct similar\nexperiments on other open-source, large language models\nalong with more in-depth characterization of compute and\nenergy for not just inference, but also for the training/fine-\ntuning of these models.\n---\n[ref_id=samsi2024] It is our hope that this paper provides\na baseline for inference with LLMs and fosters a broader\ndiscussion of the challenges and opportunities in this field.\n---\n[ref_id=samsi2024] ACKNOWLEDGEMENTS\nThe authors acknowledge the MIT SuperCloud [18] and\nLincoln Laboratory Supercomputing Center for providing HPC\nand consultation resources that have contributed to the research\nresults reported within this paper.\n---\n[ref_id=samsi2024] The authors acknowledge the\n---\n[ref_id=samsi2024] For example, we find that, at a minimum, 8\nV100 GPUs each with 32 GB of RAM or 4 A100 GPUs\neach with 80GB of memory are required for any meaningful\ninferences with the 65B LLaMA model.\n---\n[ref_id=samsi2024] of 256. For this configuration, we ran on four A100 GPUs\nand 8, 16, 32 V100 GPUs. These results are summarized in\nTables IV and V. In all cases, the streaming multiprocessors\n(SM) utilization as reported by the DCGM utility was observed\nto be in the 94%-95% range. For the A100 GPUs, the average\nSM utilization rises to 98% when the maximum generated\noutput length is increased to 2048. Given that the model is\nsharded in a manner that enables us to load it fully in GPU\nmemory and run inference on a non-trivial amount of data,\nwe expect memory utilization to be low depending on the\nspecific model parameters and input sizes used. Thus, on\nthe four 80GB A100 nodes, the memory utilization varies\nbetween 23%-27% depending the maximum generated output\nlength. This under-utilization of memory implies that it may\nbe possible to co-locate multiple models on the same set\nof GPUs to increase aggregate throughput and potentially\nreduce cloud compute costs or improve system utilization at\na supercomputer center. With new GPU sharing capabilities\nsuch as Multi-Process Service (MPS) [27] and Multi-Instance\nGPU (MIG) [28], a single GPU may be shared by diverse\nworkloads for an overall improvement in system throughput\nas shown in recent work [29]. The optimal GPU configuration\nfor sharing LLMs and other workloads is a part of our future\nwork in this area.\nModel Shards Output Length Max. Memory Util. Avg. SM Util.\n4 256 23.36 95.00\n4 512 24.54 98.81\n4 1024 24.85 98.85\n4 2048 27.00 98.00\nTABLE IV: A100 Utilization: This table shows GPU utiliza-\ntion for 80GB A100 GPUs and LLaMA 65B with 4 shards,\nbatch size of 64 averaged across both datasets used in this\npaper.\nModel Shards Output Length Max. Memory Util. Avg. SM Util.\n8 256 24.25 94.75\n16 256 13.33 95.00\n32 256 6.66 95.66\nTABLE V: V100 Utilization: This table shows GPU utiliza-\ntion for 32GB V100 GPUs and LLaMA 65B with 8, 16, 32\nshards, a batch size of 64 and maximum generated output\nlength of 256 averaged across both datasets used in this paper.\nWe limit this result to an ouptut length of 256 because longer\noutputs on 8 V100 GPUs are not possible given memory limits\nof the GPU.\nV. D ISCUSSION\nIn this paper, we show the results of benchmarking a\nrepresentative large language model on NVIDIA GPUs. We\nshow baseline results from smaller models (7B, 13B) and\ncompare them against the largest available version (65B) of\nLLaMA. We also examine the inference performance and en-\nergy across distributed settings and different configurations by\nvarying model parameters, input data, and hardware configu-\nrations. By comparing a natural language instruction following\ndataset (Alpaca) and a mathematical question-answer dataset\n(GSM8K), we also find that the complexity of the input dataset\ncan affect the model performance for a given set of hyper-\nparameters and hardware configuration.\nGiven the size of LLMs and the limits imposed by current\nhardware, inference with large models can impose onerous\nrequirements. For example, we find that, at a minimum, 8\nV100 GPUs each with 32 GB of RAM or 4 A100 GPUs\neach with 80GB of memory are required for any meaningful\ninferences with the 65B LLaMA model. In each case among\nour experiments, we shard the model evenly across all GPUs in\norder to fit the model/data; however, this results in only 20%-\n25% of the GPU memory being utilized at any given time. This\nover-provisioning of resources represents new opportunities\nfor resource sharing across multiple workloads in the latest\nNVIDIA GPUs. The Multi-Process Service (MPS) [27] and\nMulti-Instance GPU (MIG) [28] are new capabilities that\nenable GPU sharing across different workloads. Although\nidentifying the optimal MPS or MIG configuration for a\ngiven set of workloads is challenging, recent work [29] has\ndeveloped new techniques to exploit these capabilities in order\nto dynamically partition GPU resources. This opens up the\npotential to optimally partition high-end GPUs such as the\nA100s or H100s to co-locate multiple LLMs for inference\u2014\nwith the potential of only minimal degradation to computa-\ntional performance.\nFinally, as AI compute requirements have increased, there\nis an increasing focus on approaches to reduce the carbon\nand energy footprints of datacenters by making larger models\nleaner or more efficient. Approaches such as model quan-\ntization, distillation, sparsification, etc. are being developed\nto reduce the compute required for AI along with the de-\nvelopment of custom, energy-efficient hardware for inference\nand training. However, simple interventions like GPU power\ncapping is available to be deployed today\u2014our preliminary\nanalysis with LLM inference in this paper suggests that power\ncapping can be an effective tool for reducing inference energy.\nIf applied at the datacenter-scale, this intervention has the\npotential to reduce overall energy usage in the long-run as new\napproaches are developed to address the energy consumption\nof AI compute.\nAs part of our future plans, we aim to conduct similar\nexperiments on other open-source, large language models\nalong with more in-depth characterization of compute and\nenergy for not just inference, but also for the training/fine-\ntuning of these models. It is our hope that this paper provides\na baseline for inference with LLMs and fosters a broader\ndiscussion of the challenges and opportunities in this field.\nACKNOWLEDGEMENTS\nThe authors acknowledge the MIT SuperCloud [18] and\nLincoln Laboratory Supercomputing Center for providing HPC\nand consultation resources that have contributed to the research\nresults reported within this paper. The authors acknowledge the\n---\n[ref_id=samsi2024] 2: Baseline comparison of inference perfor-\nmance/latency between LLaMA 7B, 13B and 65B: inference\nperformance comparisons on the minimum set of hardware\nrequired to run inference (see Table II) across model sizes\nand between V100s and A100s.\n---\n[ref_id=samsi2024] (a) Results from the Alpaca dataset.\n(b) Results from GSM8K dataset\nFig. 2: Baseline comparison of inference perfor-\nmance/latency between LLaMA 7B, 13B and 65B: inference\nperformance comparisons on the minimum set of hardware\nrequired to run inference (see Table II) across model sizes\nand between V100s and A100s.\nFig. 3: Baseline energy per second (Watts) estimates of per-\nforming inference with LLaMA 7B, 13B, and 65B: inference\nenergy comparisons on the minimum set of hardware/settings\nrequired (see Table II) with Alpaca and GSM8K on a log-scale.\nColor indicates device (V100/A100), bars indicate average\nquantities and lines indicate error bars. Energy is averaged\nover maximum generation lengths of 256, 512, and 1024 due\nto near-identical energy/size trends for each generation length.\nFig. 4: Energy per second (Watts) estimates of LLaMA 65B\nacross batch sizes of 64/128/256/256 and 8/16/32 shards for\nmax generation length 512 : inference energy estimates on\nAlpaca and GSM8K on log-scale. Color indicates batch size.\nB. Energy per Second: LLaMA 65B\nWe first take a look at the amount of energy inference costs\nper unit time in seconds. Figures 4 and 5 show a more in-\ndepth look of the energy inference costs of LLaMA 65B across\ndifferent batch sizes and degrees of sharding. Specifically,\nFigure 4 shows energy costs for maximum generation length\n512 and Figure 5 shows energy costs for 1024.\nOverall, we see an average increase in energy per second\nwith the number of shards. While there is a slight correlation\nas energy per second increases with increasing batch size,\nincreasing the number of shards always increases the wattage.\nIndeed, the energy per second increases with the number of\nshards even at the same batch size (e.g., the energy of inference\nat batch size 64, going from 16 shards to 32 shards). For both\ndatasets, increasing the max generation length from 512 to\n1024 does seem to increase the energy per second for each\nbatch size within each shard configuration, but the overall\neffect is less clear or consistent. Overall, we see that the energy\nper second for inference with LLaMA 65B is on the order of\n300 Watts to 1 Kilowatt from the lower shard configuration of\n8 GPUs to the higher end of 32 GPUs.\nC. Energy per Decoded Token: LLaMA 65B\nMoving on to energy per each decoded output token, we see\nthat in Figures 6 and 7 that energy per token tends to follow\na similar pattern in relation to the number of shards: as the\nnumber of shards increases, the energy per output token also\n---\n[ref_id=samsi2024] responses and the total time needed to run inference over the\ninput data.\nWe monitor GPUs using the nvidia-smi [23] and\nNVIDIA DCGM [24] utilities to study GPU utilization, energy,\npower draw, etc. during our experiments. The nvidia-smi\nutility is used to capture GPU usage over time at 100ms\nintervals and the DCGM monitoring tool is used to capture\naggregate GPU energy in Joules for the rank-0 node. For a\nmulti-node, multi-GPU model, we multiply the rank-0 energy\nby the number of nodes used. Maximum power draw on GPUs\nis capped at 250 Watts unless otherwise stated. Due to limits\non resource availability, we mainly use V100 GPUs for larger-\nscale distributed experiments (i.e., for 8, 16, and 32 shards)\nand A100 GPUs for smaller scale experiments.\nInference energy metrics are calculated by combining the\ninference metrics above with the energy data collected from\nour GPUs using NVIDIA\u2019s utilities described above. Specif-\nically, energy per second is defined as the total aggregate\nGPU energy spent from a single experiment/job (across all\nshards) divided by the total run time of that experiment/job in\nseconds. A single experiment/job denotes a single run through\nall 4,096 prompts under a specified batch size. Energy per\ntoken and energy per response are similarly defined as total\nenergy divided by the number of decoded output tokens and\nthe number of responses as defined above, respectively.\nIV. R ESULTS\nA. Baselines: LLaMA 7B, 13B, & 65B\n1) Inference Performance: We begin our analysis with\na baseline comparison of LLaMA 65B with smaller-scale\nLLaMA models: LLaMA 7B and 13B. The goal is to under-\nstand the following: what do inference performance and energy\ntrade-offs look like for the different sizes of LLaMA under the\nbare-minimum set of resources required to have them running\ninference? This question can be important for researchers\nand users who have may not have limitless computational\nresources and hardware acceleration or may be constrained\nin terms of GPU memory, etc.\nGiven the sizes of the models, the size of the data, and the\nhardware memory limits, we only show results from experi-\nments that were possible for a given combination of parameters\n(i.e., for some models, certain combinations of batch size and\nnumber of shards are infeasible due to memory limits of the\nunderlying GPUs). Table II shows the bare minimum hardware\nrequirements for each LLaMA variant and the maximum batch\nsize possible for each combination, assuming no further model\ncompression, optimization, quantization, distillation etc.\nWith these limits in mind, we present the inference per-\nformance of LLaMA 7B, 13B, and 65B on the Alpaca and\nGSM8K datasets with the bare minimum hardware settings in\nFigure 2. The plots in Figure 2 show a baseline comparison of\ninference performance of the three LLaMA variants on both\nthe V100 and A100 GPUs respectively. For each model, in\nline with the spirit of the bare minimum settings, inference\nis done with a batch size of 64 and an maximum generation\nlength of 256. The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256. These limits are imposed by a combination of GPU\nmemory, model size, response length and the number of GPUs.\nWhile the 65B model can sharded across 6 V100 GPUs, we\nuse 8 since the model architecture makes it better suited for\nbalanced sharding across 8 GPUs.\nModel Size V100 32GB A100 80GB\nCount Max. Batch size Count Max. Batch size\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n13B on two GPUs in each case whereas the 65B model was\nrun on 8 V100 GPUs and 4 A100 GPUs respectively due to\nthe size of the model and available memory on the GPU(s).\nAs expected, we observe that the A100 outperforms V100\non both the Alpaca and GSM8K datasets: particularly for the\nsmaller LLaMA 7B and 13B, we see anywhere from a 2\ntimes (7B) to a 1.25 times increase (13B) in inference latency\non the A100 when compared to the V100 across words per\nsecond, tokens per second, and responses per second. Faster\nresponse rates and inference are likely due to the fact that\nthe number of computations, directly related to the number\nof parameters of said model, involved in the 7B and 13B\nmodels are significantly lower than the 65B model. We do note\nthat for LLaMA 65B, we see a much smaller improvement in\nusing the A100 over the V100; however, since the 65B model\nrequires sharding across two (A100) or four (V100) compute\nnodes at the mininum, this could result in additional latency\nto each forward pass of the model, explaining the smaller\nimprovements. We also observe that while LLaMA 7B exhibits\na considerable improvement in inference throughput on both\nAlpaca and GSM8K with the A100, the improvement is much\nlarger for Alpaca than GSM8K. This can also be attributed to\nthe different complexities of inputs from each dataset.\n2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above. For both the Alpaca and\nGSM8K datasets, we see that there is a considerable increase\nin the energy per second across all LLaMA sizes when using\nthe A100 over the V100 where the most considerable increase\nis for the smallest 7B model. Although Figure 2 shows a\nconsiderable increase in inference throughput from using the\nA100, Figure 3 shows us that this improvement does not come\nfor free: it comes at an increased energy cost per second.\nMoreover, for the largest LLaMA 65B, it is less clear whether\nthe increased inference energy per second (Figure 3) is worth\nthe small improvement in inference throughput in terms of\nwords/token/responses per second (Figure 2).\n---\n[ref_id=samsi2024] Page 4\n---\n[ref_id=samsi2024] responses and the total time needed to run inference over the\ninput data.\n---\n[ref_id=samsi2024] We monitor GPUs using the nvidia-smi [23] and\nNVIDIA DCGM [24] utilities to study GPU utilization, energy,\npower draw, etc.\n---\n[ref_id=samsi2024] during our experiments.\n---\n[ref_id=samsi2024] The nvidia-smi\nutility is used to capture GPU usage over time at 100ms\nintervals and the DCGM monitoring tool is used to capture\naggregate GPU energy in Joules for the rank-0 node.\n---\n[ref_id=samsi2024] For a\nmulti-node, multi-GPU model, we multiply the rank-0 energy\nby the number of nodes used.\n---\n[ref_id=samsi2024] Maximum power draw on GPUs\nis capped at 250 Watts unless otherwise stated.\n---\n[ref_id=samsi2024] Due to limits\non resource availability, we mainly use V100 GPUs for larger-\nscale distributed experiments (i.e., for 8, 16, and 32 shards)\nand A100 GPUs for smaller scale experiments.\n---\n[ref_id=samsi2024] Inference energy metrics are calculated by combining the\ninference metrics above with the energy data collected from\nour GPUs using NVIDIA\u2019s utilities described above.\n---\n[ref_id=samsi2024] Specif-\nically, energy per second is defined as the total aggregate\nGPU energy spent from a single experiment/job (across all\nshards) divided by the total run time of that experiment/job in\nseconds.\n---\n[ref_id=samsi2024] A single experiment/job denotes a single run through\nall 4,096 prompts under a specified batch size.\n---\n[ref_id=samsi2024] Energy per\ntoken and energy per response are similarly defined as total\nenergy divided by the number of decoded output tokens and\nthe number of responses as defined above, respectively.\n---\n[ref_id=samsi2024] IV.\n---\n[ref_id=samsi2024] R ESULTS\nA.\n---\n[ref_id=samsi2024] Baselines: LLaMA 7B, 13B, & 65B\n1) Inference Performance: We begin our analysis with\na baseline comparison of LLaMA 65B with smaller-scale\nLLaMA models: LLaMA 7B and 13B.\n---\n[ref_id=samsi2024] The goal is to under-\nstand the following: what do inference performance and energy\ntrade-offs look like for the different sizes of LLaMA under the\nbare-minimum set of resources required to have them running\ninference?\n---\n[ref_id=samsi2024] This question can be important for researchers\nand users who have may not have limitless computational\nresources and hardware acceleration or may be constrained\nin terms of GPU memory, etc.\n---\n[ref_id=samsi2024] Given the sizes of the models, the size of the data, and the\nhardware memory limits, we only show results from experi-\nments that were possible for a given combination of parameters\n(i.e., for some models, certain combinations of batch size and\nnumber of shards are infeasible due to memory limits of the\nunderlying GPUs).\n---\n[ref_id=samsi2024] Table II shows the bare minimum hardware\nrequirements for each LLaMA variant and the maximum batch\nsize possible for each combination, assuming no further model\ncompression, optimization, quantization, distillation etc.\n---\n[ref_id=samsi2024] With these limits in mind, we present the inference per-\nformance of LLaMA 7B, 13B, and 65B on the Alpaca and\nGSM8K datasets with the bare minimum hardware settings in\nFigure 2.\n---\n[ref_id=samsi2024] The plots in Figure 2 show a baseline comparison of\ninference performance of the three LLaMA variants on both\nthe V100 and A100 GPUs respectively.\n---\n[ref_id=samsi2024] For each model, in\nline with the spirit of the bare minimum settings, inference\nis done with a batch size of 64 and an maximum generation\nlength of 256.\n---\n[ref_id=samsi2024] The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256.\n---\n[ref_id=samsi2024] These limits are imposed by a combination of GPU\nmemory, model size, response length and the number of GPUs.\n---\n[ref_id=samsi2024] While the 65B model can sharded across 6 V100 GPUs, we\nuse 8 since the model architecture makes it better suited for\nbalanced sharding across 8 GPUs.\n---\n[ref_id=samsi2024] Model Size V100 32GB A100 80GB\nCount Max.\n---\n[ref_id=samsi2024] Batch size Count Max.\n---\n[ref_id=samsi2024] Batch size\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n13B on two GPUs in each case whereas the 65B model was\nrun on 8 V100 GPUs and 4 A100 GPUs respectively due to\nthe size of the model and available memory on the GPU(s).\n---\n[ref_id=samsi2024] As expected, we observe that the A100 outperforms V100\non both the Alpaca and GSM8K datasets: particularly for the\nsmaller LLaMA 7B and 13B, we see anywhere from a 2\ntimes (7B) to a 1.25 times increase (13B) in inference latency\non the A100 when compared to the V100 across words per\nsecond, tokens per second, and responses per second.\n---\n[ref_id=samsi2024] Faster\nresponse rates and inference are likely due to the fact that\nthe number of computations, directly related to the number\nof parameters of said model, involved in the 7B and 13B\nmodels are significantly lower than the 65B model.\n---\n[ref_id=samsi2024] We do note\nthat for LLaMA 65B, we see a much smaller improvement in\nusing the A100 over the V100;\n---\n[ref_id=samsi2024] however, since the 65B model\nrequires sharding across two (A100) or four (V100) compute\nnodes at the mininum, this could result in additional latency\nto each forward pass of the model, explaining the smaller\nimprovements.\n---\n[ref_id=samsi2024] We also observe that while LLaMA 7B exhibits\na considerable improvement in inference throughput on both\nAlpaca and GSM8K with the A100, the improvement is much\nlarger for Alpaca than GSM8K.\n---\n[ref_id=samsi2024] This can also be attributed to\nthe different complexities of inputs from each dataset.\n---\n[ref_id=samsi2024] 2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above.\n---\n[ref_id=samsi2024] For both the Alpaca and\nGSM8K datasets, we see that there is a considerable increase\nin the energy per second across all LLaMA sizes when using\nthe A100 over the V100 where the most considerable increase\nis for the smallest 7B model.\n---\n[ref_id=samsi2024] Although Figure 2 shows a\nconsiderable increase in inference throughput from using the\nA100, Figure 3 shows us that this improvement does not come\nfor free: it comes at an increased energy cost per second.\n---\n[ref_id=samsi2024] Moreover, for the largest LLaMA 65B, it is less clear whether\nthe increased inference energy per second (Figure 3) is worth\nthe small improvement in inference throughput in terms of\nwords/token/responses per second (Figure 2).\n---\n[ref_id=samsi2024] 2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above.\n---\n[ref_id=samsi2024] responses and the total time needed to run inference over the\ninput data.\nWe monitor GPUs using the nvidia-smi [23] and\nNVIDIA DCGM [24] utilities to study GPU utilization, energy,\npower draw, etc. during our experiments. The nvidia-smi\nutility is used to capture GPU usage over time at 100ms\nintervals and the DCGM monitoring tool is used to capture\naggregate GPU energy in Joules for the rank-0 node. For a\nmulti-node, multi-GPU model, we multiply the rank-0 energy\nby the number of nodes used. Maximum power draw on GPUs\nis capped at 250 Watts unless otherwise stated. Due to limits\non resource availability, we mainly use V100 GPUs for larger-\nscale distributed experiments (i.e., for 8, 16, and 32 shards)\nand A100 GPUs for smaller scale experiments.\nInference energy metrics are calculated by combining the\ninference metrics above with the energy data collected from\nour GPUs using NVIDIA\u2019s utilities described above. Specif-\nically, energy per second is defined as the total aggregate\nGPU energy spent from a single experiment/job (across all\nshards) divided by the total run time of that experiment/job in\nseconds. A single experiment/job denotes a single run through\nall 4,096 prompts under a specified batch size. Energy per\ntoken and energy per response are similarly defined as total\nenergy divided by the number of decoded output tokens and\nthe number of responses as defined above, respectively.\nIV. R ESULTS\nA. Baselines: LLaMA 7B, 13B, & 65B\n1) Inference Performance: We begin our analysis with\na baseline comparison of LLaMA 65B with smaller-scale\nLLaMA models: LLaMA 7B and 13B. The goal is to under-\nstand the following: what do inference performance and energy\ntrade-offs look like for the different sizes of LLaMA under the\nbare-minimum set of resources required to have them running\ninference? This question can be important for researchers\nand users who have may not have limitless computational\nresources and hardware acceleration or may be constrained\nin terms of GPU memory, etc.\nGiven the sizes of the models, the size of the data, and the\nhardware memory limits, we only show results from experi-\nments that were possible for a given combination of parameters\n(i.e., for some models, certain combinations of batch size and\nnumber of shards are infeasible due to memory limits of the\nunderlying GPUs). Table II shows the bare minimum hardware\nrequirements for each LLaMA variant and the maximum batch\nsize possible for each combination, assuming no further model\ncompression, optimization, quantization, distillation etc.\nWith these limits in mind, we present the inference per-\nformance of LLaMA 7B, 13B, and 65B on the Alpaca and\nGSM8K datasets with the bare minimum hardware settings in\nFigure 2. The plots in Figure 2 show a baseline comparison of\ninference performance of the three LLaMA variants on both\nthe V100 and A100 GPUs respectively. For each model, in\nline with the spirit of the bare minimum settings, inference\nis done with a batch size of 64 and an maximum generation\nlength of 256. The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256. These limits are imposed by a combination of GPU\nmemory, model size, response length and the number of GPUs.\nWhile the 65B model can sharded across 6 V100 GPUs, we\nuse 8 since the model architecture makes it better suited for\nbalanced sharding across 8 GPUs.\nModel Size V100 32GB A100 80GB\nCount Max. Batch size Count Max. Batch size\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n13B on two GPUs in each case whereas the 65B model was\nrun on 8 V100 GPUs and 4 A100 GPUs respectively due to\nthe size of the model and available memory on the GPU(s).\nAs expected, we observe that the A100 outperforms V100\non both the Alpaca and GSM8K datasets: particularly for the\nsmaller LLaMA 7B and 13B, we see anywhere from a 2\ntimes (7B) to a 1.25 times increase (13B) in inference latency\non the A100 when compared to the V100 across words per\nsecond, tokens per second, and responses per second. Faster\nresponse rates and inference are likely due to the fact that\nthe number of computations, directly related to the number\nof parameters of said model, involved in the 7B and 13B\nmodels are significantly lower than the 65B model. We do note\nthat for LLaMA 65B, we see a much smaller improvement in\nusing the A100 over the V100; however, since the 65B model\nrequires sharding across two (A100) or four (V100) compute\nnodes at the mininum, this could result in additional latency\nto each forward pass of the model, explaining the smaller\nimprovements. We also observe that while LLaMA 7B exhibits\na considerable improvement in inference throughput on both\nAlpaca and GSM8K with the A100, the improvement is much\nlarger for Alpaca than GSM8K. This can also be attributed to\nthe different complexities of inputs from each dataset.\n2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above. For both the Alpaca and\nGSM8K datasets, we see that there is a considerable increase\nin the energy per second across all LLaMA sizes when using\nthe A100 over the V100 where the most considerable increase\nis for the smallest 7B model. Although Figure 2 shows a\nconsiderable increase in inference throughput from using the\nA100, Figure 3 shows us that this improvement does not come\nfor free: it comes at an increased energy cost per second.\nMoreover, for the largest LLaMA 65B, it is less clear whether\nthe increased inference energy per second (Figure 3) is worth\nthe small improvement in inference throughput in terms of\nwords/token/responses per second (Figure 2).\n---\n[ref_id=chen2024] The\nvalues for LLaMA3-70B are also presented.\n---\n[ref_id=chen2024] converter. This converter splits the model computation graph\ninto slices, interleaved with attention operators. It also re-\norders the operators and coordinates the computation and\ncommunication pipelines, enabling effective overlapping of\ncommunication and computation tasks.\nMoreover, with model-attention disaggregation, running\nthe inference process with only a single batch results in under-\nutilization of resources, as the memory device remains idle\nwhen the computation device is active, and vice versa. To\naddress this inefficiency and resource wastage, we introduce\nstaggered pipelining, an advanced technique that increases the\nhardware utilization. With staggered pipelining, we run multi-\nple batches concurrently and optimize the workflow to ensure\nthat both the computation and memory devices are working\nsimultaneously, minimizing resource waste and maximizing\nsystem performance.\nTo validate our analysis, we develop and evaluate Lam-\nina, a distributed heterogeneous LLM inference system with\nmodel-attention disaggregation. We also conduct extensive\nevaluations to mirror the real-world LLM services with a het-\nerogeneous cluster made up of H100 and H20 GPUs, tested\nwith various models and request traces collected from the\nproduction environments of LLM service providers. Experi-\nmental results that our system can achieve up to16.1 \u223c 90.1%\nhigher throughput with similar hardware cost than existing so-\nlutions. Although Lamina experiences a slightly larger latency\nthan homogeneous solutions for the larger (2.39\u00d7 on average)\nbatch sizes and additional networking and scheduling costs,\nthe latency is still within the SLO of online interactive LLM\nservices.\n2 Background: The Underutilization of GPUs\nin LLM Decoding\nTo comprehensively understand the challenges and limita-\ntions present in current LLM decoding implementation with\nhomogeneous hardware, this section will provide a detailed\nperformance analysis of LLM decoding with LLaMA3-70B\nmodel as a representative LLM. The specific notations used\nin this analysis are explained in Table 2.\nTable 2: Notations used in the performance analysis. The\nvalues for LLaMA3-70B are also presented.\nParameter Description Typical Value\nN Number of parameters in LLM. 70 billion\nd Hidden dimension. 8192\nL Layers of the LLM. 80\nG GQA group size. 8\ne Bytes per element. 2\nB Batch size. 1 \u223c 1024\nl Sequence length. 128 \u223c 32768\n2.1 Preliminaries\nModern large language models (LLMs) primarily rely on the\ntransformer architecture [49]. In a transformer-based LLM,\neach input token is first mapped to a word embedding of di-\nmension d. These embeddings then pass through a series of\ntransformer blocks. The final output embeddings are multi-\nplied by a sampling matrix to generate the predicted likeli-\nhoods for the next token.\nWithin each transformer block, the input embeddings are\nprojected into three distinct vectors: query (qi), key (ki), and\nvalue (vi), all of which have the same dimension d as hidden\nstates. These vectors are processed through an attention oper-\nator to compute attention scores. The attention scores are then\nweighted by a matrix Wout to produce the output embeddings\nyi of the attention layer.\nqi = Wqxi, ki = Wkxi, vi = Wvxi,\nai =\nn\n\u2211\nj=1\nsoftmax\n\u0012q\u22a4\ni k j\u221a\nd\n\u0013\nv j, \u22c6\nyi = Woutai.\nThe output yi is then passed through a feedforward network\nthat scales it into an intermediate vector space, followed by\nanother matrix multiplication to scale it back:\nx\u2032\ni = Wproj \u00b7 fact (Wfc \u00b7 yi) .\nAlthough the transformer block involves various transfor-\nmations, there are actually only two kind of computationally\nexpensive operations, which are the attention operator (de-\nnoted by \u22c6 in the equations) and the other matrix projection\nsteps. Thus, in the following of this section, we will conduct\na quantitative analysis based on the roofline model [50] and\nexperimental measurements to evaluate these two kinds of\noperators. This analysis will highlight the differing charac-\nteristics of attention and non-attention operators during the\ndecoding phase, which explains why current LLM decoding\nimplementations with homogeneous hardware often lead to\nunderutilization of GPUs, thus motivating the need for het-\nerogeneous architectures.\n2.2 Hardware Underutilization\n2.2.1 The Underutilization in Non-Attention Operators\nTo improve GPU utilization in LLM decoding, continuous\nbatching is widely adopted [16, 20, 46]. By processing multi-\nple inputs concurrently, the model parameters in GPU mem-\nory can be reused, making the workload more computation-\nintensive. For a batch ofB requests, the non-attention operator\nrequires approximately 2NB floating-point operations. Addi-\ntionally, these operators involve loading model parameterseN\nand reading/writing a total of2eBd input and output data from\n3\n---\n[ref_id=samsi2024] The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256.\n---\n[ref_id=samsi2024] responses and the total time needed to run inference over the\ninput data.\nWe monitor GPUs using the nvidia-smi [23] and\nNVIDIA DCGM [24] utilities to study GPU utilization, energy,\npower draw, etc. during our experiments. The nvidia-smi\nutility is used to capture GPU usage over time at 100ms\nintervals and the DCGM monitoring tool is used to capture\naggregate GPU energy in Joules for the rank-0 node. For a\nmulti-node, multi-GPU model, we multiply the rank-0 energy\nby the number of nodes used. Maximum power draw on GPUs\nis capped at 250 Watts unless otherwise stated. Due to limits\non resource availability, we mainly use V100 GPUs for larger-\nscale distributed experiments (i.e., for 8, 16, and 32 shards)\nand A100 GPUs for smaller scale experiments.\nInference energy metrics are calculated by combining the\ninference metrics above with the energy data collected from\nour GPUs using NVIDIA\u2019s utilities described above. Specif-\nically, energy per second is defined as the total aggregate\nGPU energy spent from a single experiment/job (across all\nshards) divided by the total run time of that experiment/job in\nseconds. A single experiment/job denotes a single run through\nall 4,096 prompts under a specified batch size. Energy per\ntoken and energy per response are similarly defined as total\nenergy divided by the number of decoded output tokens and\nthe number of responses as defined above, respectively.\nIV. R ESULTS\nA. Baselines: LLaMA 7B, 13B, & 65B\n1) Inference Performance: We begin our analysis with\na baseline comparison of LLaMA 65B with smaller-scale\nLLaMA models: LLaMA 7B and 13B. The goal is to under-\nstand the following: what do inference performance and energy\ntrade-offs look like for the different sizes of LLaMA under the\nbare-minimum set of resources required to have them running\ninference? This question can be important for researchers\nand users who have may not have limitless computational\nresources and hardware acceleration or may be constrained\nin terms of GPU memory, etc.\nGiven the sizes of the models, the size of the data, and the\nhardware memory limits, we only show results from experi-\nments that were possible for a given combination of parameters\n(i.e., for some models, certain combinations of batch size and\nnumber of shards are infeasible due to memory limits of the\nunderlying GPUs). Table II shows the bare minimum hardware\nrequirements for each LLaMA variant and the maximum batch\nsize possible for each combination, assuming no further model\ncompression, optimization, quantization, distillation etc.\nWith these limits in mind, we present the inference per-\nformance of LLaMA 7B, 13B, and 65B on the Alpaca and\nGSM8K datasets with the bare minimum hardware settings in\nFigure 2. The plots in Figure 2 show a baseline comparison of\ninference performance of the three LLaMA variants on both\nthe V100 and A100 GPUs respectively. For each model, in\nline with the spirit of the bare minimum settings, inference\nis done with a batch size of 64 and an maximum generation\nlength of 256. The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256. These limits are imposed by a combination of GPU\nmemory, model size, response length and the number of GPUs.\nWhile the 65B model can sharded across 6 V100 GPUs, we\nuse 8 since the model architecture makes it better suited for\nbalanced sharding across 8 GPUs.\nModel Size V100 32GB A100 80GB\nCount Max. Batch size Count Max. Batch size\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n13B on two GPUs in each case whereas the 65B model was\nrun on 8 V100 GPUs and 4 A100 GPUs respectively due to\nthe size of the model and available memory on the GPU(s).\nAs expected, we observe that the A100 outperforms V100\non both the Alpaca and GSM8K datasets: particularly for the\nsmaller LLaMA 7B and 13B, we see anywhere from a 2\ntimes (7B) to a 1.25 times increase (13B) in inference latency\non the A100 when compared to the V100 across words per\nsecond, tokens per second, and responses per second. Faster\nresponse rates and inference are likely due to the fact that\nthe number of computations, directly related to the number\nof parameters of said model, involved in the 7B and 13B\nmodels are significantly lower than the 65B model. We do note\nthat for LLaMA 65B, we see a much smaller improvement in\nusing the A100 over the V100; however, since the 65B model\nrequires sharding across two (A100) or four (V100) compute\nnodes at the mininum, this could result in additional latency\nto each forward pass of the model, explaining the smaller\nimprovements. We also observe that while LLaMA 7B exhibits\na considerable improvement in inference throughput on both\nAlpaca and GSM8K with the A100, the improvement is much\nlarger for Alpaca than GSM8K. This can also be attributed to\nthe different complexities of inputs from each dataset.\n2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above. For both the Alpaca and\nGSM8K datasets, we see that there is a considerable increase\nin the energy per second across all LLaMA sizes when using\nthe A100 over the V100 where the most considerable increase\nis for the smallest 7B model. Although Figure 2 shows a\nconsiderable increase in inference throughput from using the\nA100, Figure 3 shows us that this improvement does not come\nfor free: it comes at an increased energy cost per second.\nMoreover, for the largest LLaMA 65B, it is less clear whether\nthe increased inference energy per second (Figure 3) is worth\nthe small improvement in inference throughput in terms of\nwords/token/responses per second (Figure 2).\n---\n[ref_id=chen2024] 2 Background: The Underutilization of GPUs\nin LLM Decoding\nTo comprehensively understand the challenges and limita-\ntions present in current LLM decoding implementation with\nhomogeneous hardware, this section will provide a detailed\nperformance analysis of LLM decoding with LLaMA3-70B\nmodel as a representative LLM.\n---\n[ref_id=chen2024] converter. This converter splits the model computation graph\ninto slices, interleaved with attention operators. It also re-\norders the operators and coordinates the computation and\ncommunication pipelines, enabling effective overlapping of\ncommunication and computation tasks.\nMoreover, with model-attention disaggregation, running\nthe inference process with only a single batch results in under-\nutilization of resources, as the memory device remains idle\nwhen the computation device is active, and vice versa. To\naddress this inefficiency and resource wastage, we introduce\nstaggered pipelining, an advanced technique that increases the\nhardware utilization. With staggered pipelining, we run multi-\nple batches concurrently and optimize the workflow to ensure\nthat both the computation and memory devices are working\nsimultaneously, minimizing resource waste and maximizing\nsystem performance.\nTo validate our analysis, we develop and evaluate Lam-\nina, a distributed heterogeneous LLM inference system with\nmodel-attention disaggregation. We also conduct extensive\nevaluations to mirror the real-world LLM services with a het-\nerogeneous cluster made up of H100 and H20 GPUs, tested\nwith various models and request traces collected from the\nproduction environments of LLM service providers. Experi-\nmental results that our system can achieve up to16.1 \u223c 90.1%\nhigher throughput with similar hardware cost than existing so-\nlutions. Although Lamina experiences a slightly larger latency\nthan homogeneous solutions for the larger (2.39\u00d7 on average)\nbatch sizes and additional networking and scheduling costs,\nthe latency is still within the SLO of online interactive LLM\nservices.\n2 Background: The Underutilization of GPUs\nin LLM Decoding\nTo comprehensively understand the challenges and limita-\ntions present in current LLM decoding implementation with\nhomogeneous hardware, this section will provide a detailed\nperformance analysis of LLM decoding with LLaMA3-70B\nmodel as a representative LLM. The specific notations used\nin this analysis are explained in Table 2.\nTable 2: Notations used in the performance analysis. The\nvalues for LLaMA3-70B are also presented.\nParameter Description Typical Value\nN Number of parameters in LLM. 70 billion\nd Hidden dimension. 8192\nL Layers of the LLM. 80\nG GQA group size. 8\ne Bytes per element. 2\nB Batch size. 1 \u223c 1024\nl Sequence length. 128 \u223c 32768\n2.1 Preliminaries\nModern large language models (LLMs) primarily rely on the\ntransformer architecture [49]. In a transformer-based LLM,\neach input token is first mapped to a word embedding of di-\nmension d. These embeddings then pass through a series of\ntransformer blocks. The final output embeddings are multi-\nplied by a sampling matrix to generate the predicted likeli-\nhoods for the next token.\nWithin each transformer block, the input embeddings are\nprojected into three distinct vectors: query (qi), key (ki), and\nvalue (vi), all of which have the same dimension d as hidden\nstates. These vectors are processed through an attention oper-\nator to compute attention scores. The attention scores are then\nweighted by a matrix Wout to produce the output embeddings\nyi of the attention layer.\nqi = Wqxi, ki = Wkxi, vi = Wvxi,\nai =\nn\n\u2211\nj=1\nsoftmax\n\u0012q\u22a4\ni k j\u221a\nd\n\u0013\nv j, \u22c6\nyi = Woutai.\nThe output yi is then passed through a feedforward network\nthat scales it into an intermediate vector space, followed by\nanother matrix multiplication to scale it back:\nx\u2032\ni = Wproj \u00b7 fact (Wfc \u00b7 yi) .\nAlthough the transformer block involves various transfor-\nmations, there are actually only two kind of computationally\nexpensive operations, which are the attention operator (de-\nnoted by \u22c6 in the equations) and the other matrix projection\nsteps. Thus, in the following of this section, we will conduct\na quantitative analysis based on the roofline model [50] and\nexperimental measurements to evaluate these two kinds of\noperators. This analysis will highlight the differing charac-\nteristics of attention and non-attention operators during the\ndecoding phase, which explains why current LLM decoding\nimplementations with homogeneous hardware often lead to\nunderutilization of GPUs, thus motivating the need for het-\nerogeneous architectures.\n2.2 Hardware Underutilization\n2.2.1 The Underutilization in Non-Attention Operators\nTo improve GPU utilization in LLM decoding, continuous\nbatching is widely adopted [16, 20, 46]. By processing multi-\nple inputs concurrently, the model parameters in GPU mem-\nory can be reused, making the workload more computation-\nintensive. For a batch ofB requests, the non-attention operator\nrequires approximately 2NB floating-point operations. Addi-\ntionally, these operators involve loading model parameterseN\nand reading/writing a total of2eBd input and output data from\n3\n\n---\n\nNow answer the following question based ONLY on the context above.\n\nQuestion: What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?\n\nAdditional info (JSON): {\"answer_unit\": \"V100_32GB_GPU\", \"question_id\": \"q320\"}\n\nIMPORTANT: The \"answer_unit\" field specifies the expected format/unit for answer_value.\n- If answer_unit is a unit (e.g., \"kW\", \"USD\"), express answer_value as a number in that unit (no unit name).\n- If answer_unit is \"is_blank\", answer_value should be the exact identifier/name from context that answers the question.\n- If the answer is a numeric range, format as [lower,upper].\n\nReturn STRICT JSON with these keys in order:\n- explanation          (1-3 sentences explaining how context supports the answer and how you applied answer_unit; or \"is_blank\")\n- answer               (short natural-language response)\n- answer_value         (the value matching the expected format; or \"is_blank\")\n- ref_id               (list of document ids from context used as evidence; or \"is_blank\")\n- ref_url              (list of URLs for the cited documents; or \"is_blank\")\n- supporting_materials (verbatim quote, table reference, or figure reference from the cited document; or \"is_blank\")\n\nJSON Answer:",
    "retrieved_snippets": [
      {
        "node_id": "rubei2025:sec5:p8:s161",
        "document_title": "Prompt engineering and its implications on the energy consumption of Large Language Models",
        "text": "Since the GPU provided\nonly 8GB of RAM, we used the quantized version of the\nLlama model i.e., we used 16-bit float rather than the default\n32-bit.",
        "score": 0.7585,
        "rank": 1,
        "metadata": {
          "document_id": "rubei2025",
          "document_title": "Prompt engineering and its implications on the energy consumption of Large Language Models",
          "paragraph_id": "rubei2025:sec5:p8",
          "section_id": "rubei2025:sec5",
          "sentence_index": 161
        }
      },
      {
        "node_id": "rubei2025:sec5:p8",
        "document_title": "Prompt engineering and its implications on the energy consumption of Large Language Models",
        "text": "TABLE I: Summary of the Experimental Settings\nModel Llama3 8B - Instruct\nSnippets 1,000\nPETs 3\nCustom Prompts 5\nRepetitions 5\nPause 10 seconds\nMetrics (Performance) Energy Consumption, Execution Time\nMetrics (Accuracy) Exact Match, Edit Distance\nincomplete Java snippets retrieved from the code-completion\ndataset of CodeXGLUE. As discussed in Section IV the overall\nexecution requires more than 250 hours. We calculated an\naverage test time per snippet of about 900 seconds. Therefore,\nwe limit ourselves to 1,000 snippets. with the abovementioned\nPETs\nAs discussed in Section III-B, we defined five distinct\nconfigurations for each query. Consequently, we tested every\ncombination of prompting techniques and the use of custom\ntags. To ensure experimental reliability, each test is repeated\nfive times [33], [34], with a ten-second pause between each test\nto mitigate potential tail effects [34], [35]. We use two metrics\nto evaluate energy consumption and execution time, and two\nprimary metrics (exact match and edit distance) to assess the\nimpact of different configurations on accuracy. These metrics\nalign with those used in the original evaluation of the code\ncompletion benchmark suite by the authors of CodeXGLUE.\nAll the experiments have been conducted on an isolated\ndesktop equipped with an AMD Ryzen 7 5800X 3.8GHz CPU\nand an Nvidia Geforce RTX 4060 TI (8 GB VRAM). 5 The\noperating system is Xubuntu 23.04. Since the GPU provided\nonly 8GB of RAM, we used the quantized version of the\nLlama model i.e., we used 16-bit float rather than the default\n32-bit.\nIV. E XPERIMENTAL RESULTS\nAnswering RQ1: Figure 3a shows the energy consumption\nof the three prompt techniques applied to the five different\nconfigurations. In particular, with the default configuration C0,\nzero-shot is the most energy-efficient, with an average cost of\nabout 0.000016 kWh. one-shot and a few-shots consumed an\naverage of 0.000035 kWh and 0.000054 kWh, respectively.\nCustom tags can contribute to reducing the energy consump-\ntion of the video card. As shown in Fig. 3a, the best config-\nuration is the C2 (explanation in prompts). While the zero-\nshot technique passed from 0.0000157 (of C0) to 0.0000146\n(-7%), one-shot and few-shots reduced the consumption from\n0.0000347 to 0.0000174 (-99%) and from 0.0000537 to\n0.0000293 (-83%) comparing with the default configuration\nC0, respectively. It is also interesting to see the results of C4,\nin which we do not specify any role in the system token. The\nconsumption increased from 0.0000157 to 0.000189 kWh for\nzero-shot and from 0.0000347 to 0.000181 kWh for one-shot.\n5https://www.nvidia.com/en-us/geforce/graphics-cards/40-series/rtx-4060-\n4060ti/\nThe reason is that the model started to generate completely\nnew code snippets when asked to finalize the code given as\ninput. The few-shots technique seems to be less affected by\nthis problem. The sequence of example questions and answers\ninstructed the model on the behaviour despite the lack of the\nsystem role specification.\nConcerning the execution time, Figure 3b reports the results\nobtained for all the prompt configurations. Similar to energy\nconsumption, the usage of custom tags provides a general\nimprovement in performance. In particular, the one-shot and\nfew-shots reduced the average time from 1.54 seconds of\nconfiguration C0 to 0.74 (-52%) and from 2.1 to 1.09 (-48%),\nrespectively, using configuration C2. The zero-shot technique\nperformed better using C1, reporting an improvement from\n0.74 seconds to 0.63 (-14.8%). Similarly, for the energy\nconsumption, in the case of C4, we can notice a remarkable\nincrease in execution time for zero-shot and one-shot.\nAnswer to RQ 1: Our study reveals that custom tags\ncan reduce the energy consumption of LLMs across the\nthree prompt engineering techniques tested for source\ncode completion tasks.\nAnswering RQ2: Figure 4 depicts the obtained results in\nterms of accuracy metrics. In particular, Figure 4a shows the\neffects of custom tags on exact match performance across\ndifferent prompt engineering techniques. Overall, we observe\nan increase in exact matches for configuration C1-C3 in com-\nparison with the default configuration C0. Notably, zero-shot\nshows the greatest improvement with C1, where exact matches\nrise from 63 to 82, reflecting a 23% increase. Both one-\nshot and few-shots see substantial gains with C3, achieving\napproximately a 44% improvement. Interestingly, with C4,\nzero-shot fails to achieve any exact matches.\nFigure 4b shows the impact of custom tags on edit distance\nmetrics, where an edit distance of 0 indicates a perfect result.\nOverall, custom tags contributed to a reduction in edit distance,\nwith C2 emerging as the most effective configuration across all\nprompt engineering techniques. Specifically, zero-shot showed\na 24% improvement, one-shot achieved a 64% reduction, and\nfew-shots improved by 70%. Results for zero-shot and one-\nshot are omitted for C4 because, with this configuration,\nthe LLM produced uncontrolled responses. As a result, it\nwas impossible to calculate edit distance accurately, as the\noutputs included both code and explanatory text. Despite\nlacking explicit role definitions, few-shots continued to yield\nsatisfactory results.\nAnswer to RQ 2: Prompt customizations enhanced\nthe accuracy of the tested PETs, showing a positive\ntrend with increased exact matches and reduced edit\ndistances.",
        "score": 0.7585,
        "rank": 1,
        "metadata": {
          "document_id": "rubei2025",
          "document_title": "Prompt engineering and its implications on the energy consumption of Large Language Models",
          "page": 5,
          "paragraph_index": 8,
          "section_id": "rubei2025:sec5",
          "section_index": 5
        }
      },
      {
        "node_id": "samsi2024:sec1:p1:s32",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The LLaMA model is available in a\nnumber of sizes but, in most cases, its larger variants typically\nrequire multiple high-end GPUs for both training and in-\nference (assuming no further compression/distillation).",
        "score": 0.7495,
        "rank": 2,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec1:p1",
          "section_id": "samsi2024:sec1",
          "sentence_index": 32
        }
      },
      {
        "node_id": "samsi2024:sec1:p1",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "From Words to Watts: Benchmarking the Energy\nCosts of Large Language Model Inference\nSiddharth Samsi \u2217\u00a7, Dan Zhao \u2020, Joseph McDonald \u2217, Baolin Li \u2021, Adam Michaleas \u2217,\nMichael Jones \u2217, William Bergeron \u2217, Jeremy Kepner \u2217, Devesh Tiwari \u2021, Vijay Gadepally \u2217\n\u2217 MIT, \u2020 NYU, \u2021 Northeastern University\nAbstract\u2014Large language models (LLMs) have exploded in\npopularity due to their new generative capabilities that go far\nbeyond prior state-of-the-art. These technologies are increasingly\nbeing leveraged in various domains such as law, finance, and\nmedicine. However, these models carry significant computational\nchallenges, especially the compute and energy costs required for\ninference. Inference energy costs already receive less attention\nthan the energy costs of training LLMs\u2014despite how often these\nlarge models are called on to conduct inference in reality (e.g.,\nChatGPT). As these state-of-the-art LLMs see increasing usage\nand deployment in various domains, a better understanding\nof their resource utilization is crucial for cost-savings, scaling\nperformance, efficient hardware usage, and optimal inference\nstrategies.\nIn this paper, we describe experiments conducted to study the\ncomputational and energy utilization of inference with LLMs. We\nbenchmark and conduct a preliminary analysis of the inference\nperformance and inference energy costs of different sizes of\nLLaMA\u2014a recent state-of-the-art LLM\u2014developed by Meta AI\non two generations of popular GPUs (NVIDIA V100 & A100)\nand two datasets (Alpaca and GSM8K) to reflect the diverse\nset of tasks/benchmarks for LLMs in research and practice.\nWe present the results of multi-node, multi-GPU inference using\nmodel sharding across up to 32 GPUs. To our knowledge, our\nwork is the one of the first to study LLM inference performance\nfrom the perspective of computational and energy resources at\nthis scale.\nIndex Terms \u2014Large Language Models, Natural Language\nProcessing, Inference, Green AI, LLM, NLP, Deep Learning,\nDistributed Computing, Energy, Sustainability\nI. I NTRODUCTION\nGenerative models (GenAI) are able to produce new content\nfrom synthesizing text, images, and audio from which it\u2019s\ntrained on. While GenAI is not entirely new, the recent\napplication and broad availability of this technology via tools\nsuch as Stable Diffusion [1], OpenAI\u2019s ChatGPT, Google\u2019s\nBard and integration into the Microsoft Bing search engine\nhas captured the imagination of the world and led to a massive\nsurge in interest in deploying these types of models across a\nDISTRIBUTION STATEMENT A. Approved for public release. Distribu-\ntion is unlimited. This material is based upon work supported by the Assistant\nSecretary of Defense for Research and Engineering under Air Force Contract\nNo. FA8702-15-D-0001, and United States Air Force Research Laboratory\nCooperative Agreement Number FA8750-19-2-1000. Any opinions, findings,\nconclusions or recommendations expressed in this material are those of the\nauthor(s) and do not necessarily reflect the views of the Assistant Secretary\nof Defense for Research and Engineering, or the United States Air Force.\nThe U.S. Government is authorized to reproduce and distribute reprints for\nGovernment purposes notwithstanding any copyright notation herein.\nvariety of domains ranging such as education, government,\nengineering, law, finance and many more.\nThe popularity of these models has also put a spotlight on\nmany societal concerns stemming from their usage. From ethi-\ncal concerns ranging from violations of copyright laws [2], [3]\nto safety concerns arising from the fact that these models are\ncapable of hallucinating or fabricating information, concerns\nabout these models in the educational and medical domain [4],\n[5], their carbon footprint, and many more.\nIn this paper, we focus primarily on understanding the\nsignificant amount of resources\u2014time, computation, and\nenergy\u2014required for using and deploying some of the large\nlanguage models (LLM) like those that underlie ChatGPT,\nBard, etc. Several prior works have estimated the compute\nand energy costs of training language models. Works like [6]\ndiscuss the carbon footprint of language models such as BERT,\nELMo, and precursors to larger models such as GPT-3 and\nGPT-4 that power some of the popular AI chatbots today. Oth-\ners have also looked to larger language models; for instance,\nthe largest NVIDIA Megatron-LM model required 3,072 A100\nGPUs [7]\u2013[9] for its training. While the complete details (time\nand resources used) of compute required for training GPT-\n3/4 are not available, several estimates for training [10], [11]\nand inference are publicly available. As industry attempts to\nshore up competitive moats and restrict information regarding\ntheir underlying LLM technologies, these details can become\nless reliable and available. Compounding this issue, estimates\nfor inference are even less readily available [12] despite their\nsignificant share of energy costs and their likely larger impact\non the environment [13]\u2014especially since model inference\ncalls can occur more frequently than training/fine-tuning for\nreal-world deployments and applications.\nWe present the results of our inference experiments on\nLLaMA [14]: an open sourced pre-trained large language\nmodels by Meta AI. The LLaMA model is available in a\nnumber of sizes but, in most cases, its larger variants typically\nrequire multiple high-end GPUs for both training and in-\nference (assuming no further compression/distillation). While\nour emphasis is on characterizing the compute performance\nand energy used for multi-node, multi-GPU inference, we\nalso include results from single node instances using smaller\nvariants of the model as a baseline comparison. We hope our\nwork will help illustrate some of the compute performance\nand energy utilization characteristics of LLM inference. We\nalso hope that our experiments, analysis, and data on real-\narXiv:2310.03003v1  [cs.CL]  4 Oct 2023",
        "score": 0.7495,
        "rank": 2,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "page": 1,
          "paragraph_index": 1,
          "section_id": "samsi2024:sec1",
          "section_index": 1
        }
      },
      {
        "node_id": "samsi2024:sec8:p25",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "of 256. For this configuration, we ran on four A100 GPUs\nand 8, 16, 32 V100 GPUs. These results are summarized in\nTables IV and V. In all cases, the streaming multiprocessors\n(SM) utilization as reported by the DCGM utility was observed\nto be in the 94%-95% range. For the A100 GPUs, the average\nSM utilization rises to 98% when the maximum generated\noutput length is increased to 2048. Given that the model is\nsharded in a manner that enables us to load it fully in GPU\nmemory and run inference on a non-trivial amount of data,\nwe expect memory utilization to be low depending on the\nspecific model parameters and input sizes used. Thus, on\nthe four 80GB A100 nodes, the memory utilization varies\nbetween 23%-27% depending the maximum generated output\nlength. This under-utilization of memory implies that it may\nbe possible to co-locate multiple models on the same set\nof GPUs to increase aggregate throughput and potentially\nreduce cloud compute costs or improve system utilization at\na supercomputer center. With new GPU sharing capabilities\nsuch as Multi-Process Service (MPS) [27] and Multi-Instance\nGPU (MIG) [28], a single GPU may be shared by diverse\nworkloads for an overall improvement in system throughput\nas shown in recent work [29]. The optimal GPU configuration\nfor sharing LLMs and other workloads is a part of our future\nwork in this area.\nModel Shards Output Length Max. Memory Util. Avg. SM Util.\n4 256 23.36 95.00\n4 512 24.54 98.81\n4 1024 24.85 98.85\n4 2048 27.00 98.00\nTABLE IV: A100 Utilization: This table shows GPU utiliza-\ntion for 80GB A100 GPUs and LLaMA 65B with 4 shards,\nbatch size of 64 averaged across both datasets used in this\npaper.\nModel Shards Output Length Max. Memory Util. Avg. SM Util.\n8 256 24.25 94.75\n16 256 13.33 95.00\n32 256 6.66 95.66\nTABLE V: V100 Utilization: This table shows GPU utiliza-\ntion for 32GB V100 GPUs and LLaMA 65B with 8, 16, 32\nshards, a batch size of 64 and maximum generated output\nlength of 256 averaged across both datasets used in this paper.\nWe limit this result to an ouptut length of 256 because longer\noutputs on 8 V100 GPUs are not possible given memory limits\nof the GPU.\nV. D ISCUSSION\nIn this paper, we show the results of benchmarking a\nrepresentative large language model on NVIDIA GPUs. We\nshow baseline results from smaller models (7B, 13B) and\ncompare them against the largest available version (65B) of\nLLaMA. We also examine the inference performance and en-\nergy across distributed settings and different configurations by\nvarying model parameters, input data, and hardware configu-\nrations. By comparing a natural language instruction following\ndataset (Alpaca) and a mathematical question-answer dataset\n(GSM8K), we also find that the complexity of the input dataset\ncan affect the model performance for a given set of hyper-\nparameters and hardware configuration.\nGiven the size of LLMs and the limits imposed by current\nhardware, inference with large models can impose onerous\nrequirements. For example, we find that, at a minimum, 8\nV100 GPUs each with 32 GB of RAM or 4 A100 GPUs\neach with 80GB of memory are required for any meaningful\ninferences with the 65B LLaMA model. In each case among\nour experiments, we shard the model evenly across all GPUs in\norder to fit the model/data; however, this results in only 20%-\n25% of the GPU memory being utilized at any given time. This\nover-provisioning of resources represents new opportunities\nfor resource sharing across multiple workloads in the latest\nNVIDIA GPUs. The Multi-Process Service (MPS) [27] and\nMulti-Instance GPU (MIG) [28] are new capabilities that\nenable GPU sharing across different workloads. Although\nidentifying the optimal MPS or MIG configuration for a\ngiven set of workloads is challenging, recent work [29] has\ndeveloped new techniques to exploit these capabilities in order\nto dynamically partition GPU resources. This opens up the\npotential to optimally partition high-end GPUs such as the\nA100s or H100s to co-locate multiple LLMs for inference\u2014\nwith the potential of only minimal degradation to computa-\ntional performance.\nFinally, as AI compute requirements have increased, there\nis an increasing focus on approaches to reduce the carbon\nand energy footprints of datacenters by making larger models\nleaner or more efficient. Approaches such as model quan-\ntization, distillation, sparsification, etc. are being developed\nto reduce the compute required for AI along with the de-\nvelopment of custom, energy-efficient hardware for inference\nand training. However, simple interventions like GPU power\ncapping is available to be deployed today\u2014our preliminary\nanalysis with LLM inference in this paper suggests that power\ncapping can be an effective tool for reducing inference energy.\nIf applied at the datacenter-scale, this intervention has the\npotential to reduce overall energy usage in the long-run as new\napproaches are developed to address the energy consumption\nof AI compute.\nAs part of our future plans, we aim to conduct similar\nexperiments on other open-source, large language models\nalong with more in-depth characterization of compute and\nenergy for not just inference, but also for the training/fine-\ntuning of these models. It is our hope that this paper provides\na baseline for inference with LLMs and fosters a broader\ndiscussion of the challenges and opportunities in this field.\nACKNOWLEDGEMENTS\nThe authors acknowledge the MIT SuperCloud [18] and\nLincoln Laboratory Supercomputing Center for providing HPC\nand consultation resources that have contributed to the research\nresults reported within this paper. The authors acknowledge the",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "page": 8,
          "paragraph_index": 25,
          "section_id": "samsi2024:sec8",
          "section_index": 8
        }
      },
      {
        "node_id": "samsi2024:sec8",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Page 8",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "page": 8,
          "section_index": 8
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s232",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "of 256.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 232
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s233",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "For this configuration, we ran on four A100 GPUs\nand 8, 16, 32 V100 GPUs.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 233
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s234",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "These results are summarized in\nTables IV and V.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 234
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s235",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "In all cases, the streaming multiprocessors\n(SM) utilization as reported by the DCGM utility was observed\nto be in the 94%-95% range.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 235
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s236",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "For the A100 GPUs, the average\nSM utilization rises to 98% when the maximum generated\noutput length is increased to 2048.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 236
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s237",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Given that the model is\nsharded in a manner that enables us to load it fully in GPU\nmemory and run inference on a non-trivial amount of data,\nwe expect memory utilization to be low depending on the\nspecific model parameters and input sizes used.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 237
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s238",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Thus, on\nthe four 80GB A100 nodes, the memory utilization varies\nbetween 23%-27% depending the maximum generated output\nlength.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 238
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s239",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "This under-utilization of memory implies that it may\nbe possible to co-locate multiple models on the same set\nof GPUs to increase aggregate throughput and potentially\nreduce cloud compute costs or improve system utilization at\na supercomputer center.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 239
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s240",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "With new GPU sharing capabilities\nsuch as Multi-Process Service (MPS) [27] and Multi-Instance\nGPU (MIG) [28], a single GPU may be shared by diverse\nworkloads for an overall improvement in system throughput\nas shown in recent work [29].",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 240
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s241",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The optimal GPU configuration\nfor sharing LLMs and other workloads is a part of our future\nwork in this area.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 241
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s242",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Model Shards Output Length Max.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 242
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s243",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Memory Util.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 243
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s244",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Avg.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 244
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s245",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "SM Util.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 245
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s246",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "4 256 23.36 95.00\n4 512 24.54 98.81\n4 1024 24.85 98.85\n4 2048 27.00 98.00\nTABLE IV: A100 Utilization: This table shows GPU utiliza-\ntion for 80GB A100 GPUs and LLaMA 65B with 4 shards,\nbatch size of 64 averaged across both datasets used in this\npaper.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 246
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s247",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Model Shards Output Length Max.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 247
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s248",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Memory Util.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 248
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s249",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Avg.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 249
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s250",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "SM Util.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 250
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s251",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "8 256 24.25 94.75\n16 256 13.33 95.00\n32 256 6.66 95.66\nTABLE V: V100 Utilization: This table shows GPU utiliza-\ntion for 32GB V100 GPUs and LLaMA 65B with 8, 16, 32\nshards, a batch size of 64 and maximum generated output\nlength of 256 averaged across both datasets used in this paper.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 251
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s252",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "We limit this result to an ouptut length of 256 because longer\noutputs on 8 V100 GPUs are not possible given memory limits\nof the GPU.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 252
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s253",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "V.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 253
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s254",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "D ISCUSSION\nIn this paper, we show the results of benchmarking a\nrepresentative large language model on NVIDIA GPUs.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 254
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s255",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "We\nshow baseline results from smaller models (7B, 13B) and\ncompare them against the largest available version (65B) of\nLLaMA.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 255
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s256",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "We also examine the inference performance and en-\nergy across distributed settings and different configurations by\nvarying model parameters, input data, and hardware configu-\nrations.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 256
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s257",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "By comparing a natural language instruction following\ndataset (Alpaca) and a mathematical question-answer dataset\n(GSM8K), we also find that the complexity of the input dataset\ncan affect the model performance for a given set of hyper-\nparameters and hardware configuration.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 257
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s258",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Given the size of LLMs and the limits imposed by current\nhardware, inference with large models can impose onerous\nrequirements.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 258
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s259",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "For example, we find that, at a minimum, 8\nV100 GPUs each with 32 GB of RAM or 4 A100 GPUs\neach with 80GB of memory are required for any meaningful\ninferences with the 65B LLaMA model.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 259
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s260",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "In each case among\nour experiments, we shard the model evenly across all GPUs in\norder to fit the model/data;",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 260
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s261",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "however, this results in only 20%-\n25% of the GPU memory being utilized at any given time.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 261
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s262",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "This\nover-provisioning of resources represents new opportunities\nfor resource sharing across multiple workloads in the latest\nNVIDIA GPUs.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 262
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s263",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The Multi-Process Service (MPS) [27] and\nMulti-Instance GPU (MIG) [28] are new capabilities that\nenable GPU sharing across different workloads.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 263
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s264",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Although\nidentifying the optimal MPS or MIG configuration for a\ngiven set of workloads is challenging, recent work [29] has\ndeveloped new techniques to exploit these capabilities in order\nto dynamically partition GPU resources.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 264
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s265",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "This opens up the\npotential to optimally partition high-end GPUs such as the\nA100s or H100s to co-locate multiple LLMs for inference\u2014\nwith the potential of only minimal degradation to computa-\ntional performance.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 265
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s266",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Finally, as AI compute requirements have increased, there\nis an increasing focus on approaches to reduce the carbon\nand energy footprints of datacenters by making larger models\nleaner or more efficient.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 266
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s267",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Approaches such as model quan-\ntization, distillation, sparsification, etc.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 267
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s268",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "are being developed\nto reduce the compute required for AI along with the de-\nvelopment of custom, energy-efficient hardware for inference\nand training.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 268
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s269",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "However, simple interventions like GPU power\ncapping is available to be deployed today\u2014our preliminary\nanalysis with LLM inference in this paper suggests that power\ncapping can be an effective tool for reducing inference energy.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 269
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s270",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "If applied at the datacenter-scale, this intervention has the\npotential to reduce overall energy usage in the long-run as new\napproaches are developed to address the energy consumption\nof AI compute.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 270
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s271",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "As part of our future plans, we aim to conduct similar\nexperiments on other open-source, large language models\nalong with more in-depth characterization of compute and\nenergy for not just inference, but also for the training/fine-\ntuning of these models.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 271
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s272",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "It is our hope that this paper provides\na baseline for inference with LLMs and fosters a broader\ndiscussion of the challenges and opportunities in this field.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 272
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s273",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "ACKNOWLEDGEMENTS\nThe authors acknowledge the MIT SuperCloud [18] and\nLincoln Laboratory Supercomputing Center for providing HPC\nand consultation resources that have contributed to the research\nresults reported within this paper.",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 273
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s274",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The authors acknowledge the",
        "score": 0.7317,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 274
        }
      },
      {
        "node_id": "samsi2024:sec8:p25:s259",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "For example, we find that, at a minimum, 8\nV100 GPUs each with 32 GB of RAM or 4 A100 GPUs\neach with 80GB of memory are required for any meaningful\ninferences with the 65B LLaMA model.",
        "score": 0.7216,
        "rank": 4,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec8:p25",
          "section_id": "samsi2024:sec8",
          "sentence_index": 259
        }
      },
      {
        "node_id": "samsi2024:sec8:p25",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "of 256. For this configuration, we ran on four A100 GPUs\nand 8, 16, 32 V100 GPUs. These results are summarized in\nTables IV and V. In all cases, the streaming multiprocessors\n(SM) utilization as reported by the DCGM utility was observed\nto be in the 94%-95% range. For the A100 GPUs, the average\nSM utilization rises to 98% when the maximum generated\noutput length is increased to 2048. Given that the model is\nsharded in a manner that enables us to load it fully in GPU\nmemory and run inference on a non-trivial amount of data,\nwe expect memory utilization to be low depending on the\nspecific model parameters and input sizes used. Thus, on\nthe four 80GB A100 nodes, the memory utilization varies\nbetween 23%-27% depending the maximum generated output\nlength. This under-utilization of memory implies that it may\nbe possible to co-locate multiple models on the same set\nof GPUs to increase aggregate throughput and potentially\nreduce cloud compute costs or improve system utilization at\na supercomputer center. With new GPU sharing capabilities\nsuch as Multi-Process Service (MPS) [27] and Multi-Instance\nGPU (MIG) [28], a single GPU may be shared by diverse\nworkloads for an overall improvement in system throughput\nas shown in recent work [29]. The optimal GPU configuration\nfor sharing LLMs and other workloads is a part of our future\nwork in this area.\nModel Shards Output Length Max. Memory Util. Avg. SM Util.\n4 256 23.36 95.00\n4 512 24.54 98.81\n4 1024 24.85 98.85\n4 2048 27.00 98.00\nTABLE IV: A100 Utilization: This table shows GPU utiliza-\ntion for 80GB A100 GPUs and LLaMA 65B with 4 shards,\nbatch size of 64 averaged across both datasets used in this\npaper.\nModel Shards Output Length Max. Memory Util. Avg. SM Util.\n8 256 24.25 94.75\n16 256 13.33 95.00\n32 256 6.66 95.66\nTABLE V: V100 Utilization: This table shows GPU utiliza-\ntion for 32GB V100 GPUs and LLaMA 65B with 8, 16, 32\nshards, a batch size of 64 and maximum generated output\nlength of 256 averaged across both datasets used in this paper.\nWe limit this result to an ouptut length of 256 because longer\noutputs on 8 V100 GPUs are not possible given memory limits\nof the GPU.\nV. D ISCUSSION\nIn this paper, we show the results of benchmarking a\nrepresentative large language model on NVIDIA GPUs. We\nshow baseline results from smaller models (7B, 13B) and\ncompare them against the largest available version (65B) of\nLLaMA. We also examine the inference performance and en-\nergy across distributed settings and different configurations by\nvarying model parameters, input data, and hardware configu-\nrations. By comparing a natural language instruction following\ndataset (Alpaca) and a mathematical question-answer dataset\n(GSM8K), we also find that the complexity of the input dataset\ncan affect the model performance for a given set of hyper-\nparameters and hardware configuration.\nGiven the size of LLMs and the limits imposed by current\nhardware, inference with large models can impose onerous\nrequirements. For example, we find that, at a minimum, 8\nV100 GPUs each with 32 GB of RAM or 4 A100 GPUs\neach with 80GB of memory are required for any meaningful\ninferences with the 65B LLaMA model. In each case among\nour experiments, we shard the model evenly across all GPUs in\norder to fit the model/data; however, this results in only 20%-\n25% of the GPU memory being utilized at any given time. This\nover-provisioning of resources represents new opportunities\nfor resource sharing across multiple workloads in the latest\nNVIDIA GPUs. The Multi-Process Service (MPS) [27] and\nMulti-Instance GPU (MIG) [28] are new capabilities that\nenable GPU sharing across different workloads. Although\nidentifying the optimal MPS or MIG configuration for a\ngiven set of workloads is challenging, recent work [29] has\ndeveloped new techniques to exploit these capabilities in order\nto dynamically partition GPU resources. This opens up the\npotential to optimally partition high-end GPUs such as the\nA100s or H100s to co-locate multiple LLMs for inference\u2014\nwith the potential of only minimal degradation to computa-\ntional performance.\nFinally, as AI compute requirements have increased, there\nis an increasing focus on approaches to reduce the carbon\nand energy footprints of datacenters by making larger models\nleaner or more efficient. Approaches such as model quan-\ntization, distillation, sparsification, etc. are being developed\nto reduce the compute required for AI along with the de-\nvelopment of custom, energy-efficient hardware for inference\nand training. However, simple interventions like GPU power\ncapping is available to be deployed today\u2014our preliminary\nanalysis with LLM inference in this paper suggests that power\ncapping can be an effective tool for reducing inference energy.\nIf applied at the datacenter-scale, this intervention has the\npotential to reduce overall energy usage in the long-run as new\napproaches are developed to address the energy consumption\nof AI compute.\nAs part of our future plans, we aim to conduct similar\nexperiments on other open-source, large language models\nalong with more in-depth characterization of compute and\nenergy for not just inference, but also for the training/fine-\ntuning of these models. It is our hope that this paper provides\na baseline for inference with LLMs and fosters a broader\ndiscussion of the challenges and opportunities in this field.\nACKNOWLEDGEMENTS\nThe authors acknowledge the MIT SuperCloud [18] and\nLincoln Laboratory Supercomputing Center for providing HPC\nand consultation resources that have contributed to the research\nresults reported within this paper. The authors acknowledge the",
        "score": 0.7216,
        "rank": 4,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "page": 8,
          "paragraph_index": 25,
          "section_id": "samsi2024:sec8",
          "section_index": 8
        }
      },
      {
        "node_id": "samsi2024:sec5:p6:s151",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "2: Baseline comparison of inference perfor-\nmance/latency between LLaMA 7B, 13B and 65B: inference\nperformance comparisons on the minimum set of hardware\nrequired to run inference (see Table II) across model sizes\nand between V100s and A100s.",
        "score": 0.7178,
        "rank": 5,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec5:p6",
          "section_id": "samsi2024:sec5",
          "sentence_index": 151
        }
      },
      {
        "node_id": "samsi2024:sec5:p6",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "(a) Results from the Alpaca dataset.\n(b) Results from GSM8K dataset\nFig. 2: Baseline comparison of inference perfor-\nmance/latency between LLaMA 7B, 13B and 65B: inference\nperformance comparisons on the minimum set of hardware\nrequired to run inference (see Table II) across model sizes\nand between V100s and A100s.\nFig. 3: Baseline energy per second (Watts) estimates of per-\nforming inference with LLaMA 7B, 13B, and 65B: inference\nenergy comparisons on the minimum set of hardware/settings\nrequired (see Table II) with Alpaca and GSM8K on a log-scale.\nColor indicates device (V100/A100), bars indicate average\nquantities and lines indicate error bars. Energy is averaged\nover maximum generation lengths of 256, 512, and 1024 due\nto near-identical energy/size trends for each generation length.\nFig. 4: Energy per second (Watts) estimates of LLaMA 65B\nacross batch sizes of 64/128/256/256 and 8/16/32 shards for\nmax generation length 512 : inference energy estimates on\nAlpaca and GSM8K on log-scale. Color indicates batch size.\nB. Energy per Second: LLaMA 65B\nWe first take a look at the amount of energy inference costs\nper unit time in seconds. Figures 4 and 5 show a more in-\ndepth look of the energy inference costs of LLaMA 65B across\ndifferent batch sizes and degrees of sharding. Specifically,\nFigure 4 shows energy costs for maximum generation length\n512 and Figure 5 shows energy costs for 1024.\nOverall, we see an average increase in energy per second\nwith the number of shards. While there is a slight correlation\nas energy per second increases with increasing batch size,\nincreasing the number of shards always increases the wattage.\nIndeed, the energy per second increases with the number of\nshards even at the same batch size (e.g., the energy of inference\nat batch size 64, going from 16 shards to 32 shards). For both\ndatasets, increasing the max generation length from 512 to\n1024 does seem to increase the energy per second for each\nbatch size within each shard configuration, but the overall\neffect is less clear or consistent. Overall, we see that the energy\nper second for inference with LLaMA 65B is on the order of\n300 Watts to 1 Kilowatt from the lower shard configuration of\n8 GPUs to the higher end of 32 GPUs.\nC. Energy per Decoded Token: LLaMA 65B\nMoving on to energy per each decoded output token, we see\nthat in Figures 6 and 7 that energy per token tends to follow\na similar pattern in relation to the number of shards: as the\nnumber of shards increases, the energy per output token also",
        "score": 0.7178,
        "rank": 5,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "page": 5,
          "paragraph_index": 6,
          "section_id": "samsi2024:sec5",
          "section_index": 5
        }
      },
      {
        "node_id": "samsi2024:sec4:p5",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "responses and the total time needed to run inference over the\ninput data.\nWe monitor GPUs using the nvidia-smi [23] and\nNVIDIA DCGM [24] utilities to study GPU utilization, energy,\npower draw, etc. during our experiments. The nvidia-smi\nutility is used to capture GPU usage over time at 100ms\nintervals and the DCGM monitoring tool is used to capture\naggregate GPU energy in Joules for the rank-0 node. For a\nmulti-node, multi-GPU model, we multiply the rank-0 energy\nby the number of nodes used. Maximum power draw on GPUs\nis capped at 250 Watts unless otherwise stated. Due to limits\non resource availability, we mainly use V100 GPUs for larger-\nscale distributed experiments (i.e., for 8, 16, and 32 shards)\nand A100 GPUs for smaller scale experiments.\nInference energy metrics are calculated by combining the\ninference metrics above with the energy data collected from\nour GPUs using NVIDIA\u2019s utilities described above. Specif-\nically, energy per second is defined as the total aggregate\nGPU energy spent from a single experiment/job (across all\nshards) divided by the total run time of that experiment/job in\nseconds. A single experiment/job denotes a single run through\nall 4,096 prompts under a specified batch size. Energy per\ntoken and energy per response are similarly defined as total\nenergy divided by the number of decoded output tokens and\nthe number of responses as defined above, respectively.\nIV. R ESULTS\nA. Baselines: LLaMA 7B, 13B, & 65B\n1) Inference Performance: We begin our analysis with\na baseline comparison of LLaMA 65B with smaller-scale\nLLaMA models: LLaMA 7B and 13B. The goal is to under-\nstand the following: what do inference performance and energy\ntrade-offs look like for the different sizes of LLaMA under the\nbare-minimum set of resources required to have them running\ninference? This question can be important for researchers\nand users who have may not have limitless computational\nresources and hardware acceleration or may be constrained\nin terms of GPU memory, etc.\nGiven the sizes of the models, the size of the data, and the\nhardware memory limits, we only show results from experi-\nments that were possible for a given combination of parameters\n(i.e., for some models, certain combinations of batch size and\nnumber of shards are infeasible due to memory limits of the\nunderlying GPUs). Table II shows the bare minimum hardware\nrequirements for each LLaMA variant and the maximum batch\nsize possible for each combination, assuming no further model\ncompression, optimization, quantization, distillation etc.\nWith these limits in mind, we present the inference per-\nformance of LLaMA 7B, 13B, and 65B on the Alpaca and\nGSM8K datasets with the bare minimum hardware settings in\nFigure 2. The plots in Figure 2 show a baseline comparison of\ninference performance of the three LLaMA variants on both\nthe V100 and A100 GPUs respectively. For each model, in\nline with the spirit of the bare minimum settings, inference\nis done with a batch size of 64 and an maximum generation\nlength of 256. The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256. These limits are imposed by a combination of GPU\nmemory, model size, response length and the number of GPUs.\nWhile the 65B model can sharded across 6 V100 GPUs, we\nuse 8 since the model architecture makes it better suited for\nbalanced sharding across 8 GPUs.\nModel Size V100 32GB A100 80GB\nCount Max. Batch size Count Max. Batch size\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n13B on two GPUs in each case whereas the 65B model was\nrun on 8 V100 GPUs and 4 A100 GPUs respectively due to\nthe size of the model and available memory on the GPU(s).\nAs expected, we observe that the A100 outperforms V100\non both the Alpaca and GSM8K datasets: particularly for the\nsmaller LLaMA 7B and 13B, we see anywhere from a 2\ntimes (7B) to a 1.25 times increase (13B) in inference latency\non the A100 when compared to the V100 across words per\nsecond, tokens per second, and responses per second. Faster\nresponse rates and inference are likely due to the fact that\nthe number of computations, directly related to the number\nof parameters of said model, involved in the 7B and 13B\nmodels are significantly lower than the 65B model. We do note\nthat for LLaMA 65B, we see a much smaller improvement in\nusing the A100 over the V100; however, since the 65B model\nrequires sharding across two (A100) or four (V100) compute\nnodes at the mininum, this could result in additional latency\nto each forward pass of the model, explaining the smaller\nimprovements. We also observe that while LLaMA 7B exhibits\na considerable improvement in inference throughput on both\nAlpaca and GSM8K with the A100, the improvement is much\nlarger for Alpaca than GSM8K. This can also be attributed to\nthe different complexities of inputs from each dataset.\n2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above. For both the Alpaca and\nGSM8K datasets, we see that there is a considerable increase\nin the energy per second across all LLaMA sizes when using\nthe A100 over the V100 where the most considerable increase\nis for the smallest 7B model. Although Figure 2 shows a\nconsiderable increase in inference throughput from using the\nA100, Figure 3 shows us that this improvement does not come\nfor free: it comes at an increased energy cost per second.\nMoreover, for the largest LLaMA 65B, it is less clear whether\nthe increased inference energy per second (Figure 3) is worth\nthe small improvement in inference throughput in terms of\nwords/token/responses per second (Figure 2).",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "page": 4,
          "paragraph_index": 5,
          "section_id": "samsi2024:sec4",
          "section_index": 4
        }
      },
      {
        "node_id": "samsi2024:sec4",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Page 4",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "page": 4,
          "section_index": 4
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s112",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "responses and the total time needed to run inference over the\ninput data.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 112
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s113",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "We monitor GPUs using the nvidia-smi [23] and\nNVIDIA DCGM [24] utilities to study GPU utilization, energy,\npower draw, etc.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 113
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s114",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "during our experiments.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 114
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s115",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The nvidia-smi\nutility is used to capture GPU usage over time at 100ms\nintervals and the DCGM monitoring tool is used to capture\naggregate GPU energy in Joules for the rank-0 node.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 115
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s116",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "For a\nmulti-node, multi-GPU model, we multiply the rank-0 energy\nby the number of nodes used.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 116
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s117",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Maximum power draw on GPUs\nis capped at 250 Watts unless otherwise stated.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 117
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s118",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Due to limits\non resource availability, we mainly use V100 GPUs for larger-\nscale distributed experiments (i.e., for 8, 16, and 32 shards)\nand A100 GPUs for smaller scale experiments.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 118
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s119",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Inference energy metrics are calculated by combining the\ninference metrics above with the energy data collected from\nour GPUs using NVIDIA\u2019s utilities described above.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 119
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s120",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Specif-\nically, energy per second is defined as the total aggregate\nGPU energy spent from a single experiment/job (across all\nshards) divided by the total run time of that experiment/job in\nseconds.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 120
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s121",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "A single experiment/job denotes a single run through\nall 4,096 prompts under a specified batch size.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 121
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s122",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Energy per\ntoken and energy per response are similarly defined as total\nenergy divided by the number of decoded output tokens and\nthe number of responses as defined above, respectively.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 122
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s123",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "IV.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 123
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s124",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "R ESULTS\nA.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 124
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s125",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Baselines: LLaMA 7B, 13B, & 65B\n1) Inference Performance: We begin our analysis with\na baseline comparison of LLaMA 65B with smaller-scale\nLLaMA models: LLaMA 7B and 13B.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 125
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s126",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The goal is to under-\nstand the following: what do inference performance and energy\ntrade-offs look like for the different sizes of LLaMA under the\nbare-minimum set of resources required to have them running\ninference?",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 126
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s127",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "This question can be important for researchers\nand users who have may not have limitless computational\nresources and hardware acceleration or may be constrained\nin terms of GPU memory, etc.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 127
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s128",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Given the sizes of the models, the size of the data, and the\nhardware memory limits, we only show results from experi-\nments that were possible for a given combination of parameters\n(i.e., for some models, certain combinations of batch size and\nnumber of shards are infeasible due to memory limits of the\nunderlying GPUs).",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 128
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s129",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Table II shows the bare minimum hardware\nrequirements for each LLaMA variant and the maximum batch\nsize possible for each combination, assuming no further model\ncompression, optimization, quantization, distillation etc.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 129
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s130",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "With these limits in mind, we present the inference per-\nformance of LLaMA 7B, 13B, and 65B on the Alpaca and\nGSM8K datasets with the bare minimum hardware settings in\nFigure 2.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 130
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s131",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The plots in Figure 2 show a baseline comparison of\ninference performance of the three LLaMA variants on both\nthe V100 and A100 GPUs respectively.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 131
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s132",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "For each model, in\nline with the spirit of the bare minimum settings, inference\nis done with a batch size of 64 and an maximum generation\nlength of 256.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 132
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s133",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 133
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s134",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "These limits are imposed by a combination of GPU\nmemory, model size, response length and the number of GPUs.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 134
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s135",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "While the 65B model can sharded across 6 V100 GPUs, we\nuse 8 since the model architecture makes it better suited for\nbalanced sharding across 8 GPUs.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 135
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s136",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Model Size V100 32GB A100 80GB\nCount Max.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 136
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s137",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Batch size Count Max.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 137
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s138",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Batch size\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n13B on two GPUs in each case whereas the 65B model was\nrun on 8 V100 GPUs and 4 A100 GPUs respectively due to\nthe size of the model and available memory on the GPU(s).",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 138
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s139",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "As expected, we observe that the A100 outperforms V100\non both the Alpaca and GSM8K datasets: particularly for the\nsmaller LLaMA 7B and 13B, we see anywhere from a 2\ntimes (7B) to a 1.25 times increase (13B) in inference latency\non the A100 when compared to the V100 across words per\nsecond, tokens per second, and responses per second.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 139
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s140",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Faster\nresponse rates and inference are likely due to the fact that\nthe number of computations, directly related to the number\nof parameters of said model, involved in the 7B and 13B\nmodels are significantly lower than the 65B model.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 140
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s141",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "We do note\nthat for LLaMA 65B, we see a much smaller improvement in\nusing the A100 over the V100;",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 141
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s142",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "however, since the 65B model\nrequires sharding across two (A100) or four (V100) compute\nnodes at the mininum, this could result in additional latency\nto each forward pass of the model, explaining the smaller\nimprovements.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 142
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s143",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "We also observe that while LLaMA 7B exhibits\na considerable improvement in inference throughput on both\nAlpaca and GSM8K with the A100, the improvement is much\nlarger for Alpaca than GSM8K.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 143
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s144",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "This can also be attributed to\nthe different complexities of inputs from each dataset.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 144
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s145",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 145
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s146",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "For both the Alpaca and\nGSM8K datasets, we see that there is a considerable increase\nin the energy per second across all LLaMA sizes when using\nthe A100 over the V100 where the most considerable increase\nis for the smallest 7B model.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 146
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s147",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Although Figure 2 shows a\nconsiderable increase in inference throughput from using the\nA100, Figure 3 shows us that this improvement does not come\nfor free: it comes at an increased energy cost per second.",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 147
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s148",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Moreover, for the largest LLaMA 65B, it is less clear whether\nthe increased inference energy per second (Figure 3) is worth\nthe small improvement in inference throughput in terms of\nwords/token/responses per second (Figure 2).",
        "score": 0.7167,
        "rank": 6,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 148
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s145",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above.",
        "score": 0.7072,
        "rank": 7,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 145
        }
      },
      {
        "node_id": "samsi2024:sec4:p5",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "responses and the total time needed to run inference over the\ninput data.\nWe monitor GPUs using the nvidia-smi [23] and\nNVIDIA DCGM [24] utilities to study GPU utilization, energy,\npower draw, etc. during our experiments. The nvidia-smi\nutility is used to capture GPU usage over time at 100ms\nintervals and the DCGM monitoring tool is used to capture\naggregate GPU energy in Joules for the rank-0 node. For a\nmulti-node, multi-GPU model, we multiply the rank-0 energy\nby the number of nodes used. Maximum power draw on GPUs\nis capped at 250 Watts unless otherwise stated. Due to limits\non resource availability, we mainly use V100 GPUs for larger-\nscale distributed experiments (i.e., for 8, 16, and 32 shards)\nand A100 GPUs for smaller scale experiments.\nInference energy metrics are calculated by combining the\ninference metrics above with the energy data collected from\nour GPUs using NVIDIA\u2019s utilities described above. Specif-\nically, energy per second is defined as the total aggregate\nGPU energy spent from a single experiment/job (across all\nshards) divided by the total run time of that experiment/job in\nseconds. A single experiment/job denotes a single run through\nall 4,096 prompts under a specified batch size. Energy per\ntoken and energy per response are similarly defined as total\nenergy divided by the number of decoded output tokens and\nthe number of responses as defined above, respectively.\nIV. R ESULTS\nA. Baselines: LLaMA 7B, 13B, & 65B\n1) Inference Performance: We begin our analysis with\na baseline comparison of LLaMA 65B with smaller-scale\nLLaMA models: LLaMA 7B and 13B. The goal is to under-\nstand the following: what do inference performance and energy\ntrade-offs look like for the different sizes of LLaMA under the\nbare-minimum set of resources required to have them running\ninference? This question can be important for researchers\nand users who have may not have limitless computational\nresources and hardware acceleration or may be constrained\nin terms of GPU memory, etc.\nGiven the sizes of the models, the size of the data, and the\nhardware memory limits, we only show results from experi-\nments that were possible for a given combination of parameters\n(i.e., for some models, certain combinations of batch size and\nnumber of shards are infeasible due to memory limits of the\nunderlying GPUs). Table II shows the bare minimum hardware\nrequirements for each LLaMA variant and the maximum batch\nsize possible for each combination, assuming no further model\ncompression, optimization, quantization, distillation etc.\nWith these limits in mind, we present the inference per-\nformance of LLaMA 7B, 13B, and 65B on the Alpaca and\nGSM8K datasets with the bare minimum hardware settings in\nFigure 2. The plots in Figure 2 show a baseline comparison of\ninference performance of the three LLaMA variants on both\nthe V100 and A100 GPUs respectively. For each model, in\nline with the spirit of the bare minimum settings, inference\nis done with a batch size of 64 and an maximum generation\nlength of 256. The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256. These limits are imposed by a combination of GPU\nmemory, model size, response length and the number of GPUs.\nWhile the 65B model can sharded across 6 V100 GPUs, we\nuse 8 since the model architecture makes it better suited for\nbalanced sharding across 8 GPUs.\nModel Size V100 32GB A100 80GB\nCount Max. Batch size Count Max. Batch size\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n13B on two GPUs in each case whereas the 65B model was\nrun on 8 V100 GPUs and 4 A100 GPUs respectively due to\nthe size of the model and available memory on the GPU(s).\nAs expected, we observe that the A100 outperforms V100\non both the Alpaca and GSM8K datasets: particularly for the\nsmaller LLaMA 7B and 13B, we see anywhere from a 2\ntimes (7B) to a 1.25 times increase (13B) in inference latency\non the A100 when compared to the V100 across words per\nsecond, tokens per second, and responses per second. Faster\nresponse rates and inference are likely due to the fact that\nthe number of computations, directly related to the number\nof parameters of said model, involved in the 7B and 13B\nmodels are significantly lower than the 65B model. We do note\nthat for LLaMA 65B, we see a much smaller improvement in\nusing the A100 over the V100; however, since the 65B model\nrequires sharding across two (A100) or four (V100) compute\nnodes at the mininum, this could result in additional latency\nto each forward pass of the model, explaining the smaller\nimprovements. We also observe that while LLaMA 7B exhibits\na considerable improvement in inference throughput on both\nAlpaca and GSM8K with the A100, the improvement is much\nlarger for Alpaca than GSM8K. This can also be attributed to\nthe different complexities of inputs from each dataset.\n2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above. For both the Alpaca and\nGSM8K datasets, we see that there is a considerable increase\nin the energy per second across all LLaMA sizes when using\nthe A100 over the V100 where the most considerable increase\nis for the smallest 7B model. Although Figure 2 shows a\nconsiderable increase in inference throughput from using the\nA100, Figure 3 shows us that this improvement does not come\nfor free: it comes at an increased energy cost per second.\nMoreover, for the largest LLaMA 65B, it is less clear whether\nthe increased inference energy per second (Figure 3) is worth\nthe small improvement in inference throughput in terms of\nwords/token/responses per second (Figure 2).",
        "score": 0.7072,
        "rank": 7,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "page": 4,
          "paragraph_index": 5,
          "section_id": "samsi2024:sec4",
          "section_index": 4
        }
      },
      {
        "node_id": "chen2024:sec3:p5:s73",
        "document_title": "Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation",
        "text": "The\nvalues for LLaMA3-70B are also presented.",
        "score": 0.6911,
        "rank": 8,
        "metadata": {
          "document_id": "chen2024",
          "document_title": "Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation",
          "paragraph_id": "chen2024:sec3:p5",
          "section_id": "chen2024:sec3",
          "sentence_index": 73
        }
      },
      {
        "node_id": "chen2024:sec3:p5",
        "document_title": "Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation",
        "text": "converter. This converter splits the model computation graph\ninto slices, interleaved with attention operators. It also re-\norders the operators and coordinates the computation and\ncommunication pipelines, enabling effective overlapping of\ncommunication and computation tasks.\nMoreover, with model-attention disaggregation, running\nthe inference process with only a single batch results in under-\nutilization of resources, as the memory device remains idle\nwhen the computation device is active, and vice versa. To\naddress this inefficiency and resource wastage, we introduce\nstaggered pipelining, an advanced technique that increases the\nhardware utilization. With staggered pipelining, we run multi-\nple batches concurrently and optimize the workflow to ensure\nthat both the computation and memory devices are working\nsimultaneously, minimizing resource waste and maximizing\nsystem performance.\nTo validate our analysis, we develop and evaluate Lam-\nina, a distributed heterogeneous LLM inference system with\nmodel-attention disaggregation. We also conduct extensive\nevaluations to mirror the real-world LLM services with a het-\nerogeneous cluster made up of H100 and H20 GPUs, tested\nwith various models and request traces collected from the\nproduction environments of LLM service providers. Experi-\nmental results that our system can achieve up to16.1 \u223c 90.1%\nhigher throughput with similar hardware cost than existing so-\nlutions. Although Lamina experiences a slightly larger latency\nthan homogeneous solutions for the larger (2.39\u00d7 on average)\nbatch sizes and additional networking and scheduling costs,\nthe latency is still within the SLO of online interactive LLM\nservices.\n2 Background: The Underutilization of GPUs\nin LLM Decoding\nTo comprehensively understand the challenges and limita-\ntions present in current LLM decoding implementation with\nhomogeneous hardware, this section will provide a detailed\nperformance analysis of LLM decoding with LLaMA3-70B\nmodel as a representative LLM. The specific notations used\nin this analysis are explained in Table 2.\nTable 2: Notations used in the performance analysis. The\nvalues for LLaMA3-70B are also presented.\nParameter Description Typical Value\nN Number of parameters in LLM. 70 billion\nd Hidden dimension. 8192\nL Layers of the LLM. 80\nG GQA group size. 8\ne Bytes per element. 2\nB Batch size. 1 \u223c 1024\nl Sequence length. 128 \u223c 32768\n2.1 Preliminaries\nModern large language models (LLMs) primarily rely on the\ntransformer architecture [49]. In a transformer-based LLM,\neach input token is first mapped to a word embedding of di-\nmension d. These embeddings then pass through a series of\ntransformer blocks. The final output embeddings are multi-\nplied by a sampling matrix to generate the predicted likeli-\nhoods for the next token.\nWithin each transformer block, the input embeddings are\nprojected into three distinct vectors: query (qi), key (ki), and\nvalue (vi), all of which have the same dimension d as hidden\nstates. These vectors are processed through an attention oper-\nator to compute attention scores. The attention scores are then\nweighted by a matrix Wout to produce the output embeddings\nyi of the attention layer.\nqi = Wqxi, ki = Wkxi, vi = Wvxi,\nai =\nn\n\u2211\nj=1\nsoftmax\n\u0012q\u22a4\ni k j\u221a\nd\n\u0013\nv j, \u22c6\nyi = Woutai.\nThe output yi is then passed through a feedforward network\nthat scales it into an intermediate vector space, followed by\nanother matrix multiplication to scale it back:\nx\u2032\ni = Wproj \u00b7 fact (Wfc \u00b7 yi) .\nAlthough the transformer block involves various transfor-\nmations, there are actually only two kind of computationally\nexpensive operations, which are the attention operator (de-\nnoted by \u22c6 in the equations) and the other matrix projection\nsteps. Thus, in the following of this section, we will conduct\na quantitative analysis based on the roofline model [50] and\nexperimental measurements to evaluate these two kinds of\noperators. This analysis will highlight the differing charac-\nteristics of attention and non-attention operators during the\ndecoding phase, which explains why current LLM decoding\nimplementations with homogeneous hardware often lead to\nunderutilization of GPUs, thus motivating the need for het-\nerogeneous architectures.\n2.2 Hardware Underutilization\n2.2.1 The Underutilization in Non-Attention Operators\nTo improve GPU utilization in LLM decoding, continuous\nbatching is widely adopted [16, 20, 46]. By processing multi-\nple inputs concurrently, the model parameters in GPU mem-\nory can be reused, making the workload more computation-\nintensive. For a batch ofB requests, the non-attention operator\nrequires approximately 2NB floating-point operations. Addi-\ntionally, these operators involve loading model parameterseN\nand reading/writing a total of2eBd input and output data from\n3",
        "score": 0.6911,
        "rank": 8,
        "metadata": {
          "document_id": "chen2024",
          "document_title": "Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation",
          "page": 3,
          "paragraph_index": 5,
          "section_id": "chen2024:sec3",
          "section_index": 3
        }
      },
      {
        "node_id": "samsi2024:sec4:p5:s133",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256.",
        "score": 0.6816,
        "rank": 9,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec4:p5",
          "section_id": "samsi2024:sec4",
          "sentence_index": 133
        }
      },
      {
        "node_id": "samsi2024:sec4:p5",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "responses and the total time needed to run inference over the\ninput data.\nWe monitor GPUs using the nvidia-smi [23] and\nNVIDIA DCGM [24] utilities to study GPU utilization, energy,\npower draw, etc. during our experiments. The nvidia-smi\nutility is used to capture GPU usage over time at 100ms\nintervals and the DCGM monitoring tool is used to capture\naggregate GPU energy in Joules for the rank-0 node. For a\nmulti-node, multi-GPU model, we multiply the rank-0 energy\nby the number of nodes used. Maximum power draw on GPUs\nis capped at 250 Watts unless otherwise stated. Due to limits\non resource availability, we mainly use V100 GPUs for larger-\nscale distributed experiments (i.e., for 8, 16, and 32 shards)\nand A100 GPUs for smaller scale experiments.\nInference energy metrics are calculated by combining the\ninference metrics above with the energy data collected from\nour GPUs using NVIDIA\u2019s utilities described above. Specif-\nically, energy per second is defined as the total aggregate\nGPU energy spent from a single experiment/job (across all\nshards) divided by the total run time of that experiment/job in\nseconds. A single experiment/job denotes a single run through\nall 4,096 prompts under a specified batch size. Energy per\ntoken and energy per response are similarly defined as total\nenergy divided by the number of decoded output tokens and\nthe number of responses as defined above, respectively.\nIV. R ESULTS\nA. Baselines: LLaMA 7B, 13B, & 65B\n1) Inference Performance: We begin our analysis with\na baseline comparison of LLaMA 65B with smaller-scale\nLLaMA models: LLaMA 7B and 13B. The goal is to under-\nstand the following: what do inference performance and energy\ntrade-offs look like for the different sizes of LLaMA under the\nbare-minimum set of resources required to have them running\ninference? This question can be important for researchers\nand users who have may not have limitless computational\nresources and hardware acceleration or may be constrained\nin terms of GPU memory, etc.\nGiven the sizes of the models, the size of the data, and the\nhardware memory limits, we only show results from experi-\nments that were possible for a given combination of parameters\n(i.e., for some models, certain combinations of batch size and\nnumber of shards are infeasible due to memory limits of the\nunderlying GPUs). Table II shows the bare minimum hardware\nrequirements for each LLaMA variant and the maximum batch\nsize possible for each combination, assuming no further model\ncompression, optimization, quantization, distillation etc.\nWith these limits in mind, we present the inference per-\nformance of LLaMA 7B, 13B, and 65B on the Alpaca and\nGSM8K datasets with the bare minimum hardware settings in\nFigure 2. The plots in Figure 2 show a baseline comparison of\ninference performance of the three LLaMA variants on both\nthe V100 and A100 GPUs respectively. For each model, in\nline with the spirit of the bare minimum settings, inference\nis done with a batch size of 64 and an maximum generation\nlength of 256. The 7B model was run on a single GPU and\nTABLE II: Baseline configurations for LLaMA 7B, 13B,\nand 65B: This table lists the bare minimum hardware required\nfor different models and the maximum batch size possible\ngiven the bare minimum hardware for a max response length\nof 256. These limits are imposed by a combination of GPU\nmemory, model size, response length and the number of GPUs.\nWhile the 65B model can sharded across 6 V100 GPUs, we\nuse 8 since the model architecture makes it better suited for\nbalanced sharding across 8 GPUs.\nModel Size V100 32GB A100 80GB\nCount Max. Batch size Count Max. Batch size\n7B 1 64 1 64\n13B 2 64 1 64\n65B 8 64 4 128\n13B on two GPUs in each case whereas the 65B model was\nrun on 8 V100 GPUs and 4 A100 GPUs respectively due to\nthe size of the model and available memory on the GPU(s).\nAs expected, we observe that the A100 outperforms V100\non both the Alpaca and GSM8K datasets: particularly for the\nsmaller LLaMA 7B and 13B, we see anywhere from a 2\ntimes (7B) to a 1.25 times increase (13B) in inference latency\non the A100 when compared to the V100 across words per\nsecond, tokens per second, and responses per second. Faster\nresponse rates and inference are likely due to the fact that\nthe number of computations, directly related to the number\nof parameters of said model, involved in the 7B and 13B\nmodels are significantly lower than the 65B model. We do note\nthat for LLaMA 65B, we see a much smaller improvement in\nusing the A100 over the V100; however, since the 65B model\nrequires sharding across two (A100) or four (V100) compute\nnodes at the mininum, this could result in additional latency\nto each forward pass of the model, explaining the smaller\nimprovements. We also observe that while LLaMA 7B exhibits\na considerable improvement in inference throughput on both\nAlpaca and GSM8K with the A100, the improvement is much\nlarger for Alpaca than GSM8K. This can also be attributed to\nthe different complexities of inputs from each dataset.\n2) Inference Energy: Figure 3 shows a comparison of the\nenergy per second required to run inference on LLaMA 7B,\n13B, and 65B, with different GPUs under the same bare mini-\nmum hardware settings as the above. For both the Alpaca and\nGSM8K datasets, we see that there is a considerable increase\nin the energy per second across all LLaMA sizes when using\nthe A100 over the V100 where the most considerable increase\nis for the smallest 7B model. Although Figure 2 shows a\nconsiderable increase in inference throughput from using the\nA100, Figure 3 shows us that this improvement does not come\nfor free: it comes at an increased energy cost per second.\nMoreover, for the largest LLaMA 65B, it is less clear whether\nthe increased inference energy per second (Figure 3) is worth\nthe small improvement in inference throughput in terms of\nwords/token/responses per second (Figure 2).",
        "score": 0.6816,
        "rank": 9,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "page": 4,
          "paragraph_index": 5,
          "section_id": "samsi2024:sec4",
          "section_index": 4
        }
      },
      {
        "node_id": "chen2024:sec3:p5:s70",
        "document_title": "Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation",
        "text": "2 Background: The Underutilization of GPUs\nin LLM Decoding\nTo comprehensively understand the challenges and limita-\ntions present in current LLM decoding implementation with\nhomogeneous hardware, this section will provide a detailed\nperformance analysis of LLM decoding with LLaMA3-70B\nmodel as a representative LLM.",
        "score": 0.6751,
        "rank": 10,
        "metadata": {
          "document_id": "chen2024",
          "document_title": "Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation",
          "paragraph_id": "chen2024:sec3:p5",
          "section_id": "chen2024:sec3",
          "sentence_index": 70
        }
      },
      {
        "node_id": "chen2024:sec3:p5",
        "document_title": "Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation",
        "text": "converter. This converter splits the model computation graph\ninto slices, interleaved with attention operators. It also re-\norders the operators and coordinates the computation and\ncommunication pipelines, enabling effective overlapping of\ncommunication and computation tasks.\nMoreover, with model-attention disaggregation, running\nthe inference process with only a single batch results in under-\nutilization of resources, as the memory device remains idle\nwhen the computation device is active, and vice versa. To\naddress this inefficiency and resource wastage, we introduce\nstaggered pipelining, an advanced technique that increases the\nhardware utilization. With staggered pipelining, we run multi-\nple batches concurrently and optimize the workflow to ensure\nthat both the computation and memory devices are working\nsimultaneously, minimizing resource waste and maximizing\nsystem performance.\nTo validate our analysis, we develop and evaluate Lam-\nina, a distributed heterogeneous LLM inference system with\nmodel-attention disaggregation. We also conduct extensive\nevaluations to mirror the real-world LLM services with a het-\nerogeneous cluster made up of H100 and H20 GPUs, tested\nwith various models and request traces collected from the\nproduction environments of LLM service providers. Experi-\nmental results that our system can achieve up to16.1 \u223c 90.1%\nhigher throughput with similar hardware cost than existing so-\nlutions. Although Lamina experiences a slightly larger latency\nthan homogeneous solutions for the larger (2.39\u00d7 on average)\nbatch sizes and additional networking and scheduling costs,\nthe latency is still within the SLO of online interactive LLM\nservices.\n2 Background: The Underutilization of GPUs\nin LLM Decoding\nTo comprehensively understand the challenges and limita-\ntions present in current LLM decoding implementation with\nhomogeneous hardware, this section will provide a detailed\nperformance analysis of LLM decoding with LLaMA3-70B\nmodel as a representative LLM. The specific notations used\nin this analysis are explained in Table 2.\nTable 2: Notations used in the performance analysis. The\nvalues for LLaMA3-70B are also presented.\nParameter Description Typical Value\nN Number of parameters in LLM. 70 billion\nd Hidden dimension. 8192\nL Layers of the LLM. 80\nG GQA group size. 8\ne Bytes per element. 2\nB Batch size. 1 \u223c 1024\nl Sequence length. 128 \u223c 32768\n2.1 Preliminaries\nModern large language models (LLMs) primarily rely on the\ntransformer architecture [49]. In a transformer-based LLM,\neach input token is first mapped to a word embedding of di-\nmension d. These embeddings then pass through a series of\ntransformer blocks. The final output embeddings are multi-\nplied by a sampling matrix to generate the predicted likeli-\nhoods for the next token.\nWithin each transformer block, the input embeddings are\nprojected into three distinct vectors: query (qi), key (ki), and\nvalue (vi), all of which have the same dimension d as hidden\nstates. These vectors are processed through an attention oper-\nator to compute attention scores. The attention scores are then\nweighted by a matrix Wout to produce the output embeddings\nyi of the attention layer.\nqi = Wqxi, ki = Wkxi, vi = Wvxi,\nai =\nn\n\u2211\nj=1\nsoftmax\n\u0012q\u22a4\ni k j\u221a\nd\n\u0013\nv j, \u22c6\nyi = Woutai.\nThe output yi is then passed through a feedforward network\nthat scales it into an intermediate vector space, followed by\nanother matrix multiplication to scale it back:\nx\u2032\ni = Wproj \u00b7 fact (Wfc \u00b7 yi) .\nAlthough the transformer block involves various transfor-\nmations, there are actually only two kind of computationally\nexpensive operations, which are the attention operator (de-\nnoted by \u22c6 in the equations) and the other matrix projection\nsteps. Thus, in the following of this section, we will conduct\na quantitative analysis based on the roofline model [50] and\nexperimental measurements to evaluate these two kinds of\noperators. This analysis will highlight the differing charac-\nteristics of attention and non-attention operators during the\ndecoding phase, which explains why current LLM decoding\nimplementations with homogeneous hardware often lead to\nunderutilization of GPUs, thus motivating the need for het-\nerogeneous architectures.\n2.2 Hardware Underutilization\n2.2.1 The Underutilization in Non-Attention Operators\nTo improve GPU utilization in LLM decoding, continuous\nbatching is widely adopted [16, 20, 46]. By processing multi-\nple inputs concurrently, the model parameters in GPU mem-\nory can be reused, making the workload more computation-\nintensive. For a batch ofB requests, the non-attention operator\nrequires approximately 2NB floating-point operations. Addi-\ntionally, these operators involve loading model parameterseN\nand reading/writing a total of2eBd input and output data from\n3",
        "score": 0.6751,
        "rank": 10,
        "metadata": {
          "document_id": "chen2024",
          "document_title": "Efficient Heterogeneous Large Language Model Decoding with Model-Attention Disaggregation",
          "page": 3,
          "paragraph_index": 5,
          "section_id": "chen2024:sec3",
          "section_index": 3
        }
      }
    ],
    "num_snippets": 100,
    "retry_count": 0
  },
  {
    "id": "q321",
    "question": "When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?",
    "gt_value": "16.7",
    "gt_unit": "requests",
    "gt_ref": "['li2025b']",
    "pred_value": "[10, 50]",
    "pred_unit": "requests",
    "pred_ref": "[\"li2025b\"]",
    "pred_explanation": "The context states that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10\u201350 medium-length responses, depending on deployment location. Although the data center location (Arizona) is mentioned in the context as having specific water efficiency characteristics, no location-specific adjustment to the 10\u201350 range is provided. Thus, the baseline range from the context applies.",
    "raw_response": "{\n  \"explanation\": \"The context states that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10\u201350 medium-length responses, depending on deployment location. Although the data center location (Arizona) is mentioned in the context as having specific water efficiency characteristics, no location-specific adjustment to the 10\u201350 range is provided. Thus, the baseline range from the context applies.\",\n  \"answer\": \"Between 10 and 50 user requests\",\n  \"answer_value\": [10, 50],\n  \"ref_id\": [\"li2025b\"],\n  \"ref_url\": [],\n  \"supporting_materials\": \"Additionally, GPT-3 needs to \u201cdrink\u201d (i.e., consume) a500ml bottle of waterfor roughly 10 \u2013 50 medium-length responses, depending on when and where it is deployed.\"\n}",
    "value_correct": false,
    "ref_score": 1.0,
    "na_correct": true,
    "weighted_score": 0.25,
    "latency_seconds": 58.87095522880554,
    "retrieval_seconds": 15.294182300567627,
    "generation_seconds": 43.57652688026428,
    "error": null,
    "pred_ref_url": [],
    "pred_supporting_materials": "Additionally, GPT-3 needs to \u201cdrink\u201d (i.e., consume) a500ml bottle of waterfor roughly 10 \u2013 50 medium-length responses, depending on when and where it is deployed.",
    "rendered_prompt": "You will answer a question using ONLY the provided context snippets.\nIf the context does not clearly support an answer, use \"is_blank\".\n\nContext snippets from documents:\n[ref_id=li2025b] Additionally, GPT-3 needs to \u201cdrink\u201d (i.e., consume) a500ml bottle of waterfor roughly 10 \u2013 50\nmedium-length responses, depending on when and where it is deployed.\n---\n[ref_id=li2025b] 2027, which is more than the total annual water withdrawal of 4 \u2013 6 Denmark or half of the United King-\ndom.3 Simultaneously, a total of 0.38 \u2013 0.60 billion cubic meters of water will be evaporated and considered\n\u201cconsumption\u201d due to the global AI demand in 2027. Moreover, these global estimates will be exceeded by\nthe total water withdrawal and consumption attributed to AI in the U.S. alone in 2028 if the projection in [1]\ncomes to fruition.\nDespite its profound environmental and societal impact, the increasing water footprint of AI has received\ndisproportionately less attention from the AI community as well as the general public. For example, while\nthe scope-2 carbon emissions are routinely included as part of AI model cards, even scope-1 direct water\nusage (either withdrawal or consumption) is missing, let alone scope-2 water usage. This may impede inno-\nvations to enable water sustainability and build truly sustainable AI. Crucially, water and carbon footprints\nare complementary to, not substitutable of, each other for understanding the environmental impacts. In-\ndeed, optimizing for carbon efficiency does not necessarily result in, and may even worsen, water efficiency,\nwhich varies with the fuel mixes for electricity generation and outside weather in a unique way [1,9].\nTo ensure that the growth in AI does not exacerbate the global water stresses or outweigh the environ-\nmental benefits it provides, it is a critical time to uncover and address AI\u2019s hidden water footprint amid the\nincreasingly severe freshwater scarcity crisis, worsened extended droughts, and quickly aging public water\ninfrastructure. The urgency can also be reflected in part by the recent commitment to \u201cWater Positive by\n2030\u201d from industry leaders, including Google [4] and Microsoft [6], and by the inclusion of water foot-\nprint as a key metric into the world\u2019s first international standard on sustainable AI to be published by the\nISO/IEC [10].\nIn this paper, we advocate for a holistic approach to sustainable AI that extends beyond the carbon foot-\nprint to also address the water footprint. Specifically, we present a principled methodology to estimate AI\u2019s\ntotal water footprint, including both operational water and embodied water. By taking the GPT-3 model\nwith 175 billion parameters as an example [11], we show that training GPT-3 in Microsoft\u2019s U.S. data cen-\nters can consume a total of 5.4 million liters of water, including700,000 liters of scope-1 on-site water con-\nsumption. Additionally, GPT-3 needs to \u201cdrink\u201d (i.e., consume) a500ml bottle of waterfor roughly 10 \u2013 50\nmedium-length responses, depending on when and where it is deployed.\nNext, we show that WUE (Water Usage Effectiveness, a measure of water efficiency) varies both spatially\nand temporally, suggesting that judiciously deciding \u201cwhen\u201d and \u201cwhere\u201d to train a large AI model can\nsignificantly cut the water footprint. We also emphasize the need for increasing transparency of AI\u2019 water\nfootprint, including disclosing more information about operational data and keeping users informed of the\nruntime water efficiency. Finally, we highlight the necessity of holistically addressing water footprint along\nwith carbon footprint to enable truly sustainable AI \u2014 the water footprint of AI can no longer stay under the\nradar.\n2 Background\n2.1 Water Withdrawal vs. Water Consumption\nThere are two related but different concepts \u2014 water withdrawal and water consumption, both of which are\nimportant for understanding the impacts on water stress and availability [12,13].\n\u2022 Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either tem-\nporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding\nwater used for hydroelectricity generation) [12]. As water is a finite shared resource, water withdrawal\nindicates the level of competition as well as dependence on water resources among different sectors.\n\u2022 Water consumption:It is defined as \u201cwater withdrawal minus water discharge\u201d, and means the amount\nof water \u201cevaporated, transpired, incorporated into products or crops, or otherwise removed from the im-\nmediate water environment\u201d [13]. Water consumption reflects the impact on downstream water availability\nand is crucial for assessing watershed-level scarcity [12].\nThese two types of water usage correspond to two different water footprints, i.e., water withdrawal foot-\n3The scope definition of water usage [8] is in line with that of carbon emissions and is discussed in Section 2.2. Our scope-2 water\nwithdrawal (and consumption when applicable) is for location-based electricity generation throughout the paper. Large data centers\noften adopt sustainability programs (e.g., renewable purchasing agreements) to offset their location-based electricity usage and thus\nmay have lower market-based carbon and water footprints.\n2\n---\n[ref_id=li2025b] 0.1 0.3 0.5 0.7 0.9\nCarbon (kg/kWh)\n0\n4\n8\n12\n16Water (L/kWh)\nAKMS\nHIOA\nMROE\nNWPP\nNYUP\nAZNM\n(a) Carbon/water efficiency\nMON TUE WED THU FRI\n1.5\n1.8\n2.1\n2.4\n2.7\n3.0Water (L/kWh)\n0.20\n0.25\n0.30\n0.35\n0.40\nCarbon (kg/kWh) (b) Hourly carbon/water efficiency\nMON TUE WED THU FRI\n0\n20\n40\n60Percentage\nCoal\nNatural Gas\nNuclear\nOil\nWind\nSolar\nHydro\nOther (c) Hourly energy fuel mixes\nFigure 2: (a) The U.S. eGRID-level scope-2 water consumption intensity factor vs. carbon emission rate [8, 33].\nThe dashed line represents a linear regression model, showing that the eGRID-level scope-2 carbon emission and water\nconsumption efficiencies are not aligned.(b) A 5-day snapshot of scope-2 carbon emission rate and water consumption\nintensity in Virginia, starting from April 4, 2022. The values are calculated based on the fuel mixes, carbon emission\nrate and water consumption intensity for each fuel type [8,20,33]. The scope-2 carbon and water efficiencies only have\na weak Pearson correlation coefficient of 0.06 in Virginia.(c) A 5-day snapshot of energy fuel mixes serving Virginia,\nstarting from April 4, 2022 [20].\nper-request server energy consumption of 0.004 kWh for our conversation task. The PUE, WUE, and EWIF\nare the same as those used for estimating the training water consumption. Our estimate of inference water\nconsumption for GPT-3 is on the conservative side, and the actual water consumption could be several times\nhigher. Specifically, when considering service level objectives (SLOs) for LLM response times in enterprise-\ngrade Nvidia DGX H100 systems for conversation tasks, the inference server energy consumption for a\nmuch smaller model (e.g., Llama-3-70B) is already approximately 0.010 kWh per medium-sized request\nwhen using a state-of-the-art LLM inference solution and accounting for non-GPU server overhead [30]. For\nthe Falcon-180B model, which is comparable in size to GPT-3-175B, the server energy consumption reaches\napproximately 0.016 kWh per medium-sized request [30]. Furthermore, we emphasize that Microsoft\u2019s\ndata centers already have some of the lowest on-site WUE in the industry. If the same model is deployed\nin a third-party colocation data center, the scope-1 direct water consumption is expected to be several times\nhigher. Additionally, our EWIF for the U.S. (3.14 L/kWh) is conservative and significantly lower than the\n4.35 L/kWh recently reported by [1].\nWhile no official information is available on the resource consumption, some subsequent models like\nGPT-4 could consume substantially more energy and water than GPT-3 for processing the same request\n[31, 32]. With continued efforts to reduce AI\u2019s computational demand and improve the overall water effi-\nciency, the water consumption per request may decrease in the future. However, the total water consump-\ntion is likely to continue rising due to the growing demand for AI services and the increasing scale of AI\napplications [1].\n4 Our Recommendations\nWe provide our recommendations to address AI\u2019s water footprint from the scheduling and policy perspec-\ntives, making future AI more environmentally sustainable.\n4.1 More Transparency and Comprehensive Reporting\nDespite its growing importance, AI\u2019s water footprint has received relatively less attention. For example,\nwhile AI model cards routinely include carbon emissions and serve as an important reporting framework\nfor understanding AI\u2019s environmental impacts, they currently omit information on AI\u2019s water consumption.\nThe lack of transparency may obstruct efforts to drive innovations that enhance water sustainability and\nsupport truly sustainable AI. As an initial step to raise awareness among end users about the water resource\nimpacts of their AI usage, we recommend tracking and reporting AI\u2019s water consumption in AI model cards\nand/or through cloud dashboards.\nMoreover, a comprehensive understanding and reporting of AI\u2019s scope-2 water consumption associ-\nated with electricity generation remain limited. Although data centers have increasingly adopted climate-\n6\n---\n[ref_id=li2025b] Page 6\n---\n[ref_id=li2025b] 0.1 0.3 0.5 0.7 0.9\nCarbon (kg/kWh)\n0\n4\n8\n12\n16Water (L/kWh)\nAKMS\nHIOA\nMROE\nNWPP\nNYUP\nAZNM\n(a) Carbon/water efficiency\nMON TUE WED THU FRI\n1.5\n1.8\n2.1\n2.4\n2.7\n3.0Water (L/kWh)\n0.20\n0.25\n0.30\n0.35\n0.40\nCarbon (kg/kWh) (b) Hourly carbon/water efficiency\nMON TUE WED THU FRI\n0\n20\n40\n60Percentage\nCoal\nNatural Gas\nNuclear\nOil\nWind\nSolar\nHydro\nOther (c) Hourly energy fuel mixes\nFigure 2: (a) The U.S.\n---\n[ref_id=li2025b] eGRID-level scope-2 water consumption intensity factor vs.\n---\n[ref_id=li2025b] carbon emission rate [8, 33].\n---\n[ref_id=li2025b] The dashed line represents a linear regression model, showing that the eGRID-level scope-2 carbon emission and water\nconsumption efficiencies are not aligned.(b) A 5-day snapshot of scope-2 carbon emission rate and water consumption\nintensity in Virginia, starting from April 4, 2022.\n---\n[ref_id=li2025b] The values are calculated based on the fuel mixes, carbon emission\nrate and water consumption intensity for each fuel type [8,20,33].\n---\n[ref_id=li2025b] The scope-2 carbon and water efficiencies only have\na weak Pearson correlation coefficient of 0.06 in Virginia.(c) A 5-day snapshot of energy fuel mixes serving Virginia,\nstarting from April 4, 2022 [20].\n---\n[ref_id=li2025b] per-request server energy consumption of 0.004 kWh for our conversation task.\n---\n[ref_id=li2025b] The PUE, WUE, and EWIF\nare the same as those used for estimating the training water consumption.\n---\n[ref_id=li2025b] Our estimate of inference water\nconsumption for GPT-3 is on the conservative side, and the actual water consumption could be several times\nhigher.\n---\n[ref_id=li2025b] Specifically, when considering service level objectives (SLOs) for LLM response times in enterprise-\ngrade Nvidia DGX H100 systems for conversation tasks, the inference server energy consumption for a\nmuch smaller model (e.g., Llama-3-70B) is already approximately 0.010 kWh per medium-sized request\nwhen using a state-of-the-art LLM inference solution and accounting for non-GPU server overhead [30].\n---\n[ref_id=li2025b] For\nthe Falcon-180B model, which is comparable in size to GPT-3-175B, the server energy consumption reaches\napproximately 0.016 kWh per medium-sized request [30].\n---\n[ref_id=li2025b] Furthermore, we emphasize that Microsoft\u2019s\ndata centers already have some of the lowest on-site WUE in the industry.\n---\n[ref_id=li2025b] If the same model is deployed\nin a third-party colocation data center, the scope-1 direct water consumption is expected to be several times\nhigher.\n---\n[ref_id=li2025b] Additionally, our EWIF for the U.S.\n---\n[ref_id=li2025b] (3.14 L/kWh) is conservative and significantly lower than the\n4.35 L/kWh recently reported by [1].\n---\n[ref_id=li2025b] While no official information is available on the resource consumption, some subsequent models like\nGPT-4 could consume substantially more energy and water than GPT-3 for processing the same request\n[31, 32].\n---\n[ref_id=li2025b] With continued efforts to reduce AI\u2019s computational demand and improve the overall water effi-\nciency, the water consumption per request may decrease in the future.\n---\n[ref_id=li2025b] However, the total water consump-\ntion is likely to continue rising due to the growing demand for AI services and the increasing scale of AI\napplications [1].\n---\n[ref_id=li2025b] 4 Our Recommendations\nWe provide our recommendations to address AI\u2019s water footprint from the scheduling and policy perspec-\ntives, making future AI more environmentally sustainable.\n---\n[ref_id=li2025b] 4.1 More Transparency and Comprehensive Reporting\nDespite its growing importance, AI\u2019s water footprint has received relatively less attention.\n---\n[ref_id=li2025b] For example,\nwhile AI model cards routinely include carbon emissions and serve as an important reporting framework\nfor understanding AI\u2019s environmental impacts, they currently omit information on AI\u2019s water consumption.\n---\n[ref_id=li2025b] The lack of transparency may obstruct efforts to drive innovations that enhance water sustainability and\nsupport truly sustainable AI.\n---\n[ref_id=li2025b] As an initial step to raise awareness among end users about the water resource\nimpacts of their AI usage, we recommend tracking and reporting AI\u2019s water consumption in AI model cards\nand/or through cloud dashboards.\n---\n[ref_id=li2025b] Moreover, a comprehensive understanding and reporting of AI\u2019s scope-2 water consumption associ-\nated with electricity generation remain limited.\n---\n[ref_id=li2025b] Although data centers have increasingly adopted climate-\n6\n---\n[ref_id=li2025b] Our estimate of inference water\nconsumption for GPT-3 is on the conservative side, and the actual water consumption could be several times\nhigher.\n---\n[ref_id=li2025b] 0.1 0.3 0.5 0.7 0.9\nCarbon (kg/kWh)\n0\n4\n8\n12\n16Water (L/kWh)\nAKMS\nHIOA\nMROE\nNWPP\nNYUP\nAZNM\n(a) Carbon/water efficiency\nMON TUE WED THU FRI\n1.5\n1.8\n2.1\n2.4\n2.7\n3.0Water (L/kWh)\n0.20\n0.25\n0.30\n0.35\n0.40\nCarbon (kg/kWh) (b) Hourly carbon/water efficiency\nMON TUE WED THU FRI\n0\n20\n40\n60Percentage\nCoal\nNatural Gas\nNuclear\nOil\nWind\nSolar\nHydro\nOther (c) Hourly energy fuel mixes\nFigure 2: (a) The U.S. eGRID-level scope-2 water consumption intensity factor vs. carbon emission rate [8, 33].\nThe dashed line represents a linear regression model, showing that the eGRID-level scope-2 carbon emission and water\nconsumption efficiencies are not aligned.(b) A 5-day snapshot of scope-2 carbon emission rate and water consumption\nintensity in Virginia, starting from April 4, 2022. The values are calculated based on the fuel mixes, carbon emission\nrate and water consumption intensity for each fuel type [8,20,33]. The scope-2 carbon and water efficiencies only have\na weak Pearson correlation coefficient of 0.06 in Virginia.(c) A 5-day snapshot of energy fuel mixes serving Virginia,\nstarting from April 4, 2022 [20].\nper-request server energy consumption of 0.004 kWh for our conversation task. The PUE, WUE, and EWIF\nare the same as those used for estimating the training water consumption. Our estimate of inference water\nconsumption for GPT-3 is on the conservative side, and the actual water consumption could be several times\nhigher. Specifically, when considering service level objectives (SLOs) for LLM response times in enterprise-\ngrade Nvidia DGX H100 systems for conversation tasks, the inference server energy consumption for a\nmuch smaller model (e.g., Llama-3-70B) is already approximately 0.010 kWh per medium-sized request\nwhen using a state-of-the-art LLM inference solution and accounting for non-GPU server overhead [30]. For\nthe Falcon-180B model, which is comparable in size to GPT-3-175B, the server energy consumption reaches\napproximately 0.016 kWh per medium-sized request [30]. Furthermore, we emphasize that Microsoft\u2019s\ndata centers already have some of the lowest on-site WUE in the industry. If the same model is deployed\nin a third-party colocation data center, the scope-1 direct water consumption is expected to be several times\nhigher. Additionally, our EWIF for the U.S. (3.14 L/kWh) is conservative and significantly lower than the\n4.35 L/kWh recently reported by [1].\nWhile no official information is available on the resource consumption, some subsequent models like\nGPT-4 could consume substantially more energy and water than GPT-3 for processing the same request\n[31, 32]. With continued efforts to reduce AI\u2019s computational demand and improve the overall water effi-\nciency, the water consumption per request may decrease in the future. However, the total water consump-\ntion is likely to continue rising due to the growing demand for AI services and the increasing scale of AI\napplications [1].\n4 Our Recommendations\nWe provide our recommendations to address AI\u2019s water footprint from the scheduling and policy perspec-\ntives, making future AI more environmentally sustainable.\n4.1 More Transparency and Comprehensive Reporting\nDespite its growing importance, AI\u2019s water footprint has received relatively less attention. For example,\nwhile AI model cards routinely include carbon emissions and serve as an important reporting framework\nfor understanding AI\u2019s environmental impacts, they currently omit information on AI\u2019s water consumption.\nThe lack of transparency may obstruct efforts to drive innovations that enhance water sustainability and\nsupport truly sustainable AI. As an initial step to raise awareness among end users about the water resource\nimpacts of their AI usage, we recommend tracking and reporting AI\u2019s water consumption in AI model cards\nand/or through cloud dashboards.\nMoreover, a comprehensive understanding and reporting of AI\u2019s scope-2 water consumption associ-\nated with electricity generation remain limited. Although data centers have increasingly adopted climate-\n6\n---\n[ref_id=li2025b] Making AI Less \u201cThirsty\u201d: Uncovering and Addressing the\nSecret Water Footprint of AI Models\nPengfei Li\nUC Riverside\nJianyi Yang\nUC Riverside\nMohammad A. Islam\nUT Arlington\nShaolei Ren1\nUC Riverside\nAbstract\nThe growing carbon footprint of artificial intelligence (AI) has been undergoing public scrutiny. Nonethe-\nless, the equally important water (withdrawal and consumption) footprint of AI has largely remained under\nthe radar. For example, training the GPT-3 language model in Microsoft\u2019s state-of-the-art U.S. data centers\ncan directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret. More\ncritically, the global AI demand is projected to account for 4.2 \u2013 6.6 billion cubic meters of water withdrawal\nin 2027, which is more than the total annual water withdrawal of 4 \u2013 6 Denmark or half of the United King-\ndom. This is concerning, as freshwater scarcity has become one of the most pressing challenges. To respond\nto the global water challenges, AI can, and also must, take social responsibility and lead by example by ad-\ndressing its own water footprint. In this paper, we provide a principled methodology to estimate the water\nfootprint of AI, and also discuss the unique spatial-temporal diversities of AI\u2019s runtime water efficiency.\nFinally, we highlight the necessity of holistically addressing water footprint along with carbon footprint to\nenable truly sustainable AI.\n1 Introduction\nArtificial intelligence (AI) has enabled remarkable breakthroughs in numerous areas of critical importance,\nincluding tackling global challenges such as climate change. On the other hand, many AI models, especially\nlarge generative ones like GPT-4, are trained and deployed on energy-hungry servers in warehouse-scale\ndata centers, accelerating the data center energy consumption at an unprecedented rate [1]. As a result,\nAI\u2019s carbon footprint has been undergoing scrutiny, driving the recent progress in AI carbon efficiency [2,3].\nHowever, AI\u2019s water footprint \u2014 many millions of liters of freshwater consumed for cooling the servers and\nfor electricity generation \u2014 has largely remained under the radar and keeps escalating. If not properly\naddressed, AI\u2019s water footprint can potentially become a major roadblock to sustainability and create social\nconflicts as freshwater resources suitable for human use are extremely limited and unevenly distributed.\nAs acknowledged in Google\u2019s sustainability report [4] and the recent U.S. data center energy report [1],\nthe expansion of AI products and services is a key driver of the rapid increase in data center water consump-\ntion. Even excluding the water usage in leased third-party colocation facilities, one technology company\u2019s\nself-owned data centers alone directly withdrew 29 billion liters and consumed (i.e., evaporated) more than\n23 billion liters of freshwater for on-site cooling in 2023, nearly 80% of which was potable water [4].2 This\namount of annual water consumption even rivals that of a major household-name beverage company [5].\nImportantly, the company\u2019s data center water consumption increased by\u223c20% from 2021 to 2022 and by\n\u223c17% from 2022 to 2023 [4], and another technology company\u2019s data center water consumption saw\u223c34%\nand \u223c22% increases over the same periods, respectively [6]. Furthermore, according to the recent U.S. data\ncenter energy report, the total annual on-site water consumption by U.S. data centers in 2028 could double\nor even quadruple the 2023 level, reaching approximately 150 \u2013 280 billion liters and further stressing the\nwater infrastructures [1].\nAI represents the fastest expanding workloads in data centers [1, 4]. For example, a recent study sug-\ngests that the global AI could consume 85 \u2013 134 TWh of electricity in 2027 [7], whereas a more aggressive\nprojection by the recent U.S. data center energy report predicts that AI servers\u2019 electricity consumption in\nthe U.S. alone will surpass 150 \u2013 300 TWh in 2028 [1]. Even considering the lower estimate, the combined\nscope-1 and scope-2 water withdrawal of global AI is projected to reach 4.2 \u2013 6.6 billion cubic metersin\n1Corresponding author: Shaolei Ren (shaolei@ucr.edu), University of California, Riverside.\n2The detailed difference between water withdrawal and water consumption is presented in Section 2.1.\n1\narXiv:2304.03271v5  [cs.LG]  26 Mar 2025\n---\n[ref_id=li2025b] Page 1\n---\n[ref_id=li2025b] Making AI Less \u201cThirsty\u201d: Uncovering and Addressing the\nSecret Water Footprint of AI Models\nPengfei Li\nUC Riverside\nJianyi Yang\nUC Riverside\nMohammad A.\n---\n[ref_id=li2025b] Islam\nUT Arlington\nShaolei Ren1\nUC Riverside\nAbstract\nThe growing carbon footprint of artificial intelligence (AI) has been undergoing public scrutiny.\n---\n[ref_id=li2025b] Nonethe-\nless, the equally important water (withdrawal and consumption) footprint of AI has largely remained under\nthe radar.\n---\n[ref_id=li2025b] For example, training the GPT-3 language model in Microsoft\u2019s state-of-the-art U.S.\n---\n[ref_id=li2025b] data centers\ncan directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.\n---\n[ref_id=li2025b] More\ncritically, the global AI demand is projected to account for 4.2 \u2013 6.6 billion cubic meters of water withdrawal\nin 2027, which is more than the total annual water withdrawal of 4 \u2013 6 Denmark or half of the United King-\ndom.\n---\n[ref_id=li2025b] This is concerning, as freshwater scarcity has become one of the most pressing challenges.\n---\n[ref_id=li2025b] To respond\nto the global water challenges, AI can, and also must, take social responsibility and lead by example by ad-\ndressing its own water footprint.\n---\n[ref_id=li2025b] In this paper, we provide a principled methodology to estimate the water\nfootprint of AI, and also discuss the unique spatial-temporal diversities of AI\u2019s runtime water efficiency.\n---\n[ref_id=li2025b] Finally, we highlight the necessity of holistically addressing water footprint along with carbon footprint to\nenable truly sustainable AI.\n---\n[ref_id=li2025b] 1 Introduction\nArtificial intelligence (AI) has enabled remarkable breakthroughs in numerous areas of critical importance,\nincluding tackling global challenges such as climate change.\n---\n[ref_id=li2025b] On the other hand, many AI models, especially\nlarge generative ones like GPT-4, are trained and deployed on energy-hungry servers in warehouse-scale\ndata centers, accelerating the data center energy consumption at an unprecedented rate [1].\n---\n[ref_id=li2025b] As a result,\nAI\u2019s carbon footprint has been undergoing scrutiny, driving the recent progress in AI carbon efficiency [2,3].\n---\n[ref_id=li2025b] However, AI\u2019s water footprint \u2014 many millions of liters of freshwater consumed for cooling the servers and\nfor electricity generation \u2014 has largely remained under the radar and keeps escalating.\n---\n[ref_id=li2025b] If not properly\naddressed, AI\u2019s water footprint can potentially become a major roadblock to sustainability and create social\nconflicts as freshwater resources suitable for human use are extremely limited and unevenly distributed.\n---\n[ref_id=li2025b] As acknowledged in Google\u2019s sustainability report [4] and the recent U.S.\n---\n[ref_id=li2025b] data center energy report [1],\nthe expansion of AI products and services is a key driver of the rapid increase in data center water consump-\ntion.\n---\n[ref_id=li2025b] Even excluding the water usage in leased third-party colocation facilities, one technology company\u2019s\nself-owned data centers alone directly withdrew 29 billion liters and consumed (i.e., evaporated) more than\n23 billion liters of freshwater for on-site cooling in 2023, nearly 80% of which was potable water [4].2 This\namount of annual water consumption even rivals that of a major household-name beverage company [5].\n---\n[ref_id=li2025b] Importantly, the company\u2019s data center water consumption increased by\u223c20% from 2021 to 2022 and by\n\u223c17% from 2022 to 2023 [4], and another technology company\u2019s data center water consumption saw\u223c34%\nand \u223c22% increases over the same periods, respectively [6].\n---\n[ref_id=li2025b] Furthermore, according to the recent U.S.\n---\n[ref_id=li2025b] data\ncenter energy report, the total annual on-site water consumption by U.S.\n---\n[ref_id=li2025b] data centers in 2028 could double\nor even quadruple the 2023 level, reaching approximately 150 \u2013 280 billion liters and further stressing the\nwater infrastructures [1].\n---\n[ref_id=li2025b] AI represents the fastest expanding workloads in data centers [1, 4].\n---\n[ref_id=li2025b] For example, a recent study sug-\ngests that the global AI could consume 85 \u2013 134 TWh of electricity in 2027 [7], whereas a more aggressive\nprojection by the recent U.S.\n---\n[ref_id=li2025b] data center energy report predicts that AI servers\u2019 electricity consumption in\nthe U.S.\n---\n[ref_id=li2025b] alone will surpass 150 \u2013 300 TWh in 2028 [1].\n---\n[ref_id=li2025b] Even considering the lower estimate, the combined\nscope-1 and scope-2 water withdrawal of global AI is projected to reach 4.2 \u2013 6.6 billion cubic metersin\n1Corresponding author: Shaolei Ren (shaolei@ucr.edu), University of California, Riverside.\n---\n[ref_id=li2025b] 2The detailed difference between water withdrawal and water consumption is presented in Section 2.1.\n---\n[ref_id=li2025b] 1\narXiv:2304.03271v5  [cs.LG]  26 Mar 2025\n---\n[ref_id=li2025b] Then, using GPT-\n3 as an example, we show that a large AI model can consume millions of liters of water for training.\n---\n[ref_id=li2025b] conscious cooling system designs to minimize on-site water consumption [4,17,19], these efforts primarily\nfocus on scope-1 water usage while largely overlooking scope-2 impacts. Just as addressing scope-2 carbon\nemissions is important for mitigating climate change, it is equally crucial to address scope-2 water con-\nsumption to reduce AI\u2019s \u201ctrue water cost\u201d, as noted by the recent U.S. data center energy report [1]. To\nbetter reflect the true impacts of data centers on water resources, some technology companies such as Meta\nhave begun to include scope-2 water consumption in their sustainability reports [18]. We recommend the\nreporting of scope-2 water consumption as a standard practice. This approach makes the off-site water\nconsumption visible to AI model developers as well as end users and can unlock new opportunities for\ndemand-side flexibility, thereby alleviating the overall strain on water resources.\nFinally, despite the enormous scope-3 supply-chain water footprint [23], there is limited data available\nfor embodied water usage by chip manufacturing. We recommend further research on scope-3 water con-\nsumption to achieve a comprehensive understanding of AI\u2019s overall water footprint and to foster corporate\nwater stewardship.\n4.2 \u201cWhen\u201d and \u201cWhere\u201d Matter\nJudiciously deciding \u201cwhen\u201d and \u201cwhere\u201d to train a large AI model can significantly affect the water foot-\nprint. The water efficiency exhibits a spatial-temporal diversity \u2014 on-site water efficiency changes due to\nvariations of outside weather conditions, and off-site water efficiency changes due to variations of the grid\u2019s\nenergy fuel mixes to meet time-varying demands (Figure 2). Therefore, we can dynamically schedule AI\ntraining and inference in a water-wise manner to cut the water footprint. For example, we may schedule\nAI training at midnight and/or in a data center with better water efficiency. Likewise, if informed of the\nreal-time water efficiency, some water-conscious users may prefer to use AI inference during water-efficient\nhours and/or in water-efficient data centers, which can reduce AI\u2019s water footprint by enabling demand-side\nflexibility.\n4.3 \u201cFollow the Sun\u201d or \u201cUnfollow the Sun\u201d\nTo cut the carbon footprint, it is preferable to \u201cfollow the sun\u201d when solar energy is more abundant. Nonethe-\nless, to cut the water footprint, it may be more appealing to \u201cunfollow the sun\u201d to avoid high-temperature\nhours of a day when WUE is high. This conflict can also be shown in Figure 2(a) and Figure 2(b), where we\nsee misalignment between the scope-2 water consumption intensity factor and carbon emission rate: mini-\nmizing one footprint might increase the other footprint. This observation further corroborates the previous\nfinding that the environmental impacts of carbon and water footprints are not substitutable [1,9]. Therefore,\nto judiciously achieve a balance between \u201cfollow the sun\u201d for carbon efficiency and \u201cunfollow the sun\u201d for\nwater efficiency, we need to reconcile the potential water-carbon conflicts by using holistic approaches that\nare both carbon-efficient and water-wise.\n5 Conclusion\nIn this paper, we uncover AI\u2019s water usage as a critical concern for socially responsible and environmentally\nsustainable AI. We present a principled methodology to estimate AI\u2019s water footprint. Then, using GPT-\n3 as an example, we show that a large AI model can consume millions of liters of water for training. We\nalso discuss that the scope-1 and scope-2 water efficiencies vary spatially and temporally \u2014 judiciously\ndeciding \u201cwhen\u201d and \u201cwhere\u201d to run a large AI model can significantly cut the water footprint. In addition,\nwe recommend increased transparency and comprehensive reporting of AI\u2019s water footprint, and highlight\nthe necessity of holistically addressing the water footprint along with the carbon footprint to build truly\nsustainable AI.\nAI\u2019s water footprint can no longer stay under the radar and must be addressed as a priority as part of the collective\nefforts to combat global water challenges.\nReferences\n[1] Arman Shehabi, Sarah J. Smith, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakar Siddik, Billie\nHolecek, Jonathan Koomey, Eric Masanet, and Dale Sartor. 2024 United States data center energy usage\nreport. Lawrence Berkeley National Laboratory LBNL-2001637, December 2024.\n7\n---\n[ref_id=li2025b] With continued efforts to reduce AI\u2019s computational demand and improve the overall water effi-\nciency, the water consumption per request may decrease in the future.\n---\n[ref_id=li2025b] 0.1 0.3 0.5 0.7 0.9\nCarbon (kg/kWh)\n0\n4\n8\n12\n16Water (L/kWh)\nAKMS\nHIOA\nMROE\nNWPP\nNYUP\nAZNM\n(a) Carbon/water efficiency\nMON TUE WED THU FRI\n1.5\n1.8\n2.1\n2.4\n2.7\n3.0Water (L/kWh)\n0.20\n0.25\n0.30\n0.35\n0.40\nCarbon (kg/kWh) (b) Hourly carbon/water efficiency\nMON TUE WED THU FRI\n0\n20\n40\n60Percentage\nCoal\nNatural Gas\nNuclear\nOil\nWind\nSolar\nHydro\nOther (c) Hourly energy fuel mixes\nFigure 2: (a) The U.S. eGRID-level scope-2 water consumption intensity factor vs. carbon emission rate [8, 33].\nThe dashed line represents a linear regression model, showing that the eGRID-level scope-2 carbon emission and water\nconsumption efficiencies are not aligned.(b) A 5-day snapshot of scope-2 carbon emission rate and water consumption\nintensity in Virginia, starting from April 4, 2022. The values are calculated based on the fuel mixes, carbon emission\nrate and water consumption intensity for each fuel type [8,20,33]. The scope-2 carbon and water efficiencies only have\na weak Pearson correlation coefficient of 0.06 in Virginia.(c) A 5-day snapshot of energy fuel mixes serving Virginia,\nstarting from April 4, 2022 [20].\nper-request server energy consumption of 0.004 kWh for our conversation task. The PUE, WUE, and EWIF\nare the same as those used for estimating the training water consumption. Our estimate of inference water\nconsumption for GPT-3 is on the conservative side, and the actual water consumption could be several times\nhigher. Specifically, when considering service level objectives (SLOs) for LLM response times in enterprise-\ngrade Nvidia DGX H100 systems for conversation tasks, the inference server energy consumption for a\nmuch smaller model (e.g., Llama-3-70B) is already approximately 0.010 kWh per medium-sized request\nwhen using a state-of-the-art LLM inference solution and accounting for non-GPU server overhead [30]. For\nthe Falcon-180B model, which is comparable in size to GPT-3-175B, the server energy consumption reaches\napproximately 0.016 kWh per medium-sized request [30]. Furthermore, we emphasize that Microsoft\u2019s\ndata centers already have some of the lowest on-site WUE in the industry. If the same model is deployed\nin a third-party colocation data center, the scope-1 direct water consumption is expected to be several times\nhigher. Additionally, our EWIF for the U.S. (3.14 L/kWh) is conservative and significantly lower than the\n4.35 L/kWh recently reported by [1].\nWhile no official information is available on the resource consumption, some subsequent models like\nGPT-4 could consume substantially more energy and water than GPT-3 for processing the same request\n[31, 32]. With continued efforts to reduce AI\u2019s computational demand and improve the overall water effi-\nciency, the water consumption per request may decrease in the future. However, the total water consump-\ntion is likely to continue rising due to the growing demand for AI services and the increasing scale of AI\napplications [1].\n4 Our Recommendations\nWe provide our recommendations to address AI\u2019s water footprint from the scheduling and policy perspec-\ntives, making future AI more environmentally sustainable.\n4.1 More Transparency and Comprehensive Reporting\nDespite its growing importance, AI\u2019s water footprint has received relatively less attention. For example,\nwhile AI model cards routinely include carbon emissions and serve as an important reporting framework\nfor understanding AI\u2019s environmental impacts, they currently omit information on AI\u2019s water consumption.\nThe lack of transparency may obstruct efforts to drive innovations that enhance water sustainability and\nsupport truly sustainable AI. As an initial step to raise awareness among end users about the water resource\nimpacts of their AI usage, we recommend tracking and reporting AI\u2019s water consumption in AI model cards\nand/or through cloud dashboards.\nMoreover, a comprehensive understanding and reporting of AI\u2019s scope-2 water consumption associ-\nated with electricity generation remain limited. Although data centers have increasingly adopted climate-\n6\n---\n[ref_id=li2025b] conscious cooling system designs to minimize on-site water consumption [4,17,19], these efforts primarily\nfocus on scope-1 water usage while largely overlooking scope-2 impacts. Just as addressing scope-2 carbon\nemissions is important for mitigating climate change, it is equally crucial to address scope-2 water con-\nsumption to reduce AI\u2019s \u201ctrue water cost\u201d, as noted by the recent U.S. data center energy report [1]. To\nbetter reflect the true impacts of data centers on water resources, some technology companies such as Meta\nhave begun to include scope-2 water consumption in their sustainability reports [18]. We recommend the\nreporting of scope-2 water consumption as a standard practice. This approach makes the off-site water\nconsumption visible to AI model developers as well as end users and can unlock new opportunities for\ndemand-side flexibility, thereby alleviating the overall strain on water resources.\nFinally, despite the enormous scope-3 supply-chain water footprint [23], there is limited data available\nfor embodied water usage by chip manufacturing. We recommend further research on scope-3 water con-\nsumption to achieve a comprehensive understanding of AI\u2019s overall water footprint and to foster corporate\nwater stewardship.\n4.2 \u201cWhen\u201d and \u201cWhere\u201d Matter\nJudiciously deciding \u201cwhen\u201d and \u201cwhere\u201d to train a large AI model can significantly affect the water foot-\nprint. The water efficiency exhibits a spatial-temporal diversity \u2014 on-site water efficiency changes due to\nvariations of outside weather conditions, and off-site water efficiency changes due to variations of the grid\u2019s\nenergy fuel mixes to meet time-varying demands (Figure 2). Therefore, we can dynamically schedule AI\ntraining and inference in a water-wise manner to cut the water footprint. For example, we may schedule\nAI training at midnight and/or in a data center with better water efficiency. Likewise, if informed of the\nreal-time water efficiency, some water-conscious users may prefer to use AI inference during water-efficient\nhours and/or in water-efficient data centers, which can reduce AI\u2019s water footprint by enabling demand-side\nflexibility.\n4.3 \u201cFollow the Sun\u201d or \u201cUnfollow the Sun\u201d\nTo cut the carbon footprint, it is preferable to \u201cfollow the sun\u201d when solar energy is more abundant. Nonethe-\nless, to cut the water footprint, it may be more appealing to \u201cunfollow the sun\u201d to avoid high-temperature\nhours of a day when WUE is high. This conflict can also be shown in Figure 2(a) and Figure 2(b), where we\nsee misalignment between the scope-2 water consumption intensity factor and carbon emission rate: mini-\nmizing one footprint might increase the other footprint. This observation further corroborates the previous\nfinding that the environmental impacts of carbon and water footprints are not substitutable [1,9]. Therefore,\nto judiciously achieve a balance between \u201cfollow the sun\u201d for carbon efficiency and \u201cunfollow the sun\u201d for\nwater efficiency, we need to reconcile the potential water-carbon conflicts by using holistic approaches that\nare both carbon-efficient and water-wise.\n5 Conclusion\nIn this paper, we uncover AI\u2019s water usage as a critical concern for socially responsible and environmentally\nsustainable AI. We present a principled methodology to estimate AI\u2019s water footprint. Then, using GPT-\n3 as an example, we show that a large AI model can consume millions of liters of water for training. We\nalso discuss that the scope-1 and scope-2 water efficiencies vary spatially and temporally \u2014 judiciously\ndeciding \u201cwhen\u201d and \u201cwhere\u201d to run a large AI model can significantly cut the water footprint. In addition,\nwe recommend increased transparency and comprehensive reporting of AI\u2019s water footprint, and highlight\nthe necessity of holistically addressing the water footprint along with the carbon footprint to build truly\nsustainable AI.\nAI\u2019s water footprint can no longer stay under the radar and must be addressed as a priority as part of the collective\nefforts to combat global water challenges.\nReferences\n[1] Arman Shehabi, Sarah J. Smith, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakar Siddik, Billie\nHolecek, Jonathan Koomey, Eric Masanet, and Dale Sartor. 2024 United States data center energy usage\nreport. Lawrence Berkeley National Laboratory LBNL-2001637, December 2024.\n7\n---\n[ref_id=li2025b] Page 7\n---\n[ref_id=li2025b] conscious cooling system designs to minimize on-site water consumption [4,17,19], these efforts primarily\nfocus on scope-1 water usage while largely overlooking scope-2 impacts.\n---\n[ref_id=li2025b] Just as addressing scope-2 carbon\nemissions is important for mitigating climate change, it is equally crucial to address scope-2 water con-\nsumption to reduce AI\u2019s \u201ctrue water cost\u201d, as noted by the recent U.S.\n---\n[ref_id=li2025b] data center energy report [1].\n---\n[ref_id=li2025b] To\nbetter reflect the true impacts of data centers on water resources, some technology companies such as Meta\nhave begun to include scope-2 water consumption in their sustainability reports [18].\n---\n[ref_id=li2025b] We recommend the\nreporting of scope-2 water consumption as a standard practice.\n---\n[ref_id=li2025b] This approach makes the off-site water\nconsumption visible to AI model developers as well as end users and can unlock new opportunities for\ndemand-side flexibility, thereby alleviating the overall strain on water resources.\n---\n[ref_id=li2025b] Finally, despite the enormous scope-3 supply-chain water footprint [23], there is limited data available\nfor embodied water usage by chip manufacturing.\n---\n[ref_id=li2025b] We recommend further research on scope-3 water con-\nsumption to achieve a comprehensive understanding of AI\u2019s overall water footprint and to foster corporate\nwater stewardship.\n---\n[ref_id=li2025b] 4.2 \u201cWhen\u201d and \u201cWhere\u201d Matter\nJudiciously deciding \u201cwhen\u201d and \u201cwhere\u201d to train a large AI model can significantly affect the water foot-\nprint.\n---\n[ref_id=li2025b] The water efficiency exhibits a spatial-temporal diversity \u2014 on-site water efficiency changes due to\nvariations of outside weather conditions, and off-site water efficiency changes due to variations of the grid\u2019s\nenergy fuel mixes to meet time-varying demands (Figure 2).\n---\n[ref_id=li2025b] Therefore, we can dynamically schedule AI\ntraining and inference in a water-wise manner to cut the water footprint.\n---\n[ref_id=li2025b] For example, we may schedule\nAI training at midnight and/or in a data center with better water efficiency.\n---\n[ref_id=li2025b] Likewise, if informed of the\nreal-time water efficiency, some water-conscious users may prefer to use AI inference during water-efficient\nhours and/or in water-efficient data centers, which can reduce AI\u2019s water footprint by enabling demand-side\nflexibility.\n---\n[ref_id=li2025b] 4.3 \u201cFollow the Sun\u201d or \u201cUnfollow the Sun\u201d\nTo cut the carbon footprint, it is preferable to \u201cfollow the sun\u201d when solar energy is more abundant.\n---\n[ref_id=li2025b] Nonethe-\nless, to cut the water footprint, it may be more appealing to \u201cunfollow the sun\u201d to avoid high-temperature\nhours of a day when WUE is high.\n---\n[ref_id=li2025b] This conflict can also be shown in Figure 2(a) and Figure 2(b), where we\nsee misalignment between the scope-2 water consumption intensity factor and carbon emission rate: mini-\nmizing one footprint might increase the other footprint.\n---\n[ref_id=li2025b] This observation further corroborates the previous\nfinding that the environmental impacts of carbon and water footprints are not substitutable [1,9].\n---\n[ref_id=li2025b] Therefore,\nto judiciously achieve a balance between \u201cfollow the sun\u201d for carbon efficiency and \u201cunfollow the sun\u201d for\nwater efficiency, we need to reconcile the potential water-carbon conflicts by using holistic approaches that\nare both carbon-efficient and water-wise.\n---\n[ref_id=li2025b] 5 Conclusion\nIn this paper, we uncover AI\u2019s water usage as a critical concern for socially responsible and environmentally\nsustainable AI.\n---\n[ref_id=li2025b] We present a principled methodology to estimate AI\u2019s water footprint.\n---\n[ref_id=li2025b] Then, using GPT-\n3 as an example, we show that a large AI model can consume millions of liters of water for training.\n---\n[ref_id=li2025b] We\nalso discuss that the scope-1 and scope-2 water efficiencies vary spatially and temporally \u2014 judiciously\ndeciding \u201cwhen\u201d and \u201cwhere\u201d to run a large AI model can significantly cut the water footprint.\n---\n[ref_id=li2025b] In addition,\nwe recommend increased transparency and comprehensive reporting of AI\u2019s water footprint, and highlight\nthe necessity of holistically addressing the water footprint along with the carbon footprint to build truly\nsustainable AI.\n---\n[ref_id=li2025b] AI\u2019s water footprint can no longer stay under the radar and must be addressed as a priority as part of the collective\nefforts to combat global water challenges.\n---\n[ref_id=li2025b] References\n[1] Arman Shehabi, Sarah J.\n---\n[ref_id=li2025b] Smith, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakar Siddik, Billie\nHolecek, Jonathan Koomey, Eric Masanet, and Dale Sartor.\n---\n[ref_id=li2025b] 2024 United States data center energy usage\nreport.\n---\n[ref_id=li2025b] Lawrence Berkeley National Laboratory LBNL-2001637, December 2024.\n---\n[ref_id=li2025b] 7\n---\n[ref_id=jegham2025] GPT-4o. To simulate real-world usage dynamics, we apply a monthly prompt growth rate of 20%\nfrom January to May 2025, reflecting the documented increase in ChatGPT\u2019s weekly active user\nbase from 300 million to 800 million between December 2024 and April 2025 [72]. This is followed\nby a decaying growth pattern from June to December, yielding a total of approximately 772 billion\nGPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].\nWithin these queries, we conservatively assume an 80%/20% split between short and medium-length\nprompts based on typical usage patterns. Scaling the per-query energy estimates accordingly, we find\nthat GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269\nMWh at maximum, as seen in Figure 5. These values exceed the total electricity consumption of\n35,000 U.S. residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even\n325 universities (390,650 MWh) annually.\n6.3 Estimated 2025 Annual Water Footprint of GPT-4o Inference\nAs showcased in Figure 5, we translate estimated cooling and infrastructure-related water usage into\nreal-world benchmarks. Based on scaled inference volumes, GPT-4o\u2019s annual water consumption is\nprojected to be between 1,334,991 kiloliters (kL) and 1,579,680 kL. These quantities are roughly\nequivalent to filling over 500 Olympic-sized pools or to supporting the annual drinking needs of 1.2\nmillion people. Importantly, this consumption refers to evaporated freshwater permanently removed\nfrom local ecosystems rather than recycled. GPT-4o alone is responsible for evaporating an amount\nof freshwater equivalent to the annual drinking needs of almost 1.2 million people.\n6.4 Estimated 2025 Annual Carbon Footprint of GPT-4o Inference\nWe further examine GPT-4o\u2019s environmental footprint through estimated carbon emissions from\nelectricity usage, as seen in Figure 5. Our projections indicate annual emissions of approximately\n138,125 tons of CO2e at minimum and 163,441 tons at maximum. These figures are comparable to the\nannual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately\n272 transatlantic flights between Boston and London. In sequestration terms, offsetting GPT-4o\u2019s\nannual emissions would require over 138,000 acres of average U.S. forest, an area roughly equivalent\nto the size of Chicago. These results showcase that the aggregation of hundreds of millions of requests\nper day can already impose a substantial environmental burden. This burden is only expected to grow\nas AI usage continues to scale.\n7 GPT-5 Adaptive Model Routing Case Study\nThe launch of GPT-5 [74] introduced adaptive model routing, a mechanism that allows the system to\nautomatically determine whether to use a fast variant or a more computationally intensive \u201cThinking\u201d\nmodel for complex reasoning tasks. This unification eliminates the need for manual model selection\nwhere the model dynamically scales its reasoning effort based on prompt complexity.\nHowever, this adaptability introduces substantial variability in energy consumption across reasoning\nmodes, as shown in Figure 6. For medium-length queries, the average energy consumption ranges\nfrom 2.33Wh for minimal reasoning to 17.15Wh for high reasoning, representing a more than seven-\nfold increase. Despite this variance, GPT-5 remains relatively efficient at lower reasoning levels. For\ninstance, a short, minimal reasoning query consumes only 0.67 Wh, a value comparable to GPT-4o\u2019s\n0.42 Wh per short prompt. Conversely, a long, high-reasoning query reaches an average of 33.8 Wh,\ncomparable to the upper bounds observed among the most energy-intensive models analyzed in this\nstudy.\nThese results suggest that while adaptive routing optimizes computational resources by tailoring\ninference depth to task complexity, it also amplifies the environmental footprint of cognitively\ndemanding prompts. This finding underscores the growing importance of prompt-level efficiency\nanalysis for next-generation LLMs that blend lightweight and high-reasoning architectures within a\nunified system.\n11\n---\n[ref_id=jegham2025] Page 11\n---\n[ref_id=jegham2025] GPT-4o.\n---\n[ref_id=jegham2025] To simulate real-world usage dynamics, we apply a monthly prompt growth rate of 20%\nfrom January to May 2025, reflecting the documented increase in ChatGPT\u2019s weekly active user\nbase from 300 million to 800 million between December 2024 and April 2025 [72].\n---\n[ref_id=jegham2025] This is followed\nby a decaying growth pattern from June to December, yielding a total of approximately 772 billion\nGPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].\n---\n[ref_id=jegham2025] Within these queries, we conservatively assume an 80%/20% split between short and medium-length\nprompts based on typical usage patterns.\n---\n[ref_id=jegham2025] Scaling the per-query energy estimates accordingly, we find\nthat GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269\nMWh at maximum, as seen in Figure 5.\n---\n[ref_id=jegham2025] These values exceed the total electricity consumption of\n35,000 U.S.\n---\n[ref_id=jegham2025] residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even\n325 universities (390,650 MWh) annually.\n---\n[ref_id=jegham2025] 6.3 Estimated 2025 Annual Water Footprint of GPT-4o Inference\nAs showcased in Figure 5, we translate estimated cooling and infrastructure-related water usage into\nreal-world benchmarks.\n---\n[ref_id=jegham2025] Based on scaled inference volumes, GPT-4o\u2019s annual water consumption is\nprojected to be between 1,334,991 kiloliters (kL) and 1,579,680 kL.\n---\n[ref_id=jegham2025] These quantities are roughly\nequivalent to filling over 500 Olympic-sized pools or to supporting the annual drinking needs of 1.2\nmillion people.\n---\n[ref_id=jegham2025] Importantly, this consumption refers to evaporated freshwater permanently removed\nfrom local ecosystems rather than recycled.\n---\n[ref_id=jegham2025] GPT-4o alone is responsible for evaporating an amount\nof freshwater equivalent to the annual drinking needs of almost 1.2 million people.\n---\n[ref_id=jegham2025] 6.4 Estimated 2025 Annual Carbon Footprint of GPT-4o Inference\nWe further examine GPT-4o\u2019s environmental footprint through estimated carbon emissions from\nelectricity usage, as seen in Figure 5.\n---\n[ref_id=jegham2025] Our projections indicate annual emissions of approximately\n138,125 tons of CO2e at minimum and 163,441 tons at maximum.\n---\n[ref_id=jegham2025] These figures are comparable to the\nannual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately\n272 transatlantic flights between Boston and London.\n---\n[ref_id=jegham2025] In sequestration terms, offsetting GPT-4o\u2019s\nannual emissions would require over 138,000 acres of average U.S.\n---\n[ref_id=jegham2025] forest, an area roughly equivalent\nto the size of Chicago.\n---\n[ref_id=jegham2025] These results showcase that the aggregation of hundreds of millions of requests\nper day can already impose a substantial environmental burden.\n---\n[ref_id=jegham2025] This burden is only expected to grow\nas AI usage continues to scale.\n---\n[ref_id=jegham2025] 7 GPT-5 Adaptive Model Routing Case Study\nThe launch of GPT-5 [74] introduced adaptive model routing, a mechanism that allows the system to\nautomatically determine whether to use a fast variant or a more computationally intensive \u201cThinking\u201d\nmodel for complex reasoning tasks.\n---\n[ref_id=jegham2025] This unification eliminates the need for manual model selection\nwhere the model dynamically scales its reasoning effort based on prompt complexity.\n---\n[ref_id=jegham2025] However, this adaptability introduces substantial variability in energy consumption across reasoning\nmodes, as shown in Figure 6.\n---\n[ref_id=jegham2025] For medium-length queries, the average energy consumption ranges\nfrom 2.33Wh for minimal reasoning to 17.15Wh for high reasoning, representing a more than seven-\nfold increase.\n---\n[ref_id=jegham2025] Despite this variance, GPT-5 remains relatively efficient at lower reasoning levels.\n---\n[ref_id=jegham2025] For\ninstance, a short, minimal reasoning query consumes only 0.67 Wh, a value comparable to GPT-4o\u2019s\n0.42 Wh per short prompt.\n---\n[ref_id=jegham2025] Conversely, a long, high-reasoning query reaches an average of 33.8 Wh,\ncomparable to the upper bounds observed among the most energy-intensive models analyzed in this\nstudy.\n---\n[ref_id=jegham2025] These results suggest that while adaptive routing optimizes computational resources by tailoring\ninference depth to task complexity, it also amplifies the environmental footprint of cognitively\ndemanding prompts.\n---\n[ref_id=jegham2025] This finding underscores the growing importance of prompt-level efficiency\nanalysis for next-generation LLMs that blend lightweight and high-reasoning architectures within a\nunified system.\n---\n[ref_id=jegham2025] 11\n---\n[ref_id=jegham2025] Figure 5: (Top Left) Per-query and daily energy consumption of GPT-4o. (Top Right) Estimated total\nannual energy usage of GPT-4o in 2025. (Bottom Left) The estimated 2025 annual water consumption\nof GPT-4o. (Bottom Right) The estimated 2025 annual carbon emissions of GPT-4o.\n5.3 Validation Against Public Disclosures\nPublic disclosures of inference-level energy and carbon data remain limited, but a few recent state-\nments provide useful reference points for cross-validation. In June 2025, OpenAI CEO Sam Altman\nreported that the default ChatGPT model consumed approximately 0.34 Wh per query [68]. Knowing\nthat GPT-4o was the default deployment at that time, this estimate likely corresponds to GPT-4o-level\ninference. Our framework estimates 0.42 Wh ( \u00b10.13 Wh) for a short GPT-4o prompt (0.37 Wh\nwithout datacenter overhead), within 19% of Altman\u2019s figure. Similarly, the results for Mistral Large 2\nalign closely with Mistral\u2019s published life-cycle assessment (LCA) report [69], which cites approxi-\nmately 1.14 gCO2e per 400-token query. Our corresponding estimate for 300 tokens (0.82 gCO2e,\n\u00b10.10 gCO2e) scales to roughly 1.09 gCO2e when normalized to 400 tokens, showcasing alignment\nwithin one standard deviation. Together, these alignments between independent disclosures and our\nmodeled results suggest that the framework reproduces realistic operational conditions for modern\nLLM inference.\n6 GPT-4o Environmental Impact Case Study\n6.1 Energy Cost of a Single GPT-4o User Session\nBased on Reuters [70], the average ChatGPT user sends approximately eight queries per day as of\nApril 2025. Based on this, we quantify the per-user energy impact of GPT-4o interactions against\nfamiliar digital activities as presented in Figure 5. A single short GPT-4o query consumes 0.42\nWh (\u00b10.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.\nScaling to a typical daily usage pattern, the cumulative energy reaches 3.73 Wh (\u00b10.358 Wh). For\nmedium-length queries, this increases to 9.71 Wh (\u00b11.106 Wh). These results highlight that even\nlimited daily engagement with GPT-4o can impose an energy cost comparable to charging two\nsmartphones to full capacity (approximately 10 Wh), illustrating the tangible environmental footprint\nof conversational AI. While the individual per-query costs appear modest, their aggregation across\nmillions of users introduces a rapidly compounding, largely invisible load on the environment.\n6.2 Estimated 2025 Annual Energy Consumption of GPT-4o Inference\nTo estimate the annual energy demand of GPT-4o in 2025, we consider a baseline of 1 billion queries\nper day across all ChatGPT deployments, a figure reported by OpenAI as of December 2024 [71].\nGiven GPT-4o\u2019s status as the default model, we conservatively attribute 700 million daily queries to\n10\n---\n[ref_id=jegham2025] Page 10\n---\n[ref_id=jegham2025] Figure 5: (Top Left) Per-query and daily energy consumption of GPT-4o.\n---\n[ref_id=jegham2025] (Top Right) Estimated total\nannual energy usage of GPT-4o in 2025.\n---\n[ref_id=jegham2025] (Bottom Left) The estimated 2025 annual water consumption\nof GPT-4o.\n---\n[ref_id=jegham2025] (Bottom Right) The estimated 2025 annual carbon emissions of GPT-4o.\n---\n[ref_id=jegham2025] 5.3 Validation Against Public Disclosures\nPublic disclosures of inference-level energy and carbon data remain limited, but a few recent state-\nments provide useful reference points for cross-validation.\n---\n[ref_id=jegham2025] In June 2025, OpenAI CEO Sam Altman\nreported that the default ChatGPT model consumed approximately 0.34 Wh per query [68].\n---\n[ref_id=jegham2025] Knowing\nthat GPT-4o was the default deployment at that time, this estimate likely corresponds to GPT-4o-level\ninference.\n---\n[ref_id=jegham2025] Our framework estimates 0.42 Wh ( \u00b10.13 Wh) for a short GPT-4o prompt (0.37 Wh\nwithout datacenter overhead), within 19% of Altman\u2019s figure.\n---\n[ref_id=jegham2025] Similarly, the results for Mistral Large 2\nalign closely with Mistral\u2019s published life-cycle assessment (LCA) report [69], which cites approxi-\nmately 1.14 gCO2e per 400-token query.\n---\n[ref_id=jegham2025] Our corresponding estimate for 300 tokens (0.82 gCO2e,\n\u00b10.10 gCO2e) scales to roughly 1.09 gCO2e when normalized to 400 tokens, showcasing alignment\nwithin one standard deviation.\n---\n[ref_id=jegham2025] Together, these alignments between independent disclosures and our\nmodeled results suggest that the framework reproduces realistic operational conditions for modern\nLLM inference.\n---\n[ref_id=jegham2025] 6 GPT-4o Environmental Impact Case Study\n6.1 Energy Cost of a Single GPT-4o User Session\nBased on Reuters [70], the average ChatGPT user sends approximately eight queries per day as of\nApril 2025.\n---\n[ref_id=jegham2025] Based on this, we quantify the per-user energy impact of GPT-4o interactions against\nfamiliar digital activities as presented in Figure 5.\n---\n[ref_id=jegham2025] A single short GPT-4o query consumes 0.42\nWh (\u00b10.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.\n---\n[ref_id=jegham2025] Scaling to a typical daily usage pattern, the cumulative energy reaches 3.73 Wh (\u00b10.358 Wh).\n---\n[ref_id=jegham2025] For\nmedium-length queries, this increases to 9.71 Wh (\u00b11.106 Wh).\n---\n[ref_id=jegham2025] These results highlight that even\nlimited daily engagement with GPT-4o can impose an energy cost comparable to charging two\nsmartphones to full capacity (approximately 10 Wh), illustrating the tangible environmental footprint\nof conversational AI.\n---\n[ref_id=jegham2025] While the individual per-query costs appear modest, their aggregation across\nmillions of users introduces a rapidly compounding, largely invisible load on the environment.\n---\n[ref_id=jegham2025] 6.2 Estimated 2025 Annual Energy Consumption of GPT-4o Inference\nTo estimate the annual energy demand of GPT-4o in 2025, we consider a baseline of 1 billion queries\nper day across all ChatGPT deployments, a figure reported by OpenAI as of December 2024 [71].\n---\n[ref_id=jegham2025] Given GPT-4o\u2019s status as the default model, we conservatively attribute 700 million daily queries to\n10\n---\n[ref_id=jegham2025] Training\nGPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric\ntons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for\ncooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool.\n---\n[ref_id=jegham2025] How Hungry is AI? Benchmarking Energy, Water, and\nCarbon Footprint of LLM Inference\nNidhal Jegham1,2\nnidhal.jegham@uri.edu\nMarwan Abdelatti3\nmabdelat@providence.edu\nChan Young Koh1\nckoh04@uri.edu\nLassad Elmoubarki2\nlassad.elmoubarki@tbs.rnu.tn\nAbdeltawab Hendawi1\u2217\nhendawi@uri.edu\n1 University of Rhode Island 2 University of Tunis 3 Providence College\nLive Dashboard: Power BI Dashboard\nAbstract\nThis paper introduces an infrastructure-aware benchmarking framework for quanti-\nfying the environmental footprint of LLM inference across 30 state-of-the-art mod-\nels in commercial datacenters. The framework combines public API performance\ndata with company-specific environmental multipliers and statistical inference of\nhardware configurations. We additionally utilize cross-efficiency Data Envelop-\nment Analysis (DEA) to rank models by performance relative to environmental cost\nand provide a dynamically updated dashboard that visualizes model-level energy,\nwater, and carbon metrics. Results show the most energy-intensive models exceed\n29 Wh per long prompt, over 65 \u00d7 the most efficient systems. Even a 0.42 Wh\nshort query, when scaled to 700M queries/day, aggregates to annual electricity\ncomparable to 35,000 U.S. homes, evaporative freshwater equal to the annual drink-\ning needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest\nto offset. These findings highlight a growing paradox: as AI becomes cheaper\nand faster, global adoption drives disproportionate resource consumption. Our\nmethodology offers a standardized, empirically grounded basis for sustainability\nbenchmarking and accountability in AI deployment.\n1 Introduction\nLarge language models (LLMs) have moved beyond research labs and are now embedded in search\nengines, virtual assistants, education platforms, and enterprise tools [1, 2, 3, 4]. Models like GPT-4o\n[5] and Claude-3.7 Sonnet [6] represent state-of-the-art systems, while open-source alternatives such\nas LLaMA-3 [7] and DeepSeek-V3 [8] reflect growing accessibility and experimentation. On top of\nthat, the emergence of reasoning models such as DeepSeek-R1 [9], o1 [10], and o3-mini [11] marks\na shift toward multi-step logic and chain-of-thought reasoning.\nHowever, the advancement of LLMs does involve shortcomings in environmental aspects. Training\nGPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric\ntons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for\ncooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool. Yet while training\nhas been the focus of sustainability discussions, inference is emerging as the primary contributor to\nenvironmental costs. In contrast to training, which is conducted once or at intervals, inference occurs\nconsistently and on a large scale. Recent estimates suggest inference can account for up to 90% of a\nmodel\u2019s total lifecycle energy use [14, 15].\n\u2217Corresponding author.\narXiv:2505.09598v6  [cs.CY]  24 Nov 2025\n\n---\n\nNow answer the following question based ONLY on the context above.\n\nQuestion: When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?\n\nAdditional info (JSON): {\"answer_unit\": \"requests\", \"question_id\": \"q321\"}\n\nIMPORTANT: The \"answer_unit\" field specifies the expected format/unit for answer_value.\n- If answer_unit is a unit (e.g., \"kW\", \"USD\"), express answer_value as a number in that unit (no unit name).\n- If answer_unit is \"is_blank\", answer_value should be the exact identifier/name from context that answers the question.\n- If the answer is a numeric range, format as [lower,upper].\n\nReturn STRICT JSON with these keys in order:\n- explanation          (1-3 sentences explaining how context supports the answer and how you applied answer_unit; or \"is_blank\")\n- answer               (short natural-language response)\n- answer_value         (the value matching the expected format; or \"is_blank\")\n- ref_id               (list of document ids from context used as evidence; or \"is_blank\")\n- ref_url              (list of URLs for the cited documents; or \"is_blank\")\n- supporting_materials (verbatim quote, table reference, or figure reference from the cited document; or \"is_blank\")\n\nJSON Answer:",
    "retrieved_snippets": [
      {
        "node_id": "li2025b:sec2:p2:s44",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Additionally, GPT-3 needs to \u201cdrink\u201d (i.e., consume) a500ml bottle of waterfor roughly 10 \u2013 50\nmedium-length responses, depending on when and where it is deployed.",
        "score": 0.7704,
        "rank": 1,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec2:p2",
          "section_id": "li2025b:sec2",
          "sentence_index": 44
        }
      },
      {
        "node_id": "li2025b:sec2:p2",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "2027, which is more than the total annual water withdrawal of 4 \u2013 6 Denmark or half of the United King-\ndom.3 Simultaneously, a total of 0.38 \u2013 0.60 billion cubic meters of water will be evaporated and considered\n\u201cconsumption\u201d due to the global AI demand in 2027. Moreover, these global estimates will be exceeded by\nthe total water withdrawal and consumption attributed to AI in the U.S. alone in 2028 if the projection in [1]\ncomes to fruition.\nDespite its profound environmental and societal impact, the increasing water footprint of AI has received\ndisproportionately less attention from the AI community as well as the general public. For example, while\nthe scope-2 carbon emissions are routinely included as part of AI model cards, even scope-1 direct water\nusage (either withdrawal or consumption) is missing, let alone scope-2 water usage. This may impede inno-\nvations to enable water sustainability and build truly sustainable AI. Crucially, water and carbon footprints\nare complementary to, not substitutable of, each other for understanding the environmental impacts. In-\ndeed, optimizing for carbon efficiency does not necessarily result in, and may even worsen, water efficiency,\nwhich varies with the fuel mixes for electricity generation and outside weather in a unique way [1,9].\nTo ensure that the growth in AI does not exacerbate the global water stresses or outweigh the environ-\nmental benefits it provides, it is a critical time to uncover and address AI\u2019s hidden water footprint amid the\nincreasingly severe freshwater scarcity crisis, worsened extended droughts, and quickly aging public water\ninfrastructure. The urgency can also be reflected in part by the recent commitment to \u201cWater Positive by\n2030\u201d from industry leaders, including Google [4] and Microsoft [6], and by the inclusion of water foot-\nprint as a key metric into the world\u2019s first international standard on sustainable AI to be published by the\nISO/IEC [10].\nIn this paper, we advocate for a holistic approach to sustainable AI that extends beyond the carbon foot-\nprint to also address the water footprint. Specifically, we present a principled methodology to estimate AI\u2019s\ntotal water footprint, including both operational water and embodied water. By taking the GPT-3 model\nwith 175 billion parameters as an example [11], we show that training GPT-3 in Microsoft\u2019s U.S. data cen-\nters can consume a total of 5.4 million liters of water, including700,000 liters of scope-1 on-site water con-\nsumption. Additionally, GPT-3 needs to \u201cdrink\u201d (i.e., consume) a500ml bottle of waterfor roughly 10 \u2013 50\nmedium-length responses, depending on when and where it is deployed.\nNext, we show that WUE (Water Usage Effectiveness, a measure of water efficiency) varies both spatially\nand temporally, suggesting that judiciously deciding \u201cwhen\u201d and \u201cwhere\u201d to train a large AI model can\nsignificantly cut the water footprint. We also emphasize the need for increasing transparency of AI\u2019 water\nfootprint, including disclosing more information about operational data and keeping users informed of the\nruntime water efficiency. Finally, we highlight the necessity of holistically addressing water footprint along\nwith carbon footprint to enable truly sustainable AI \u2014 the water footprint of AI can no longer stay under the\nradar.\n2 Background\n2.1 Water Withdrawal vs. Water Consumption\nThere are two related but different concepts \u2014 water withdrawal and water consumption, both of which are\nimportant for understanding the impacts on water stress and availability [12,13].\n\u2022 Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either tem-\nporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding\nwater used for hydroelectricity generation) [12]. As water is a finite shared resource, water withdrawal\nindicates the level of competition as well as dependence on water resources among different sectors.\n\u2022 Water consumption:It is defined as \u201cwater withdrawal minus water discharge\u201d, and means the amount\nof water \u201cevaporated, transpired, incorporated into products or crops, or otherwise removed from the im-\nmediate water environment\u201d [13]. Water consumption reflects the impact on downstream water availability\nand is crucial for assessing watershed-level scarcity [12].\nThese two types of water usage correspond to two different water footprints, i.e., water withdrawal foot-\n3The scope definition of water usage [8] is in line with that of carbon emissions and is discussed in Section 2.2. Our scope-2 water\nwithdrawal (and consumption when applicable) is for location-based electricity generation throughout the paper. Large data centers\noften adopt sustainability programs (e.g., renewable purchasing agreements) to offset their location-based electricity usage and thus\nmay have lower market-based carbon and water footprints.\n2",
        "score": 0.7704,
        "rank": 1,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "page": 2,
          "paragraph_index": 2,
          "section_id": "li2025b:sec2",
          "section_index": 2
        }
      },
      {
        "node_id": "li2025b:sec6:p17",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "0.1 0.3 0.5 0.7 0.9\nCarbon (kg/kWh)\n0\n4\n8\n12\n16Water (L/kWh)\nAKMS\nHIOA\nMROE\nNWPP\nNYUP\nAZNM\n(a) Carbon/water efficiency\nMON TUE WED THU FRI\n1.5\n1.8\n2.1\n2.4\n2.7\n3.0Water (L/kWh)\n0.20\n0.25\n0.30\n0.35\n0.40\nCarbon (kg/kWh) (b) Hourly carbon/water efficiency\nMON TUE WED THU FRI\n0\n20\n40\n60Percentage\nCoal\nNatural Gas\nNuclear\nOil\nWind\nSolar\nHydro\nOther (c) Hourly energy fuel mixes\nFigure 2: (a) The U.S. eGRID-level scope-2 water consumption intensity factor vs. carbon emission rate [8, 33].\nThe dashed line represents a linear regression model, showing that the eGRID-level scope-2 carbon emission and water\nconsumption efficiencies are not aligned.(b) A 5-day snapshot of scope-2 carbon emission rate and water consumption\nintensity in Virginia, starting from April 4, 2022. The values are calculated based on the fuel mixes, carbon emission\nrate and water consumption intensity for each fuel type [8,20,33]. The scope-2 carbon and water efficiencies only have\na weak Pearson correlation coefficient of 0.06 in Virginia.(c) A 5-day snapshot of energy fuel mixes serving Virginia,\nstarting from April 4, 2022 [20].\nper-request server energy consumption of 0.004 kWh for our conversation task. The PUE, WUE, and EWIF\nare the same as those used for estimating the training water consumption. Our estimate of inference water\nconsumption for GPT-3 is on the conservative side, and the actual water consumption could be several times\nhigher. Specifically, when considering service level objectives (SLOs) for LLM response times in enterprise-\ngrade Nvidia DGX H100 systems for conversation tasks, the inference server energy consumption for a\nmuch smaller model (e.g., Llama-3-70B) is already approximately 0.010 kWh per medium-sized request\nwhen using a state-of-the-art LLM inference solution and accounting for non-GPU server overhead [30]. For\nthe Falcon-180B model, which is comparable in size to GPT-3-175B, the server energy consumption reaches\napproximately 0.016 kWh per medium-sized request [30]. Furthermore, we emphasize that Microsoft\u2019s\ndata centers already have some of the lowest on-site WUE in the industry. If the same model is deployed\nin a third-party colocation data center, the scope-1 direct water consumption is expected to be several times\nhigher. Additionally, our EWIF for the U.S. (3.14 L/kWh) is conservative and significantly lower than the\n4.35 L/kWh recently reported by [1].\nWhile no official information is available on the resource consumption, some subsequent models like\nGPT-4 could consume substantially more energy and water than GPT-3 for processing the same request\n[31, 32]. With continued efforts to reduce AI\u2019s computational demand and improve the overall water effi-\nciency, the water consumption per request may decrease in the future. However, the total water consump-\ntion is likely to continue rising due to the growing demand for AI services and the increasing scale of AI\napplications [1].\n4 Our Recommendations\nWe provide our recommendations to address AI\u2019s water footprint from the scheduling and policy perspec-\ntives, making future AI more environmentally sustainable.\n4.1 More Transparency and Comprehensive Reporting\nDespite its growing importance, AI\u2019s water footprint has received relatively less attention. For example,\nwhile AI model cards routinely include carbon emissions and serve as an important reporting framework\nfor understanding AI\u2019s environmental impacts, they currently omit information on AI\u2019s water consumption.\nThe lack of transparency may obstruct efforts to drive innovations that enhance water sustainability and\nsupport truly sustainable AI. As an initial step to raise awareness among end users about the water resource\nimpacts of their AI usage, we recommend tracking and reporting AI\u2019s water consumption in AI model cards\nand/or through cloud dashboards.\nMoreover, a comprehensive understanding and reporting of AI\u2019s scope-2 water consumption associ-\nated with electricity generation remain limited. Although data centers have increasingly adopted climate-\n6",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "page": 6,
          "paragraph_index": 17,
          "section_id": "li2025b:sec6",
          "section_index": 6
        }
      },
      {
        "node_id": "li2025b:sec6",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Page 6",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "page": 6,
          "section_index": 6
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s153",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "0.1 0.3 0.5 0.7 0.9\nCarbon (kg/kWh)\n0\n4\n8\n12\n16Water (L/kWh)\nAKMS\nHIOA\nMROE\nNWPP\nNYUP\nAZNM\n(a) Carbon/water efficiency\nMON TUE WED THU FRI\n1.5\n1.8\n2.1\n2.4\n2.7\n3.0Water (L/kWh)\n0.20\n0.25\n0.30\n0.35\n0.40\nCarbon (kg/kWh) (b) Hourly carbon/water efficiency\nMON TUE WED THU FRI\n0\n20\n40\n60Percentage\nCoal\nNatural Gas\nNuclear\nOil\nWind\nSolar\nHydro\nOther (c) Hourly energy fuel mixes\nFigure 2: (a) The U.S.",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 153
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s154",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "eGRID-level scope-2 water consumption intensity factor vs.",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 154
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s155",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "carbon emission rate [8, 33].",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 155
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s156",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "The dashed line represents a linear regression model, showing that the eGRID-level scope-2 carbon emission and water\nconsumption efficiencies are not aligned.(b) A 5-day snapshot of scope-2 carbon emission rate and water consumption\nintensity in Virginia, starting from April 4, 2022.",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 156
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s157",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "The values are calculated based on the fuel mixes, carbon emission\nrate and water consumption intensity for each fuel type [8,20,33].",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 157
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s158",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "The scope-2 carbon and water efficiencies only have\na weak Pearson correlation coefficient of 0.06 in Virginia.(c) A 5-day snapshot of energy fuel mixes serving Virginia,\nstarting from April 4, 2022 [20].",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 158
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s159",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "per-request server energy consumption of 0.004 kWh for our conversation task.",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 159
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s160",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "The PUE, WUE, and EWIF\nare the same as those used for estimating the training water consumption.",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 160
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s161",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Our estimate of inference water\nconsumption for GPT-3 is on the conservative side, and the actual water consumption could be several times\nhigher.",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 161
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s162",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Specifically, when considering service level objectives (SLOs) for LLM response times in enterprise-\ngrade Nvidia DGX H100 systems for conversation tasks, the inference server energy consumption for a\nmuch smaller model (e.g., Llama-3-70B) is already approximately 0.010 kWh per medium-sized request\nwhen using a state-of-the-art LLM inference solution and accounting for non-GPU server overhead [30].",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 162
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s163",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "For\nthe Falcon-180B model, which is comparable in size to GPT-3-175B, the server energy consumption reaches\napproximately 0.016 kWh per medium-sized request [30].",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 163
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s164",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Furthermore, we emphasize that Microsoft\u2019s\ndata centers already have some of the lowest on-site WUE in the industry.",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 164
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s165",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "If the same model is deployed\nin a third-party colocation data center, the scope-1 direct water consumption is expected to be several times\nhigher.",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 165
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s166",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Additionally, our EWIF for the U.S.",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 166
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s167",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "(3.14 L/kWh) is conservative and significantly lower than the\n4.35 L/kWh recently reported by [1].",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 167
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s168",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "While no official information is available on the resource consumption, some subsequent models like\nGPT-4 could consume substantially more energy and water than GPT-3 for processing the same request\n[31, 32].",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 168
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s169",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "With continued efforts to reduce AI\u2019s computational demand and improve the overall water effi-\nciency, the water consumption per request may decrease in the future.",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 169
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s170",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "However, the total water consump-\ntion is likely to continue rising due to the growing demand for AI services and the increasing scale of AI\napplications [1].",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 170
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s171",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "4 Our Recommendations\nWe provide our recommendations to address AI\u2019s water footprint from the scheduling and policy perspec-\ntives, making future AI more environmentally sustainable.",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 171
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s172",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "4.1 More Transparency and Comprehensive Reporting\nDespite its growing importance, AI\u2019s water footprint has received relatively less attention.",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 172
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s173",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "For example,\nwhile AI model cards routinely include carbon emissions and serve as an important reporting framework\nfor understanding AI\u2019s environmental impacts, they currently omit information on AI\u2019s water consumption.",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 173
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s174",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "The lack of transparency may obstruct efforts to drive innovations that enhance water sustainability and\nsupport truly sustainable AI.",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 174
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s175",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "As an initial step to raise awareness among end users about the water resource\nimpacts of their AI usage, we recommend tracking and reporting AI\u2019s water consumption in AI model cards\nand/or through cloud dashboards.",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 175
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s176",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Moreover, a comprehensive understanding and reporting of AI\u2019s scope-2 water consumption associ-\nated with electricity generation remain limited.",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 176
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s177",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Although data centers have increasingly adopted climate-\n6",
        "score": 0.7501,
        "rank": 2,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 177
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s161",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Our estimate of inference water\nconsumption for GPT-3 is on the conservative side, and the actual water consumption could be several times\nhigher.",
        "score": 0.7446,
        "rank": 3,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 161
        }
      },
      {
        "node_id": "li2025b:sec6:p17",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "0.1 0.3 0.5 0.7 0.9\nCarbon (kg/kWh)\n0\n4\n8\n12\n16Water (L/kWh)\nAKMS\nHIOA\nMROE\nNWPP\nNYUP\nAZNM\n(a) Carbon/water efficiency\nMON TUE WED THU FRI\n1.5\n1.8\n2.1\n2.4\n2.7\n3.0Water (L/kWh)\n0.20\n0.25\n0.30\n0.35\n0.40\nCarbon (kg/kWh) (b) Hourly carbon/water efficiency\nMON TUE WED THU FRI\n0\n20\n40\n60Percentage\nCoal\nNatural Gas\nNuclear\nOil\nWind\nSolar\nHydro\nOther (c) Hourly energy fuel mixes\nFigure 2: (a) The U.S. eGRID-level scope-2 water consumption intensity factor vs. carbon emission rate [8, 33].\nThe dashed line represents a linear regression model, showing that the eGRID-level scope-2 carbon emission and water\nconsumption efficiencies are not aligned.(b) A 5-day snapshot of scope-2 carbon emission rate and water consumption\nintensity in Virginia, starting from April 4, 2022. The values are calculated based on the fuel mixes, carbon emission\nrate and water consumption intensity for each fuel type [8,20,33]. The scope-2 carbon and water efficiencies only have\na weak Pearson correlation coefficient of 0.06 in Virginia.(c) A 5-day snapshot of energy fuel mixes serving Virginia,\nstarting from April 4, 2022 [20].\nper-request server energy consumption of 0.004 kWh for our conversation task. The PUE, WUE, and EWIF\nare the same as those used for estimating the training water consumption. Our estimate of inference water\nconsumption for GPT-3 is on the conservative side, and the actual water consumption could be several times\nhigher. Specifically, when considering service level objectives (SLOs) for LLM response times in enterprise-\ngrade Nvidia DGX H100 systems for conversation tasks, the inference server energy consumption for a\nmuch smaller model (e.g., Llama-3-70B) is already approximately 0.010 kWh per medium-sized request\nwhen using a state-of-the-art LLM inference solution and accounting for non-GPU server overhead [30]. For\nthe Falcon-180B model, which is comparable in size to GPT-3-175B, the server energy consumption reaches\napproximately 0.016 kWh per medium-sized request [30]. Furthermore, we emphasize that Microsoft\u2019s\ndata centers already have some of the lowest on-site WUE in the industry. If the same model is deployed\nin a third-party colocation data center, the scope-1 direct water consumption is expected to be several times\nhigher. Additionally, our EWIF for the U.S. (3.14 L/kWh) is conservative and significantly lower than the\n4.35 L/kWh recently reported by [1].\nWhile no official information is available on the resource consumption, some subsequent models like\nGPT-4 could consume substantially more energy and water than GPT-3 for processing the same request\n[31, 32]. With continued efforts to reduce AI\u2019s computational demand and improve the overall water effi-\nciency, the water consumption per request may decrease in the future. However, the total water consump-\ntion is likely to continue rising due to the growing demand for AI services and the increasing scale of AI\napplications [1].\n4 Our Recommendations\nWe provide our recommendations to address AI\u2019s water footprint from the scheduling and policy perspec-\ntives, making future AI more environmentally sustainable.\n4.1 More Transparency and Comprehensive Reporting\nDespite its growing importance, AI\u2019s water footprint has received relatively less attention. For example,\nwhile AI model cards routinely include carbon emissions and serve as an important reporting framework\nfor understanding AI\u2019s environmental impacts, they currently omit information on AI\u2019s water consumption.\nThe lack of transparency may obstruct efforts to drive innovations that enhance water sustainability and\nsupport truly sustainable AI. As an initial step to raise awareness among end users about the water resource\nimpacts of their AI usage, we recommend tracking and reporting AI\u2019s water consumption in AI model cards\nand/or through cloud dashboards.\nMoreover, a comprehensive understanding and reporting of AI\u2019s scope-2 water consumption associ-\nated with electricity generation remain limited. Although data centers have increasingly adopted climate-\n6",
        "score": 0.7446,
        "rank": 3,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "page": 6,
          "paragraph_index": 17,
          "section_id": "li2025b:sec6",
          "section_index": 6
        }
      },
      {
        "node_id": "li2025b:sec1:p1",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Making AI Less \u201cThirsty\u201d: Uncovering and Addressing the\nSecret Water Footprint of AI Models\nPengfei Li\nUC Riverside\nJianyi Yang\nUC Riverside\nMohammad A. Islam\nUT Arlington\nShaolei Ren1\nUC Riverside\nAbstract\nThe growing carbon footprint of artificial intelligence (AI) has been undergoing public scrutiny. Nonethe-\nless, the equally important water (withdrawal and consumption) footprint of AI has largely remained under\nthe radar. For example, training the GPT-3 language model in Microsoft\u2019s state-of-the-art U.S. data centers\ncan directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret. More\ncritically, the global AI demand is projected to account for 4.2 \u2013 6.6 billion cubic meters of water withdrawal\nin 2027, which is more than the total annual water withdrawal of 4 \u2013 6 Denmark or half of the United King-\ndom. This is concerning, as freshwater scarcity has become one of the most pressing challenges. To respond\nto the global water challenges, AI can, and also must, take social responsibility and lead by example by ad-\ndressing its own water footprint. In this paper, we provide a principled methodology to estimate the water\nfootprint of AI, and also discuss the unique spatial-temporal diversities of AI\u2019s runtime water efficiency.\nFinally, we highlight the necessity of holistically addressing water footprint along with carbon footprint to\nenable truly sustainable AI.\n1 Introduction\nArtificial intelligence (AI) has enabled remarkable breakthroughs in numerous areas of critical importance,\nincluding tackling global challenges such as climate change. On the other hand, many AI models, especially\nlarge generative ones like GPT-4, are trained and deployed on energy-hungry servers in warehouse-scale\ndata centers, accelerating the data center energy consumption at an unprecedented rate [1]. As a result,\nAI\u2019s carbon footprint has been undergoing scrutiny, driving the recent progress in AI carbon efficiency [2,3].\nHowever, AI\u2019s water footprint \u2014 many millions of liters of freshwater consumed for cooling the servers and\nfor electricity generation \u2014 has largely remained under the radar and keeps escalating. If not properly\naddressed, AI\u2019s water footprint can potentially become a major roadblock to sustainability and create social\nconflicts as freshwater resources suitable for human use are extremely limited and unevenly distributed.\nAs acknowledged in Google\u2019s sustainability report [4] and the recent U.S. data center energy report [1],\nthe expansion of AI products and services is a key driver of the rapid increase in data center water consump-\ntion. Even excluding the water usage in leased third-party colocation facilities, one technology company\u2019s\nself-owned data centers alone directly withdrew 29 billion liters and consumed (i.e., evaporated) more than\n23 billion liters of freshwater for on-site cooling in 2023, nearly 80% of which was potable water [4].2 This\namount of annual water consumption even rivals that of a major household-name beverage company [5].\nImportantly, the company\u2019s data center water consumption increased by\u223c20% from 2021 to 2022 and by\n\u223c17% from 2022 to 2023 [4], and another technology company\u2019s data center water consumption saw\u223c34%\nand \u223c22% increases over the same periods, respectively [6]. Furthermore, according to the recent U.S. data\ncenter energy report, the total annual on-site water consumption by U.S. data centers in 2028 could double\nor even quadruple the 2023 level, reaching approximately 150 \u2013 280 billion liters and further stressing the\nwater infrastructures [1].\nAI represents the fastest expanding workloads in data centers [1, 4]. For example, a recent study sug-\ngests that the global AI could consume 85 \u2013 134 TWh of electricity in 2027 [7], whereas a more aggressive\nprojection by the recent U.S. data center energy report predicts that AI servers\u2019 electricity consumption in\nthe U.S. alone will surpass 150 \u2013 300 TWh in 2028 [1]. Even considering the lower estimate, the combined\nscope-1 and scope-2 water withdrawal of global AI is projected to reach 4.2 \u2013 6.6 billion cubic metersin\n1Corresponding author: Shaolei Ren (shaolei@ucr.edu), University of California, Riverside.\n2The detailed difference between water withdrawal and water consumption is presented in Section 2.1.\n1\narXiv:2304.03271v5  [cs.LG]  26 Mar 2025",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "page": 1,
          "paragraph_index": 1,
          "section_id": "li2025b:sec1",
          "section_index": 1
        }
      },
      {
        "node_id": "li2025b:sec1",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Page 1",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "page": 1,
          "section_index": 1
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s1",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Making AI Less \u201cThirsty\u201d: Uncovering and Addressing the\nSecret Water Footprint of AI Models\nPengfei Li\nUC Riverside\nJianyi Yang\nUC Riverside\nMohammad A.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 1
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s2",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Islam\nUT Arlington\nShaolei Ren1\nUC Riverside\nAbstract\nThe growing carbon footprint of artificial intelligence (AI) has been undergoing public scrutiny.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 2
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s3",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Nonethe-\nless, the equally important water (withdrawal and consumption) footprint of AI has largely remained under\nthe radar.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 3
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s4",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "For example, training the GPT-3 language model in Microsoft\u2019s state-of-the-art U.S.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 4
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s5",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "data centers\ncan directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 5
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s6",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "More\ncritically, the global AI demand is projected to account for 4.2 \u2013 6.6 billion cubic meters of water withdrawal\nin 2027, which is more than the total annual water withdrawal of 4 \u2013 6 Denmark or half of the United King-\ndom.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 6
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s7",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "This is concerning, as freshwater scarcity has become one of the most pressing challenges.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 7
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s8",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "To respond\nto the global water challenges, AI can, and also must, take social responsibility and lead by example by ad-\ndressing its own water footprint.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 8
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s9",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "In this paper, we provide a principled methodology to estimate the water\nfootprint of AI, and also discuss the unique spatial-temporal diversities of AI\u2019s runtime water efficiency.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 9
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s10",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Finally, we highlight the necessity of holistically addressing water footprint along with carbon footprint to\nenable truly sustainable AI.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 10
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s11",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "1 Introduction\nArtificial intelligence (AI) has enabled remarkable breakthroughs in numerous areas of critical importance,\nincluding tackling global challenges such as climate change.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 11
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s12",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "On the other hand, many AI models, especially\nlarge generative ones like GPT-4, are trained and deployed on energy-hungry servers in warehouse-scale\ndata centers, accelerating the data center energy consumption at an unprecedented rate [1].",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 12
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s13",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "As a result,\nAI\u2019s carbon footprint has been undergoing scrutiny, driving the recent progress in AI carbon efficiency [2,3].",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 13
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s14",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "However, AI\u2019s water footprint \u2014 many millions of liters of freshwater consumed for cooling the servers and\nfor electricity generation \u2014 has largely remained under the radar and keeps escalating.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 14
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s15",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "If not properly\naddressed, AI\u2019s water footprint can potentially become a major roadblock to sustainability and create social\nconflicts as freshwater resources suitable for human use are extremely limited and unevenly distributed.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 15
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s16",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "As acknowledged in Google\u2019s sustainability report [4] and the recent U.S.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 16
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s17",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "data center energy report [1],\nthe expansion of AI products and services is a key driver of the rapid increase in data center water consump-\ntion.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 17
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s18",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Even excluding the water usage in leased third-party colocation facilities, one technology company\u2019s\nself-owned data centers alone directly withdrew 29 billion liters and consumed (i.e., evaporated) more than\n23 billion liters of freshwater for on-site cooling in 2023, nearly 80% of which was potable water [4].2 This\namount of annual water consumption even rivals that of a major household-name beverage company [5].",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 18
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s19",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Importantly, the company\u2019s data center water consumption increased by\u223c20% from 2021 to 2022 and by\n\u223c17% from 2022 to 2023 [4], and another technology company\u2019s data center water consumption saw\u223c34%\nand \u223c22% increases over the same periods, respectively [6].",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 19
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s20",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Furthermore, according to the recent U.S.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 20
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s21",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "data\ncenter energy report, the total annual on-site water consumption by U.S.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 21
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s22",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "data centers in 2028 could double\nor even quadruple the 2023 level, reaching approximately 150 \u2013 280 billion liters and further stressing the\nwater infrastructures [1].",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 22
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s23",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "AI represents the fastest expanding workloads in data centers [1, 4].",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 23
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s24",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "For example, a recent study sug-\ngests that the global AI could consume 85 \u2013 134 TWh of electricity in 2027 [7], whereas a more aggressive\nprojection by the recent U.S.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 24
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s25",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "data center energy report predicts that AI servers\u2019 electricity consumption in\nthe U.S.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 25
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s26",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "alone will surpass 150 \u2013 300 TWh in 2028 [1].",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 26
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s27",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Even considering the lower estimate, the combined\nscope-1 and scope-2 water withdrawal of global AI is projected to reach 4.2 \u2013 6.6 billion cubic metersin\n1Corresponding author: Shaolei Ren (shaolei@ucr.edu), University of California, Riverside.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 27
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s28",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "2The detailed difference between water withdrawal and water consumption is presented in Section 2.1.",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 28
        }
      },
      {
        "node_id": "li2025b:sec1:p1:s29",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "1\narXiv:2304.03271v5  [cs.LG]  26 Mar 2025",
        "score": 0.7383,
        "rank": 4,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec1:p1",
          "section_id": "li2025b:sec1",
          "sentence_index": 29
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s198",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Then, using GPT-\n3 as an example, we show that a large AI model can consume millions of liters of water for training.",
        "score": 0.7364,
        "rank": 5,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 198
        }
      },
      {
        "node_id": "li2025b:sec7:p18",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "conscious cooling system designs to minimize on-site water consumption [4,17,19], these efforts primarily\nfocus on scope-1 water usage while largely overlooking scope-2 impacts. Just as addressing scope-2 carbon\nemissions is important for mitigating climate change, it is equally crucial to address scope-2 water con-\nsumption to reduce AI\u2019s \u201ctrue water cost\u201d, as noted by the recent U.S. data center energy report [1]. To\nbetter reflect the true impacts of data centers on water resources, some technology companies such as Meta\nhave begun to include scope-2 water consumption in their sustainability reports [18]. We recommend the\nreporting of scope-2 water consumption as a standard practice. This approach makes the off-site water\nconsumption visible to AI model developers as well as end users and can unlock new opportunities for\ndemand-side flexibility, thereby alleviating the overall strain on water resources.\nFinally, despite the enormous scope-3 supply-chain water footprint [23], there is limited data available\nfor embodied water usage by chip manufacturing. We recommend further research on scope-3 water con-\nsumption to achieve a comprehensive understanding of AI\u2019s overall water footprint and to foster corporate\nwater stewardship.\n4.2 \u201cWhen\u201d and \u201cWhere\u201d Matter\nJudiciously deciding \u201cwhen\u201d and \u201cwhere\u201d to train a large AI model can significantly affect the water foot-\nprint. The water efficiency exhibits a spatial-temporal diversity \u2014 on-site water efficiency changes due to\nvariations of outside weather conditions, and off-site water efficiency changes due to variations of the grid\u2019s\nenergy fuel mixes to meet time-varying demands (Figure 2). Therefore, we can dynamically schedule AI\ntraining and inference in a water-wise manner to cut the water footprint. For example, we may schedule\nAI training at midnight and/or in a data center with better water efficiency. Likewise, if informed of the\nreal-time water efficiency, some water-conscious users may prefer to use AI inference during water-efficient\nhours and/or in water-efficient data centers, which can reduce AI\u2019s water footprint by enabling demand-side\nflexibility.\n4.3 \u201cFollow the Sun\u201d or \u201cUnfollow the Sun\u201d\nTo cut the carbon footprint, it is preferable to \u201cfollow the sun\u201d when solar energy is more abundant. Nonethe-\nless, to cut the water footprint, it may be more appealing to \u201cunfollow the sun\u201d to avoid high-temperature\nhours of a day when WUE is high. This conflict can also be shown in Figure 2(a) and Figure 2(b), where we\nsee misalignment between the scope-2 water consumption intensity factor and carbon emission rate: mini-\nmizing one footprint might increase the other footprint. This observation further corroborates the previous\nfinding that the environmental impacts of carbon and water footprints are not substitutable [1,9]. Therefore,\nto judiciously achieve a balance between \u201cfollow the sun\u201d for carbon efficiency and \u201cunfollow the sun\u201d for\nwater efficiency, we need to reconcile the potential water-carbon conflicts by using holistic approaches that\nare both carbon-efficient and water-wise.\n5 Conclusion\nIn this paper, we uncover AI\u2019s water usage as a critical concern for socially responsible and environmentally\nsustainable AI. We present a principled methodology to estimate AI\u2019s water footprint. Then, using GPT-\n3 as an example, we show that a large AI model can consume millions of liters of water for training. We\nalso discuss that the scope-1 and scope-2 water efficiencies vary spatially and temporally \u2014 judiciously\ndeciding \u201cwhen\u201d and \u201cwhere\u201d to run a large AI model can significantly cut the water footprint. In addition,\nwe recommend increased transparency and comprehensive reporting of AI\u2019s water footprint, and highlight\nthe necessity of holistically addressing the water footprint along with the carbon footprint to build truly\nsustainable AI.\nAI\u2019s water footprint can no longer stay under the radar and must be addressed as a priority as part of the collective\nefforts to combat global water challenges.\nReferences\n[1] Arman Shehabi, Sarah J. Smith, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakar Siddik, Billie\nHolecek, Jonathan Koomey, Eric Masanet, and Dale Sartor. 2024 United States data center energy usage\nreport. Lawrence Berkeley National Laboratory LBNL-2001637, December 2024.\n7",
        "score": 0.7364,
        "rank": 5,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "page": 7,
          "paragraph_index": 18,
          "section_id": "li2025b:sec7",
          "section_index": 7
        }
      },
      {
        "node_id": "li2025b:sec6:p17:s169",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "With continued efforts to reduce AI\u2019s computational demand and improve the overall water effi-\nciency, the water consumption per request may decrease in the future.",
        "score": 0.734,
        "rank": 6,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec6:p17",
          "section_id": "li2025b:sec6",
          "sentence_index": 169
        }
      },
      {
        "node_id": "li2025b:sec6:p17",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "0.1 0.3 0.5 0.7 0.9\nCarbon (kg/kWh)\n0\n4\n8\n12\n16Water (L/kWh)\nAKMS\nHIOA\nMROE\nNWPP\nNYUP\nAZNM\n(a) Carbon/water efficiency\nMON TUE WED THU FRI\n1.5\n1.8\n2.1\n2.4\n2.7\n3.0Water (L/kWh)\n0.20\n0.25\n0.30\n0.35\n0.40\nCarbon (kg/kWh) (b) Hourly carbon/water efficiency\nMON TUE WED THU FRI\n0\n20\n40\n60Percentage\nCoal\nNatural Gas\nNuclear\nOil\nWind\nSolar\nHydro\nOther (c) Hourly energy fuel mixes\nFigure 2: (a) The U.S. eGRID-level scope-2 water consumption intensity factor vs. carbon emission rate [8, 33].\nThe dashed line represents a linear regression model, showing that the eGRID-level scope-2 carbon emission and water\nconsumption efficiencies are not aligned.(b) A 5-day snapshot of scope-2 carbon emission rate and water consumption\nintensity in Virginia, starting from April 4, 2022. The values are calculated based on the fuel mixes, carbon emission\nrate and water consumption intensity for each fuel type [8,20,33]. The scope-2 carbon and water efficiencies only have\na weak Pearson correlation coefficient of 0.06 in Virginia.(c) A 5-day snapshot of energy fuel mixes serving Virginia,\nstarting from April 4, 2022 [20].\nper-request server energy consumption of 0.004 kWh for our conversation task. The PUE, WUE, and EWIF\nare the same as those used for estimating the training water consumption. Our estimate of inference water\nconsumption for GPT-3 is on the conservative side, and the actual water consumption could be several times\nhigher. Specifically, when considering service level objectives (SLOs) for LLM response times in enterprise-\ngrade Nvidia DGX H100 systems for conversation tasks, the inference server energy consumption for a\nmuch smaller model (e.g., Llama-3-70B) is already approximately 0.010 kWh per medium-sized request\nwhen using a state-of-the-art LLM inference solution and accounting for non-GPU server overhead [30]. For\nthe Falcon-180B model, which is comparable in size to GPT-3-175B, the server energy consumption reaches\napproximately 0.016 kWh per medium-sized request [30]. Furthermore, we emphasize that Microsoft\u2019s\ndata centers already have some of the lowest on-site WUE in the industry. If the same model is deployed\nin a third-party colocation data center, the scope-1 direct water consumption is expected to be several times\nhigher. Additionally, our EWIF for the U.S. (3.14 L/kWh) is conservative and significantly lower than the\n4.35 L/kWh recently reported by [1].\nWhile no official information is available on the resource consumption, some subsequent models like\nGPT-4 could consume substantially more energy and water than GPT-3 for processing the same request\n[31, 32]. With continued efforts to reduce AI\u2019s computational demand and improve the overall water effi-\nciency, the water consumption per request may decrease in the future. However, the total water consump-\ntion is likely to continue rising due to the growing demand for AI services and the increasing scale of AI\napplications [1].\n4 Our Recommendations\nWe provide our recommendations to address AI\u2019s water footprint from the scheduling and policy perspec-\ntives, making future AI more environmentally sustainable.\n4.1 More Transparency and Comprehensive Reporting\nDespite its growing importance, AI\u2019s water footprint has received relatively less attention. For example,\nwhile AI model cards routinely include carbon emissions and serve as an important reporting framework\nfor understanding AI\u2019s environmental impacts, they currently omit information on AI\u2019s water consumption.\nThe lack of transparency may obstruct efforts to drive innovations that enhance water sustainability and\nsupport truly sustainable AI. As an initial step to raise awareness among end users about the water resource\nimpacts of their AI usage, we recommend tracking and reporting AI\u2019s water consumption in AI model cards\nand/or through cloud dashboards.\nMoreover, a comprehensive understanding and reporting of AI\u2019s scope-2 water consumption associ-\nated with electricity generation remain limited. Although data centers have increasingly adopted climate-\n6",
        "score": 0.734,
        "rank": 6,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "page": 6,
          "paragraph_index": 17,
          "section_id": "li2025b:sec6",
          "section_index": 6
        }
      },
      {
        "node_id": "li2025b:sec7:p18",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "conscious cooling system designs to minimize on-site water consumption [4,17,19], these efforts primarily\nfocus on scope-1 water usage while largely overlooking scope-2 impacts. Just as addressing scope-2 carbon\nemissions is important for mitigating climate change, it is equally crucial to address scope-2 water con-\nsumption to reduce AI\u2019s \u201ctrue water cost\u201d, as noted by the recent U.S. data center energy report [1]. To\nbetter reflect the true impacts of data centers on water resources, some technology companies such as Meta\nhave begun to include scope-2 water consumption in their sustainability reports [18]. We recommend the\nreporting of scope-2 water consumption as a standard practice. This approach makes the off-site water\nconsumption visible to AI model developers as well as end users and can unlock new opportunities for\ndemand-side flexibility, thereby alleviating the overall strain on water resources.\nFinally, despite the enormous scope-3 supply-chain water footprint [23], there is limited data available\nfor embodied water usage by chip manufacturing. We recommend further research on scope-3 water con-\nsumption to achieve a comprehensive understanding of AI\u2019s overall water footprint and to foster corporate\nwater stewardship.\n4.2 \u201cWhen\u201d and \u201cWhere\u201d Matter\nJudiciously deciding \u201cwhen\u201d and \u201cwhere\u201d to train a large AI model can significantly affect the water foot-\nprint. The water efficiency exhibits a spatial-temporal diversity \u2014 on-site water efficiency changes due to\nvariations of outside weather conditions, and off-site water efficiency changes due to variations of the grid\u2019s\nenergy fuel mixes to meet time-varying demands (Figure 2). Therefore, we can dynamically schedule AI\ntraining and inference in a water-wise manner to cut the water footprint. For example, we may schedule\nAI training at midnight and/or in a data center with better water efficiency. Likewise, if informed of the\nreal-time water efficiency, some water-conscious users may prefer to use AI inference during water-efficient\nhours and/or in water-efficient data centers, which can reduce AI\u2019s water footprint by enabling demand-side\nflexibility.\n4.3 \u201cFollow the Sun\u201d or \u201cUnfollow the Sun\u201d\nTo cut the carbon footprint, it is preferable to \u201cfollow the sun\u201d when solar energy is more abundant. Nonethe-\nless, to cut the water footprint, it may be more appealing to \u201cunfollow the sun\u201d to avoid high-temperature\nhours of a day when WUE is high. This conflict can also be shown in Figure 2(a) and Figure 2(b), where we\nsee misalignment between the scope-2 water consumption intensity factor and carbon emission rate: mini-\nmizing one footprint might increase the other footprint. This observation further corroborates the previous\nfinding that the environmental impacts of carbon and water footprints are not substitutable [1,9]. Therefore,\nto judiciously achieve a balance between \u201cfollow the sun\u201d for carbon efficiency and \u201cunfollow the sun\u201d for\nwater efficiency, we need to reconcile the potential water-carbon conflicts by using holistic approaches that\nare both carbon-efficient and water-wise.\n5 Conclusion\nIn this paper, we uncover AI\u2019s water usage as a critical concern for socially responsible and environmentally\nsustainable AI. We present a principled methodology to estimate AI\u2019s water footprint. Then, using GPT-\n3 as an example, we show that a large AI model can consume millions of liters of water for training. We\nalso discuss that the scope-1 and scope-2 water efficiencies vary spatially and temporally \u2014 judiciously\ndeciding \u201cwhen\u201d and \u201cwhere\u201d to run a large AI model can significantly cut the water footprint. In addition,\nwe recommend increased transparency and comprehensive reporting of AI\u2019s water footprint, and highlight\nthe necessity of holistically addressing the water footprint along with the carbon footprint to build truly\nsustainable AI.\nAI\u2019s water footprint can no longer stay under the radar and must be addressed as a priority as part of the collective\nefforts to combat global water challenges.\nReferences\n[1] Arman Shehabi, Sarah J. Smith, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakar Siddik, Billie\nHolecek, Jonathan Koomey, Eric Masanet, and Dale Sartor. 2024 United States data center energy usage\nreport. Lawrence Berkeley National Laboratory LBNL-2001637, December 2024.\n7",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "page": 7,
          "paragraph_index": 18,
          "section_id": "li2025b:sec7",
          "section_index": 7
        }
      },
      {
        "node_id": "li2025b:sec7",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Page 7",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "page": 7,
          "section_index": 7
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s178",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "conscious cooling system designs to minimize on-site water consumption [4,17,19], these efforts primarily\nfocus on scope-1 water usage while largely overlooking scope-2 impacts.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 178
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s179",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Just as addressing scope-2 carbon\nemissions is important for mitigating climate change, it is equally crucial to address scope-2 water con-\nsumption to reduce AI\u2019s \u201ctrue water cost\u201d, as noted by the recent U.S.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 179
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s180",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "data center energy report [1].",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 180
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s181",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "To\nbetter reflect the true impacts of data centers on water resources, some technology companies such as Meta\nhave begun to include scope-2 water consumption in their sustainability reports [18].",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 181
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s182",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "We recommend the\nreporting of scope-2 water consumption as a standard practice.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 182
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s183",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "This approach makes the off-site water\nconsumption visible to AI model developers as well as end users and can unlock new opportunities for\ndemand-side flexibility, thereby alleviating the overall strain on water resources.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 183
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s184",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Finally, despite the enormous scope-3 supply-chain water footprint [23], there is limited data available\nfor embodied water usage by chip manufacturing.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 184
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s185",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "We recommend further research on scope-3 water con-\nsumption to achieve a comprehensive understanding of AI\u2019s overall water footprint and to foster corporate\nwater stewardship.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 185
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s186",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "4.2 \u201cWhen\u201d and \u201cWhere\u201d Matter\nJudiciously deciding \u201cwhen\u201d and \u201cwhere\u201d to train a large AI model can significantly affect the water foot-\nprint.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 186
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s187",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "The water efficiency exhibits a spatial-temporal diversity \u2014 on-site water efficiency changes due to\nvariations of outside weather conditions, and off-site water efficiency changes due to variations of the grid\u2019s\nenergy fuel mixes to meet time-varying demands (Figure 2).",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 187
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s188",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Therefore, we can dynamically schedule AI\ntraining and inference in a water-wise manner to cut the water footprint.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 188
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s189",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "For example, we may schedule\nAI training at midnight and/or in a data center with better water efficiency.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 189
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s190",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Likewise, if informed of the\nreal-time water efficiency, some water-conscious users may prefer to use AI inference during water-efficient\nhours and/or in water-efficient data centers, which can reduce AI\u2019s water footprint by enabling demand-side\nflexibility.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 190
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s191",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "4.3 \u201cFollow the Sun\u201d or \u201cUnfollow the Sun\u201d\nTo cut the carbon footprint, it is preferable to \u201cfollow the sun\u201d when solar energy is more abundant.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 191
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s192",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Nonethe-\nless, to cut the water footprint, it may be more appealing to \u201cunfollow the sun\u201d to avoid high-temperature\nhours of a day when WUE is high.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 192
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s193",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "This conflict can also be shown in Figure 2(a) and Figure 2(b), where we\nsee misalignment between the scope-2 water consumption intensity factor and carbon emission rate: mini-\nmizing one footprint might increase the other footprint.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 193
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s194",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "This observation further corroborates the previous\nfinding that the environmental impacts of carbon and water footprints are not substitutable [1,9].",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 194
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s195",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Therefore,\nto judiciously achieve a balance between \u201cfollow the sun\u201d for carbon efficiency and \u201cunfollow the sun\u201d for\nwater efficiency, we need to reconcile the potential water-carbon conflicts by using holistic approaches that\nare both carbon-efficient and water-wise.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 195
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s196",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "5 Conclusion\nIn this paper, we uncover AI\u2019s water usage as a critical concern for socially responsible and environmentally\nsustainable AI.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 196
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s197",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "We present a principled methodology to estimate AI\u2019s water footprint.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 197
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s198",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Then, using GPT-\n3 as an example, we show that a large AI model can consume millions of liters of water for training.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 198
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s199",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "We\nalso discuss that the scope-1 and scope-2 water efficiencies vary spatially and temporally \u2014 judiciously\ndeciding \u201cwhen\u201d and \u201cwhere\u201d to run a large AI model can significantly cut the water footprint.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 199
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s200",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "In addition,\nwe recommend increased transparency and comprehensive reporting of AI\u2019s water footprint, and highlight\nthe necessity of holistically addressing the water footprint along with the carbon footprint to build truly\nsustainable AI.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 200
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s201",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "AI\u2019s water footprint can no longer stay under the radar and must be addressed as a priority as part of the collective\nefforts to combat global water challenges.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 201
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s202",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "References\n[1] Arman Shehabi, Sarah J.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 202
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s203",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Smith, Alex Hubbard, Alex Newkirk, Nuoa Lei, Md Abu Bakar Siddik, Billie\nHolecek, Jonathan Koomey, Eric Masanet, and Dale Sartor.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 203
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s204",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "2024 United States data center energy usage\nreport.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 204
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s205",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "Lawrence Berkeley National Laboratory LBNL-2001637, December 2024.",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 205
        }
      },
      {
        "node_id": "li2025b:sec7:p18:s206",
        "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
        "text": "7",
        "score": 0.727,
        "rank": 7,
        "metadata": {
          "document_id": "li2025b",
          "document_title": "Making AI Less 'Thirsty': Uncovering and Addressing the Secret Water Footprint of AI Models",
          "paragraph_id": "li2025b:sec7:p18",
          "section_id": "li2025b:sec7",
          "sentence_index": 206
        }
      },
      {
        "node_id": "jegham2025:sec11:p19",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "GPT-4o. To simulate real-world usage dynamics, we apply a monthly prompt growth rate of 20%\nfrom January to May 2025, reflecting the documented increase in ChatGPT\u2019s weekly active user\nbase from 300 million to 800 million between December 2024 and April 2025 [72]. This is followed\nby a decaying growth pattern from June to December, yielding a total of approximately 772 billion\nGPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].\nWithin these queries, we conservatively assume an 80%/20% split between short and medium-length\nprompts based on typical usage patterns. Scaling the per-query energy estimates accordingly, we find\nthat GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269\nMWh at maximum, as seen in Figure 5. These values exceed the total electricity consumption of\n35,000 U.S. residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even\n325 universities (390,650 MWh) annually.\n6.3 Estimated 2025 Annual Water Footprint of GPT-4o Inference\nAs showcased in Figure 5, we translate estimated cooling and infrastructure-related water usage into\nreal-world benchmarks. Based on scaled inference volumes, GPT-4o\u2019s annual water consumption is\nprojected to be between 1,334,991 kiloliters (kL) and 1,579,680 kL. These quantities are roughly\nequivalent to filling over 500 Olympic-sized pools or to supporting the annual drinking needs of 1.2\nmillion people. Importantly, this consumption refers to evaporated freshwater permanently removed\nfrom local ecosystems rather than recycled. GPT-4o alone is responsible for evaporating an amount\nof freshwater equivalent to the annual drinking needs of almost 1.2 million people.\n6.4 Estimated 2025 Annual Carbon Footprint of GPT-4o Inference\nWe further examine GPT-4o\u2019s environmental footprint through estimated carbon emissions from\nelectricity usage, as seen in Figure 5. Our projections indicate annual emissions of approximately\n138,125 tons of CO2e at minimum and 163,441 tons at maximum. These figures are comparable to the\nannual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately\n272 transatlantic flights between Boston and London. In sequestration terms, offsetting GPT-4o\u2019s\nannual emissions would require over 138,000 acres of average U.S. forest, an area roughly equivalent\nto the size of Chicago. These results showcase that the aggregation of hundreds of millions of requests\nper day can already impose a substantial environmental burden. This burden is only expected to grow\nas AI usage continues to scale.\n7 GPT-5 Adaptive Model Routing Case Study\nThe launch of GPT-5 [74] introduced adaptive model routing, a mechanism that allows the system to\nautomatically determine whether to use a fast variant or a more computationally intensive \u201cThinking\u201d\nmodel for complex reasoning tasks. This unification eliminates the need for manual model selection\nwhere the model dynamically scales its reasoning effort based on prompt complexity.\nHowever, this adaptability introduces substantial variability in energy consumption across reasoning\nmodes, as shown in Figure 6. For medium-length queries, the average energy consumption ranges\nfrom 2.33Wh for minimal reasoning to 17.15Wh for high reasoning, representing a more than seven-\nfold increase. Despite this variance, GPT-5 remains relatively efficient at lower reasoning levels. For\ninstance, a short, minimal reasoning query consumes only 0.67 Wh, a value comparable to GPT-4o\u2019s\n0.42 Wh per short prompt. Conversely, a long, high-reasoning query reaches an average of 33.8 Wh,\ncomparable to the upper bounds observed among the most energy-intensive models analyzed in this\nstudy.\nThese results suggest that while adaptive routing optimizes computational resources by tailoring\ninference depth to task complexity, it also amplifies the environmental footprint of cognitively\ndemanding prompts. This finding underscores the growing importance of prompt-level efficiency\nanalysis for next-generation LLMs that blend lightweight and high-reasoning architectures within a\nunified system.\n11",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "page": 11,
          "paragraph_index": 19,
          "section_id": "jegham2025:sec11",
          "section_index": 11
        }
      },
      {
        "node_id": "jegham2025:sec11",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Page 11",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "page": 11,
          "section_index": 11
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s225",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "GPT-4o.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 225
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s226",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "To simulate real-world usage dynamics, we apply a monthly prompt growth rate of 20%\nfrom January to May 2025, reflecting the documented increase in ChatGPT\u2019s weekly active user\nbase from 300 million to 800 million between December 2024 and April 2025 [72].",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 226
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s227",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "This is followed\nby a decaying growth pattern from June to December, yielding a total of approximately 772 billion\nGPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 227
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s228",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Within these queries, we conservatively assume an 80%/20% split between short and medium-length\nprompts based on typical usage patterns.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 228
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s229",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Scaling the per-query energy estimates accordingly, we find\nthat GPT-4o inference would require approximately 391,509 MWh annually at minimum and 463,269\nMWh at maximum, as seen in Figure 5.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 229
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s230",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "These values exceed the total electricity consumption of\n35,000 U.S.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 230
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s231",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even\n325 universities (390,650 MWh) annually.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 231
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s232",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "6.3 Estimated 2025 Annual Water Footprint of GPT-4o Inference\nAs showcased in Figure 5, we translate estimated cooling and infrastructure-related water usage into\nreal-world benchmarks.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 232
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s233",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Based on scaled inference volumes, GPT-4o\u2019s annual water consumption is\nprojected to be between 1,334,991 kiloliters (kL) and 1,579,680 kL.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 233
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s234",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "These quantities are roughly\nequivalent to filling over 500 Olympic-sized pools or to supporting the annual drinking needs of 1.2\nmillion people.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 234
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s235",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Importantly, this consumption refers to evaporated freshwater permanently removed\nfrom local ecosystems rather than recycled.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 235
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s236",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "GPT-4o alone is responsible for evaporating an amount\nof freshwater equivalent to the annual drinking needs of almost 1.2 million people.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 236
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s237",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "6.4 Estimated 2025 Annual Carbon Footprint of GPT-4o Inference\nWe further examine GPT-4o\u2019s environmental footprint through estimated carbon emissions from\nelectricity usage, as seen in Figure 5.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 237
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s238",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Our projections indicate annual emissions of approximately\n138,125 tons of CO2e at minimum and 163,441 tons at maximum.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 238
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s239",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "These figures are comparable to the\nannual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately\n272 transatlantic flights between Boston and London.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 239
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s240",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "In sequestration terms, offsetting GPT-4o\u2019s\nannual emissions would require over 138,000 acres of average U.S.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 240
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s241",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "forest, an area roughly equivalent\nto the size of Chicago.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 241
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s242",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "These results showcase that the aggregation of hundreds of millions of requests\nper day can already impose a substantial environmental burden.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 242
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s243",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "This burden is only expected to grow\nas AI usage continues to scale.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 243
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s244",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "7 GPT-5 Adaptive Model Routing Case Study\nThe launch of GPT-5 [74] introduced adaptive model routing, a mechanism that allows the system to\nautomatically determine whether to use a fast variant or a more computationally intensive \u201cThinking\u201d\nmodel for complex reasoning tasks.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 244
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s245",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "This unification eliminates the need for manual model selection\nwhere the model dynamically scales its reasoning effort based on prompt complexity.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 245
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s246",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "However, this adaptability introduces substantial variability in energy consumption across reasoning\nmodes, as shown in Figure 6.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 246
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s247",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "For medium-length queries, the average energy consumption ranges\nfrom 2.33Wh for minimal reasoning to 17.15Wh for high reasoning, representing a more than seven-\nfold increase.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 247
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s248",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Despite this variance, GPT-5 remains relatively efficient at lower reasoning levels.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 248
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s249",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "For\ninstance, a short, minimal reasoning query consumes only 0.67 Wh, a value comparable to GPT-4o\u2019s\n0.42 Wh per short prompt.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 249
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s250",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Conversely, a long, high-reasoning query reaches an average of 33.8 Wh,\ncomparable to the upper bounds observed among the most energy-intensive models analyzed in this\nstudy.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 250
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s251",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "These results suggest that while adaptive routing optimizes computational resources by tailoring\ninference depth to task complexity, it also amplifies the environmental footprint of cognitively\ndemanding prompts.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 251
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s252",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "This finding underscores the growing importance of prompt-level efficiency\nanalysis for next-generation LLMs that blend lightweight and high-reasoning architectures within a\nunified system.",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 252
        }
      },
      {
        "node_id": "jegham2025:sec11:p19:s253",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "11",
        "score": 0.7238,
        "rank": 8,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec11:p19",
          "section_id": "jegham2025:sec11",
          "sentence_index": 253
        }
      },
      {
        "node_id": "jegham2025:sec10:p17",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Figure 5: (Top Left) Per-query and daily energy consumption of GPT-4o. (Top Right) Estimated total\nannual energy usage of GPT-4o in 2025. (Bottom Left) The estimated 2025 annual water consumption\nof GPT-4o. (Bottom Right) The estimated 2025 annual carbon emissions of GPT-4o.\n5.3 Validation Against Public Disclosures\nPublic disclosures of inference-level energy and carbon data remain limited, but a few recent state-\nments provide useful reference points for cross-validation. In June 2025, OpenAI CEO Sam Altman\nreported that the default ChatGPT model consumed approximately 0.34 Wh per query [68]. Knowing\nthat GPT-4o was the default deployment at that time, this estimate likely corresponds to GPT-4o-level\ninference. Our framework estimates 0.42 Wh ( \u00b10.13 Wh) for a short GPT-4o prompt (0.37 Wh\nwithout datacenter overhead), within 19% of Altman\u2019s figure. Similarly, the results for Mistral Large 2\nalign closely with Mistral\u2019s published life-cycle assessment (LCA) report [69], which cites approxi-\nmately 1.14 gCO2e per 400-token query. Our corresponding estimate for 300 tokens (0.82 gCO2e,\n\u00b10.10 gCO2e) scales to roughly 1.09 gCO2e when normalized to 400 tokens, showcasing alignment\nwithin one standard deviation. Together, these alignments between independent disclosures and our\nmodeled results suggest that the framework reproduces realistic operational conditions for modern\nLLM inference.\n6 GPT-4o Environmental Impact Case Study\n6.1 Energy Cost of a Single GPT-4o User Session\nBased on Reuters [70], the average ChatGPT user sends approximately eight queries per day as of\nApril 2025. Based on this, we quantify the per-user energy impact of GPT-4o interactions against\nfamiliar digital activities as presented in Figure 5. A single short GPT-4o query consumes 0.42\nWh (\u00b10.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.\nScaling to a typical daily usage pattern, the cumulative energy reaches 3.73 Wh (\u00b10.358 Wh). For\nmedium-length queries, this increases to 9.71 Wh (\u00b11.106 Wh). These results highlight that even\nlimited daily engagement with GPT-4o can impose an energy cost comparable to charging two\nsmartphones to full capacity (approximately 10 Wh), illustrating the tangible environmental footprint\nof conversational AI. While the individual per-query costs appear modest, their aggregation across\nmillions of users introduces a rapidly compounding, largely invisible load on the environment.\n6.2 Estimated 2025 Annual Energy Consumption of GPT-4o Inference\nTo estimate the annual energy demand of GPT-4o in 2025, we consider a baseline of 1 billion queries\nper day across all ChatGPT deployments, a figure reported by OpenAI as of December 2024 [71].\nGiven GPT-4o\u2019s status as the default model, we conservatively attribute 700 million daily queries to\n10",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "page": 10,
          "paragraph_index": 17,
          "section_id": "jegham2025:sec10",
          "section_index": 10
        }
      },
      {
        "node_id": "jegham2025:sec10",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Page 10",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "page": 10,
          "section_index": 10
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s204",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Figure 5: (Top Left) Per-query and daily energy consumption of GPT-4o.",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 204
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s205",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "(Top Right) Estimated total\nannual energy usage of GPT-4o in 2025.",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 205
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s206",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "(Bottom Left) The estimated 2025 annual water consumption\nof GPT-4o.",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 206
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s207",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "(Bottom Right) The estimated 2025 annual carbon emissions of GPT-4o.",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 207
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s208",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "5.3 Validation Against Public Disclosures\nPublic disclosures of inference-level energy and carbon data remain limited, but a few recent state-\nments provide useful reference points for cross-validation.",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 208
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s209",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "In June 2025, OpenAI CEO Sam Altman\nreported that the default ChatGPT model consumed approximately 0.34 Wh per query [68].",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 209
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s210",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Knowing\nthat GPT-4o was the default deployment at that time, this estimate likely corresponds to GPT-4o-level\ninference.",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 210
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s211",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Our framework estimates 0.42 Wh ( \u00b10.13 Wh) for a short GPT-4o prompt (0.37 Wh\nwithout datacenter overhead), within 19% of Altman\u2019s figure.",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 211
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s212",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Similarly, the results for Mistral Large 2\nalign closely with Mistral\u2019s published life-cycle assessment (LCA) report [69], which cites approxi-\nmately 1.14 gCO2e per 400-token query.",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 212
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s213",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Our corresponding estimate for 300 tokens (0.82 gCO2e,\n\u00b10.10 gCO2e) scales to roughly 1.09 gCO2e when normalized to 400 tokens, showcasing alignment\nwithin one standard deviation.",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 213
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s214",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Together, these alignments between independent disclosures and our\nmodeled results suggest that the framework reproduces realistic operational conditions for modern\nLLM inference.",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 214
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s215",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "6 GPT-4o Environmental Impact Case Study\n6.1 Energy Cost of a Single GPT-4o User Session\nBased on Reuters [70], the average ChatGPT user sends approximately eight queries per day as of\nApril 2025.",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 215
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s216",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Based on this, we quantify the per-user energy impact of GPT-4o interactions against\nfamiliar digital activities as presented in Figure 5.",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 216
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s217",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "A single short GPT-4o query consumes 0.42\nWh (\u00b10.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 217
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s218",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Scaling to a typical daily usage pattern, the cumulative energy reaches 3.73 Wh (\u00b10.358 Wh).",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 218
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s219",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "For\nmedium-length queries, this increases to 9.71 Wh (\u00b11.106 Wh).",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 219
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s220",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "These results highlight that even\nlimited daily engagement with GPT-4o can impose an energy cost comparable to charging two\nsmartphones to full capacity (approximately 10 Wh), illustrating the tangible environmental footprint\nof conversational AI.",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 220
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s221",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "While the individual per-query costs appear modest, their aggregation across\nmillions of users introduces a rapidly compounding, largely invisible load on the environment.",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 221
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s222",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "6.2 Estimated 2025 Annual Energy Consumption of GPT-4o Inference\nTo estimate the annual energy demand of GPT-4o in 2025, we consider a baseline of 1 billion queries\nper day across all ChatGPT deployments, a figure reported by OpenAI as of December 2024 [71].",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 222
        }
      },
      {
        "node_id": "jegham2025:sec10:p17:s223",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Given GPT-4o\u2019s status as the default model, we conservatively attribute 700 million daily queries to\n10",
        "score": 0.7224,
        "rank": 9,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec10:p17",
          "section_id": "jegham2025:sec10",
          "sentence_index": 223
        }
      },
      {
        "node_id": "jegham2025:sec1:p1:s14",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "Training\nGPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric\ntons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for\ncooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool.",
        "score": 0.7212,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec1:p1",
          "section_id": "jegham2025:sec1",
          "sentence_index": 14
        }
      },
      {
        "node_id": "jegham2025:sec1:p1",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "How Hungry is AI? Benchmarking Energy, Water, and\nCarbon Footprint of LLM Inference\nNidhal Jegham1,2\nnidhal.jegham@uri.edu\nMarwan Abdelatti3\nmabdelat@providence.edu\nChan Young Koh1\nckoh04@uri.edu\nLassad Elmoubarki2\nlassad.elmoubarki@tbs.rnu.tn\nAbdeltawab Hendawi1\u2217\nhendawi@uri.edu\n1 University of Rhode Island 2 University of Tunis 3 Providence College\nLive Dashboard: Power BI Dashboard\nAbstract\nThis paper introduces an infrastructure-aware benchmarking framework for quanti-\nfying the environmental footprint of LLM inference across 30 state-of-the-art mod-\nels in commercial datacenters. The framework combines public API performance\ndata with company-specific environmental multipliers and statistical inference of\nhardware configurations. We additionally utilize cross-efficiency Data Envelop-\nment Analysis (DEA) to rank models by performance relative to environmental cost\nand provide a dynamically updated dashboard that visualizes model-level energy,\nwater, and carbon metrics. Results show the most energy-intensive models exceed\n29 Wh per long prompt, over 65 \u00d7 the most efficient systems. Even a 0.42 Wh\nshort query, when scaled to 700M queries/day, aggregates to annual electricity\ncomparable to 35,000 U.S. homes, evaporative freshwater equal to the annual drink-\ning needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest\nto offset. These findings highlight a growing paradox: as AI becomes cheaper\nand faster, global adoption drives disproportionate resource consumption. Our\nmethodology offers a standardized, empirically grounded basis for sustainability\nbenchmarking and accountability in AI deployment.\n1 Introduction\nLarge language models (LLMs) have moved beyond research labs and are now embedded in search\nengines, virtual assistants, education platforms, and enterprise tools [1, 2, 3, 4]. Models like GPT-4o\n[5] and Claude-3.7 Sonnet [6] represent state-of-the-art systems, while open-source alternatives such\nas LLaMA-3 [7] and DeepSeek-V3 [8] reflect growing accessibility and experimentation. On top of\nthat, the emergence of reasoning models such as DeepSeek-R1 [9], o1 [10], and o3-mini [11] marks\na shift toward multi-step logic and chain-of-thought reasoning.\nHowever, the advancement of LLMs does involve shortcomings in environmental aspects. Training\nGPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric\ntons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for\ncooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool. Yet while training\nhas been the focus of sustainability discussions, inference is emerging as the primary contributor to\nenvironmental costs. In contrast to training, which is conducted once or at intervals, inference occurs\nconsistently and on a large scale. Recent estimates suggest inference can account for up to 90% of a\nmodel\u2019s total lifecycle energy use [14, 15].\n\u2217Corresponding author.\narXiv:2505.09598v6  [cs.CY]  24 Nov 2025",
        "score": 0.7212,
        "rank": 10,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "page": 1,
          "paragraph_index": 1,
          "section_id": "jegham2025:sec1",
          "section_index": 1
        }
      }
    ],
    "num_snippets": 152,
    "retry_count": 0
  },
  {
    "id": "q322",
    "question": "What is the estimated CO2 emission in metric tons for one year of average US home energy use?",
    "gt_value": "8.3",
    "gt_unit": "metric tons",
    "gt_ref": "['dodge2022']",
    "pred_value": "8.3",
    "pred_unit": "metric tons",
    "pred_ref": "[\"dodge2022\"]",
    "pred_explanation": "The context from ref_id=dodge2022 explicitly states that the emissions from one year of average US home energy use (including electricity, natural gas, liquid petroleum gas, and fuel oil) total 8.3 metric tons of CO2. This matches the required answer unit of 'metric tons'.",
    "raw_response": "{\n  \"explanation\": \"The context from ref_id=dodge2022 explicitly states that the emissions from one year of average US home energy use (including electricity, natural gas, liquid petroleum gas, and fuel oil) total 8.3 metric tons of CO2. This matches the required answer unit of 'metric tons'.\",\n  \"answer\": \"8.3 metric tons\",\n  \"answer_value\": 8.3,\n  \"ref_id\": [\"dodge2022\"],\n  \"ref_url\": [],\n  \"supporting_materials\": \"one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)\"\n}",
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true,
    "weighted_score": 1.0,
    "latency_seconds": 49.88896632194519,
    "retrieval_seconds": 13.213131427764893,
    "generation_seconds": 36.675687074661255,
    "error": null,
    "pred_ref_url": [],
    "pred_supporting_materials": "one average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil)",
    "rendered_prompt": "You will answer a question using ONLY the provided context snippets.\nIf the context does not clearly support an answer, use \"is_blank\".\n\nContext snippets from documents:\n[ref_id=dodge2022] Even partially trained, experiments of\nthis size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from\nelectricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).\n---\n[ref_id=dodge2022] Measuring the Carbon Intensity of AI in Cloud Instances FAccT \u201922, June 21\u201324, 2022, Seoul, Republic of Korea\nFig. 2. Emissions for our 11 experiments described in \u00a74. For each model we show a vertical blue bar, where the top of the bar is\nthe max, the bottom is the min, and the black line represents the average emissions (across regions and time of year). First and\nfourth quartiles are represented by the light blue at the top and bottom of each vertical blue bar. The largest training runs (e.g., 6\nbillion parameter LM) releases a significant amount of emissions, no matter the region (and recall the 6 billion parameter LM is only\ntrained for 13% of a full run, so a full run would emit about an order of magnitude more emissions than reported here). The smallest\nexperiments emit very little. Presented on a log scale, with references on the right indicating equivalent sources of emissions per the\nUnited States Environmental Protection Agency [46].\nThe largest experiment in our set is the 6 billion parameter transformer, and that model is only partially trained (as\ndescribed in \u00a74, it is only trained for about 13% of the time needed to converge). Even partially trained, experiments of\nthis size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from\nelectricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year). Perhaps\nunsurprisingly, even the most efficient region of those we examined for that experiment still leads to more emissions\nthan a full barrel of oil. If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of\nCO2 (depending on the region it was run in).\nComparing against previous work on measuring emissions can be challenging without full information about data\nand model parallelism, GPU utilization, the number of weight updates, and other relevant factors; while we don\u2019t have\nexperiments covering the same models as previous work on estimating CO2, we can make approximate comparisons\nalong three dimensions: a) kWh per GPU hour, b) CO 2 grams per GPU hour, and c) CO 2 grams per kWh. Here we\ncompare against [34] and [33] which report information about training especially large models. Their estimates also\n9\n---\n[ref_id=morrison2025] (energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquivalent to...\n---\n[ref_id=morrison2025] Published as a conference paper at ICLR 2025\nTable 1: We developed our models in five groups, based on parameter count and architecture: less than 1\nbillion, 1 billion, 7 billion, and 13 billion parameters, and our mixture-of-experts model with 1 billion active\nand 7 billion total parameters. We found that \u223c70% of our developmental environmental impact came from\ndeveloping the 7B and 13B models, and the total impact was emissions equivalent to 2.1 tanker trucks\u2019 worth\nof gasoline, and equal to about 7 and a half years of water used by the average person in the United States.\nGPU\nHours\nTotal\nMWh # Runs\nCarbon\nEmissions\n(tCO2eq)\nEquivalent to...\n(energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquivalent to...\n(water usage,\n1 person)\n<1B 29k 19 20 6 1 yr, 4 mo 24 3 mo\n7B 269k 196 375 65 13 yrs, 6 mo 252 2 yrs, 7 mo\n13B 191k 116 156 46 9 yrs, 7 mo 402 3 yrs, 7 mo\nMoE 27k 19 35 6 1 yr, 4 mo 24 3 mo\nTotal 680k 459 813 159 33 yrs, 1 mo 843 7 yrs, 5 mo\nHardware manufacturing NVIDIA does not release the embodied carbon emissions or water\nconsumption about the hardware it produces, so we assume the same embodied carbon emissions\nas Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU. There\nis little public information on how much water is required to produce a single GPU, though chip\nmanufacturing facilities require millions of liters per day. 10 Some estimates11 place TSMC water\nusage at 12.33 liters per square centimeter of hardware, which equals 100.4 liters per H100, which\nwe use for our analysis.\nWe additionally estimate the environmental impact from mining rare earth metals used during man-\nufacturing, assuming an H100 is 0.1% rare earth metal by mass. Mining 1 kg of rare earth materials\nconsumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-\ninch silicon wafer weighs 125 grams 12 and produces about 63 H100s. 13 14 Together, these add an\nadditional 2.2 liters consumed and 0.013 kg CO2eq per GPU.\nInternally, we assume a 4 year lifespan for our GPUs, which leads to an embodied emissions of\n0.013 kg of CO2eq and 0.003 liters of water consumed per GPU hour when the estimated embodied\nimpacts is amortized over the assumed lifetime of the GPU. We used 1.65 million GPU hours in\ntotal, leading to a total of 22 tCO2eq emitted and 4.8 kL of water consumed during manufacturing.\nDevelopment Before launching our final training runs for each model, we ran a series of controlled\nexperiments to stabilize and improve our training setup, to explore different parameter initializations\nand mid-training recipes, and to determine our final hyperparameters and data mixtures through\nscaling law experiments (Bhagia et al., 2024). We ran these in five distinct groups: small models\nwith less than 1 billion parameters, 1 billion parameter models, 7 billion parameter models, 13\nbillion parameter models, and our mixture-of-experts model. We report detailed development costs\nfor each group in Table 1.\nUnsurprisingly, we find that the majority of development costs ( \u223c70%) were incurred at the 7 and\n13 billion parameter scale, due to both the relative size of the model and our own prioritization, and\nwe see this both in the total environmental impact and the number of individual runs per category.\nUsing our data center\u2019s efficiency factors, we find that our development runs led to 159 tCO 2eq\nemitted and 843 kL of water consumed.\nFinal training runs Finally, we fully trained our series of models, ranging from 20 million to\n13 billion active parameters, with detailed information provided in Table 2. As we saw during\ndevelopment, the majority of the cost incurred came from training our 7B and 13B models, which\nwe trained to 2 to 5 trillion tokens. We also see that the 1B dense model required about as much\nenergy per trillion tokens as the MoE model with 1B active parameters, though the MoE model was\nslightly less efficient, most likely due to the extra compute required for routing tokens. In summary,\nwe find that our training runs led to 312 tCO2eq emitted and 1,921 kL of water consumed.\n10https://www.azcentral.com/story/opinion/op-ed/joannaallhands/2024/06/12/\ntsmc-arizona-water-use-recycling/74059522007/\n11https://www.semiconductor-digest.com/water-supply-challenges-for-the-semiconductor-industry/\n12https://web.archive.org/web/20131207002716/http://wafercare.com/Page.aspx?id=1012\n13https://anysilicon.com/die-per-wafer-formula-free-calculators/\n14https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/\n6\n---\n[ref_id=morrison2025] (energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquiv.\n---\n[ref_id=morrison2025] Published as a conference paper at ICLR 2025\nTable 2: We list the estimated power usage, carbon emissions, and water consumption from training our\ndense transformers, ranging from 20 million to 13 billion parameters, trained on 1.7 to 5.6 trillion tokens, and\na mixture-of-experts model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens. We\nfind that the environmental impact is quite high, even for our relatively small models. Training our series of\nmodels emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and\nconsumed equivalent water to the average person in the U.S. for about 17 years.\n* One of the original OLMo 7B models was trained on LUMI, which runs entirely on hydroelectric power. See\nGroeneveld et al. (2024) for more information.\n\u2020 denotes unreleased models that were trained for various internal experiments.\nPower\nUsage\n(MWh)\nCarbon\nEmissions\n(tCO2eq)\nEquiv. to...\n(energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquiv. to...\n(water usage,\n1 person, U.S.)\nGemma 2B & 9B - 131 25 yrs, 11 mo - -\nLlama 2 7B 81 31 6 yrs, 1 mo - -\nLlama 2 13B 162 62 12 yrs, 2 mo - -\nLlama 3.1 8B - 420 83 years - -\nLlama 3.2 1B - 107 14 years - -\nOLMo 20M\u2020 0.8 0.3 3 weeks 1 3 days\nOLMo 60M\u2020 1.2 0.4 1 month 1.6 5 days\nOLMo 150M\u2020 2.4 1 2 mo, 1 wk 3.6 12 days\nOLMo 300M\u2020 5 2 5 months 5.9 19 days\nOLMo 700M\u2020 8 3 7 months 10 33 days\nOLMo 7B\u2020 67 22 4 yrs, 4 mo 87 9 months\nOLMo 1B (3T) 30 10 2 years 39 4 months\nOLMo 7B 149 0* - 0* -\nOLMo 7B (Twin) 114 70 13 yrs, 10 mo 487 4 yrs, 4 mo\nOLMo (04|07)24 7B 95 32 6 yrs, 4 mo 122 1 yr, 1 mo\nOLMo 2 7B 157 52 10 yrs, 4 mo 202 1 yr, 9 mo\nOLMo 2 13B 230 101 21 years 892 7 yrs, 10 mo\nOLMoE 0924 54 18 3 yrs, 7 mo 70 7 months\nTotal (Ours) 913 312 65 years 1,921 17 yrs, 1 mo\nPutting it in perspective In total, our series of models led to at least 493 tCO2eq emitted. Using\nthe U.S. Environmental Protection Agency\u2019s Greenhouse Gas Equivalencies Calculator 15, this is\nequivalent to 6.5 tanker trucks\u2019 worth of gasoline burned, emissions from the average yearly energy\nuse for 98.2 homes in the U.S., or the amount of carbon sequestered by 472 acres of U.S. forests in\none year. We additionally estimate we consumed at least 2,769 kL of water, which is equivalent to\nabout 24 and a half years of water consumption by the average person in the U.S.16\nOther Costs In this work, we strive to provide a thorough accounting of the total cost of develop-\ning our models. However, there remain a number of sources of emissions and water consumption\nthat are difficult, if not impossible to comprehensively measure without access to proprietary in-\nformation across a range of industries, such as transportation and end of life hardware disposal.\nWhile the costs we report above represent a large portion of the total development process, more\ntransparency is needed to understand the full impact of model training.\n4.2 S IMULATING DEPLOYMENT & INFERENCE\nWe report simulated inference costs; that is, we explore the question of what our models\u2019 impact\nmight be if they were put into production. In contrast to \u00a74.1, where we reported the actual im-\npact from our actions, this section reports partial estimates of Scope 3 carbon emissions and water\nconsumption: the impact from the downstream actions of others using our models. We include\ncomparisons with recent instruction-tuned models as well.\nIn Table 3, we display 1) power and energy costs, 2) carbon and water consumption, and 3) the\ntime to complete 100 requests. We additionally report \u201cbreakeven\u201d points, that is the number of\n15https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator\n16https://www.epa.gov/watersense/statistics-and-facts\n7\n---\n[ref_id=dodge2022] One phone charge is estimated to emit 8.22 \u00d7 10\u22126 metric tons (using US national weighted\naverage CO2 marginal emission rate for delivered electricity), one mile driven is estimated to emit 3.98 \u00d7 10\u22124 metric\ntons (using average US passenger vehicle, which gets 22.5 miles per gallon of gasoline), one gallon of gasoline consumed\nis estimated to emit 8.887 \u00d7 10\u22123 metric tons, one barrel of crude oil consumed is estimated to emit 0.43 metric tons,\none average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating\nelectricity, natural gas, liquid petroleum, and fuel oil), and one rail car of coal is estimated to emit 181.29 metric tons.\n---\n[ref_id=dodge2022] FAccT \u201922, June 21\u201324, 2022, Seoul, Republic of Korea Dodge et al.\nFig. 1. Carbon emissions that would be emitted from training BERT (language modeling on 8 V100s for 36 hours) in 16 different\nregions (one region per line) at different times throughout the year. Each line is relatively flat, indicating the emissions in a single\nregion during different months are relatively similar. There is large variation between the least carbon-intensive regions (the lowest\nlines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run\ncan be very impactful ( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions).\nemitted from training BERT (see \u00a74 for more details) on 8 V100 GPUs for 36 hours in 16 different regions (one region\nper line) at different times throughout the year.\nWhat do emissions look like across the 11 experiments described in \u00a74? In Figure 2 we show results for all 11\nexperiments, which cover two BERT experiments (finetuning and language modeling), partial training of a 6.1 billion\nparameter Transformer, 3 sizes of DenseNets, and five sizes of Vision Transformers. Each experiment is represented by\na vertical blue bar showing the range of emissions that would be emitted for that experiment across different regions.\nThe top of the blue bar is the emissions from running that experiment in the region with the most emissions, the bottom\nis the emissions from running that experiment in the region with the least emissions, the black line represents the\naverage, and the light blue regions are the top and bottom quartiles.\nIn Figure 2 we also include estimates of equivalent sources of emissions per the United States Environmental\nProtection Agency [46]. One phone charge is estimated to emit 8.22 \u00d7 10\u22126 metric tons (using US national weighted\naverage CO2 marginal emission rate for delivered electricity), one mile driven is estimated to emit 3.98 \u00d7 10\u22124 metric\ntons (using average US passenger vehicle, which gets 22.5 miles per gallon of gasoline), one gallon of gasoline consumed\nis estimated to emit 8.887 \u00d7 10\u22123 metric tons, one barrel of crude oil consumed is estimated to emit 0.43 metric tons,\none average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating\nelectricity, natural gas, liquid petroleum, and fuel oil), and one rail car of coal is estimated to emit 181.29 metric tons.\n8\n---\n[ref_id=wu2021b] For the residential sector in the\nUS, space heating and cooling contributed to over 40% of the total electricity consumption [U.S.\n---\n[ref_id=wu2021b] Socio-Technological Challenges and Opportunities: Paths Forward A PREPRINT\ninfrastructure may be better served by (co-)designing agile applications and data centers, such as [\u00cd\u00f1igo Goiri et al.,\n2015, Radovanovic et al., 2021, Lin et al., 2021, Zhang et al., 2021], that implicitly assume power \ufb02uctuations and\nvariability.\nAchieving pervasive connectivity in the presence of resiliency, security, and privacy requirements can be challenging\ngiven the interrelated nature of the problems. For instance, current approaches to achieving resilient computing often\nrely on replication and redundancy which not only increases cost but also enlarges the surface of security and privacy\npreservation. Migrating applications to another region, which may be necessary due to power constraints, can expose\ndata to further security threats. To retain physical security of data, applications may be hosted in limited geographical\nregions but this can increase application vulnerability to catastrophic events. In addition, secure and resilient computing\ninfrastructures can often come with signi\ufb01cant environmental implications. Hence, any computing infrastructure\nsolutions must be cognizant of the multifaceted nature of the problems being addressed.\nSustainability. Resource limitations, climate change, water depletion, electronic waste, ecosystem damage, and\nenvironmental racism are just a few of the topics under the larger sustainability umbrella. There is increased focus on\nthese topics from both industrial and political institutions. All major technology companies have pledged to reduce or\neliminate their carbon footprint in the next decade by reducing the environmental impact associated with manufacturing\nand using their products. Examples of such commitments are Facebook achieving NetZero in operational emissions\nin 2020 and across its value chain by 2030 [Facebook], Apple\u2019s pledge for 100% carbon neutral supply chain by\n2030 [Apple], Microsoft\u2019s goal of being carbon negative by 2030 [Smith], and Google\u2019s aim of 24x7 carbon free data\ncenters [Google, b]. In 2020, Amazon, Google, Facebook, and Microsoft were the top four technology companies\nthat purchased signi\ufb01cant renewable energy capacities, accounting for 30% of the cumulative total from corporations\nglobally [Schechner, 2021]. In addition, countries and trading zones are legislating carbon emission requirements.\nChina has committed to be carbon free by 2060 [Myers, 2020] and the EU has committed to cut carbon emissions by\n55% by 2030 [BBC, 2021].\nSustainability targets and the associated regulations will continue to increase, and hardware must be manufactured with\nless planetary impact, use less energy while in operation, and produce less e-waste at the end of life [Orcuttarchive, 2015,\nChang et al., 2017]. Existing practices such as the move to hyperscale data centers have already reduced IT\u2019s carbon\nfootprint by consolidating and sharing computing resources, and operating those resources more ef\ufb01ciently (Figure 1)\n\u2013 AWS [Amazon, 2019], Azure [Microsoft, 2020], Google cloud [Evans and Gao, 2016], and Facebook datacenter\ninfrastructures [Lee and Rowe, 2020]. In fact, data center electricity consumption has slowed down signi\ufb01cantly. The\ntotal energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated\n24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020]. Furthermore, despite the\nincrease in the global data center energy use, the number of users bene\ufb01ting from the cloud infrastructure have increased\neven more\u2014from 2010-18, the global data center compute instances increased by 5.5 times with an estimated 6%\nincrease in the global data center energy use.\nHowever, more can be done \u2013we must go beyond ef\ufb01ciency optimization and build a sustainable ecosystem to achieve\nenvironmentally-sustainable computing. For instance, develop expandable hardware and software stack that facilitate\nsigni\ufb01cantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020] and 4\nto 5 years for servers [Ascierto and Lawrence, 2020]. Modular system design will enable component-level upgrades\nwithout having to decommission the system at its entirety, reducing overall electronic waste and the environmental\nfootprint [Fairphone]. Other actions being discussed or implemented require manufacturers to make their systems more\nrepairable, resulting in increased product lifetime [Wiens and Gordon-Byrne, 2017, Alsever, 2021]. Another option is\nto reduce the number of devices in an average household. In the US, for example, the average household is equipped\nwith an average of 25 connected devices [Deloitte, 2021]. In many cases, smartphones might be powerful enough for\nthe task, but a tablet or laptop is needed for a viable keyboard or a larger viewing platform. Some of these additional\ndevices could be replaced with a virtual reality or augmented reality solution that is both portable and can provide\nvirtual keyboards and visual clarity without requiring additional hardware. To achieve an environmentally sustainable\ncomputing future, we will have to build a circular economy for computing that supports the design principle of reduce,\nreuse, repair, and recycle. These and other potential solutions likely require a complete redesign of the software and\nhardware stacks both at the edge, within the cloud, and in the edge-cloud collaborative execution environment, in order\nto provide resilient, long lasting, innovative solutions.\nMaking technology more sustainable is only one part of the technical challenge. There is another side to the story\u2014there\nare signi\ufb01cant sustainability bene\ufb01ts resulting from computing technology. Programs, such as Farm Beats [Microsoft,\n2015], address how to optimize operations on a farm using technology that is inexpensive and readily available to\npeople in rural areas. Food production accounts for 19% of the world\u2019s carbon emissions, and producing food more\nef\ufb01ciently and with less toxicity has long term bene\ufb01ts for the world [Gates, 2021]. For the residential sector in the\nUS, space heating and cooling contributed to over 40% of the total electricity consumption [U.S. Energy Information\n5\n---\n[ref_id=jegham2025] homes, evaporative freshwater equal to the annual drink-\ning needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest\nto offset.\n---\n[ref_id=jegham2025] How Hungry is AI? Benchmarking Energy, Water, and\nCarbon Footprint of LLM Inference\nNidhal Jegham1,2\nnidhal.jegham@uri.edu\nMarwan Abdelatti3\nmabdelat@providence.edu\nChan Young Koh1\nckoh04@uri.edu\nLassad Elmoubarki2\nlassad.elmoubarki@tbs.rnu.tn\nAbdeltawab Hendawi1\u2217\nhendawi@uri.edu\n1 University of Rhode Island 2 University of Tunis 3 Providence College\nLive Dashboard: Power BI Dashboard\nAbstract\nThis paper introduces an infrastructure-aware benchmarking framework for quanti-\nfying the environmental footprint of LLM inference across 30 state-of-the-art mod-\nels in commercial datacenters. The framework combines public API performance\ndata with company-specific environmental multipliers and statistical inference of\nhardware configurations. We additionally utilize cross-efficiency Data Envelop-\nment Analysis (DEA) to rank models by performance relative to environmental cost\nand provide a dynamically updated dashboard that visualizes model-level energy,\nwater, and carbon metrics. Results show the most energy-intensive models exceed\n29 Wh per long prompt, over 65 \u00d7 the most efficient systems. Even a 0.42 Wh\nshort query, when scaled to 700M queries/day, aggregates to annual electricity\ncomparable to 35,000 U.S. homes, evaporative freshwater equal to the annual drink-\ning needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest\nto offset. These findings highlight a growing paradox: as AI becomes cheaper\nand faster, global adoption drives disproportionate resource consumption. Our\nmethodology offers a standardized, empirically grounded basis for sustainability\nbenchmarking and accountability in AI deployment.\n1 Introduction\nLarge language models (LLMs) have moved beyond research labs and are now embedded in search\nengines, virtual assistants, education platforms, and enterprise tools [1, 2, 3, 4]. Models like GPT-4o\n[5] and Claude-3.7 Sonnet [6] represent state-of-the-art systems, while open-source alternatives such\nas LLaMA-3 [7] and DeepSeek-V3 [8] reflect growing accessibility and experimentation. On top of\nthat, the emergence of reasoning models such as DeepSeek-R1 [9], o1 [10], and o3-mini [11] marks\na shift toward multi-step logic and chain-of-thought reasoning.\nHowever, the advancement of LLMs does involve shortcomings in environmental aspects. Training\nGPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric\ntons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for\ncooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool. Yet while training\nhas been the focus of sustainability discussions, inference is emerging as the primary contributor to\nenvironmental costs. In contrast to training, which is conducted once or at intervals, inference occurs\nconsistently and on a large scale. Recent estimates suggest inference can account for up to 90% of a\nmodel\u2019s total lifecycle energy use [14, 15].\n\u2217Corresponding author.\narXiv:2505.09598v6  [cs.CY]  24 Nov 2025\n---\n[ref_id=morrison2025] Environmental Protection Agency\u2019s Greenhouse Gas Equivalencies Calculator 15, this is\nequivalent to 6.5 tanker trucks\u2019 worth of gasoline burned, emissions from the average yearly energy\nuse for 98.2 homes in the U.S., or the amount of carbon sequestered by 472 acres of U.S.\n---\n[ref_id=morrison2025] Published as a conference paper at ICLR 2025\nTable 2: We list the estimated power usage, carbon emissions, and water consumption from training our\ndense transformers, ranging from 20 million to 13 billion parameters, trained on 1.7 to 5.6 trillion tokens, and\na mixture-of-experts model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens. We\nfind that the environmental impact is quite high, even for our relatively small models. Training our series of\nmodels emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and\nconsumed equivalent water to the average person in the U.S. for about 17 years.\n* One of the original OLMo 7B models was trained on LUMI, which runs entirely on hydroelectric power. See\nGroeneveld et al. (2024) for more information.\n\u2020 denotes unreleased models that were trained for various internal experiments.\nPower\nUsage\n(MWh)\nCarbon\nEmissions\n(tCO2eq)\nEquiv. to...\n(energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquiv. to...\n(water usage,\n1 person, U.S.)\nGemma 2B & 9B - 131 25 yrs, 11 mo - -\nLlama 2 7B 81 31 6 yrs, 1 mo - -\nLlama 2 13B 162 62 12 yrs, 2 mo - -\nLlama 3.1 8B - 420 83 years - -\nLlama 3.2 1B - 107 14 years - -\nOLMo 20M\u2020 0.8 0.3 3 weeks 1 3 days\nOLMo 60M\u2020 1.2 0.4 1 month 1.6 5 days\nOLMo 150M\u2020 2.4 1 2 mo, 1 wk 3.6 12 days\nOLMo 300M\u2020 5 2 5 months 5.9 19 days\nOLMo 700M\u2020 8 3 7 months 10 33 days\nOLMo 7B\u2020 67 22 4 yrs, 4 mo 87 9 months\nOLMo 1B (3T) 30 10 2 years 39 4 months\nOLMo 7B 149 0* - 0* -\nOLMo 7B (Twin) 114 70 13 yrs, 10 mo 487 4 yrs, 4 mo\nOLMo (04|07)24 7B 95 32 6 yrs, 4 mo 122 1 yr, 1 mo\nOLMo 2 7B 157 52 10 yrs, 4 mo 202 1 yr, 9 mo\nOLMo 2 13B 230 101 21 years 892 7 yrs, 10 mo\nOLMoE 0924 54 18 3 yrs, 7 mo 70 7 months\nTotal (Ours) 913 312 65 years 1,921 17 yrs, 1 mo\nPutting it in perspective In total, our series of models led to at least 493 tCO2eq emitted. Using\nthe U.S. Environmental Protection Agency\u2019s Greenhouse Gas Equivalencies Calculator 15, this is\nequivalent to 6.5 tanker trucks\u2019 worth of gasoline burned, emissions from the average yearly energy\nuse for 98.2 homes in the U.S., or the amount of carbon sequestered by 472 acres of U.S. forests in\none year. We additionally estimate we consumed at least 2,769 kL of water, which is equivalent to\nabout 24 and a half years of water consumption by the average person in the U.S.16\nOther Costs In this work, we strive to provide a thorough accounting of the total cost of develop-\ning our models. However, there remain a number of sources of emissions and water consumption\nthat are difficult, if not impossible to comprehensively measure without access to proprietary in-\nformation across a range of industries, such as transportation and end of life hardware disposal.\nWhile the costs we report above represent a large portion of the total development process, more\ntransparency is needed to understand the full impact of model training.\n4.2 S IMULATING DEPLOYMENT & INFERENCE\nWe report simulated inference costs; that is, we explore the question of what our models\u2019 impact\nmight be if they were put into production. In contrast to \u00a74.1, where we reported the actual im-\npact from our actions, this section reports partial estimates of Scope 3 carbon emissions and water\nconsumption: the impact from the downstream actions of others using our models. We include\ncomparisons with recent instruction-tuned models as well.\nIn Table 3, we display 1) power and energy costs, 2) carbon and water consumption, and 3) the\ntime to complete 100 requests. We additionally report \u201cbreakeven\u201d points, that is the number of\n15https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator\n16https://www.epa.gov/watersense/statistics-and-facts\n7\n---\n[ref_id=patterson2021] [USE21]  US   Energy   Information   Administration,   2021,   FAQ   How   much   carbon   dioxide   is   produced   per   kilowatt   hour   of   \nU.S.\n---\n[ref_id=patterson2021] 17   \n[Jou21]   Jouppi,   N.,   Yoon,   D-H,   Jablin,   T.,   Kurian,   G.,   Laudon,   J.,   Li,   S.,   Ma,   P.,   Ma,   X.,   Patil,   N.,Prasad,   S.,   Young,   C.,   \nZhou,   Z.,   and   Patterson,   D.,   May   2021.   Ten   Lessons   From   Three   Generations   Shaped   Google\u2019s   TPUv4i,   to  \nappear,    the   48th   International   Symposium   on   Computer   Architecture.   \n[Kap20]  Kaplan,   J.,   McCandlish,   S.,   Henighan,   T.,   Brown,   T.B.,   Chess,   B.,   Child,   R.,   Gray,   S.,   Radford,   A.,   Wu,   J.   and   \nAmodei,   D.,   2020.   Scaling   laws   for   neural   language   models.   arXiv   preprint   arXiv:2001.08361.   \n[K\u00e4r18]   K\u00e4rcher   B.   Formation   and   radiative   forcing   of   contrail   cirrus.    Nature   communication s.   2018   May   8;9(1):1-7.   \nhttps://www.nature.com/articles/s41467-018-04068-0 .   \n[Kuc18]   Kuczmarski,   J.   and   Johnson,   M.,   2018.   Gender-aware   natural   language   \ntranslation. www.tdcommons.org/dpubs_series/1577/ .   \n[Lac19]   Lacoste,   A.,   Luccioni,   A.,   Schmidt,   V.   and   Dandres,   T.,   2019.   Quantifying   the   carbon   emissions   of   machine   \nlearning.    arXiv   preprint   arXiv:1910.09700 .   \n[Lan20]   Lannelongue,   L.,   Grealey,   J.   and   Inouye,   M.,   2020.   Green   algorithms:   Quantifying   the   carbon   footprint   of   \ncomputation.    arXiv:   2007.07610 .   \n[Leo19]   Leopold,   G.   March   19,   2019,   AWS   to   Offer   Nvidia\u2019s   T4   GPUs   for   AI   Inferencing,   \nwww.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/    .   \n[Lep20]   Lepikhin,   D.,   Lee,   H.,   Xu,   Y.,   Chen,   D.,   Firat,   O.,   Huang,   Y.,   Krikun,   M.,   Shazeer,   N.   and   Chen,   Z.,   2020.   GShard:  \nScaling   giant   models   with   conditional   computation   and   automatic   sharding.    arXiv   preprint   arXiv:2006.16668 .   \n[Li21]   Li,   S.,   Tan,   M.,   Pang,   R.,   Li,   A.,   Cheng,   L.,   Le,   Q.   and   Jouppi,   N.P.,   2021.   Searching   for   Fast   Model   Families   on   \nDatacenter   Accelerators.    arXiv   preprint   arXiv:2102.05610 .   \n[Liu18]   Liu,   H.,   Simonyan,   K.   and   Yang,   Y.,   2018.   Darts:   Differentiable   architecture   search.    arXiv   preprint   \narXiv:1806.09055 .   \n[Luc21]   Luccioni,   A.,   and   Schmidt,   V..   March   2021,   Private   Communication.   \n[Mas20]  Masanet,   E.,   Shehabi,   A.,   Lei,   N.,   Smith,   S.   and   Koomey,   J.,   2020.   Recalibrating   global   datacenter   energy-use   \nestimates.    Science ,   367(6481),   pp.984-986.   \nhttps://datacenters.lbl.gov/sites/default/files/Masanet_et_al_Science_2020.full_.pdf .   \n[Mas21]  Masanet,   E.,   March   24,   2021,    Data   Center   Energy   Analysis:   Past,   Present,   and   Future ,   lecture   at   UCSB.   \n[Mer19]   Mehrabi,   N.,   Morstatter,   F.,   Saxena,   N.,   Lerman,   K.   and   Galstyan,   A.,   2019.   A   survey   on   bias   and   fairness   in   \nmachine   learning.   arXiv   preprint   arXiv:1908.09635.    https://arxiv.org/pdf/1908.09635.pdf .   \n[Pha18]  Pham,   H.,   Guan,   M.,   Zoph,   B.,   Le,   Q.   and   Dean,   J.,   2018,   July.   Efficient   neural   architecture   search   via   \nparameters   sharing.   In   International   Conference   on   Machine   Learning   (pp.   4095-4104).   PMLR.     arXiv   preprint   \narXiv:1802.03268 .   \n[Rad20]  Radovanovic,   A.   April   22,   2020,   Our   datacenters   now   work   harder   when   the   sun   shines   and   wind   blows,   \nhttps://blog.google/inside-google/infrastructure/data-centers-work-harder-sun-shines-wind-blows   \n[Raf19]   Raffel,   C.,   Shazeer,   N.,   Roberts,   A.,   Lee,   K.,   Narang,   S.,   Matena,   M.,   Zhou,   Y.,   Li,   W.   and   Liu,   P.J.,   2019.   \nExploring   the   limits   of   transfer   learning   with   a   unified   text-to-text   transformer.    arXiv   preprint   arXiv:1910.10683 .   \n[Rit20]   Ritchie,   H.,   October   22,   2020,   Climate   change   and   flying:   what   share   of   global   CO2   emissions   come   from   \naviation?    https://ourworldindata.org/co2-emissions-from-aviation    .   \n[Ryo14]  Ryor,   J.N.   and   Tawney,   L.E.T.H.A.,   2014.   Utility-Scale   Renewable   Energy:   Understanding   Cost   Parity.   Paris:   \nWorld   Resources   Institute.   \nhttps://www.ctc-n.org/sites/www.ctc-n.org/files/resources/wri14_factsheets_utility_scale_v4.pdf .   \n[San20]  Sanh,   V.,   Debut,   L.,   Chaumond,   J.   and   Wolf,   T.,   2019.   DistilBERT,   a   distilled   version   of   BERT:   smaller,   faster,   \ncheaper   and   lighter.    arXiv   preprint   arXiv:1910.01108 .   \n[Sch20]   Schwartz,   R.,   Dodge,   J.,   Smith,   N.A.   and   Etzioni,   O.,   2020.   Green   AI.    Communications   of   the   ACM ,   63(12),   \npp.54-63.    https://cacm.acm.org/magazines/2020/12/248800-green-ai/fulltext .   \n[Sch21]   Schleier-Smith,   J.,    Sreekanti,   V.,   Khandelwal,   A.,   Carreira,   J.,   Yadwadkar,   N.,   Popa,   R.,    Joseph   E.   Gonzalez,J.,  \nIon   Stoica,   I.,   and   David   A.   Patterson,   D.,   2021   What   Serverless   Computing   Is   and   Should   Become:   The   Next   \nPhase   of   Cloud   Computing,    Communications   of   the   ACM,    64(5) .     \n[Sha17]  Shazeer,   N.,   Mirhoseini,   A.,   Maziarz,   K.,   Davis,   A.,   Le,   Q.,   Hinton,   G.   and   Dean,   J.,   2017.   Outrageously   large   \nneural   networks:   The   sparsely-gated   mixture-of-experts   layer.   ICLR   2017.     arXiv   preprint   arXiv:1701.06538 .   \n[So19]   So,   D.,   Le,   Q.   and   Liang,   C.,   2019,   May.   The   Evolved   Transformer.   In   International   Conference   on   Machine   \nLearning   2019   (pp.   5877-5886).   PMLR.     arXiv   preprint   arXiv:1901.11117 .   \n[Str19]   Strubell,   E.,   Ganesh,   A.   and   McCallum,   A.,   2019.   Energy   and   policy   considerations   for   deep   learning   in   NLP.   \nACL   2019.     arXiv   preprint   arXiv:1906.02243 .   \n[Sut21]   Sutskever,   I.   Personal   Communication,   February   4,   2021.   \n[Tan19]   Tan,   M.   and   Le,   Q.,   2019,   May.   EfficientNet:   Rethinking   model   scaling   for   convolutional   neural   networks.   In   \nInternational   Conference   on   Machine   Learning   (pp.   6105-6114).   PMLR.    arXiv   preprint   arXiv:1905.11946 .   \n[USE21]  US   Energy   Information   Administration,   2021,   FAQ   How   much   carbon   dioxide   is   produced   per   kilowatt   hour   of   \nU.S.   electricity   generation?    https://www.eia.gov/tools/faqs/faq.php?id=74&t=11 .   \n[Vas17]   Vaswani,   A.,   Shazeer,   N.,   Parmar,   N.,   Uszkoreit,   J.,   Jones,   L.,   Gomez,   A.N.,   Kaiser,   L.   and   Polosukhin,   I.,   2017.   \nAttention   is   all   you   need.   NeurIPS   2017.     arXiv   preprint   arXiv:1706.03762 .   \n[Wan20]  Wang,   Y.,   Yao,   Q.,   Kwok,   J.T.   and   Ni,   L.M.,   2020.   Generalizing   from   a   few   examples:   A   survey   on   few-shot   \nlearning.    ACM   Computing   Surveys ,   53(3),   pp.1-34.\n---\n[ref_id=han2024] A.4 Energy Consumption for Training a Generative AI Model\nWe consider Llama-3.1 as an example generative AI model. According to the model card [36], the training\nprocess of Llama-3.1 (including 8B, 70B, and 405B) utilizes a cumulative of 39.3 million GPU hours of com-\nputation on H100-80GB hardware, and each GPU has a thermal design power of 700 watts. Considering\nMeta\u2019s 2023 PUE of 1.08 [37] and excluding the non-GPU overhead for servers, we estimate the total train-\ning energy consumption as approximately 30 GWh. Our estimation method follows Meta\u2019s guideline [36]\nand is conservative, as it excludes the substantial non-GPU energy overheads (e.g., CPUs) associated with\nserver operations.\nA.5 Average Emission for Each LA-NYC Round Trip by Car\nWe use the 2023 national average emission rate for light-duty vehicles (gasoline) provided by the U.S. De-\npartment of Transportation [104]. The emission rate accounts for tailpipe exhaust, tire wear and brake wear.\nSpecifically, the average PM2.5 emission rate is 0.008 grams/mile (including 0.004 grams/mile for exhaust,\n0.003 grams/mile for brake wear, and 0.001 grams/mile for tire wear), and the average NOx emission rate\nis 0.199 grams/mile for exhaust. We see that half of PM2.5 for light-duty vehicles comes from brake and tire\nwear (0.004 gram/miles), which are also produced by other types of vehicles including electric vehicles.\nThe distance for a round-trip between Los Angeles, California, and New York City, New York, is about 5,580\nmiles. Thus, the average auto emissions for each LA-NYC round trip are estimated as 44.64 grams of PM2.5\nand 1110.42 grams of NOx.\nA.6 State-wide Electricity Consumption by U.S. Data Centers in 2023\nWe show in Fig. 10 the state-wide data center electricity consumption in 2023 [5]. It can be seen that Virginia,\nTexas and California have the highest data center electricity consumption in 2023. The total national elec-\ntricity consumption reported by EPRI is slightly lower than the values in [4], and we scale it up accordingly\nin our calculations to ensure consistency.\n0\n20\n40\n60\n80\n100\nPercentile\n(a)Electricity consumption map\nState\nElectricity\nConsumption\n(TWh)\nState\nElectricity\nConsumption\n(TWh)\nState\nElectricity\nConsumption\n(TWh)\nV A 33.85 OH 2.36 ID 0.15\nTX 21.81 SC 2.02 WI 0.15\nCA 9.33 WY 1.86 MD 0.10\nIL 7.45 KY 1.62 LA 0.08\nOR 6.41 CO 1.51 SD 0.07\nAZ 6.25 AL 1.49 ME 0.03\nIA 6.19 FL 1.38 NH 0.02\nGA 6.18 TN 1.33 RI 0.02\nWA 5.17 OK 1.23 KS <0.01\nPA 4.59 MA 1.06 AR <0.01\nNY 4.07 MO 0.97 DE <0.01\nNJ 4.04 MN 0.82 DC <0.01\nNE 3.96 MT 0.58 MS <0.01\nND 3.92 MI 0.53 VT <0.01\nNV 3.42 NM 0.40 WV <0.01\nNC 2.67 CT 0.26\nUT 2.56 IN 0.19\n(b)Electricity consumption by state (descending order)\nFigure 10:State-level electricity consumption of U.S. data centers in 2023 [5].\nB Additional Results for Health-Informed GLB\nB.1 Details of the experiment setup\nWe use Meta\u2019s electricity consumption for each U.S. data center location in 2023 [37] for our experiments.\nTable 7 summarizes the baseline annual energy loadWi for each data centeri. Since data centers are mostly\nstable loads in practice, the hourly workload for each location is calculated asli = Wi\nT , whereTrepresents\nthe total number of hours in the study period. The total hourly workload is then computed asMt = PN\ni=1 li.\nWe use the annual average industrial electricity prices in different states provided by the EIA [109].\nThe health pricep h\ni,t ($/MWh) and carbon emission rater c\ni,t (ton/MWh) are based on data provided by\nWattTime [77]. WattTime divides the U.S. into more than 100 regions and provides marginal health prices\nand carbon emission rates for each region. These values are updated every 5 minutes.\n28\n---\n[ref_id=han2024] Page 28\n---\n[ref_id=han2024] A.4 Energy Consumption for Training a Generative AI Model\nWe consider Llama-3.1 as an example generative AI model.\n---\n[ref_id=han2024] According to the model card [36], the training\nprocess of Llama-3.1 (including 8B, 70B, and 405B) utilizes a cumulative of 39.3 million GPU hours of com-\nputation on H100-80GB hardware, and each GPU has a thermal design power of 700 watts.\n---\n[ref_id=han2024] Considering\nMeta\u2019s 2023 PUE of 1.08 [37] and excluding the non-GPU overhead for servers, we estimate the total train-\ning energy consumption as approximately 30 GWh.\n---\n[ref_id=han2024] Our estimation method follows Meta\u2019s guideline [36]\nand is conservative, as it excludes the substantial non-GPU energy overheads (e.g., CPUs) associated with\nserver operations.\n---\n[ref_id=han2024] A.5 Average Emission for Each LA-NYC Round Trip by Car\nWe use the 2023 national average emission rate for light-duty vehicles (gasoline) provided by the U.S.\n---\n[ref_id=han2024] De-\npartment of Transportation [104].\n---\n[ref_id=han2024] The emission rate accounts for tailpipe exhaust, tire wear and brake wear.\n---\n[ref_id=han2024] Specifically, the average PM2.5 emission rate is 0.008 grams/mile (including 0.004 grams/mile for exhaust,\n0.003 grams/mile for brake wear, and 0.001 grams/mile for tire wear), and the average NOx emission rate\nis 0.199 grams/mile for exhaust.\n---\n[ref_id=han2024] We see that half of PM2.5 for light-duty vehicles comes from brake and tire\nwear (0.004 gram/miles), which are also produced by other types of vehicles including electric vehicles.\n---\n[ref_id=han2024] The distance for a round-trip between Los Angeles, California, and New York City, New York, is about 5,580\nmiles.\n---\n[ref_id=han2024] Thus, the average auto emissions for each LA-NYC round trip are estimated as 44.64 grams of PM2.5\nand 1110.42 grams of NOx.\n---\n[ref_id=han2024] A.6 State-wide Electricity Consumption by U.S.\n---\n[ref_id=han2024] Data Centers in 2023\nWe show in Fig.\n---\n[ref_id=han2024] 10 the state-wide data center electricity consumption in 2023 [5].\n---\n[ref_id=han2024] It can be seen that Virginia,\nTexas and California have the highest data center electricity consumption in 2023.\n---\n[ref_id=han2024] The total national elec-\ntricity consumption reported by EPRI is slightly lower than the values in [4], and we scale it up accordingly\nin our calculations to ensure consistency.\n---\n[ref_id=han2024] 0\n20\n40\n60\n80\n100\nPercentile\n(a)Electricity consumption map\nState\nElectricity\nConsumption\n(TWh)\nState\nElectricity\nConsumption\n(TWh)\nState\nElectricity\nConsumption\n(TWh)\nV A 33.85 OH 2.36 ID 0.15\nTX 21.81 SC 2.02 WI 0.15\nCA 9.33 WY 1.86 MD 0.10\nIL 7.45 KY 1.62 LA 0.08\nOR 6.41 CO 1.51 SD 0.07\nAZ 6.25 AL 1.49 ME 0.03\nIA 6.19 FL 1.38 NH 0.02\nGA 6.18 TN 1.33 RI 0.02\nWA 5.17 OK 1.23 KS <0.01\nPA 4.59 MA 1.06 AR <0.01\nNY 4.07 MO 0.97 DE <0.01\nNJ 4.04 MN 0.82 DC <0.01\nNE 3.96 MT 0.58 MS <0.01\nND 3.92 MI 0.53 VT <0.01\nNV 3.42 NM 0.40 WV <0.01\nNC 2.67 CT 0.26\nUT 2.56 IN 0.19\n(b)Electricity consumption by state (descending order)\nFigure 10:State-level electricity consumption of U.S.\n---\n[ref_id=han2024] data centers in 2023 [5].\n---\n[ref_id=han2024] B Additional Results for Health-Informed GLB\nB.1 Details of the experiment setup\nWe use Meta\u2019s electricity consumption for each U.S.\n---\n[ref_id=han2024] data center location in 2023 [37] for our experiments.\n---\n[ref_id=han2024] Table 7 summarizes the baseline annual energy loadWi for each data centeri.\n---\n[ref_id=han2024] Since data centers are mostly\nstable loads in practice, the hourly workload for each location is calculated asli = Wi\nT , whereTrepresents\nthe total number of hours in the study period.\n---\n[ref_id=han2024] The total hourly workload is then computed asMt = PN\ni=1 li.\n---\n[ref_id=han2024] We use the annual average industrial electricity prices in different states provided by the EIA [109].\n---\n[ref_id=han2024] The health pricep h\ni,t ($/MWh) and carbon emission rater c\ni,t (ton/MWh) are based on data provided by\nWattTime [77].\n---\n[ref_id=han2024] WattTime divides the U.S.\n---\n[ref_id=han2024] into more than 100 regions and provides marginal health prices\nand carbon emission rates for each region.\n---\n[ref_id=han2024] These values are updated every 5 minutes.\n---\n[ref_id=han2024] 28\n---\n[ref_id=strubell2019] Environmental Protection Agency (EPA)\nprovides average CO 2 produced (in pounds per\nkilowatt-hour) for power consumed in the U.S.\n---\n[ref_id=strubell2019] model training and development likely make up\na substantial portion of the greenhouse gas emis-\nsions attributed to many NLP researchers.\nTo heighten the awareness of the NLP commu-\nnity to this issue and promote mindful practice and\npolicy, we characterize the dollar cost and carbon\nemissions that result from training the neural net-\nworks at the core of many state-of-the-art NLP\nmodels. We do this by estimating the kilowatts\nof energy required to train a variety of popular\noff-the-shelf NLP models, which can be converted\nto approximate carbon emissions and electricity\ncosts. To estimate the even greater resources re-\nquired to transfer an existing model to a new task\nor develop new models, we perform a case study\nof the full computational resources required for the\ndevelopment and tuning of a recent state-of-the-art\nNLP pipeline (\nStrubell et al. , 2018). We conclude\nwith recommendations to the community based on\nour \ufb01ndings, namely: (1) Time to retrain and sen-\nsitivity to hyperparameters should be reported for\nNLP machine learning models; (2) academic re-\nsearchers need equitable access to computational\nresources; and (3) researchers should prioritize de-\nveloping ef\ufb01cient models and hardware.\n2 Methods\nTo quantify the computational and environmen-\ntal cost of training deep neural network mod-\nels for NLP , we perform an analysis of the en-\nergy required to train a variety of popular off-\nthe-shelf NLP models, as well as a case study of\nthe complete sum of resources required to develop\nLISA (\nStrubell et al., 2018), a state-of-the-art NLP\nmodel from EMNLP 2018, including all tuning\nand experimentation.\nWe measure energy use as follows. We train the\nmodels described in \u00a7\n2.1 using the default settings\nprovided, and sample GPU and CPU power con-\nsumption during training. Each model was trained\nfor a maximum of 1 day. We train all models on\na single NVIDIA Titan X GPU, with the excep-\ntion of ELMo which was trained on 3 NVIDIA\nGTX 1080 Ti GPUs. While training, we repeat-\nedly query the NVIDIA System Management In-\nterface\n2 to sample the GPU power consumption\nand report the average over all samples. To sample\nCPU power consumption, we use Intel\u2019s Running\nAverage Power Limit interface.\n3\n2nvidia-smi: https://bit.ly/30sGEbi\n3RAPL power meter: https://bit.ly/2LObQhV\nConsumer Renew. Gas Coal Nuc.\nChina 22% 3% 65% 4%\nGermany 40% 7% 38% 13%\nUnited States 17% 35% 27% 19%\nAmazon-AWS 17% 24% 30% 26%\nGoogle 56% 14% 15% 10%\nMicrosoft 32% 23% 31% 10%\nTable 2: Percent energy sourced from: Renewable (e.g.\nhydro, solar, wind), natural gas, coal and nuclear for\nthe top 3 cloud compute providers (\nCook et al. , 2017),\ncompared to the United States, 4 China5 and Germany\n(Burger, 2019).\nWe estimate the total time expected for mod-\nels to train to completion using training times and\nhardware reported in the original papers. We then\ncalculate the power consumption in kilowatt-hours\n(kWh) as follows. Let pc be the average power\ndraw (in watts) from all CPU sockets during train-\ning, let pr be the average power draw from all\nDRAM (main memory) sockets, let pg be the aver-\nage power draw of a GPU during training, and let\ng be the number of GPUs used to train. We esti-\nmate total power consumption as combined GPU,\nCPU and DRAM consumption, then multiply this\nby Power Usage Effectiveness (PUE), which ac-\ncounts for the additional energy required to sup-\nport the compute infrastructure (mainly cooling).\nWe use a PUE coef\ufb01cient of 1.58, the 2018 global\naverage for data centers (\nAscierto, 2018). Then the\ntotal power pt required at a given instance during\ntraining is given by:\npt = 1.58t(pc + pr + gpg)\n1000 (1)\nThe U.S. Environmental Protection Agency (EPA)\nprovides average CO 2 produced (in pounds per\nkilowatt-hour) for power consumed in the U.S.\n(\nEPA, 2018), which we use to convert power to\nestimated CO2 emissions:\nCO2e = 0 .954pt (2)\nThis conversion takes into account the relative pro-\nportions of different energy sources (primarily nat-\nural gas, coal, nuclear and renewable) consumed\nto produce energy in the United States. Table\n2\nlists the relative energy sources for China, Ger-\nmany and the United States compared to the top\n5U.S. Dept. of Energy: https://bit.ly/2JTbGnI\n5China Electricity Council; trans. China Energy Portal:\nhttps://bit.ly/2QHE5O3\n\n---\n\nNow answer the following question based ONLY on the context above.\n\nQuestion: What is the estimated CO2 emission in metric tons for one year of average US home energy use?\n\nAdditional info (JSON): {\"answer_unit\": \"metric tons\", \"question_id\": \"q322\"}\n\nIMPORTANT: The \"answer_unit\" field specifies the expected format/unit for answer_value.\n- If answer_unit is a unit (e.g., \"kW\", \"USD\"), express answer_value as a number in that unit (no unit name).\n- If answer_unit is \"is_blank\", answer_value should be the exact identifier/name from context that answers the question.\n- If the answer is a numeric range, format as [lower,upper].\n\nReturn STRICT JSON with these keys in order:\n- explanation          (1-3 sentences explaining how context supports the answer and how you applied answer_unit; or \"is_blank\")\n- answer               (short natural-language response)\n- answer_value         (the value matching the expected format; or \"is_blank\")\n- ref_id               (list of document ids from context used as evidence; or \"is_blank\")\n- ref_url              (list of URLs for the cited documents; or \"is_blank\")\n- supporting_materials (verbatim quote, table reference, or figure reference from the cited document; or \"is_blank\")\n\nJSON Answer:",
    "retrieved_snippets": [
      {
        "node_id": "dodge2022:sec9:p10:s194",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "Even partially trained, experiments of\nthis size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from\nelectricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).",
        "score": 0.7363,
        "rank": 1,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec9:p10",
          "section_id": "dodge2022:sec9",
          "sentence_index": 194
        }
      },
      {
        "node_id": "dodge2022:sec9:p10",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "Measuring the Carbon Intensity of AI in Cloud Instances FAccT \u201922, June 21\u201324, 2022, Seoul, Republic of Korea\nFig. 2. Emissions for our 11 experiments described in \u00a74. For each model we show a vertical blue bar, where the top of the bar is\nthe max, the bottom is the min, and the black line represents the average emissions (across regions and time of year). First and\nfourth quartiles are represented by the light blue at the top and bottom of each vertical blue bar. The largest training runs (e.g., 6\nbillion parameter LM) releases a significant amount of emissions, no matter the region (and recall the 6 billion parameter LM is only\ntrained for 13% of a full run, so a full run would emit about an order of magnitude more emissions than reported here). The smallest\nexperiments emit very little. Presented on a log scale, with references on the right indicating equivalent sources of emissions per the\nUnited States Environmental Protection Agency [46].\nThe largest experiment in our set is the 6 billion parameter transformer, and that model is only partially trained (as\ndescribed in \u00a74, it is only trained for about 13% of the time needed to converge). Even partially trained, experiments of\nthis size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from\nelectricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year). Perhaps\nunsurprisingly, even the most efficient region of those we examined for that experiment still leads to more emissions\nthan a full barrel of oil. If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of\nCO2 (depending on the region it was run in).\nComparing against previous work on measuring emissions can be challenging without full information about data\nand model parallelism, GPU utilization, the number of weight updates, and other relevant factors; while we don\u2019t have\nexperiments covering the same models as previous work on estimating CO2, we can make approximate comparisons\nalong three dimensions: a) kWh per GPU hour, b) CO 2 grams per GPU hour, and c) CO 2 grams per kWh. Here we\ncompare against [34] and [33] which report information about training especially large models. Their estimates also\n9",
        "score": 0.7363,
        "rank": 1,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "page": 9,
          "paragraph_index": 10,
          "section_id": "dodge2022:sec9",
          "section_index": 9
        }
      },
      {
        "node_id": "morrison2025:sec6:p6:s132",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "(energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquivalent to...",
        "score": 0.7159,
        "rank": 2,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec6:p6",
          "section_id": "morrison2025:sec6",
          "sentence_index": 132
        }
      },
      {
        "node_id": "morrison2025:sec6:p6",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Published as a conference paper at ICLR 2025\nTable 1: We developed our models in five groups, based on parameter count and architecture: less than 1\nbillion, 1 billion, 7 billion, and 13 billion parameters, and our mixture-of-experts model with 1 billion active\nand 7 billion total parameters. We found that \u223c70% of our developmental environmental impact came from\ndeveloping the 7B and 13B models, and the total impact was emissions equivalent to 2.1 tanker trucks\u2019 worth\nof gasoline, and equal to about 7 and a half years of water used by the average person in the United States.\nGPU\nHours\nTotal\nMWh # Runs\nCarbon\nEmissions\n(tCO2eq)\nEquivalent to...\n(energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquivalent to...\n(water usage,\n1 person)\n<1B 29k 19 20 6 1 yr, 4 mo 24 3 mo\n7B 269k 196 375 65 13 yrs, 6 mo 252 2 yrs, 7 mo\n13B 191k 116 156 46 9 yrs, 7 mo 402 3 yrs, 7 mo\nMoE 27k 19 35 6 1 yr, 4 mo 24 3 mo\nTotal 680k 459 813 159 33 yrs, 1 mo 843 7 yrs, 5 mo\nHardware manufacturing NVIDIA does not release the embodied carbon emissions or water\nconsumption about the hardware it produces, so we assume the same embodied carbon emissions\nas Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU. There\nis little public information on how much water is required to produce a single GPU, though chip\nmanufacturing facilities require millions of liters per day. 10 Some estimates11 place TSMC water\nusage at 12.33 liters per square centimeter of hardware, which equals 100.4 liters per H100, which\nwe use for our analysis.\nWe additionally estimate the environmental impact from mining rare earth metals used during man-\nufacturing, assuming an H100 is 0.1% rare earth metal by mass. Mining 1 kg of rare earth materials\nconsumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-\ninch silicon wafer weighs 125 grams 12 and produces about 63 H100s. 13 14 Together, these add an\nadditional 2.2 liters consumed and 0.013 kg CO2eq per GPU.\nInternally, we assume a 4 year lifespan for our GPUs, which leads to an embodied emissions of\n0.013 kg of CO2eq and 0.003 liters of water consumed per GPU hour when the estimated embodied\nimpacts is amortized over the assumed lifetime of the GPU. We used 1.65 million GPU hours in\ntotal, leading to a total of 22 tCO2eq emitted and 4.8 kL of water consumed during manufacturing.\nDevelopment Before launching our final training runs for each model, we ran a series of controlled\nexperiments to stabilize and improve our training setup, to explore different parameter initializations\nand mid-training recipes, and to determine our final hyperparameters and data mixtures through\nscaling law experiments (Bhagia et al., 2024). We ran these in five distinct groups: small models\nwith less than 1 billion parameters, 1 billion parameter models, 7 billion parameter models, 13\nbillion parameter models, and our mixture-of-experts model. We report detailed development costs\nfor each group in Table 1.\nUnsurprisingly, we find that the majority of development costs ( \u223c70%) were incurred at the 7 and\n13 billion parameter scale, due to both the relative size of the model and our own prioritization, and\nwe see this both in the total environmental impact and the number of individual runs per category.\nUsing our data center\u2019s efficiency factors, we find that our development runs led to 159 tCO 2eq\nemitted and 843 kL of water consumed.\nFinal training runs Finally, we fully trained our series of models, ranging from 20 million to\n13 billion active parameters, with detailed information provided in Table 2. As we saw during\ndevelopment, the majority of the cost incurred came from training our 7B and 13B models, which\nwe trained to 2 to 5 trillion tokens. We also see that the 1B dense model required about as much\nenergy per trillion tokens as the MoE model with 1B active parameters, though the MoE model was\nslightly less efficient, most likely due to the extra compute required for routing tokens. In summary,\nwe find that our training runs led to 312 tCO2eq emitted and 1,921 kL of water consumed.\n10https://www.azcentral.com/story/opinion/op-ed/joannaallhands/2024/06/12/\ntsmc-arizona-water-use-recycling/74059522007/\n11https://www.semiconductor-digest.com/water-supply-challenges-for-the-semiconductor-industry/\n12https://web.archive.org/web/20131207002716/http://wafercare.com/Page.aspx?id=1012\n13https://anysilicon.com/die-per-wafer-formula-free-calculators/\n14https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/\n6",
        "score": 0.7159,
        "rank": 2,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "page": 6,
          "paragraph_index": 6,
          "section_id": "morrison2025:sec6",
          "section_index": 6
        }
      },
      {
        "node_id": "morrison2025:sec7:p7:s162",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "(energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquiv.",
        "score": 0.7122,
        "rank": 3,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec7:p7",
          "section_id": "morrison2025:sec7",
          "sentence_index": 162
        }
      },
      {
        "node_id": "morrison2025:sec7:p7",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Published as a conference paper at ICLR 2025\nTable 2: We list the estimated power usage, carbon emissions, and water consumption from training our\ndense transformers, ranging from 20 million to 13 billion parameters, trained on 1.7 to 5.6 trillion tokens, and\na mixture-of-experts model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens. We\nfind that the environmental impact is quite high, even for our relatively small models. Training our series of\nmodels emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and\nconsumed equivalent water to the average person in the U.S. for about 17 years.\n* One of the original OLMo 7B models was trained on LUMI, which runs entirely on hydroelectric power. See\nGroeneveld et al. (2024) for more information.\n\u2020 denotes unreleased models that were trained for various internal experiments.\nPower\nUsage\n(MWh)\nCarbon\nEmissions\n(tCO2eq)\nEquiv. to...\n(energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquiv. to...\n(water usage,\n1 person, U.S.)\nGemma 2B & 9B - 131 25 yrs, 11 mo - -\nLlama 2 7B 81 31 6 yrs, 1 mo - -\nLlama 2 13B 162 62 12 yrs, 2 mo - -\nLlama 3.1 8B - 420 83 years - -\nLlama 3.2 1B - 107 14 years - -\nOLMo 20M\u2020 0.8 0.3 3 weeks 1 3 days\nOLMo 60M\u2020 1.2 0.4 1 month 1.6 5 days\nOLMo 150M\u2020 2.4 1 2 mo, 1 wk 3.6 12 days\nOLMo 300M\u2020 5 2 5 months 5.9 19 days\nOLMo 700M\u2020 8 3 7 months 10 33 days\nOLMo 7B\u2020 67 22 4 yrs, 4 mo 87 9 months\nOLMo 1B (3T) 30 10 2 years 39 4 months\nOLMo 7B 149 0* - 0* -\nOLMo 7B (Twin) 114 70 13 yrs, 10 mo 487 4 yrs, 4 mo\nOLMo (04|07)24 7B 95 32 6 yrs, 4 mo 122 1 yr, 1 mo\nOLMo 2 7B 157 52 10 yrs, 4 mo 202 1 yr, 9 mo\nOLMo 2 13B 230 101 21 years 892 7 yrs, 10 mo\nOLMoE 0924 54 18 3 yrs, 7 mo 70 7 months\nTotal (Ours) 913 312 65 years 1,921 17 yrs, 1 mo\nPutting it in perspective In total, our series of models led to at least 493 tCO2eq emitted. Using\nthe U.S. Environmental Protection Agency\u2019s Greenhouse Gas Equivalencies Calculator 15, this is\nequivalent to 6.5 tanker trucks\u2019 worth of gasoline burned, emissions from the average yearly energy\nuse for 98.2 homes in the U.S., or the amount of carbon sequestered by 472 acres of U.S. forests in\none year. We additionally estimate we consumed at least 2,769 kL of water, which is equivalent to\nabout 24 and a half years of water consumption by the average person in the U.S.16\nOther Costs In this work, we strive to provide a thorough accounting of the total cost of develop-\ning our models. However, there remain a number of sources of emissions and water consumption\nthat are difficult, if not impossible to comprehensively measure without access to proprietary in-\nformation across a range of industries, such as transportation and end of life hardware disposal.\nWhile the costs we report above represent a large portion of the total development process, more\ntransparency is needed to understand the full impact of model training.\n4.2 S IMULATING DEPLOYMENT & INFERENCE\nWe report simulated inference costs; that is, we explore the question of what our models\u2019 impact\nmight be if they were put into production. In contrast to \u00a74.1, where we reported the actual im-\npact from our actions, this section reports partial estimates of Scope 3 carbon emissions and water\nconsumption: the impact from the downstream actions of others using our models. We include\ncomparisons with recent instruction-tuned models as well.\nIn Table 3, we display 1) power and energy costs, 2) carbon and water consumption, and 3) the\ntime to complete 100 requests. We additionally report \u201cbreakeven\u201d points, that is the number of\n15https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator\n16https://www.epa.gov/watersense/statistics-and-facts\n7",
        "score": 0.7122,
        "rank": 3,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "page": 7,
          "paragraph_index": 7,
          "section_id": "morrison2025:sec7",
          "section_index": 7
        }
      },
      {
        "node_id": "dodge2022:sec8:p8:s182",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "One phone charge is estimated to emit 8.22 \u00d7 10\u22126 metric tons (using US national weighted\naverage CO2 marginal emission rate for delivered electricity), one mile driven is estimated to emit 3.98 \u00d7 10\u22124 metric\ntons (using average US passenger vehicle, which gets 22.5 miles per gallon of gasoline), one gallon of gasoline consumed\nis estimated to emit 8.887 \u00d7 10\u22123 metric tons, one barrel of crude oil consumed is estimated to emit 0.43 metric tons,\none average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating\nelectricity, natural gas, liquid petroleum, and fuel oil), and one rail car of coal is estimated to emit 181.29 metric tons.",
        "score": 0.6902,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "paragraph_id": "dodge2022:sec8:p8",
          "section_id": "dodge2022:sec8",
          "sentence_index": 182
        }
      },
      {
        "node_id": "dodge2022:sec8:p8",
        "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
        "text": "FAccT \u201922, June 21\u201324, 2022, Seoul, Republic of Korea Dodge et al.\nFig. 1. Carbon emissions that would be emitted from training BERT (language modeling on 8 V100s for 36 hours) in 16 different\nregions (one region per line) at different times throughout the year. Each line is relatively flat, indicating the emissions in a single\nregion during different months are relatively similar. There is large variation between the least carbon-intensive regions (the lowest\nlines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run\ncan be very impactful ( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions).\nemitted from training BERT (see \u00a74 for more details) on 8 V100 GPUs for 36 hours in 16 different regions (one region\nper line) at different times throughout the year.\nWhat do emissions look like across the 11 experiments described in \u00a74? In Figure 2 we show results for all 11\nexperiments, which cover two BERT experiments (finetuning and language modeling), partial training of a 6.1 billion\nparameter Transformer, 3 sizes of DenseNets, and five sizes of Vision Transformers. Each experiment is represented by\na vertical blue bar showing the range of emissions that would be emitted for that experiment across different regions.\nThe top of the blue bar is the emissions from running that experiment in the region with the most emissions, the bottom\nis the emissions from running that experiment in the region with the least emissions, the black line represents the\naverage, and the light blue regions are the top and bottom quartiles.\nIn Figure 2 we also include estimates of equivalent sources of emissions per the United States Environmental\nProtection Agency [46]. One phone charge is estimated to emit 8.22 \u00d7 10\u22126 metric tons (using US national weighted\naverage CO2 marginal emission rate for delivered electricity), one mile driven is estimated to emit 3.98 \u00d7 10\u22124 metric\ntons (using average US passenger vehicle, which gets 22.5 miles per gallon of gasoline), one gallon of gasoline consumed\nis estimated to emit 8.887 \u00d7 10\u22123 metric tons, one barrel of crude oil consumed is estimated to emit 0.43 metric tons,\none average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating\nelectricity, natural gas, liquid petroleum, and fuel oil), and one rail car of coal is estimated to emit 181.29 metric tons.\n8",
        "score": 0.6902,
        "rank": 4,
        "metadata": {
          "document_id": "dodge2022",
          "document_title": "Measuring the Carbon Intensity of AI in Cloud Instances",
          "page": 8,
          "paragraph_index": 8,
          "section_id": "dodge2022:sec8",
          "section_index": 8
        }
      },
      {
        "node_id": "wu2021b:sec5:p7:s144",
        "document_title": "SOCIO-TECHNOLOGICAL CHALLENGES AND OPPORTUNITIES: PATHS FORWARD",
        "text": "For the residential sector in the\nUS, space heating and cooling contributed to over 40% of the total electricity consumption [U.S.",
        "score": 0.6735,
        "rank": 5,
        "metadata": {
          "document_id": "wu2021b",
          "document_title": "SOCIO-TECHNOLOGICAL CHALLENGES AND OPPORTUNITIES: PATHS FORWARD",
          "paragraph_id": "wu2021b:sec5:p7",
          "section_id": "wu2021b:sec5",
          "sentence_index": 144
        }
      },
      {
        "node_id": "wu2021b:sec5:p7",
        "document_title": "SOCIO-TECHNOLOGICAL CHALLENGES AND OPPORTUNITIES: PATHS FORWARD",
        "text": "Socio-Technological Challenges and Opportunities: Paths Forward A PREPRINT\ninfrastructure may be better served by (co-)designing agile applications and data centers, such as [\u00cd\u00f1igo Goiri et al.,\n2015, Radovanovic et al., 2021, Lin et al., 2021, Zhang et al., 2021], that implicitly assume power \ufb02uctuations and\nvariability.\nAchieving pervasive connectivity in the presence of resiliency, security, and privacy requirements can be challenging\ngiven the interrelated nature of the problems. For instance, current approaches to achieving resilient computing often\nrely on replication and redundancy which not only increases cost but also enlarges the surface of security and privacy\npreservation. Migrating applications to another region, which may be necessary due to power constraints, can expose\ndata to further security threats. To retain physical security of data, applications may be hosted in limited geographical\nregions but this can increase application vulnerability to catastrophic events. In addition, secure and resilient computing\ninfrastructures can often come with signi\ufb01cant environmental implications. Hence, any computing infrastructure\nsolutions must be cognizant of the multifaceted nature of the problems being addressed.\nSustainability. Resource limitations, climate change, water depletion, electronic waste, ecosystem damage, and\nenvironmental racism are just a few of the topics under the larger sustainability umbrella. There is increased focus on\nthese topics from both industrial and political institutions. All major technology companies have pledged to reduce or\neliminate their carbon footprint in the next decade by reducing the environmental impact associated with manufacturing\nand using their products. Examples of such commitments are Facebook achieving NetZero in operational emissions\nin 2020 and across its value chain by 2030 [Facebook], Apple\u2019s pledge for 100% carbon neutral supply chain by\n2030 [Apple], Microsoft\u2019s goal of being carbon negative by 2030 [Smith], and Google\u2019s aim of 24x7 carbon free data\ncenters [Google, b]. In 2020, Amazon, Google, Facebook, and Microsoft were the top four technology companies\nthat purchased signi\ufb01cant renewable energy capacities, accounting for 30% of the cumulative total from corporations\nglobally [Schechner, 2021]. In addition, countries and trading zones are legislating carbon emission requirements.\nChina has committed to be carbon free by 2060 [Myers, 2020] and the EU has committed to cut carbon emissions by\n55% by 2030 [BBC, 2021].\nSustainability targets and the associated regulations will continue to increase, and hardware must be manufactured with\nless planetary impact, use less energy while in operation, and produce less e-waste at the end of life [Orcuttarchive, 2015,\nChang et al., 2017]. Existing practices such as the move to hyperscale data centers have already reduced IT\u2019s carbon\nfootprint by consolidating and sharing computing resources, and operating those resources more ef\ufb01ciently (Figure 1)\n\u2013 AWS [Amazon, 2019], Azure [Microsoft, 2020], Google cloud [Evans and Gao, 2016], and Facebook datacenter\ninfrastructures [Lee and Rowe, 2020]. In fact, data center electricity consumption has slowed down signi\ufb01cantly. The\ntotal energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated\n24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020]. Furthermore, despite the\nincrease in the global data center energy use, the number of users bene\ufb01ting from the cloud infrastructure have increased\neven more\u2014from 2010-18, the global data center compute instances increased by 5.5 times with an estimated 6%\nincrease in the global data center energy use.\nHowever, more can be done \u2013we must go beyond ef\ufb01ciency optimization and build a sustainable ecosystem to achieve\nenvironmentally-sustainable computing. For instance, develop expandable hardware and software stack that facilitate\nsigni\ufb01cantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020] and 4\nto 5 years for servers [Ascierto and Lawrence, 2020]. Modular system design will enable component-level upgrades\nwithout having to decommission the system at its entirety, reducing overall electronic waste and the environmental\nfootprint [Fairphone]. Other actions being discussed or implemented require manufacturers to make their systems more\nrepairable, resulting in increased product lifetime [Wiens and Gordon-Byrne, 2017, Alsever, 2021]. Another option is\nto reduce the number of devices in an average household. In the US, for example, the average household is equipped\nwith an average of 25 connected devices [Deloitte, 2021]. In many cases, smartphones might be powerful enough for\nthe task, but a tablet or laptop is needed for a viable keyboard or a larger viewing platform. Some of these additional\ndevices could be replaced with a virtual reality or augmented reality solution that is both portable and can provide\nvirtual keyboards and visual clarity without requiring additional hardware. To achieve an environmentally sustainable\ncomputing future, we will have to build a circular economy for computing that supports the design principle of reduce,\nreuse, repair, and recycle. These and other potential solutions likely require a complete redesign of the software and\nhardware stacks both at the edge, within the cloud, and in the edge-cloud collaborative execution environment, in order\nto provide resilient, long lasting, innovative solutions.\nMaking technology more sustainable is only one part of the technical challenge. There is another side to the story\u2014there\nare signi\ufb01cant sustainability bene\ufb01ts resulting from computing technology. Programs, such as Farm Beats [Microsoft,\n2015], address how to optimize operations on a farm using technology that is inexpensive and readily available to\npeople in rural areas. Food production accounts for 19% of the world\u2019s carbon emissions, and producing food more\nef\ufb01ciently and with less toxicity has long term bene\ufb01ts for the world [Gates, 2021]. For the residential sector in the\nUS, space heating and cooling contributed to over 40% of the total electricity consumption [U.S. Energy Information\n5",
        "score": 0.6735,
        "rank": 5,
        "metadata": {
          "document_id": "wu2021b",
          "document_title": "SOCIO-TECHNOLOGICAL CHALLENGES AND OPPORTUNITIES: PATHS FORWARD",
          "page": 5,
          "paragraph_index": 7,
          "section_id": "wu2021b:sec5",
          "section_index": 5
        }
      },
      {
        "node_id": "jegham2025:sec1:p1:s7",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "homes, evaporative freshwater equal to the annual drink-\ning needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest\nto offset.",
        "score": 0.6704,
        "rank": 6,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "paragraph_id": "jegham2025:sec1:p1",
          "section_id": "jegham2025:sec1",
          "sentence_index": 7
        }
      },
      {
        "node_id": "jegham2025:sec1:p1",
        "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
        "text": "How Hungry is AI? Benchmarking Energy, Water, and\nCarbon Footprint of LLM Inference\nNidhal Jegham1,2\nnidhal.jegham@uri.edu\nMarwan Abdelatti3\nmabdelat@providence.edu\nChan Young Koh1\nckoh04@uri.edu\nLassad Elmoubarki2\nlassad.elmoubarki@tbs.rnu.tn\nAbdeltawab Hendawi1\u2217\nhendawi@uri.edu\n1 University of Rhode Island 2 University of Tunis 3 Providence College\nLive Dashboard: Power BI Dashboard\nAbstract\nThis paper introduces an infrastructure-aware benchmarking framework for quanti-\nfying the environmental footprint of LLM inference across 30 state-of-the-art mod-\nels in commercial datacenters. The framework combines public API performance\ndata with company-specific environmental multipliers and statistical inference of\nhardware configurations. We additionally utilize cross-efficiency Data Envelop-\nment Analysis (DEA) to rank models by performance relative to environmental cost\nand provide a dynamically updated dashboard that visualizes model-level energy,\nwater, and carbon metrics. Results show the most energy-intensive models exceed\n29 Wh per long prompt, over 65 \u00d7 the most efficient systems. Even a 0.42 Wh\nshort query, when scaled to 700M queries/day, aggregates to annual electricity\ncomparable to 35,000 U.S. homes, evaporative freshwater equal to the annual drink-\ning needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest\nto offset. These findings highlight a growing paradox: as AI becomes cheaper\nand faster, global adoption drives disproportionate resource consumption. Our\nmethodology offers a standardized, empirically grounded basis for sustainability\nbenchmarking and accountability in AI deployment.\n1 Introduction\nLarge language models (LLMs) have moved beyond research labs and are now embedded in search\nengines, virtual assistants, education platforms, and enterprise tools [1, 2, 3, 4]. Models like GPT-4o\n[5] and Claude-3.7 Sonnet [6] represent state-of-the-art systems, while open-source alternatives such\nas LLaMA-3 [7] and DeepSeek-V3 [8] reflect growing accessibility and experimentation. On top of\nthat, the emergence of reasoning models such as DeepSeek-R1 [9], o1 [10], and o3-mini [11] marks\na shift toward multi-step logic and chain-of-thought reasoning.\nHowever, the advancement of LLMs does involve shortcomings in environmental aspects. Training\nGPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric\ntons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for\ncooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool. Yet while training\nhas been the focus of sustainability discussions, inference is emerging as the primary contributor to\nenvironmental costs. In contrast to training, which is conducted once or at intervals, inference occurs\nconsistently and on a large scale. Recent estimates suggest inference can account for up to 90% of a\nmodel\u2019s total lifecycle energy use [14, 15].\n\u2217Corresponding author.\narXiv:2505.09598v6  [cs.CY]  24 Nov 2025",
        "score": 0.6704,
        "rank": 6,
        "metadata": {
          "document_id": "jegham2025",
          "document_title": "How Hungry is AI? Benchmarking Energy, Water and Carbon Footprint of LLM Inference",
          "page": 1,
          "paragraph_index": 1,
          "section_id": "jegham2025:sec1",
          "section_index": 1
        }
      },
      {
        "node_id": "morrison2025:sec7:p7:s166",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Environmental Protection Agency\u2019s Greenhouse Gas Equivalencies Calculator 15, this is\nequivalent to 6.5 tanker trucks\u2019 worth of gasoline burned, emissions from the average yearly energy\nuse for 98.2 homes in the U.S., or the amount of carbon sequestered by 472 acres of U.S.",
        "score": 0.6673,
        "rank": 7,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "paragraph_id": "morrison2025:sec7:p7",
          "section_id": "morrison2025:sec7",
          "sentence_index": 166
        }
      },
      {
        "node_id": "morrison2025:sec7:p7",
        "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
        "text": "Published as a conference paper at ICLR 2025\nTable 2: We list the estimated power usage, carbon emissions, and water consumption from training our\ndense transformers, ranging from 20 million to 13 billion parameters, trained on 1.7 to 5.6 trillion tokens, and\na mixture-of-experts model with 1 billion active and 7 billion total parameters, trained to 5 trillion tokens. We\nfind that the environmental impact is quite high, even for our relatively small models. Training our series of\nmodels emitted equivalent carbon to over 65 years of electricity use by the average household in the U.S., and\nconsumed equivalent water to the average person in the U.S. for about 17 years.\n* One of the original OLMo 7B models was trained on LUMI, which runs entirely on hydroelectric power. See\nGroeneveld et al. (2024) for more information.\n\u2020 denotes unreleased models that were trained for various internal experiments.\nPower\nUsage\n(MWh)\nCarbon\nEmissions\n(tCO2eq)\nEquiv. to...\n(energy usage,\n1 home, U.S.)\nWater\nConsumption\n(kL)\nEquiv. to...\n(water usage,\n1 person, U.S.)\nGemma 2B & 9B - 131 25 yrs, 11 mo - -\nLlama 2 7B 81 31 6 yrs, 1 mo - -\nLlama 2 13B 162 62 12 yrs, 2 mo - -\nLlama 3.1 8B - 420 83 years - -\nLlama 3.2 1B - 107 14 years - -\nOLMo 20M\u2020 0.8 0.3 3 weeks 1 3 days\nOLMo 60M\u2020 1.2 0.4 1 month 1.6 5 days\nOLMo 150M\u2020 2.4 1 2 mo, 1 wk 3.6 12 days\nOLMo 300M\u2020 5 2 5 months 5.9 19 days\nOLMo 700M\u2020 8 3 7 months 10 33 days\nOLMo 7B\u2020 67 22 4 yrs, 4 mo 87 9 months\nOLMo 1B (3T) 30 10 2 years 39 4 months\nOLMo 7B 149 0* - 0* -\nOLMo 7B (Twin) 114 70 13 yrs, 10 mo 487 4 yrs, 4 mo\nOLMo (04|07)24 7B 95 32 6 yrs, 4 mo 122 1 yr, 1 mo\nOLMo 2 7B 157 52 10 yrs, 4 mo 202 1 yr, 9 mo\nOLMo 2 13B 230 101 21 years 892 7 yrs, 10 mo\nOLMoE 0924 54 18 3 yrs, 7 mo 70 7 months\nTotal (Ours) 913 312 65 years 1,921 17 yrs, 1 mo\nPutting it in perspective In total, our series of models led to at least 493 tCO2eq emitted. Using\nthe U.S. Environmental Protection Agency\u2019s Greenhouse Gas Equivalencies Calculator 15, this is\nequivalent to 6.5 tanker trucks\u2019 worth of gasoline burned, emissions from the average yearly energy\nuse for 98.2 homes in the U.S., or the amount of carbon sequestered by 472 acres of U.S. forests in\none year. We additionally estimate we consumed at least 2,769 kL of water, which is equivalent to\nabout 24 and a half years of water consumption by the average person in the U.S.16\nOther Costs In this work, we strive to provide a thorough accounting of the total cost of develop-\ning our models. However, there remain a number of sources of emissions and water consumption\nthat are difficult, if not impossible to comprehensively measure without access to proprietary in-\nformation across a range of industries, such as transportation and end of life hardware disposal.\nWhile the costs we report above represent a large portion of the total development process, more\ntransparency is needed to understand the full impact of model training.\n4.2 S IMULATING DEPLOYMENT & INFERENCE\nWe report simulated inference costs; that is, we explore the question of what our models\u2019 impact\nmight be if they were put into production. In contrast to \u00a74.1, where we reported the actual im-\npact from our actions, this section reports partial estimates of Scope 3 carbon emissions and water\nconsumption: the impact from the downstream actions of others using our models. We include\ncomparisons with recent instruction-tuned models as well.\nIn Table 3, we display 1) power and energy costs, 2) carbon and water consumption, and 3) the\ntime to complete 100 requests. We additionally report \u201cbreakeven\u201d points, that is the number of\n15https://www.epa.gov/energy/greenhouse-gas-equivalencies-calculator\n16https://www.epa.gov/watersense/statistics-and-facts\n7",
        "score": 0.6673,
        "rank": 7,
        "metadata": {
          "document_id": "morrison2025",
          "document_title": "Holistically Evaluating the Environmental Impact of Creating Language Models",
          "page": 7,
          "paragraph_index": 7,
          "section_id": "morrison2025:sec7",
          "section_index": 7
        }
      },
      {
        "node_id": "patterson2021:sec17:p38:s590",
        "document_title": "Carbon Emissions and Large Neural Network Training",
        "text": "[USE21]  US   Energy   Information   Administration,   2021,   FAQ   How   much   carbon   dioxide   is   produced   per   kilowatt   hour   of   \nU.S.",
        "score": 0.6664,
        "rank": 8,
        "metadata": {
          "document_id": "patterson2021",
          "document_title": "Carbon Emissions and Large Neural Network Training",
          "paragraph_id": "patterson2021:sec17:p38",
          "section_id": "patterson2021:sec17",
          "sentence_index": 590
        }
      },
      {
        "node_id": "patterson2021:sec17:p38",
        "document_title": "Carbon Emissions and Large Neural Network Training",
        "text": "17   \n[Jou21]   Jouppi,   N.,   Yoon,   D-H,   Jablin,   T.,   Kurian,   G.,   Laudon,   J.,   Li,   S.,   Ma,   P.,   Ma,   X.,   Patil,   N.,Prasad,   S.,   Young,   C.,   \nZhou,   Z.,   and   Patterson,   D.,   May   2021.   Ten   Lessons   From   Three   Generations   Shaped   Google\u2019s   TPUv4i,   to  \nappear,    the   48th   International   Symposium   on   Computer   Architecture.   \n[Kap20]  Kaplan,   J.,   McCandlish,   S.,   Henighan,   T.,   Brown,   T.B.,   Chess,   B.,   Child,   R.,   Gray,   S.,   Radford,   A.,   Wu,   J.   and   \nAmodei,   D.,   2020.   Scaling   laws   for   neural   language   models.   arXiv   preprint   arXiv:2001.08361.   \n[K\u00e4r18]   K\u00e4rcher   B.   Formation   and   radiative   forcing   of   contrail   cirrus.    Nature   communication s.   2018   May   8;9(1):1-7.   \nhttps://www.nature.com/articles/s41467-018-04068-0 .   \n[Kuc18]   Kuczmarski,   J.   and   Johnson,   M.,   2018.   Gender-aware   natural   language   \ntranslation. www.tdcommons.org/dpubs_series/1577/ .   \n[Lac19]   Lacoste,   A.,   Luccioni,   A.,   Schmidt,   V.   and   Dandres,   T.,   2019.   Quantifying   the   carbon   emissions   of   machine   \nlearning.    arXiv   preprint   arXiv:1910.09700 .   \n[Lan20]   Lannelongue,   L.,   Grealey,   J.   and   Inouye,   M.,   2020.   Green   algorithms:   Quantifying   the   carbon   footprint   of   \ncomputation.    arXiv:   2007.07610 .   \n[Leo19]   Leopold,   G.   March   19,   2019,   AWS   to   Offer   Nvidia\u2019s   T4   GPUs   for   AI   Inferencing,   \nwww.hpcwire.com/2019/03/19/aws-upgrades-its-gpu-backed-ai-inference-platform/    .   \n[Lep20]   Lepikhin,   D.,   Lee,   H.,   Xu,   Y.,   Chen,   D.,   Firat,   O.,   Huang,   Y.,   Krikun,   M.,   Shazeer,   N.   and   Chen,   Z.,   2020.   GShard:  \nScaling   giant   models   with   conditional   computation   and   automatic   sharding.    arXiv   preprint   arXiv:2006.16668 .   \n[Li21]   Li,   S.,   Tan,   M.,   Pang,   R.,   Li,   A.,   Cheng,   L.,   Le,   Q.   and   Jouppi,   N.P.,   2021.   Searching   for   Fast   Model   Families   on   \nDatacenter   Accelerators.    arXiv   preprint   arXiv:2102.05610 .   \n[Liu18]   Liu,   H.,   Simonyan,   K.   and   Yang,   Y.,   2018.   Darts:   Differentiable   architecture   search.    arXiv   preprint   \narXiv:1806.09055 .   \n[Luc21]   Luccioni,   A.,   and   Schmidt,   V..   March   2021,   Private   Communication.   \n[Mas20]  Masanet,   E.,   Shehabi,   A.,   Lei,   N.,   Smith,   S.   and   Koomey,   J.,   2020.   Recalibrating   global   datacenter   energy-use   \nestimates.    Science ,   367(6481),   pp.984-986.   \nhttps://datacenters.lbl.gov/sites/default/files/Masanet_et_al_Science_2020.full_.pdf .   \n[Mas21]  Masanet,   E.,   March   24,   2021,    Data   Center   Energy   Analysis:   Past,   Present,   and   Future ,   lecture   at   UCSB.   \n[Mer19]   Mehrabi,   N.,   Morstatter,   F.,   Saxena,   N.,   Lerman,   K.   and   Galstyan,   A.,   2019.   A   survey   on   bias   and   fairness   in   \nmachine   learning.   arXiv   preprint   arXiv:1908.09635.    https://arxiv.org/pdf/1908.09635.pdf .   \n[Pha18]  Pham,   H.,   Guan,   M.,   Zoph,   B.,   Le,   Q.   and   Dean,   J.,   2018,   July.   Efficient   neural   architecture   search   via   \nparameters   sharing.   In   International   Conference   on   Machine   Learning   (pp.   4095-4104).   PMLR.     arXiv   preprint   \narXiv:1802.03268 .   \n[Rad20]  Radovanovic,   A.   April   22,   2020,   Our   datacenters   now   work   harder   when   the   sun   shines   and   wind   blows,   \nhttps://blog.google/inside-google/infrastructure/data-centers-work-harder-sun-shines-wind-blows   \n[Raf19]   Raffel,   C.,   Shazeer,   N.,   Roberts,   A.,   Lee,   K.,   Narang,   S.,   Matena,   M.,   Zhou,   Y.,   Li,   W.   and   Liu,   P.J.,   2019.   \nExploring   the   limits   of   transfer   learning   with   a   unified   text-to-text   transformer.    arXiv   preprint   arXiv:1910.10683 .   \n[Rit20]   Ritchie,   H.,   October   22,   2020,   Climate   change   and   flying:   what   share   of   global   CO2   emissions   come   from   \naviation?    https://ourworldindata.org/co2-emissions-from-aviation    .   \n[Ryo14]  Ryor,   J.N.   and   Tawney,   L.E.T.H.A.,   2014.   Utility-Scale   Renewable   Energy:   Understanding   Cost   Parity.   Paris:   \nWorld   Resources   Institute.   \nhttps://www.ctc-n.org/sites/www.ctc-n.org/files/resources/wri14_factsheets_utility_scale_v4.pdf .   \n[San20]  Sanh,   V.,   Debut,   L.,   Chaumond,   J.   and   Wolf,   T.,   2019.   DistilBERT,   a   distilled   version   of   BERT:   smaller,   faster,   \ncheaper   and   lighter.    arXiv   preprint   arXiv:1910.01108 .   \n[Sch20]   Schwartz,   R.,   Dodge,   J.,   Smith,   N.A.   and   Etzioni,   O.,   2020.   Green   AI.    Communications   of   the   ACM ,   63(12),   \npp.54-63.    https://cacm.acm.org/magazines/2020/12/248800-green-ai/fulltext .   \n[Sch21]   Schleier-Smith,   J.,    Sreekanti,   V.,   Khandelwal,   A.,   Carreira,   J.,   Yadwadkar,   N.,   Popa,   R.,    Joseph   E.   Gonzalez,J.,  \nIon   Stoica,   I.,   and   David   A.   Patterson,   D.,   2021   What   Serverless   Computing   Is   and   Should   Become:   The   Next   \nPhase   of   Cloud   Computing,    Communications   of   the   ACM,    64(5) .     \n[Sha17]  Shazeer,   N.,   Mirhoseini,   A.,   Maziarz,   K.,   Davis,   A.,   Le,   Q.,   Hinton,   G.   and   Dean,   J.,   2017.   Outrageously   large   \nneural   networks:   The   sparsely-gated   mixture-of-experts   layer.   ICLR   2017.     arXiv   preprint   arXiv:1701.06538 .   \n[So19]   So,   D.,   Le,   Q.   and   Liang,   C.,   2019,   May.   The   Evolved   Transformer.   In   International   Conference   on   Machine   \nLearning   2019   (pp.   5877-5886).   PMLR.     arXiv   preprint   arXiv:1901.11117 .   \n[Str19]   Strubell,   E.,   Ganesh,   A.   and   McCallum,   A.,   2019.   Energy   and   policy   considerations   for   deep   learning   in   NLP.   \nACL   2019.     arXiv   preprint   arXiv:1906.02243 .   \n[Sut21]   Sutskever,   I.   Personal   Communication,   February   4,   2021.   \n[Tan19]   Tan,   M.   and   Le,   Q.,   2019,   May.   EfficientNet:   Rethinking   model   scaling   for   convolutional   neural   networks.   In   \nInternational   Conference   on   Machine   Learning   (pp.   6105-6114).   PMLR.    arXiv   preprint   arXiv:1905.11946 .   \n[USE21]  US   Energy   Information   Administration,   2021,   FAQ   How   much   carbon   dioxide   is   produced   per   kilowatt   hour   of   \nU.S.   electricity   generation?    https://www.eia.gov/tools/faqs/faq.php?id=74&t=11 .   \n[Vas17]   Vaswani,   A.,   Shazeer,   N.,   Parmar,   N.,   Uszkoreit,   J.,   Jones,   L.,   Gomez,   A.N.,   Kaiser,   L.   and   Polosukhin,   I.,   2017.   \nAttention   is   all   you   need.   NeurIPS   2017.     arXiv   preprint   arXiv:1706.03762 .   \n[Wan20]  Wang,   Y.,   Yao,   Q.,   Kwok,   J.T.   and   Ni,   L.M.,   2020.   Generalizing   from   a   few   examples:   A   survey   on   few-shot   \nlearning.    ACM   Computing   Surveys ,   53(3),   pp.1-34.",
        "score": 0.6664,
        "rank": 8,
        "metadata": {
          "document_id": "patterson2021",
          "document_title": "Carbon Emissions and Large Neural Network Training",
          "page": 17,
          "paragraph_index": 38,
          "section_id": "patterson2021:sec17",
          "section_index": 17
        }
      },
      {
        "node_id": "han2024:sec28:p34",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "A.4 Energy Consumption for Training a Generative AI Model\nWe consider Llama-3.1 as an example generative AI model. According to the model card [36], the training\nprocess of Llama-3.1 (including 8B, 70B, and 405B) utilizes a cumulative of 39.3 million GPU hours of com-\nputation on H100-80GB hardware, and each GPU has a thermal design power of 700 watts. Considering\nMeta\u2019s 2023 PUE of 1.08 [37] and excluding the non-GPU overhead for servers, we estimate the total train-\ning energy consumption as approximately 30 GWh. Our estimation method follows Meta\u2019s guideline [36]\nand is conservative, as it excludes the substantial non-GPU energy overheads (e.g., CPUs) associated with\nserver operations.\nA.5 Average Emission for Each LA-NYC Round Trip by Car\nWe use the 2023 national average emission rate for light-duty vehicles (gasoline) provided by the U.S. De-\npartment of Transportation [104]. The emission rate accounts for tailpipe exhaust, tire wear and brake wear.\nSpecifically, the average PM2.5 emission rate is 0.008 grams/mile (including 0.004 grams/mile for exhaust,\n0.003 grams/mile for brake wear, and 0.001 grams/mile for tire wear), and the average NOx emission rate\nis 0.199 grams/mile for exhaust. We see that half of PM2.5 for light-duty vehicles comes from brake and tire\nwear (0.004 gram/miles), which are also produced by other types of vehicles including electric vehicles.\nThe distance for a round-trip between Los Angeles, California, and New York City, New York, is about 5,580\nmiles. Thus, the average auto emissions for each LA-NYC round trip are estimated as 44.64 grams of PM2.5\nand 1110.42 grams of NOx.\nA.6 State-wide Electricity Consumption by U.S. Data Centers in 2023\nWe show in Fig. 10 the state-wide data center electricity consumption in 2023 [5]. It can be seen that Virginia,\nTexas and California have the highest data center electricity consumption in 2023. The total national elec-\ntricity consumption reported by EPRI is slightly lower than the values in [4], and we scale it up accordingly\nin our calculations to ensure consistency.\n0\n20\n40\n60\n80\n100\nPercentile\n(a)Electricity consumption map\nState\nElectricity\nConsumption\n(TWh)\nState\nElectricity\nConsumption\n(TWh)\nState\nElectricity\nConsumption\n(TWh)\nV A 33.85 OH 2.36 ID 0.15\nTX 21.81 SC 2.02 WI 0.15\nCA 9.33 WY 1.86 MD 0.10\nIL 7.45 KY 1.62 LA 0.08\nOR 6.41 CO 1.51 SD 0.07\nAZ 6.25 AL 1.49 ME 0.03\nIA 6.19 FL 1.38 NH 0.02\nGA 6.18 TN 1.33 RI 0.02\nWA 5.17 OK 1.23 KS <0.01\nPA 4.59 MA 1.06 AR <0.01\nNY 4.07 MO 0.97 DE <0.01\nNJ 4.04 MN 0.82 DC <0.01\nNE 3.96 MT 0.58 MS <0.01\nND 3.92 MI 0.53 VT <0.01\nNV 3.42 NM 0.40 WV <0.01\nNC 2.67 CT 0.26\nUT 2.56 IN 0.19\n(b)Electricity consumption by state (descending order)\nFigure 10:State-level electricity consumption of U.S. data centers in 2023 [5].\nB Additional Results for Health-Informed GLB\nB.1 Details of the experiment setup\nWe use Meta\u2019s electricity consumption for each U.S. data center location in 2023 [37] for our experiments.\nTable 7 summarizes the baseline annual energy loadWi for each data centeri. Since data centers are mostly\nstable loads in practice, the hourly workload for each location is calculated asli = Wi\nT , whereTrepresents\nthe total number of hours in the study period. The total hourly workload is then computed asMt = PN\ni=1 li.\nWe use the annual average industrial electricity prices in different states provided by the EIA [109].\nThe health pricep h\ni,t ($/MWh) and carbon emission rater c\ni,t (ton/MWh) are based on data provided by\nWattTime [77]. WattTime divides the U.S. into more than 100 regions and provides marginal health prices\nand carbon emission rates for each region. These values are updated every 5 minutes.\n28",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "page": 28,
          "paragraph_index": 34,
          "section_id": "han2024:sec28",
          "section_index": 28
        }
      },
      {
        "node_id": "han2024:sec28",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "Page 28",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "page": 28,
          "section_index": 28
        }
      },
      {
        "node_id": "han2024:sec28:p34:s912",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "A.4 Energy Consumption for Training a Generative AI Model\nWe consider Llama-3.1 as an example generative AI model.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 912
        }
      },
      {
        "node_id": "han2024:sec28:p34:s913",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "According to the model card [36], the training\nprocess of Llama-3.1 (including 8B, 70B, and 405B) utilizes a cumulative of 39.3 million GPU hours of com-\nputation on H100-80GB hardware, and each GPU has a thermal design power of 700 watts.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 913
        }
      },
      {
        "node_id": "han2024:sec28:p34:s914",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "Considering\nMeta\u2019s 2023 PUE of 1.08 [37] and excluding the non-GPU overhead for servers, we estimate the total train-\ning energy consumption as approximately 30 GWh.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 914
        }
      },
      {
        "node_id": "han2024:sec28:p34:s915",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "Our estimation method follows Meta\u2019s guideline [36]\nand is conservative, as it excludes the substantial non-GPU energy overheads (e.g., CPUs) associated with\nserver operations.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 915
        }
      },
      {
        "node_id": "han2024:sec28:p34:s916",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "A.5 Average Emission for Each LA-NYC Round Trip by Car\nWe use the 2023 national average emission rate for light-duty vehicles (gasoline) provided by the U.S.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 916
        }
      },
      {
        "node_id": "han2024:sec28:p34:s917",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "De-\npartment of Transportation [104].",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 917
        }
      },
      {
        "node_id": "han2024:sec28:p34:s918",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "The emission rate accounts for tailpipe exhaust, tire wear and brake wear.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 918
        }
      },
      {
        "node_id": "han2024:sec28:p34:s919",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "Specifically, the average PM2.5 emission rate is 0.008 grams/mile (including 0.004 grams/mile for exhaust,\n0.003 grams/mile for brake wear, and 0.001 grams/mile for tire wear), and the average NOx emission rate\nis 0.199 grams/mile for exhaust.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 919
        }
      },
      {
        "node_id": "han2024:sec28:p34:s920",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "We see that half of PM2.5 for light-duty vehicles comes from brake and tire\nwear (0.004 gram/miles), which are also produced by other types of vehicles including electric vehicles.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 920
        }
      },
      {
        "node_id": "han2024:sec28:p34:s921",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "The distance for a round-trip between Los Angeles, California, and New York City, New York, is about 5,580\nmiles.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 921
        }
      },
      {
        "node_id": "han2024:sec28:p34:s922",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "Thus, the average auto emissions for each LA-NYC round trip are estimated as 44.64 grams of PM2.5\nand 1110.42 grams of NOx.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 922
        }
      },
      {
        "node_id": "han2024:sec28:p34:s923",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "A.6 State-wide Electricity Consumption by U.S.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 923
        }
      },
      {
        "node_id": "han2024:sec28:p34:s924",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "Data Centers in 2023\nWe show in Fig.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 924
        }
      },
      {
        "node_id": "han2024:sec28:p34:s925",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "10 the state-wide data center electricity consumption in 2023 [5].",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 925
        }
      },
      {
        "node_id": "han2024:sec28:p34:s926",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "It can be seen that Virginia,\nTexas and California have the highest data center electricity consumption in 2023.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 926
        }
      },
      {
        "node_id": "han2024:sec28:p34:s927",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "The total national elec-\ntricity consumption reported by EPRI is slightly lower than the values in [4], and we scale it up accordingly\nin our calculations to ensure consistency.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 927
        }
      },
      {
        "node_id": "han2024:sec28:p34:s928",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "0\n20\n40\n60\n80\n100\nPercentile\n(a)Electricity consumption map\nState\nElectricity\nConsumption\n(TWh)\nState\nElectricity\nConsumption\n(TWh)\nState\nElectricity\nConsumption\n(TWh)\nV A 33.85 OH 2.36 ID 0.15\nTX 21.81 SC 2.02 WI 0.15\nCA 9.33 WY 1.86 MD 0.10\nIL 7.45 KY 1.62 LA 0.08\nOR 6.41 CO 1.51 SD 0.07\nAZ 6.25 AL 1.49 ME 0.03\nIA 6.19 FL 1.38 NH 0.02\nGA 6.18 TN 1.33 RI 0.02\nWA 5.17 OK 1.23 KS <0.01\nPA 4.59 MA 1.06 AR <0.01\nNY 4.07 MO 0.97 DE <0.01\nNJ 4.04 MN 0.82 DC <0.01\nNE 3.96 MT 0.58 MS <0.01\nND 3.92 MI 0.53 VT <0.01\nNV 3.42 NM 0.40 WV <0.01\nNC 2.67 CT 0.26\nUT 2.56 IN 0.19\n(b)Electricity consumption by state (descending order)\nFigure 10:State-level electricity consumption of U.S.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 928
        }
      },
      {
        "node_id": "han2024:sec28:p34:s929",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "data centers in 2023 [5].",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 929
        }
      },
      {
        "node_id": "han2024:sec28:p34:s930",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "B Additional Results for Health-Informed GLB\nB.1 Details of the experiment setup\nWe use Meta\u2019s electricity consumption for each U.S.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 930
        }
      },
      {
        "node_id": "han2024:sec28:p34:s931",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "data center location in 2023 [37] for our experiments.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 931
        }
      },
      {
        "node_id": "han2024:sec28:p34:s932",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "Table 7 summarizes the baseline annual energy loadWi for each data centeri.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 932
        }
      },
      {
        "node_id": "han2024:sec28:p34:s933",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "Since data centers are mostly\nstable loads in practice, the hourly workload for each location is calculated asli = Wi\nT , whereTrepresents\nthe total number of hours in the study period.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 933
        }
      },
      {
        "node_id": "han2024:sec28:p34:s934",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "The total hourly workload is then computed asMt = PN\ni=1 li.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 934
        }
      },
      {
        "node_id": "han2024:sec28:p34:s935",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "We use the annual average industrial electricity prices in different states provided by the EIA [109].",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 935
        }
      },
      {
        "node_id": "han2024:sec28:p34:s936",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "The health pricep h\ni,t ($/MWh) and carbon emission rater c\ni,t (ton/MWh) are based on data provided by\nWattTime [77].",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 936
        }
      },
      {
        "node_id": "han2024:sec28:p34:s937",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "WattTime divides the U.S.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 937
        }
      },
      {
        "node_id": "han2024:sec28:p34:s938",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "into more than 100 regions and provides marginal health prices\nand carbon emission rates for each region.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 938
        }
      },
      {
        "node_id": "han2024:sec28:p34:s939",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "These values are updated every 5 minutes.",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 939
        }
      },
      {
        "node_id": "han2024:sec28:p34:s940",
        "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
        "text": "28",
        "score": 0.6589,
        "rank": 9,
        "metadata": {
          "document_id": "han2024",
          "document_title": "The Unpaid Toll: Quantifying the Public Health Impact of AI",
          "paragraph_id": "han2024:sec28:p34",
          "section_id": "han2024:sec28",
          "sentence_index": 940
        }
      },
      {
        "node_id": "strubell2019:sec2:p5:s57",
        "document_title": "Energy and Policy Considerations for Deep Learning in NLP",
        "text": "Environmental Protection Agency (EPA)\nprovides average CO 2 produced (in pounds per\nkilowatt-hour) for power consumed in the U.S.",
        "score": 0.6577,
        "rank": 10,
        "metadata": {
          "document_id": "strubell2019",
          "document_title": "Energy and Policy Considerations for Deep Learning in NLP",
          "paragraph_id": "strubell2019:sec2:p5",
          "section_id": "strubell2019:sec2",
          "sentence_index": 57
        }
      },
      {
        "node_id": "strubell2019:sec2:p5",
        "document_title": "Energy and Policy Considerations for Deep Learning in NLP",
        "text": "model training and development likely make up\na substantial portion of the greenhouse gas emis-\nsions attributed to many NLP researchers.\nTo heighten the awareness of the NLP commu-\nnity to this issue and promote mindful practice and\npolicy, we characterize the dollar cost and carbon\nemissions that result from training the neural net-\nworks at the core of many state-of-the-art NLP\nmodels. We do this by estimating the kilowatts\nof energy required to train a variety of popular\noff-the-shelf NLP models, which can be converted\nto approximate carbon emissions and electricity\ncosts. To estimate the even greater resources re-\nquired to transfer an existing model to a new task\nor develop new models, we perform a case study\nof the full computational resources required for the\ndevelopment and tuning of a recent state-of-the-art\nNLP pipeline (\nStrubell et al. , 2018). We conclude\nwith recommendations to the community based on\nour \ufb01ndings, namely: (1) Time to retrain and sen-\nsitivity to hyperparameters should be reported for\nNLP machine learning models; (2) academic re-\nsearchers need equitable access to computational\nresources; and (3) researchers should prioritize de-\nveloping ef\ufb01cient models and hardware.\n2 Methods\nTo quantify the computational and environmen-\ntal cost of training deep neural network mod-\nels for NLP , we perform an analysis of the en-\nergy required to train a variety of popular off-\nthe-shelf NLP models, as well as a case study of\nthe complete sum of resources required to develop\nLISA (\nStrubell et al., 2018), a state-of-the-art NLP\nmodel from EMNLP 2018, including all tuning\nand experimentation.\nWe measure energy use as follows. We train the\nmodels described in \u00a7\n2.1 using the default settings\nprovided, and sample GPU and CPU power con-\nsumption during training. Each model was trained\nfor a maximum of 1 day. We train all models on\na single NVIDIA Titan X GPU, with the excep-\ntion of ELMo which was trained on 3 NVIDIA\nGTX 1080 Ti GPUs. While training, we repeat-\nedly query the NVIDIA System Management In-\nterface\n2 to sample the GPU power consumption\nand report the average over all samples. To sample\nCPU power consumption, we use Intel\u2019s Running\nAverage Power Limit interface.\n3\n2nvidia-smi: https://bit.ly/30sGEbi\n3RAPL power meter: https://bit.ly/2LObQhV\nConsumer Renew. Gas Coal Nuc.\nChina 22% 3% 65% 4%\nGermany 40% 7% 38% 13%\nUnited States 17% 35% 27% 19%\nAmazon-AWS 17% 24% 30% 26%\nGoogle 56% 14% 15% 10%\nMicrosoft 32% 23% 31% 10%\nTable 2: Percent energy sourced from: Renewable (e.g.\nhydro, solar, wind), natural gas, coal and nuclear for\nthe top 3 cloud compute providers (\nCook et al. , 2017),\ncompared to the United States, 4 China5 and Germany\n(Burger, 2019).\nWe estimate the total time expected for mod-\nels to train to completion using training times and\nhardware reported in the original papers. We then\ncalculate the power consumption in kilowatt-hours\n(kWh) as follows. Let pc be the average power\ndraw (in watts) from all CPU sockets during train-\ning, let pr be the average power draw from all\nDRAM (main memory) sockets, let pg be the aver-\nage power draw of a GPU during training, and let\ng be the number of GPUs used to train. We esti-\nmate total power consumption as combined GPU,\nCPU and DRAM consumption, then multiply this\nby Power Usage Effectiveness (PUE), which ac-\ncounts for the additional energy required to sup-\nport the compute infrastructure (mainly cooling).\nWe use a PUE coef\ufb01cient of 1.58, the 2018 global\naverage for data centers (\nAscierto, 2018). Then the\ntotal power pt required at a given instance during\ntraining is given by:\npt = 1.58t(pc + pr + gpg)\n1000 (1)\nThe U.S. Environmental Protection Agency (EPA)\nprovides average CO 2 produced (in pounds per\nkilowatt-hour) for power consumed in the U.S.\n(\nEPA, 2018), which we use to convert power to\nestimated CO2 emissions:\nCO2e = 0 .954pt (2)\nThis conversion takes into account the relative pro-\nportions of different energy sources (primarily nat-\nural gas, coal, nuclear and renewable) consumed\nto produce energy in the United States. Table\n2\nlists the relative energy sources for China, Ger-\nmany and the United States compared to the top\n5U.S. Dept. of Energy: https://bit.ly/2JTbGnI\n5China Electricity Council; trans. China Energy Portal:\nhttps://bit.ly/2QHE5O3",
        "score": 0.6577,
        "rank": 10,
        "metadata": {
          "document_id": "strubell2019",
          "document_title": "Energy and Policy Considerations for Deep Learning in NLP",
          "page": 2,
          "paragraph_index": 5,
          "section_id": "strubell2019:sec2",
          "section_index": 2
        }
      }
    ],
    "num_snippets": 49,
    "retry_count": 0
  },
  {
    "id": "q323",
    "question": "On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?",
    "gt_value": "27.8",
    "gt_unit": "score",
    "gt_ref": "['shen2024']",
    "pred_value": "27.8",
    "pred_unit": "score",
    "pred_ref": "[\"shen2024\"]",
    "pred_explanation": "The context from ref_id=shen2024 includes Table 3, which shows the performance of JetMoE-8B on the GSM8k benchmark with a score of 27.8. This score represents the model's performance on grade school math problem-solving, which is the exact metric asked for in the question.",
    "raw_response": "{\n  \"explanation\": \"The context from ref_id=shen2024 includes Table 3, which shows the performance of JetMoE-8B on the GSM8k benchmark with a score of 27.8. This score represents the model's performance on grade school math problem-solving, which is the exact metric asked for in the question.\",\n  \"answer\": \"JetMoE-8B achieved a score of 27.8 on the GSM8k benchmark.\",\n  \"answer_value\": 27.8,\n  \"ref_id\": [\"shen2024\"],\n  \"ref_url\": [],\n  \"supporting_materials\": \"Table 3: OpenLLM leaderboard and code benchmarks results from four different models. ... GSM8k 14.5 17.3 16.9 27.8\"\n}",
    "value_correct": true,
    "ref_score": 1.0,
    "na_correct": true,
    "weighted_score": 1.0,
    "latency_seconds": 104.93511986732483,
    "retrieval_seconds": 16.0011990070343,
    "generation_seconds": 43.64814591407776,
    "error": null,
    "pred_ref_url": [],
    "pred_supporting_materials": "Table 3: OpenLLM leaderboard and code benchmarks results from four different models. ... GSM8k 14.5 17.3 16.9 27.8",
    "rendered_prompt": "You will answer a question using ONLY the provided context snippets.\nIf the context does not clearly support an answer, use \"is_blank\".\n\nContext snippets from documents:\n[ref_id=samsi2024] The second dataset is GSM8K [22], consisting of\n8,500 human crafted grade school math problems.\n---\n[ref_id=samsi2024] III. E XPERIMENTAL SETUP\nWe conducted our experiments on the MIT Supercloud\nhigh-performance computing (HPC) system [18]. This het-\nerogeneous HPC cluster consists of 448 compute nodes with\ndual Intel Xeon Gold 6248 CPUs with 384 GB of RAM\nand two NVIDIA V olta V100 GPUs with 32 GB of memory\nper node. Each node on the system has two independent\nback-end fabrics: a 100 Gb/s Intel Omnipath as well as a\n25 Gb/s Ethernet interconnect using Mellanox ConnectX-4\nadapters with all servers connected to a single, non-blocking\nArista DCS-7516 Ethernet core switch. The GPUs, Omnipath,\nand Ethernet cards are all connected to PCIe slots that route\ndirectly to the Xeon processors without any intermediary PCIe\nswitches. All experiments in this paper exclusively used the\n25 Gb/s Ethernet interconnect. The system also includes 480\nCPU-only nodes with Intel Xeon Platinum 8260 processors.\nIn addition, four nodes with NVIDIA A100 GPUs were also\navailable for experiments described in this paper. A summary\nof the hardware is shown in Table I. All experiments described\nin this paper were run exclusively on NVIDIA GPUs.\nTABLE I: Compute node configurations: This table lists\nthe types of hardware used for inference evaluations in our\nexperiments. Each node consists of 2 CPUs and 2 GPUs in\nthe configuration listed below. All GPUs are from NVIDIA.\nCPU GPU\nType Memory TDP Type Memory TDP\n(GB) (W) (GB) (W)\nIntel Xeon\nGold 6248 384 150 V100 32 250\nIntel Xeon\nPlatinum 8358 503 240 A100 80 300\nA. Models\nExperiments were performed using open-source implemen-\ntation of the pre-trained LLaMA 65B model available via\nrequest from Meta [14] and evaluation scripts available via\nGitHub [19]. This implementation of the model uses Pytorch\nand the FairScale [20] library to enable model sharding across\nmultiple GPUs and nodes. For the models, we use a decoder\ntemperature setting \u03c4 = 0 .8 and a top- p value of 0.95 in\nattempts to align our settings with the general range of values\nthat are typically used. In future work, we aim to study\nhow varying decoding temperature, top- p, and other hyper-\nparameters may affect compute performance and energy usage\nduring inference. While our main focus is on LLaMA 65B,\nwe also examine LLaMA 7B and LLaMA 13B to characterize\ninference performance and energy under the bare minimum\nsettings/resources required to run these models.\nB. Datasets\nWe used two datasets to evaluate inference performance.\nThe first is an instruction following dataset used to fine-tune\nthe Alpaca [21] model (from here on, this dataset is referred\nto as \u201cAlpaca\u201d in our paper which is not to be confused with\nthe Alpaca model). This Alpaca dataset consists of 52,000\ninstruction-following tasks, instructions/questions where some\nhave example inputs and some do not, that the model is asked\nto answer. The second dataset is GSM8K [22], consisting of\n8,500 human crafted grade school math problems. The goal\nof using these two datasets is two-fold: (1) to evaluate the\nmodel on a diverse set of tasks in NLP and (2) evaluate\nhow different types of data and its underlying dynamics\ncan impact energy and inference performance. While natural\nlanguage is more common in LLM usage and in LLM training\ndata, increasingly new capabilities have been demonstrated\nin LLMs, including the ability to solve simple mathematical\nproblems, provide/correct examples of code, and more. Math\nquestions also differ considerably from questions posed in\nnatural language which can result in smaller context windows,\ninputs/outputs of differing lengths, number of decoded tokens,\netc. This, in turn, may impact inference performance in\neither throughput rates or energy costs. For this reason, our\nbenchmarking experiments are conducted on both datasets.\nFor both datasets, we sample 4,096 inputs for our inference\nexperiments. Using the entirety of the datasets would only\nserve to increase inference time and energy used for the ex-\nperimentation unreasonably and did not provide any significant\nbenefits to the study.\nC. Evaluation\nOur goal is to evaluate the inference performance, latency,\nand inference energy costs of LLaMA 65B as a representative\nlarge language model that requires sharding across multiple\nGPUs. We intend this to be a preliminary analysis that will\nhelp guide more in-depth experiments and benchmarking for\nour future work. Our analysis also includes limited analysis of\nsmaller LLaMA variants to illustrate inference performance\nand energy trade-offs in bare-minimum hardware settings:\nnamely, LLaMA 7B and 13B. While we do not control for\nthe correctness/quality of the outputs or the complexity of the\ninputs/outputs in studying trade-offs between inference energy\nand performance, we hope to account for this as an ablative\nstudy in future work. Similarly, we do not perform a com-\nprehensive evaluation with different optimization techniques\nor inference settings available for LLMs such as modeling\nquery arrival rates, model quantization, continuous batching,\netc. which we also leave for future work.\nInference performance is measured in terms of rates: words,\ntokens, and responses per second or, equivalently, the number\nof words, tokens, and responses generated per second. When\nrunning inference with LLaMA, the model generates a string\nof text for each input until the length of the text hits a\nmaximum generation length or a stop-word is encountered.\nThe number of words are calculated by counting the number\nof words present in the output by splitting each output string\non spaces. The number of tokens is calculated using LLaMA\u2019s\nown default tokenizer by counting the number of tokens in the\ntokenized output. Lastly, the number of responses per second\nor the response rate is calculated using the total number of\n---\n[ref_id=shen2024] The key advantages of JetMoE-8B include:\n\u2022 Openness and academia-friendly: JetMoE-8B is trained using only public datasets and\nopen-source training code, making it accessible to many academia research settings.\n---\n[ref_id=shen2024] JetMoE\nFigure 1: JetMoE architecture\nTo facilitate future efforts on open foundation models, particularly MoE models, we intro-\nduce JetMoE-8B, an innovative MoE architecture inspired by ModuleFormer (Shen et al.,\n2023) that extends the concept of sparse activation to both the attention and feed-forward\nlayers. Unlike prior works that only apply sparse activation to the feed-forward layer,\nJetMoE-8B leverages sparse activation in both components to further reduce computational\ncosts while maintaining performance.\nImpressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from\nmixed open-source datasets and 30,000 H100 GPU hours. Despite its low cost, JetMoE-8B\noutperforms the Llama2-7B model, and JetMoE-8B-Chat outperforms the Llama2-13B-Chat\nmodel, demonstrating that LLM training can be much more cost-effective than generally\nthought. In addition, JetMoE-8B has 8B parameters while only activating 2B for each input\ntoken, reducing inference computation by about 70% compared to Llama2-7B.\nThe key advantages of JetMoE-8B include:\n\u2022 Openness and academia-friendly: JetMoE-8B is trained using only public datasets and\nopen-source training code, making it accessible to many academia research settings.\nThe model can also be finetuned with limited compute budgets (e.g., consumer-grade\nGPUs).\n\u2022 Sparse activation on both attention and feed-forward layers , which significantly\nreduces training and inference costs. We also propose to share the kv projection in\nattention experts to improve training stability.\n\u2022 Comprehensive open-source data mixture, which ensures high-quality training using\nonly open-source datasets.\nThese innovations in JetMoE-8B pave the way for more accessible and efficient LLMs, bene-\nfiting the broader AI research community. To foster collaboration and further advancements,\nwe have detailed all the training parameters and data mixture in this report.\n2 Model Architecture\n2.1 Mixture of Experts\nA Mixture of Experts (MoE) layer comprises N modules f1, . . ., fN and a router g(e | x).\nGiven an input x to the MoE layer, the router predicts a probability distribution over the N\n2\n---\n[ref_id=samsi2024] (b) Results from GSM8K dataset\nFig.\n---\n[ref_id=samsi2024] (a) Results from the Alpaca dataset.\n(b) Results from GSM8K dataset\nFig. 2: Baseline comparison of inference perfor-\nmance/latency between LLaMA 7B, 13B and 65B: inference\nperformance comparisons on the minimum set of hardware\nrequired to run inference (see Table II) across model sizes\nand between V100s and A100s.\nFig. 3: Baseline energy per second (Watts) estimates of per-\nforming inference with LLaMA 7B, 13B, and 65B: inference\nenergy comparisons on the minimum set of hardware/settings\nrequired (see Table II) with Alpaca and GSM8K on a log-scale.\nColor indicates device (V100/A100), bars indicate average\nquantities and lines indicate error bars. Energy is averaged\nover maximum generation lengths of 256, 512, and 1024 due\nto near-identical energy/size trends for each generation length.\nFig. 4: Energy per second (Watts) estimates of LLaMA 65B\nacross batch sizes of 64/128/256/256 and 8/16/32 shards for\nmax generation length 512 : inference energy estimates on\nAlpaca and GSM8K on log-scale. Color indicates batch size.\nB. Energy per Second: LLaMA 65B\nWe first take a look at the amount of energy inference costs\nper unit time in seconds. Figures 4 and 5 show a more in-\ndepth look of the energy inference costs of LLaMA 65B across\ndifferent batch sizes and degrees of sharding. Specifically,\nFigure 4 shows energy costs for maximum generation length\n512 and Figure 5 shows energy costs for 1024.\nOverall, we see an average increase in energy per second\nwith the number of shards. While there is a slight correlation\nas energy per second increases with increasing batch size,\nincreasing the number of shards always increases the wattage.\nIndeed, the energy per second increases with the number of\nshards even at the same batch size (e.g., the energy of inference\nat batch size 64, going from 16 shards to 32 shards). For both\ndatasets, increasing the max generation length from 512 to\n1024 does seem to increase the energy per second for each\nbatch size within each shard configuration, but the overall\neffect is less clear or consistent. Overall, we see that the energy\nper second for inference with LLaMA 65B is on the order of\n300 Watts to 1 Kilowatt from the lower shard configuration of\n8 GPUs to the higher end of 32 GPUs.\nC. Energy per Decoded Token: LLaMA 65B\nMoving on to energy per each decoded output token, we see\nthat in Figures 6 and 7 that energy per token tends to follow\na similar pattern in relation to the number of shards: as the\nnumber of shards increases, the energy per output token also\n---\n[ref_id=shen2024] Table 1 shows the key hyperparameters in JetMoE-8B.\n---\n[ref_id=shen2024] JetMoE\n4 Model Pretraining\n4.1 Infrastructures\nWe use Megatron (Shoeybi et al., 2019) as the training framework and integrate\nMegablock (Gale et al., 2023) for MoE support. We further modified the training framework\nto support MoA (Section 2.3) and z-loss (Section 2.4). Against the common practice, we\nchoose the Pipeline parallelism introduced in (Narayanan et al., 2021) instead of the expert\nparallelism for model parallel during training. This is mainly due to two reasons. First,\nSparse MoE models usually have a narrower hidden state compared to standard transformer\nmodels. Thus, the communication cost for pipeline parallelism is smaller. Second, we use\nthe dropless MoE schema introduced in Gale et al. (2023); Shen et al. (2023), which could\ncause load unbalance across experts. Thus, using expert parallel will cause an unbalanced\nload across devices and result in inefficient training. Pipeline parallelism could avoid this\nslowdown because it computes all the experts inside a layer on the same device. We con-\nduct training on a cluster containing 12 nodes and 96 H100s. Inside each node, gpus are\nconnected via NVLinks. Infiniband is used for fast communication between nodes.\n4.2 Hyper-parameters\nPtotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp\n8B 2B 24 2048 8 2 16 128 5632\nTable 1: JetMoE-8B hyperparameters.\nThe hyperparameters of JetMoE-8B are selected based on the common practice for the 1B\ntransformer language model. We replace all self-attention and MLP layers in the transformer\nwith MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every\nlayer. Such that the model has approximately two times the computation compared to a 1B\nmodel. Following ST-MoE (Zoph et al., 2022), the weight for load balancing loss and z-loss\nis set to 0.01 and 0.001, respectively. Table 1 shows the key hyperparameters in JetMoE-8B.\nJetMoE-8B is trained with the AdamW optimizer (Loshchilov & Hutter, 2017) with a maxi-\nmum learning rate of 5e-4 and a batch size of 4M tokens with sequence length of 4096. We\nemploy the Warmup-Stable-Decay (WSD) learning rate schedule introduced in Hu et al.\n(2024). This learning rate scheduler is divided into three stages: the warmup stage (denoted\nby W, representing the number of steps at the end of the warmup stage), the stable training\nstage (denoted by S), and the annealing stage (denoted by D):\nlr(s) =\n\uf8f1\n\uf8f2\n\uf8f3\ns\nW \u2217 \u03b7, s < W\n\u03b7, W < s < S\nf (s \u2212 S) \u2217 \u03b7, S < s < S + D\n(13)\nwhere 0 < f (s \u2212 S) \u2264 1 is a decreasing function of s, and \u03b7 is the maximum learning rate.\nIn our settings, the warmup stage lasts for 10 billion tokens, and the decay stage spans 250\nbillion tokens. The initial and final learning rates are set to 10% of the maximum learning\nrate. A weight decay of 0.1 and gradient clipping of 1.0 are applied during training.\n4.3 Training Data Mixture\nJetMoE-8B is trained on 1.25T tokens of primarily English data from web documents,\nmathematics, and code. Similar to the approach advocated in miniCPM (Hu et al., 2024) and\nGemma (Team et al., 2024), we increase the weight of high-quality data during the learning\nrate decay phase. The training process is divided into two phases:\n\u2022 Phase 1 (warmup and stable learning rate): The dataset includes RefinedWeb, Star-\ncoder, The Pile, peS2o from Dolma, and OpenWebMath.\n6\n---\n[ref_id=shen2024] Despite\nthis limitation, JetMoE-8B-chat exhibits strong performance across various other dimensions,\nmaking it a competitive model in the open-source LLM landscape.\n---\n[ref_id=shen2024] JetMoE\nModel MT-Bench Score\nGPT-4 9.014\nGPT-3.5-turbo 7.995\nClaude-v1 7.923\nJetMoE-8B-chat 6.681\nLlama-2-13b-chat 6.650\nVicuna-13b-v1.3 6.413\nWizardlm-13b 6.353\nLlama-2-7b-chat 6.269\nTable 4: MT-Bench score comparison of various models\nFigure 3: MT-Bench radar figure\nhave a temperature of 0.0, suggesting preciseness; and \u201dSTEM\u201d and \u201dHumanities\u201d have a\ntemperature of 0.1, implying slightly more variability than 0.0 tasks.\nJetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment,\ndemonstrating its superior performance. However, as shown in Figure 3, JetMoE-8B-chat is\nrelatively weak in coding and extraction compared to GPT-3.5-turbo. This might be due to\nthe smaller model size leading to suboptimal reasoning capability in these tasks. Despite\nthis limitation, JetMoE-8B-chat exhibits strong performance across various other dimensions,\nmaking it a competitive model in the open-source LLM landscape.\n7 Limitation and Future Works\nDue to the limited $100k budget, we can not afford any ablation study for the model\narchitecture. The hyperparameters and data mixtures are also handpicked based on the\nempirical results from previous works (Shen et al., 2023; Zoph et al., 2022; Hu et al., 2024).\nIn the future, it would be interesting to further study the actual contribution of different\ncomponents to the final results.\n8 Conclusion\nWe introduce JetMoE-8B, an open-source MoE model that achieves state-of-the-art perfor-\nmance among open-source models while maintaining high efficiency. By leveraging sparse\n10\n---\n[ref_id=shen2024] This report\nintroduces JetMoE-8B, a new LLM trained with less than$0.1 million, using\n1.25T tokens from carefully mixed open-source corpora and 30,000 H100\nGPU hours.\n---\n[ref_id=shen2024] JetMoE\nJetMoE: Reaching Llama2 Performance with 0.1M Dollars\nYikang Shen \u2217\nMIT-IBM Watson AI Lab\nyikang.shn@gmail.com\nZhen Guo\u2217\nMIT EECS\nzguo0525@mit.edu\nTianle Cai\nPrinceton University\ntianle.cai@princeton.edu\nZengyi Qin\nMyShell.ai & MIT\nqinzy@mit.edu\nAbstract\nLarge Language Models (LLMs) have achieved remarkable results, but\ntheir increasing resource demand has become a major obstacle to the devel-\nopment of powerful and accessible super-human intelligence. This report\nintroduces JetMoE-8B, a new LLM trained with less than$0.1 million, using\n1.25T tokens from carefully mixed open-source corpora and 30,000 H100\nGPU hours. Despite its low cost, the JetMoE-8B demonstrates impressive\nperformance, with JetMoE-8B outperforming the Llama2-7B model and\nJetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results\nsuggest that LLM training can be much more cost-effective than gener-\nally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-\nof-Experts (SMoE) architecture, composed of attention and feedforward\nexperts. Both layers are sparsely activated, allowing JetMoE-8B to have\n8B parameters while only activating 2B for each input token, reducing\ninference computation by about 70% compared to Llama2-7B. Moreover,\nJetMoE-8B is highly open and academia-friendly, using only public datasets\nand training code. All training parameters and data mixtures have been\ndetailed in this report to facilitate future efforts in the development of open\nfoundation models. This transparency aims to encourage collaboration and\nfurther advancements in the field of accessible and efficient LLMs. The mod-\nels are publicly available at https://github.com/myshell-ai/JetMoE.\n1 Introduction\nLarge Language Models (LLMs) have achieved remarkable results, but their increasing\nresource demand has become a major obstacle to developing powerful and accessible AI.\nAlthough modern LLMs have surpassed human performance on some tasks, they remain\ninefficient and inflexible. Most LLMs (e.g., Llama, Touvron et al. 2023; Pythia, Biderman\net al. 2023; GPT-3, Brown et al. 2020; Mistral, Jiang et al. 2023) use all of their parameters\nduring inference and training, which are referred to as dense models. Considering the\nsubstantial costs, the Mixture-of-Experts (MoE) architecture (Yuksel et al., 2012; Shazeer\net al., 2017; Du et al., 2022; Pan et al., 2024) has emerged as a popular solution, enabling\nparameter scaling while keeping computational costs modest. Recent applications of MoE\narchitectures in Transformers (Vaswani et al., 2017) have yielded successful attempts at\nscaling language models to a substantial size, accompanied by remarkable performance,\nsuch as Deepseek MoE (Dai et al., 2024), Mixtral 8x7B (Jiang et al., 2024), Grok-1 (xai-org,\n2024), and DBRX (Databricks, 2024). However, even though these models achieve excellent\nperformance, they are not truly open-sourced as the training recipes are not published and\nmay contain proprietary datasets inaccessible outside of large corporations. The open-source\ncommunity has also attempted to train MoE models, such as OpenMoE (Xue et al., 2024), but\nits performance is only on par with weak dense models with similar activation parameters,\nsuch as OpenLLaMA (Geng & Liu, 2023) and TinyLLaMA (Zhang et al., 2024a).\n\u2217Equal contribution.\n1\narXiv:2404.07413v1  [cs.CL]  11 Apr 2024\n---\n[ref_id=shen2024] 8 Conclusion\nWe introduce JetMoE-8B, an open-source MoE model that achieves state-of-the-art perfor-\nmance among open-source models while maintaining high efficiency.\n---\n[ref_id=shen2024] JetMoE\nModel MT-Bench Score\nGPT-4 9.014\nGPT-3.5-turbo 7.995\nClaude-v1 7.923\nJetMoE-8B-chat 6.681\nLlama-2-13b-chat 6.650\nVicuna-13b-v1.3 6.413\nWizardlm-13b 6.353\nLlama-2-7b-chat 6.269\nTable 4: MT-Bench score comparison of various models\nFigure 3: MT-Bench radar figure\nhave a temperature of 0.0, suggesting preciseness; and \u201dSTEM\u201d and \u201dHumanities\u201d have a\ntemperature of 0.1, implying slightly more variability than 0.0 tasks.\nJetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment,\ndemonstrating its superior performance. However, as shown in Figure 3, JetMoE-8B-chat is\nrelatively weak in coding and extraction compared to GPT-3.5-turbo. This might be due to\nthe smaller model size leading to suboptimal reasoning capability in these tasks. Despite\nthis limitation, JetMoE-8B-chat exhibits strong performance across various other dimensions,\nmaking it a competitive model in the open-source LLM landscape.\n7 Limitation and Future Works\nDue to the limited $100k budget, we can not afford any ablation study for the model\narchitecture. The hyperparameters and data mixtures are also handpicked based on the\nempirical results from previous works (Shen et al., 2023; Zoph et al., 2022; Hu et al., 2024).\nIn the future, it would be interesting to further study the actual contribution of different\ncomponents to the final results.\n8 Conclusion\nWe introduce JetMoE-8B, an open-source MoE model that achieves state-of-the-art perfor-\nmance among open-source models while maintaining high efficiency. By leveraging sparse\n10\n---\n[ref_id=shen2024] However, it serves as a good starting point for training JetMoE-8B and can be further\noptimized in future iterations.\n---\n[ref_id=shen2024] JetMoE\n\u2022 Phase 2 (decay learning rate): We include additional high-quality data to further\nimprove the model\u2019s performance.\nThe detailed data mixture can be found in Figure 2 and Table 2. It is important to note\nthat given the limited computing budget available, our data mixture might not be ideal.\nHowever, it serves as a good starting point for training JetMoE-8B and can be further\noptimized in future iterations.\nFigure 2: Pretraining data mixture\nCategory Dataset Percentage\nNL pretraining data\nRefinedweb 39.8%\nPile Wikipedia 6.7%\nPile StackExchange 4.8%\nPile arXiv 1.0%\nPile remaining 5.1%\nDolma peS2o 1.0%\nNL SFT data xP3x, OpenAssistant, OpenHermes 7.3%UltraChat, Oasst-octopack\nTextbook UltraTextbooks 4.8%\nCode pretraining data Starcoder Github 19.6%\nCode SFT data\nMagicoder-OSS, Magicoder-Evol\n3.8% Code-290k-ShareGPT, CommitPackFT\nEvol-Code Alpaca\nMath data Open-web-math, algebraic-stack 5.8%TemplateGSM, StackMathQA\nTable 2: Detailed data mixture for Phase 2\n5 Model Alignment\n5.1 Distilled Supervised Fine-Tuning (dSFT)\nThe dSFT process involves training a student language model for replying to user prompts,\nwith data generated by a teacher model (such as GPT-4 or Claude) (Wang et al., 2022; Taori\net al., 2023; Chiang et al., 2023; Tunstall et al., 2023b). The key steps are as follows:\n7\n---\n[ref_id=shen2024] Trained using a two-phase approach and a carefully curated mixture of open-source datasets,\nJetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leader-\nboard.\n---\n[ref_id=shen2024] JetMoE\nactivation in both the attention and feed-forward layers, JetMoE-8B reduces computational\ncosts while maintaining strong performance across a wide range of tasks.\nTrained using a two-phase approach and a carefully curated mixture of open-source datasets,\nJetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leader-\nboard. In addition, JetMoE-8B-Chat demonstrates competitive performance compared to\nother open-source chatbots.\nWe provide detailed training parameters and data mixture information to encourage repro-\nducibility and enable researchers to build upon our work. JetMoE-8B represents a significant\nstep forward in the development of open-source, efficient, and high-performing language\nmodels, contributing to the democratization of advanced language technologies.\nAcknowledgments\nWe express our gratitude to Shengding Hu for his valuable advice on the Phase 2 data\nmixture. We also express our gratitude to Exabits for their assistance in setting up the GPU\nclusters, and to Lepton AI for their support in setting up the chat demo.\nReferences\nabacusai. Systemchat, 2024. URL https://huggingface.co/datasets/abacusai/\nSystemChat.\najibawa 2023. Code-290k-sharegpt, 2024. URL https://huggingface.co/datasets/\najibawa-2023/Code-290k-ShareGPT .\nJason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Vozne-\nsensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine\nlearning through dynamic python bytecode transformation and graph compilation, 2024.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with\nlarge language models. arXiv preprint arXiv:2108.07732, 2021.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language\nmodel for mathematics, 2023.\nLoubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro\nvon Werra. A framework for the evaluation of code generation models. https://github.\ncom/bigcode-project/bigcode-evaluation-harness , 2022.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\nRaff, et al. Pythia: A suite for analyzing large language models across training and scaling.\narXiv preprint arXiv:2304.01373, 2023.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\ncommonsense in natural language. In Proceedings of the AAAI conference on artificial\nintelligence, volume 34, pp. 7432\u20137439, 2020.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neural information processing systems,\n33:1877\u20131901, 2020.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\n11\n---\n[ref_id=shen2024] We compare JetMoE-8B models to several external\nopen-source (OSS) LLMs, including Gemma, LLaMA2, DeepseekMoE.\n---\n[ref_id=shen2024] JetMoE\n2023), Code-Feedback (Zheng et al., 2024b), Orca-math-word-problems-200k (Mitra et al.,\n2024), SystemChat (abacusai, 2024), and Capybara (Daniele & Suphavadeeprasit, 2023).\nChat template is the same as Zephyr-7b-beta. The key hyperparameters for dSFT are a\nlearning rate of 2e-5 with an Adam optimizer, a batch size of 128, and 3 epochs.\nWe further finetune the JetMoE-8B-SFT model using dDPO on the UltraFeedback\ndataset (Cui et al., 2023), which contains binary preference labels indicating the preferred\nresponse between two options. The key hyperparameters for dDPO are a learning rate of\n5e-7 with AdamW, a batch size of 128, and 1 epoch. This fine-tuning process results in the\nJetMoE-8B-Chat model. The entire alignment process takes 60 H100 GPU hours.\n6 Evaluation\nLLaMA2 DeepseekMoE Gemma JetMoE\n# Total Params 7B 16B 2B 8B\n# Activate Params 7B 2.8B 2B 2.2B\n# Training tokens 2T 2T 2T 1.25T\nARC-challenge 53.1 53.2 48.4 48.7\nHellaswag 78.6 79.8 71.8 80.5\nMMLU 46.9 46.3 41.8 49.2\nTruthfulQA 38.8 36.1 33.1 41.7\nWinoGrande 74.0 73.7 66.3 70.2\nGSM8k 14.5 17.3 16.9 27.8\nOpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0\nMBPP (Pass@1) 20.8 34.0 28.0 34.2\nHumanEval (Pass@1) 12.8 25.0 24.4 14.6\nAll Avg. 45.5 47.3 43.2 47.6\nTable 3: OpenLLM leaderboard and code benchmarks results from four different models.\nWe measure JetMoE-8B\u2019s performance on tasks included in OpenLLM leaderboard2 and\nfrom other domains, including physical reasoning (Bisk et al., 2020), social reasoning (Sap\net al., 2019), question answering (Clark et al., 2019; Kwiatkowski et al., 2019), mathemat-\nics (Cobbe et al., 2021), commonsense reasoning (Sakaguchi et al., 2021), language model-\ning (Paperno et al., 2016), reading comprehension (Joshi et al., 2017), and more. For most\nbenchmarks, we use the same evaluation methodology as in the OpenLLM leaderboard\nto be comparable to other models.. We compare JetMoE-8B models to several external\nopen-source (OSS) LLMs, including Gemma, LLaMA2, DeepseekMoE.\nIn addition, we include HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021)\nto evaluate the code generation of the models. Utilizing the BigCode Evaluation Har-\nness (Ben Allal et al., 2022), we follow recent work on Code LLMs (Rozi`ere et al., 2024; Guo\net al., 2024a) with greedy decoding, and report the mean pass@1 (mean success rate) for the\ntwo benchmarks.\nTable 3 shows the OpenLLM leaderboard and code benchmarks results from four different\nmodels. JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM\nleaderboard, achieving the best scores in all tasks except ARC-challenge and WinoGrande.\nAdditionally, JetMoE-8B obtains the highest MBPP scores in Python programming.\nWe also evaluated our model on MT-Bench (Zheng et al., 2023) with a strong LLM judge\n(gpt-4-0613 checkpoint). The temperature configuration, following the official FastChat\nimplementation, is defined as follows: \u201dWriting\u201d and \u201dRoleplay\u201d tasks have a temperature\nof 0.7, indicating higher creativity; \u201dExtraction\u201d, \u201dMath\u201d, \u201dCoding\u201d, and \u201dReasoning\u201d tasks\n2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n9\n---\n[ref_id=shen2024] In addition, JetMoE-8B-Chat demonstrates competitive performance compared to\nother open-source chatbots.\n---\n[ref_id=shen2024] JetMoE\nactivation in both the attention and feed-forward layers, JetMoE-8B reduces computational\ncosts while maintaining strong performance across a wide range of tasks.\nTrained using a two-phase approach and a carefully curated mixture of open-source datasets,\nJetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leader-\nboard. In addition, JetMoE-8B-Chat demonstrates competitive performance compared to\nother open-source chatbots.\nWe provide detailed training parameters and data mixture information to encourage repro-\nducibility and enable researchers to build upon our work. JetMoE-8B represents a significant\nstep forward in the development of open-source, efficient, and high-performing language\nmodels, contributing to the democratization of advanced language technologies.\nAcknowledgments\nWe express our gratitude to Shengding Hu for his valuable advice on the Phase 2 data\nmixture. We also express our gratitude to Exabits for their assistance in setting up the GPU\nclusters, and to Lepton AI for their support in setting up the chat demo.\nReferences\nabacusai. Systemchat, 2024. URL https://huggingface.co/datasets/abacusai/\nSystemChat.\najibawa 2023. Code-290k-sharegpt, 2024. URL https://huggingface.co/datasets/\najibawa-2023/Code-290k-ShareGPT .\nJason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Vozne-\nsensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine\nlearning through dynamic python bytecode transformation and graph compilation, 2024.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with\nlarge language models. arXiv preprint arXiv:2108.07732, 2021.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language\nmodel for mathematics, 2023.\nLoubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro\nvon Werra. A framework for the evaluation of code generation models. https://github.\ncom/bigcode-project/bigcode-evaluation-harness , 2022.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\nRaff, et al. Pythia: A suite for analyzing large language models across training and scaling.\narXiv preprint arXiv:2304.01373, 2023.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\ncommonsense in natural language. In Proceedings of the AAAI conference on artificial\nintelligence, volume 34, pp. 7432\u20137439, 2020.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neural information processing systems,\n33:1877\u20131901, 2020.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\n11\n---\n[ref_id=shen2024] Despite its low cost, the JetMoE-8B demonstrates impressive\nperformance, with JetMoE-8B outperforming the Llama2-7B model and\nJetMoE-8B-Chat surpassing the Llama2-13B-Chat model.\n---\n[ref_id=shen2024] JetMoE\nJetMoE: Reaching Llama2 Performance with 0.1M Dollars\nYikang Shen \u2217\nMIT-IBM Watson AI Lab\nyikang.shn@gmail.com\nZhen Guo\u2217\nMIT EECS\nzguo0525@mit.edu\nTianle Cai\nPrinceton University\ntianle.cai@princeton.edu\nZengyi Qin\nMyShell.ai & MIT\nqinzy@mit.edu\nAbstract\nLarge Language Models (LLMs) have achieved remarkable results, but\ntheir increasing resource demand has become a major obstacle to the devel-\nopment of powerful and accessible super-human intelligence. This report\nintroduces JetMoE-8B, a new LLM trained with less than$0.1 million, using\n1.25T tokens from carefully mixed open-source corpora and 30,000 H100\nGPU hours. Despite its low cost, the JetMoE-8B demonstrates impressive\nperformance, with JetMoE-8B outperforming the Llama2-7B model and\nJetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results\nsuggest that LLM training can be much more cost-effective than gener-\nally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-\nof-Experts (SMoE) architecture, composed of attention and feedforward\nexperts. Both layers are sparsely activated, allowing JetMoE-8B to have\n8B parameters while only activating 2B for each input token, reducing\ninference computation by about 70% compared to Llama2-7B. Moreover,\nJetMoE-8B is highly open and academia-friendly, using only public datasets\nand training code. All training parameters and data mixtures have been\ndetailed in this report to facilitate future efforts in the development of open\nfoundation models. This transparency aims to encourage collaboration and\nfurther advancements in the field of accessible and efficient LLMs. The mod-\nels are publicly available at https://github.com/myshell-ai/JetMoE.\n1 Introduction\nLarge Language Models (LLMs) have achieved remarkable results, but their increasing\nresource demand has become a major obstacle to developing powerful and accessible AI.\nAlthough modern LLMs have surpassed human performance on some tasks, they remain\ninefficient and inflexible. Most LLMs (e.g., Llama, Touvron et al. 2023; Pythia, Biderman\net al. 2023; GPT-3, Brown et al. 2020; Mistral, Jiang et al. 2023) use all of their parameters\nduring inference and training, which are referred to as dense models. Considering the\nsubstantial costs, the Mixture-of-Experts (MoE) architecture (Yuksel et al., 2012; Shazeer\net al., 2017; Du et al., 2022; Pan et al., 2024) has emerged as a popular solution, enabling\nparameter scaling while keeping computational costs modest. Recent applications of MoE\narchitectures in Transformers (Vaswani et al., 2017) have yielded successful attempts at\nscaling language models to a substantial size, accompanied by remarkable performance,\nsuch as Deepseek MoE (Dai et al., 2024), Mixtral 8x7B (Jiang et al., 2024), Grok-1 (xai-org,\n2024), and DBRX (Databricks, 2024). However, even though these models achieve excellent\nperformance, they are not truly open-sourced as the training recipes are not published and\nmay contain proprietary datasets inaccessible outside of large corporations. The open-source\ncommunity has also attempted to train MoE models, such as OpenMoE (Xue et al., 2024), but\nits performance is only on par with weak dense models with similar activation parameters,\nsuch as OpenLLaMA (Geng & Liu, 2023) and TinyLLaMA (Zhang et al., 2024a).\n\u2217Equal contribution.\n1\narXiv:2404.07413v1  [cs.CL]  11 Apr 2024\n---\n[ref_id=shen2024] 4.3 Training Data Mixture\nJetMoE-8B is trained on 1.25T tokens of primarily English data from web documents,\nmathematics, and code.\n---\n[ref_id=shen2024] JetMoE\n4 Model Pretraining\n4.1 Infrastructures\nWe use Megatron (Shoeybi et al., 2019) as the training framework and integrate\nMegablock (Gale et al., 2023) for MoE support. We further modified the training framework\nto support MoA (Section 2.3) and z-loss (Section 2.4). Against the common practice, we\nchoose the Pipeline parallelism introduced in (Narayanan et al., 2021) instead of the expert\nparallelism for model parallel during training. This is mainly due to two reasons. First,\nSparse MoE models usually have a narrower hidden state compared to standard transformer\nmodels. Thus, the communication cost for pipeline parallelism is smaller. Second, we use\nthe dropless MoE schema introduced in Gale et al. (2023); Shen et al. (2023), which could\ncause load unbalance across experts. Thus, using expert parallel will cause an unbalanced\nload across devices and result in inefficient training. Pipeline parallelism could avoid this\nslowdown because it computes all the experts inside a layer on the same device. We con-\nduct training on a cluster containing 12 nodes and 96 H100s. Inside each node, gpus are\nconnected via NVLinks. Infiniband is used for fast communication between nodes.\n4.2 Hyper-parameters\nPtotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp\n8B 2B 24 2048 8 2 16 128 5632\nTable 1: JetMoE-8B hyperparameters.\nThe hyperparameters of JetMoE-8B are selected based on the common practice for the 1B\ntransformer language model. We replace all self-attention and MLP layers in the transformer\nwith MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every\nlayer. Such that the model has approximately two times the computation compared to a 1B\nmodel. Following ST-MoE (Zoph et al., 2022), the weight for load balancing loss and z-loss\nis set to 0.01 and 0.001, respectively. Table 1 shows the key hyperparameters in JetMoE-8B.\nJetMoE-8B is trained with the AdamW optimizer (Loshchilov & Hutter, 2017) with a maxi-\nmum learning rate of 5e-4 and a batch size of 4M tokens with sequence length of 4096. We\nemploy the Warmup-Stable-Decay (WSD) learning rate schedule introduced in Hu et al.\n(2024). This learning rate scheduler is divided into three stages: the warmup stage (denoted\nby W, representing the number of steps at the end of the warmup stage), the stable training\nstage (denoted by S), and the annealing stage (denoted by D):\nlr(s) =\n\uf8f1\n\uf8f2\n\uf8f3\ns\nW \u2217 \u03b7, s < W\n\u03b7, W < s < S\nf (s \u2212 S) \u2217 \u03b7, S < s < S + D\n(13)\nwhere 0 < f (s \u2212 S) \u2264 1 is a decreasing function of s, and \u03b7 is the maximum learning rate.\nIn our settings, the warmup stage lasts for 10 billion tokens, and the decay stage spans 250\nbillion tokens. The initial and final learning rates are set to 10% of the maximum learning\nrate. A weight decay of 0.1 and gradient clipping of 1.0 are applied during training.\n4.3 Training Data Mixture\nJetMoE-8B is trained on 1.25T tokens of primarily English data from web documents,\nmathematics, and code. Similar to the approach advocated in miniCPM (Hu et al., 2024) and\nGemma (Team et al., 2024), we increase the weight of high-quality data during the learning\nrate decay phase. The training process is divided into two phases:\n\u2022 Phase 1 (warmup and stable learning rate): The dataset includes RefinedWeb, Star-\ncoder, The Pile, peS2o from Dolma, and OpenWebMath.\n6\n---\n[ref_id=shen2024] The hyperparameters of JetMoE-8B are selected based on the common practice for the 1B\ntransformer language model.\n---\n[ref_id=shen2024] JetMoE\n4 Model Pretraining\n4.1 Infrastructures\nWe use Megatron (Shoeybi et al., 2019) as the training framework and integrate\nMegablock (Gale et al., 2023) for MoE support. We further modified the training framework\nto support MoA (Section 2.3) and z-loss (Section 2.4). Against the common practice, we\nchoose the Pipeline parallelism introduced in (Narayanan et al., 2021) instead of the expert\nparallelism for model parallel during training. This is mainly due to two reasons. First,\nSparse MoE models usually have a narrower hidden state compared to standard transformer\nmodels. Thus, the communication cost for pipeline parallelism is smaller. Second, we use\nthe dropless MoE schema introduced in Gale et al. (2023); Shen et al. (2023), which could\ncause load unbalance across experts. Thus, using expert parallel will cause an unbalanced\nload across devices and result in inefficient training. Pipeline parallelism could avoid this\nslowdown because it computes all the experts inside a layer on the same device. We con-\nduct training on a cluster containing 12 nodes and 96 H100s. Inside each node, gpus are\nconnected via NVLinks. Infiniband is used for fast communication between nodes.\n4.2 Hyper-parameters\nPtotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp\n8B 2B 24 2048 8 2 16 128 5632\nTable 1: JetMoE-8B hyperparameters.\nThe hyperparameters of JetMoE-8B are selected based on the common practice for the 1B\ntransformer language model. We replace all self-attention and MLP layers in the transformer\nwith MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every\nlayer. Such that the model has approximately two times the computation compared to a 1B\nmodel. Following ST-MoE (Zoph et al., 2022), the weight for load balancing loss and z-loss\nis set to 0.01 and 0.001, respectively. Table 1 shows the key hyperparameters in JetMoE-8B.\nJetMoE-8B is trained with the AdamW optimizer (Loshchilov & Hutter, 2017) with a maxi-\nmum learning rate of 5e-4 and a batch size of 4M tokens with sequence length of 4096. We\nemploy the Warmup-Stable-Decay (WSD) learning rate schedule introduced in Hu et al.\n(2024). This learning rate scheduler is divided into three stages: the warmup stage (denoted\nby W, representing the number of steps at the end of the warmup stage), the stable training\nstage (denoted by S), and the annealing stage (denoted by D):\nlr(s) =\n\uf8f1\n\uf8f2\n\uf8f3\ns\nW \u2217 \u03b7, s < W\n\u03b7, W < s < S\nf (s \u2212 S) \u2217 \u03b7, S < s < S + D\n(13)\nwhere 0 < f (s \u2212 S) \u2264 1 is a decreasing function of s, and \u03b7 is the maximum learning rate.\nIn our settings, the warmup stage lasts for 10 billion tokens, and the decay stage spans 250\nbillion tokens. The initial and final learning rates are set to 10% of the maximum learning\nrate. A weight decay of 0.1 and gradient clipping of 1.0 are applied during training.\n4.3 Training Data Mixture\nJetMoE-8B is trained on 1.25T tokens of primarily English data from web documents,\nmathematics, and code. Similar to the approach advocated in miniCPM (Hu et al., 2024) and\nGemma (Team et al., 2024), we increase the weight of high-quality data during the learning\nrate decay phase. The training process is divided into two phases:\n\u2022 Phase 1 (warmup and stable learning rate): The dataset includes RefinedWeb, Star-\ncoder, The Pile, peS2o from Dolma, and OpenWebMath.\n6\n---\n[ref_id=samsi2024] Interestingly, there appears to be an exception for\nthe GSM8K math problem dataset;\n---\n[ref_id=samsi2024] Fig. 5: Energy per second (Watts) estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards for\nmax generation length 1024 : inference energy estimates on\nAlpaca and GSM8K on log-scale. Color indicates batch size.\nincreases. However, we see little change in the average energy\nper token between max generation length 512 and 1024. For\ninstance, with length 512, we see that it takes about 3-4 Joules\nfor a output token, which is approximately the same amount\nfor length 512. As with energy per second, max generation\nlength seems to have a negligible effect on energy costs from\n512 to 1024. Interestingly, there appears to be an exception for\nthe GSM8K math problem dataset; there exists a \u201csweet spot\u201d\nat 16 shards where continuously increasing the batch size can\nactually reduce the energy per token at max generation length\n512. However, this disappears under max generation length\n1024 where increasing the batch size increases the energy per\ntoken. The definitive existence of this sweet spot for datasets\nof differing styles/complexities, or others like it, will require\nmore experimentation and benchmarking to establish.\nD. Energy per Response: LLaMA 65B\nFigures 8 and 9 show energy metrics in terms of responses\nfrom the 65B model. Like before, we see that increasing\nthe number of shards still tends to increase the energy costs\nof inference per response most overall while increasing the\nmaximum generation length from 512 (Figure 8) to 1024\n(Figure 9) does not induce a clear or significant effect in\ninference energy costs. Also like before, while we see slight\nincreases in energy costs per response generated within a\nshard configuration as batch size increases, but not consistently\nor significantly. Again, we see that for GSM8K, at max\nFig. 6: Energy per output token estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards\nfor max generation length 512 : inference energy estimates\non Alpaca and GSM8K on log-scale. Color indicates batch\nsize.\nFig. 7: Energy per output token estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards for\nmax generation length 1024 : inference energy estimates on\nAlpaca and GSM8K on log-scale. Color indicates batch size.\n---\n[ref_id=shen2024] JetMoE-8B represents a significant\nstep forward in the development of open-source, efficient, and high-performing language\nmodels, contributing to the democratization of advanced language technologies.\n---\n[ref_id=shen2024] JetMoE\nactivation in both the attention and feed-forward layers, JetMoE-8B reduces computational\ncosts while maintaining strong performance across a wide range of tasks.\nTrained using a two-phase approach and a carefully curated mixture of open-source datasets,\nJetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leader-\nboard. In addition, JetMoE-8B-Chat demonstrates competitive performance compared to\nother open-source chatbots.\nWe provide detailed training parameters and data mixture information to encourage repro-\nducibility and enable researchers to build upon our work. JetMoE-8B represents a significant\nstep forward in the development of open-source, efficient, and high-performing language\nmodels, contributing to the democratization of advanced language technologies.\nAcknowledgments\nWe express our gratitude to Shengding Hu for his valuable advice on the Phase 2 data\nmixture. We also express our gratitude to Exabits for their assistance in setting up the GPU\nclusters, and to Lepton AI for their support in setting up the chat demo.\nReferences\nabacusai. Systemchat, 2024. URL https://huggingface.co/datasets/abacusai/\nSystemChat.\najibawa 2023. Code-290k-sharegpt, 2024. URL https://huggingface.co/datasets/\najibawa-2023/Code-290k-ShareGPT .\nJason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Vozne-\nsensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine\nlearning through dynamic python bytecode transformation and graph compilation, 2024.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with\nlarge language models. arXiv preprint arXiv:2108.07732, 2021.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language\nmodel for mathematics, 2023.\nLoubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro\nvon Werra. A framework for the evaluation of code generation models. https://github.\ncom/bigcode-project/bigcode-evaluation-harness , 2022.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\nRaff, et al. Pythia: A suite for analyzing large language models across training and scaling.\narXiv preprint arXiv:2304.01373, 2023.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\ncommonsense in natural language. In Proceedings of the AAAI conference on artificial\nintelligence, volume 34, pp. 7432\u20137439, 2020.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neural information processing systems,\n33:1877\u20131901, 2020.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\n11\n---\n[ref_id=shen2024] Both layers are sparsely activated, allowing JetMoE-8B to have\n8B parameters while only activating 2B for each input token, reducing\ninference computation by about 70% compared to Llama2-7B.\n---\n[ref_id=shen2024] JetMoE\nJetMoE: Reaching Llama2 Performance with 0.1M Dollars\nYikang Shen \u2217\nMIT-IBM Watson AI Lab\nyikang.shn@gmail.com\nZhen Guo\u2217\nMIT EECS\nzguo0525@mit.edu\nTianle Cai\nPrinceton University\ntianle.cai@princeton.edu\nZengyi Qin\nMyShell.ai & MIT\nqinzy@mit.edu\nAbstract\nLarge Language Models (LLMs) have achieved remarkable results, but\ntheir increasing resource demand has become a major obstacle to the devel-\nopment of powerful and accessible super-human intelligence. This report\nintroduces JetMoE-8B, a new LLM trained with less than$0.1 million, using\n1.25T tokens from carefully mixed open-source corpora and 30,000 H100\nGPU hours. Despite its low cost, the JetMoE-8B demonstrates impressive\nperformance, with JetMoE-8B outperforming the Llama2-7B model and\nJetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results\nsuggest that LLM training can be much more cost-effective than gener-\nally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-\nof-Experts (SMoE) architecture, composed of attention and feedforward\nexperts. Both layers are sparsely activated, allowing JetMoE-8B to have\n8B parameters while only activating 2B for each input token, reducing\ninference computation by about 70% compared to Llama2-7B. Moreover,\nJetMoE-8B is highly open and academia-friendly, using only public datasets\nand training code. All training parameters and data mixtures have been\ndetailed in this report to facilitate future efforts in the development of open\nfoundation models. This transparency aims to encourage collaboration and\nfurther advancements in the field of accessible and efficient LLMs. The mod-\nels are publicly available at https://github.com/myshell-ai/JetMoE.\n1 Introduction\nLarge Language Models (LLMs) have achieved remarkable results, but their increasing\nresource demand has become a major obstacle to developing powerful and accessible AI.\nAlthough modern LLMs have surpassed human performance on some tasks, they remain\ninefficient and inflexible. Most LLMs (e.g., Llama, Touvron et al. 2023; Pythia, Biderman\net al. 2023; GPT-3, Brown et al. 2020; Mistral, Jiang et al. 2023) use all of their parameters\nduring inference and training, which are referred to as dense models. Considering the\nsubstantial costs, the Mixture-of-Experts (MoE) architecture (Yuksel et al., 2012; Shazeer\net al., 2017; Du et al., 2022; Pan et al., 2024) has emerged as a popular solution, enabling\nparameter scaling while keeping computational costs modest. Recent applications of MoE\narchitectures in Transformers (Vaswani et al., 2017) have yielded successful attempts at\nscaling language models to a substantial size, accompanied by remarkable performance,\nsuch as Deepseek MoE (Dai et al., 2024), Mixtral 8x7B (Jiang et al., 2024), Grok-1 (xai-org,\n2024), and DBRX (Databricks, 2024). However, even though these models achieve excellent\nperformance, they are not truly open-sourced as the training recipes are not published and\nmay contain proprietary datasets inaccessible outside of large corporations. The open-source\ncommunity has also attempted to train MoE models, such as OpenMoE (Xue et al., 2024), but\nits performance is only on par with weak dense models with similar activation parameters,\nsuch as OpenLLaMA (Geng & Liu, 2023) and TinyLLaMA (Zhang et al., 2024a).\n\u2217Equal contribution.\n1\narXiv:2404.07413v1  [cs.CL]  11 Apr 2024\n---\n[ref_id=shen2024] We measure JetMoE-8B\u2019s performance on tasks included in OpenLLM leaderboard2 and\nfrom other domains, including physical reasoning (Bisk et al., 2020), social reasoning (Sap\net al., 2019), question answering (Clark et al., 2019;\n---\n[ref_id=shen2024] JetMoE\n2023), Code-Feedback (Zheng et al., 2024b), Orca-math-word-problems-200k (Mitra et al.,\n2024), SystemChat (abacusai, 2024), and Capybara (Daniele & Suphavadeeprasit, 2023).\nChat template is the same as Zephyr-7b-beta. The key hyperparameters for dSFT are a\nlearning rate of 2e-5 with an Adam optimizer, a batch size of 128, and 3 epochs.\nWe further finetune the JetMoE-8B-SFT model using dDPO on the UltraFeedback\ndataset (Cui et al., 2023), which contains binary preference labels indicating the preferred\nresponse between two options. The key hyperparameters for dDPO are a learning rate of\n5e-7 with AdamW, a batch size of 128, and 1 epoch. This fine-tuning process results in the\nJetMoE-8B-Chat model. The entire alignment process takes 60 H100 GPU hours.\n6 Evaluation\nLLaMA2 DeepseekMoE Gemma JetMoE\n# Total Params 7B 16B 2B 8B\n# Activate Params 7B 2.8B 2B 2.2B\n# Training tokens 2T 2T 2T 1.25T\nARC-challenge 53.1 53.2 48.4 48.7\nHellaswag 78.6 79.8 71.8 80.5\nMMLU 46.9 46.3 41.8 49.2\nTruthfulQA 38.8 36.1 33.1 41.7\nWinoGrande 74.0 73.7 66.3 70.2\nGSM8k 14.5 17.3 16.9 27.8\nOpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0\nMBPP (Pass@1) 20.8 34.0 28.0 34.2\nHumanEval (Pass@1) 12.8 25.0 24.4 14.6\nAll Avg. 45.5 47.3 43.2 47.6\nTable 3: OpenLLM leaderboard and code benchmarks results from four different models.\nWe measure JetMoE-8B\u2019s performance on tasks included in OpenLLM leaderboard2 and\nfrom other domains, including physical reasoning (Bisk et al., 2020), social reasoning (Sap\net al., 2019), question answering (Clark et al., 2019; Kwiatkowski et al., 2019), mathemat-\nics (Cobbe et al., 2021), commonsense reasoning (Sakaguchi et al., 2021), language model-\ning (Paperno et al., 2016), reading comprehension (Joshi et al., 2017), and more. For most\nbenchmarks, we use the same evaluation methodology as in the OpenLLM leaderboard\nto be comparable to other models.. We compare JetMoE-8B models to several external\nopen-source (OSS) LLMs, including Gemma, LLaMA2, DeepseekMoE.\nIn addition, we include HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021)\nto evaluate the code generation of the models. Utilizing the BigCode Evaluation Har-\nness (Ben Allal et al., 2022), we follow recent work on Code LLMs (Rozi`ere et al., 2024; Guo\net al., 2024a) with greedy decoding, and report the mean pass@1 (mean success rate) for the\ntwo benchmarks.\nTable 3 shows the OpenLLM leaderboard and code benchmarks results from four different\nmodels. JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM\nleaderboard, achieving the best scores in all tasks except ARC-challenge and WinoGrande.\nAdditionally, JetMoE-8B obtains the highest MBPP scores in Python programming.\nWe also evaluated our model on MT-Bench (Zheng et al., 2023) with a strong LLM judge\n(gpt-4-0613 checkpoint). The temperature configuration, following the official FastChat\nimplementation, is defined as follows: \u201dWriting\u201d and \u201dRoleplay\u201d tasks have a temperature\nof 0.7, indicating higher creativity; \u201dExtraction\u201d, \u201dMath\u201d, \u201dCoding\u201d, and \u201dReasoning\u201d tasks\n2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n9\n---\n[ref_id=shen2024] In addition, JetMoE-8B has 8B parameters while only activating 2B for each input\ntoken, reducing inference computation by about 70% compared to Llama2-7B.\n---\n[ref_id=shen2024] JetMoE\nFigure 1: JetMoE architecture\nTo facilitate future efforts on open foundation models, particularly MoE models, we intro-\nduce JetMoE-8B, an innovative MoE architecture inspired by ModuleFormer (Shen et al.,\n2023) that extends the concept of sparse activation to both the attention and feed-forward\nlayers. Unlike prior works that only apply sparse activation to the feed-forward layer,\nJetMoE-8B leverages sparse activation in both components to further reduce computational\ncosts while maintaining performance.\nImpressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from\nmixed open-source datasets and 30,000 H100 GPU hours. Despite its low cost, JetMoE-8B\noutperforms the Llama2-7B model, and JetMoE-8B-Chat outperforms the Llama2-13B-Chat\nmodel, demonstrating that LLM training can be much more cost-effective than generally\nthought. In addition, JetMoE-8B has 8B parameters while only activating 2B for each input\ntoken, reducing inference computation by about 70% compared to Llama2-7B.\nThe key advantages of JetMoE-8B include:\n\u2022 Openness and academia-friendly: JetMoE-8B is trained using only public datasets and\nopen-source training code, making it accessible to many academia research settings.\nThe model can also be finetuned with limited compute budgets (e.g., consumer-grade\nGPUs).\n\u2022 Sparse activation on both attention and feed-forward layers , which significantly\nreduces training and inference costs. We also propose to share the kv projection in\nattention experts to improve training stability.\n\u2022 Comprehensive open-source data mixture, which ensures high-quality training using\nonly open-source datasets.\nThese innovations in JetMoE-8B pave the way for more accessible and efficient LLMs, bene-\nfiting the broader AI research community. To foster collaboration and further advancements,\nwe have detailed all the training parameters and data mixture in this report.\n2 Model Architecture\n2.1 Mixture of Experts\nA Mixture of Experts (MoE) layer comprises N modules f1, . . ., fN and a router g(e | x).\nGiven an input x to the MoE layer, the router predicts a probability distribution over the N\n2\n---\n[ref_id=shen2024] JetMoE\nFigure 1: JetMoE architecture\nTo facilitate future efforts on open foundation models, particularly MoE models, we intro-\nduce JetMoE-8B, an innovative MoE architecture inspired by ModuleFormer (Shen et al.,\n2023) that extends the concept of sparse activation to both the attention and feed-forward\nlayers. Unlike prior works that only apply sparse activation to the feed-forward layer,\nJetMoE-8B leverages sparse activation in both components to further reduce computational\ncosts while maintaining performance.\nImpressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from\nmixed open-source datasets and 30,000 H100 GPU hours. Despite its low cost, JetMoE-8B\noutperforms the Llama2-7B model, and JetMoE-8B-Chat outperforms the Llama2-13B-Chat\nmodel, demonstrating that LLM training can be much more cost-effective than generally\nthought. In addition, JetMoE-8B has 8B parameters while only activating 2B for each input\ntoken, reducing inference computation by about 70% compared to Llama2-7B.\nThe key advantages of JetMoE-8B include:\n\u2022 Openness and academia-friendly: JetMoE-8B is trained using only public datasets and\nopen-source training code, making it accessible to many academia research settings.\nThe model can also be finetuned with limited compute budgets (e.g., consumer-grade\nGPUs).\n\u2022 Sparse activation on both attention and feed-forward layers , which significantly\nreduces training and inference costs. We also propose to share the kv projection in\nattention experts to improve training stability.\n\u2022 Comprehensive open-source data mixture, which ensures high-quality training using\nonly open-source datasets.\nThese innovations in JetMoE-8B pave the way for more accessible and efficient LLMs, bene-\nfiting the broader AI research community. To foster collaboration and further advancements,\nwe have detailed all the training parameters and data mixture in this report.\n2 Model Architecture\n2.1 Mixture of Experts\nA Mixture of Experts (MoE) layer comprises N modules f1, . . ., fN and a router g(e | x).\nGiven an input x to the MoE layer, the router predicts a probability distribution over the N\n2\n---\n[ref_id=shen2024] Page 2\n---\n[ref_id=shen2024] JetMoE\nFigure 1: JetMoE architecture\nTo facilitate future efforts on open foundation models, particularly MoE models, we intro-\nduce JetMoE-8B, an innovative MoE architecture inspired by ModuleFormer (Shen et al.,\n2023) that extends the concept of sparse activation to both the attention and feed-forward\nlayers.\n---\n[ref_id=shen2024] Unlike prior works that only apply sparse activation to the feed-forward layer,\nJetMoE-8B leverages sparse activation in both components to further reduce computational\ncosts while maintaining performance.\n---\n[ref_id=shen2024] Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from\nmixed open-source datasets and 30,000 H100 GPU hours.\n---\n[ref_id=shen2024] Despite its low cost, JetMoE-8B\noutperforms the Llama2-7B model, and JetMoE-8B-Chat outperforms the Llama2-13B-Chat\nmodel, demonstrating that LLM training can be much more cost-effective than generally\nthought.\n---\n[ref_id=shen2024] In addition, JetMoE-8B has 8B parameters while only activating 2B for each input\ntoken, reducing inference computation by about 70% compared to Llama2-7B.\n---\n[ref_id=shen2024] The key advantages of JetMoE-8B include:\n\u2022 Openness and academia-friendly: JetMoE-8B is trained using only public datasets and\nopen-source training code, making it accessible to many academia research settings.\n---\n[ref_id=shen2024] The model can also be finetuned with limited compute budgets (e.g., consumer-grade\nGPUs).\n---\n[ref_id=shen2024] \u2022 Sparse activation on both attention and feed-forward layers , which significantly\nreduces training and inference costs.\n---\n[ref_id=shen2024] We also propose to share the kv projection in\nattention experts to improve training stability.\n---\n[ref_id=shen2024] \u2022 Comprehensive open-source data mixture, which ensures high-quality training using\nonly open-source datasets.\n---\n[ref_id=shen2024] These innovations in JetMoE-8B pave the way for more accessible and efficient LLMs, bene-\nfiting the broader AI research community.\n---\n[ref_id=shen2024] To foster collaboration and further advancements,\nwe have detailed all the training parameters and data mixture in this report.\n---\n[ref_id=shen2024] 2 Model Architecture\n2.1 Mixture of Experts\nA Mixture of Experts (MoE) layer comprises N modules f1, .\n---\n[ref_id=shen2024] .\n---\n[ref_id=shen2024] ., fN and a router g(e | x).\n---\n[ref_id=shen2024] Given an input x to the MoE layer, the router predicts a probability distribution over the N\n2\n\n---\n\nNow answer the following question based ONLY on the context above.\n\nQuestion: On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?\n\nAdditional info (JSON): {\"answer_unit\": \"score\", \"question_id\": \"q323\"}\n\nIMPORTANT: The \"answer_unit\" field specifies the expected format/unit for answer_value.\n- If answer_unit is a unit (e.g., \"kW\", \"USD\"), express answer_value as a number in that unit (no unit name).\n- If answer_unit is \"is_blank\", answer_value should be the exact identifier/name from context that answers the question.\n- If the answer is a numeric range, format as [lower,upper].\n\nReturn STRICT JSON with these keys in order:\n- explanation          (1-3 sentences explaining how context supports the answer and how you applied answer_unit; or \"is_blank\")\n- answer               (short natural-language response)\n- answer_value         (the value matching the expected format; or \"is_blank\")\n- ref_id               (list of document ids from context used as evidence; or \"is_blank\")\n- ref_url              (list of URLs for the cited documents; or \"is_blank\")\n- supporting_materials (verbatim quote, table reference, or figure reference from the cited document; or \"is_blank\")\n\nJSON Answer:",
    "retrieved_snippets": [
      {
        "node_id": "samsi2024:sec3:p4:s92",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "The second dataset is GSM8K [22], consisting of\n8,500 human crafted grade school math problems.",
        "score": 0.7616,
        "rank": 1,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec3:p4",
          "section_id": "samsi2024:sec3",
          "sentence_index": 92
        }
      },
      {
        "node_id": "samsi2024:sec3:p4",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "III. E XPERIMENTAL SETUP\nWe conducted our experiments on the MIT Supercloud\nhigh-performance computing (HPC) system [18]. This het-\nerogeneous HPC cluster consists of 448 compute nodes with\ndual Intel Xeon Gold 6248 CPUs with 384 GB of RAM\nand two NVIDIA V olta V100 GPUs with 32 GB of memory\nper node. Each node on the system has two independent\nback-end fabrics: a 100 Gb/s Intel Omnipath as well as a\n25 Gb/s Ethernet interconnect using Mellanox ConnectX-4\nadapters with all servers connected to a single, non-blocking\nArista DCS-7516 Ethernet core switch. The GPUs, Omnipath,\nand Ethernet cards are all connected to PCIe slots that route\ndirectly to the Xeon processors without any intermediary PCIe\nswitches. All experiments in this paper exclusively used the\n25 Gb/s Ethernet interconnect. The system also includes 480\nCPU-only nodes with Intel Xeon Platinum 8260 processors.\nIn addition, four nodes with NVIDIA A100 GPUs were also\navailable for experiments described in this paper. A summary\nof the hardware is shown in Table I. All experiments described\nin this paper were run exclusively on NVIDIA GPUs.\nTABLE I: Compute node configurations: This table lists\nthe types of hardware used for inference evaluations in our\nexperiments. Each node consists of 2 CPUs and 2 GPUs in\nthe configuration listed below. All GPUs are from NVIDIA.\nCPU GPU\nType Memory TDP Type Memory TDP\n(GB) (W) (GB) (W)\nIntel Xeon\nGold 6248 384 150 V100 32 250\nIntel Xeon\nPlatinum 8358 503 240 A100 80 300\nA. Models\nExperiments were performed using open-source implemen-\ntation of the pre-trained LLaMA 65B model available via\nrequest from Meta [14] and evaluation scripts available via\nGitHub [19]. This implementation of the model uses Pytorch\nand the FairScale [20] library to enable model sharding across\nmultiple GPUs and nodes. For the models, we use a decoder\ntemperature setting \u03c4 = 0 .8 and a top- p value of 0.95 in\nattempts to align our settings with the general range of values\nthat are typically used. In future work, we aim to study\nhow varying decoding temperature, top- p, and other hyper-\nparameters may affect compute performance and energy usage\nduring inference. While our main focus is on LLaMA 65B,\nwe also examine LLaMA 7B and LLaMA 13B to characterize\ninference performance and energy under the bare minimum\nsettings/resources required to run these models.\nB. Datasets\nWe used two datasets to evaluate inference performance.\nThe first is an instruction following dataset used to fine-tune\nthe Alpaca [21] model (from here on, this dataset is referred\nto as \u201cAlpaca\u201d in our paper which is not to be confused with\nthe Alpaca model). This Alpaca dataset consists of 52,000\ninstruction-following tasks, instructions/questions where some\nhave example inputs and some do not, that the model is asked\nto answer. The second dataset is GSM8K [22], consisting of\n8,500 human crafted grade school math problems. The goal\nof using these two datasets is two-fold: (1) to evaluate the\nmodel on a diverse set of tasks in NLP and (2) evaluate\nhow different types of data and its underlying dynamics\ncan impact energy and inference performance. While natural\nlanguage is more common in LLM usage and in LLM training\ndata, increasingly new capabilities have been demonstrated\nin LLMs, including the ability to solve simple mathematical\nproblems, provide/correct examples of code, and more. Math\nquestions also differ considerably from questions posed in\nnatural language which can result in smaller context windows,\ninputs/outputs of differing lengths, number of decoded tokens,\netc. This, in turn, may impact inference performance in\neither throughput rates or energy costs. For this reason, our\nbenchmarking experiments are conducted on both datasets.\nFor both datasets, we sample 4,096 inputs for our inference\nexperiments. Using the entirety of the datasets would only\nserve to increase inference time and energy used for the ex-\nperimentation unreasonably and did not provide any significant\nbenefits to the study.\nC. Evaluation\nOur goal is to evaluate the inference performance, latency,\nand inference energy costs of LLaMA 65B as a representative\nlarge language model that requires sharding across multiple\nGPUs. We intend this to be a preliminary analysis that will\nhelp guide more in-depth experiments and benchmarking for\nour future work. Our analysis also includes limited analysis of\nsmaller LLaMA variants to illustrate inference performance\nand energy trade-offs in bare-minimum hardware settings:\nnamely, LLaMA 7B and 13B. While we do not control for\nthe correctness/quality of the outputs or the complexity of the\ninputs/outputs in studying trade-offs between inference energy\nand performance, we hope to account for this as an ablative\nstudy in future work. Similarly, we do not perform a com-\nprehensive evaluation with different optimization techniques\nor inference settings available for LLMs such as modeling\nquery arrival rates, model quantization, continuous batching,\netc. which we also leave for future work.\nInference performance is measured in terms of rates: words,\ntokens, and responses per second or, equivalently, the number\nof words, tokens, and responses generated per second. When\nrunning inference with LLaMA, the model generates a string\nof text for each input until the length of the text hits a\nmaximum generation length or a stop-word is encountered.\nThe number of words are calculated by counting the number\nof words present in the output by splitting each output string\non spaces. The number of tokens is calculated using LLaMA\u2019s\nown default tokenizer by counting the number of tokens in the\ntokenized output. Lastly, the number of responses per second\nor the response rate is calculated using the total number of",
        "score": 0.7616,
        "rank": 1,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "page": 3,
          "paragraph_index": 4,
          "section_id": "samsi2024:sec3",
          "section_index": 3
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s35",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "The key advantages of JetMoE-8B include:\n\u2022 Openness and academia-friendly: JetMoE-8B is trained using only public datasets and\nopen-source training code, making it accessible to many academia research settings.",
        "score": 0.7283,
        "rank": 2,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 35
        }
      },
      {
        "node_id": "shen2024:sec2:p2",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\nFigure 1: JetMoE architecture\nTo facilitate future efforts on open foundation models, particularly MoE models, we intro-\nduce JetMoE-8B, an innovative MoE architecture inspired by ModuleFormer (Shen et al.,\n2023) that extends the concept of sparse activation to both the attention and feed-forward\nlayers. Unlike prior works that only apply sparse activation to the feed-forward layer,\nJetMoE-8B leverages sparse activation in both components to further reduce computational\ncosts while maintaining performance.\nImpressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from\nmixed open-source datasets and 30,000 H100 GPU hours. Despite its low cost, JetMoE-8B\noutperforms the Llama2-7B model, and JetMoE-8B-Chat outperforms the Llama2-13B-Chat\nmodel, demonstrating that LLM training can be much more cost-effective than generally\nthought. In addition, JetMoE-8B has 8B parameters while only activating 2B for each input\ntoken, reducing inference computation by about 70% compared to Llama2-7B.\nThe key advantages of JetMoE-8B include:\n\u2022 Openness and academia-friendly: JetMoE-8B is trained using only public datasets and\nopen-source training code, making it accessible to many academia research settings.\nThe model can also be finetuned with limited compute budgets (e.g., consumer-grade\nGPUs).\n\u2022 Sparse activation on both attention and feed-forward layers , which significantly\nreduces training and inference costs. We also propose to share the kv projection in\nattention experts to improve training stability.\n\u2022 Comprehensive open-source data mixture, which ensures high-quality training using\nonly open-source datasets.\nThese innovations in JetMoE-8B pave the way for more accessible and efficient LLMs, bene-\nfiting the broader AI research community. To foster collaboration and further advancements,\nwe have detailed all the training parameters and data mixture in this report.\n2 Model Architecture\n2.1 Mixture of Experts\nA Mixture of Experts (MoE) layer comprises N modules f1, . . ., fN and a router g(e | x).\nGiven an input x to the MoE layer, the router predicts a probability distribution over the N\n2",
        "score": 0.7283,
        "rank": 2,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 2,
          "paragraph_index": 2,
          "section_id": "shen2024:sec2",
          "section_index": 2
        }
      },
      {
        "node_id": "samsi2024:sec5:p6:s150",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "(b) Results from GSM8K dataset\nFig.",
        "score": 0.7167,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec5:p6",
          "section_id": "samsi2024:sec5",
          "sentence_index": 150
        }
      },
      {
        "node_id": "samsi2024:sec5:p6",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "(a) Results from the Alpaca dataset.\n(b) Results from GSM8K dataset\nFig. 2: Baseline comparison of inference perfor-\nmance/latency between LLaMA 7B, 13B and 65B: inference\nperformance comparisons on the minimum set of hardware\nrequired to run inference (see Table II) across model sizes\nand between V100s and A100s.\nFig. 3: Baseline energy per second (Watts) estimates of per-\nforming inference with LLaMA 7B, 13B, and 65B: inference\nenergy comparisons on the minimum set of hardware/settings\nrequired (see Table II) with Alpaca and GSM8K on a log-scale.\nColor indicates device (V100/A100), bars indicate average\nquantities and lines indicate error bars. Energy is averaged\nover maximum generation lengths of 256, 512, and 1024 due\nto near-identical energy/size trends for each generation length.\nFig. 4: Energy per second (Watts) estimates of LLaMA 65B\nacross batch sizes of 64/128/256/256 and 8/16/32 shards for\nmax generation length 512 : inference energy estimates on\nAlpaca and GSM8K on log-scale. Color indicates batch size.\nB. Energy per Second: LLaMA 65B\nWe first take a look at the amount of energy inference costs\nper unit time in seconds. Figures 4 and 5 show a more in-\ndepth look of the energy inference costs of LLaMA 65B across\ndifferent batch sizes and degrees of sharding. Specifically,\nFigure 4 shows energy costs for maximum generation length\n512 and Figure 5 shows energy costs for 1024.\nOverall, we see an average increase in energy per second\nwith the number of shards. While there is a slight correlation\nas energy per second increases with increasing batch size,\nincreasing the number of shards always increases the wattage.\nIndeed, the energy per second increases with the number of\nshards even at the same batch size (e.g., the energy of inference\nat batch size 64, going from 16 shards to 32 shards). For both\ndatasets, increasing the max generation length from 512 to\n1024 does seem to increase the energy per second for each\nbatch size within each shard configuration, but the overall\neffect is less clear or consistent. Overall, we see that the energy\nper second for inference with LLaMA 65B is on the order of\n300 Watts to 1 Kilowatt from the lower shard configuration of\n8 GPUs to the higher end of 32 GPUs.\nC. Energy per Decoded Token: LLaMA 65B\nMoving on to energy per each decoded output token, we see\nthat in Figures 6 and 7 that energy per token tends to follow\na similar pattern in relation to the number of shards: as the\nnumber of shards increases, the energy per output token also",
        "score": 0.7167,
        "rank": 3,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "page": 5,
          "paragraph_index": 6,
          "section_id": "samsi2024:sec5",
          "section_index": 5
        }
      },
      {
        "node_id": "shen2024:sec6:p8:s129",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "Table 1 shows the key hyperparameters in JetMoE-8B.",
        "score": 0.7103,
        "rank": 4,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec6:p8",
          "section_id": "shen2024:sec6",
          "sentence_index": 129
        }
      },
      {
        "node_id": "shen2024:sec6:p8",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\n4 Model Pretraining\n4.1 Infrastructures\nWe use Megatron (Shoeybi et al., 2019) as the training framework and integrate\nMegablock (Gale et al., 2023) for MoE support. We further modified the training framework\nto support MoA (Section 2.3) and z-loss (Section 2.4). Against the common practice, we\nchoose the Pipeline parallelism introduced in (Narayanan et al., 2021) instead of the expert\nparallelism for model parallel during training. This is mainly due to two reasons. First,\nSparse MoE models usually have a narrower hidden state compared to standard transformer\nmodels. Thus, the communication cost for pipeline parallelism is smaller. Second, we use\nthe dropless MoE schema introduced in Gale et al. (2023); Shen et al. (2023), which could\ncause load unbalance across experts. Thus, using expert parallel will cause an unbalanced\nload across devices and result in inefficient training. Pipeline parallelism could avoid this\nslowdown because it computes all the experts inside a layer on the same device. We con-\nduct training on a cluster containing 12 nodes and 96 H100s. Inside each node, gpus are\nconnected via NVLinks. Infiniband is used for fast communication between nodes.\n4.2 Hyper-parameters\nPtotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp\n8B 2B 24 2048 8 2 16 128 5632\nTable 1: JetMoE-8B hyperparameters.\nThe hyperparameters of JetMoE-8B are selected based on the common practice for the 1B\ntransformer language model. We replace all self-attention and MLP layers in the transformer\nwith MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every\nlayer. Such that the model has approximately two times the computation compared to a 1B\nmodel. Following ST-MoE (Zoph et al., 2022), the weight for load balancing loss and z-loss\nis set to 0.01 and 0.001, respectively. Table 1 shows the key hyperparameters in JetMoE-8B.\nJetMoE-8B is trained with the AdamW optimizer (Loshchilov & Hutter, 2017) with a maxi-\nmum learning rate of 5e-4 and a batch size of 4M tokens with sequence length of 4096. We\nemploy the Warmup-Stable-Decay (WSD) learning rate schedule introduced in Hu et al.\n(2024). This learning rate scheduler is divided into three stages: the warmup stage (denoted\nby W, representing the number of steps at the end of the warmup stage), the stable training\nstage (denoted by S), and the annealing stage (denoted by D):\nlr(s) =\n\uf8f1\n\uf8f2\n\uf8f3\ns\nW \u2217 \u03b7, s < W\n\u03b7, W < s < S\nf (s \u2212 S) \u2217 \u03b7, S < s < S + D\n(13)\nwhere 0 < f (s \u2212 S) \u2264 1 is a decreasing function of s, and \u03b7 is the maximum learning rate.\nIn our settings, the warmup stage lasts for 10 billion tokens, and the decay stage spans 250\nbillion tokens. The initial and final learning rates are set to 10% of the maximum learning\nrate. A weight decay of 0.1 and gradient clipping of 1.0 are applied during training.\n4.3 Training Data Mixture\nJetMoE-8B is trained on 1.25T tokens of primarily English data from web documents,\nmathematics, and code. Similar to the approach advocated in miniCPM (Hu et al., 2024) and\nGemma (Team et al., 2024), we increase the weight of high-quality data during the learning\nrate decay phase. The training process is divided into two phases:\n\u2022 Phase 1 (warmup and stable learning rate): The dataset includes RefinedWeb, Star-\ncoder, The Pile, peS2o from Dolma, and OpenWebMath.\n6",
        "score": 0.7103,
        "rank": 4,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 6,
          "paragraph_index": 8,
          "section_id": "shen2024:sec6",
          "section_index": 6
        }
      },
      {
        "node_id": "shen2024:sec10:p14:s203",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "Despite\nthis limitation, JetMoE-8B-chat exhibits strong performance across various other dimensions,\nmaking it a competitive model in the open-source LLM landscape.",
        "score": 0.7063,
        "rank": 5,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec10:p14",
          "section_id": "shen2024:sec10",
          "sentence_index": 203
        }
      },
      {
        "node_id": "shen2024:sec10:p14",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\nModel MT-Bench Score\nGPT-4 9.014\nGPT-3.5-turbo 7.995\nClaude-v1 7.923\nJetMoE-8B-chat 6.681\nLlama-2-13b-chat 6.650\nVicuna-13b-v1.3 6.413\nWizardlm-13b 6.353\nLlama-2-7b-chat 6.269\nTable 4: MT-Bench score comparison of various models\nFigure 3: MT-Bench radar figure\nhave a temperature of 0.0, suggesting preciseness; and \u201dSTEM\u201d and \u201dHumanities\u201d have a\ntemperature of 0.1, implying slightly more variability than 0.0 tasks.\nJetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment,\ndemonstrating its superior performance. However, as shown in Figure 3, JetMoE-8B-chat is\nrelatively weak in coding and extraction compared to GPT-3.5-turbo. This might be due to\nthe smaller model size leading to suboptimal reasoning capability in these tasks. Despite\nthis limitation, JetMoE-8B-chat exhibits strong performance across various other dimensions,\nmaking it a competitive model in the open-source LLM landscape.\n7 Limitation and Future Works\nDue to the limited $100k budget, we can not afford any ablation study for the model\narchitecture. The hyperparameters and data mixtures are also handpicked based on the\nempirical results from previous works (Shen et al., 2023; Zoph et al., 2022; Hu et al., 2024).\nIn the future, it would be interesting to further study the actual contribution of different\ncomponents to the final results.\n8 Conclusion\nWe introduce JetMoE-8B, an open-source MoE model that achieves state-of-the-art perfor-\nmance among open-source models while maintaining high efficiency. By leveraging sparse\n10",
        "score": 0.7063,
        "rank": 5,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 10,
          "paragraph_index": 14,
          "section_id": "shen2024:sec10",
          "section_index": 10
        }
      },
      {
        "node_id": "shen2024:sec1:p1:s2",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "This report\nintroduces JetMoE-8B, a new LLM trained with less than$0.1 million, using\n1.25T tokens from carefully mixed open-source corpora and 30,000 H100\nGPU hours.",
        "score": 0.7057,
        "rank": 6,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec1:p1",
          "section_id": "shen2024:sec1",
          "sentence_index": 2
        }
      },
      {
        "node_id": "shen2024:sec1:p1",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\nJetMoE: Reaching Llama2 Performance with 0.1M Dollars\nYikang Shen \u2217\nMIT-IBM Watson AI Lab\nyikang.shn@gmail.com\nZhen Guo\u2217\nMIT EECS\nzguo0525@mit.edu\nTianle Cai\nPrinceton University\ntianle.cai@princeton.edu\nZengyi Qin\nMyShell.ai & MIT\nqinzy@mit.edu\nAbstract\nLarge Language Models (LLMs) have achieved remarkable results, but\ntheir increasing resource demand has become a major obstacle to the devel-\nopment of powerful and accessible super-human intelligence. This report\nintroduces JetMoE-8B, a new LLM trained with less than$0.1 million, using\n1.25T tokens from carefully mixed open-source corpora and 30,000 H100\nGPU hours. Despite its low cost, the JetMoE-8B demonstrates impressive\nperformance, with JetMoE-8B outperforming the Llama2-7B model and\nJetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results\nsuggest that LLM training can be much more cost-effective than gener-\nally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-\nof-Experts (SMoE) architecture, composed of attention and feedforward\nexperts. Both layers are sparsely activated, allowing JetMoE-8B to have\n8B parameters while only activating 2B for each input token, reducing\ninference computation by about 70% compared to Llama2-7B. Moreover,\nJetMoE-8B is highly open and academia-friendly, using only public datasets\nand training code. All training parameters and data mixtures have been\ndetailed in this report to facilitate future efforts in the development of open\nfoundation models. This transparency aims to encourage collaboration and\nfurther advancements in the field of accessible and efficient LLMs. The mod-\nels are publicly available at https://github.com/myshell-ai/JetMoE.\n1 Introduction\nLarge Language Models (LLMs) have achieved remarkable results, but their increasing\nresource demand has become a major obstacle to developing powerful and accessible AI.\nAlthough modern LLMs have surpassed human performance on some tasks, they remain\ninefficient and inflexible. Most LLMs (e.g., Llama, Touvron et al. 2023; Pythia, Biderman\net al. 2023; GPT-3, Brown et al. 2020; Mistral, Jiang et al. 2023) use all of their parameters\nduring inference and training, which are referred to as dense models. Considering the\nsubstantial costs, the Mixture-of-Experts (MoE) architecture (Yuksel et al., 2012; Shazeer\net al., 2017; Du et al., 2022; Pan et al., 2024) has emerged as a popular solution, enabling\nparameter scaling while keeping computational costs modest. Recent applications of MoE\narchitectures in Transformers (Vaswani et al., 2017) have yielded successful attempts at\nscaling language models to a substantial size, accompanied by remarkable performance,\nsuch as Deepseek MoE (Dai et al., 2024), Mixtral 8x7B (Jiang et al., 2024), Grok-1 (xai-org,\n2024), and DBRX (Databricks, 2024). However, even though these models achieve excellent\nperformance, they are not truly open-sourced as the training recipes are not published and\nmay contain proprietary datasets inaccessible outside of large corporations. The open-source\ncommunity has also attempted to train MoE models, such as OpenMoE (Xue et al., 2024), but\nits performance is only on par with weak dense models with similar activation parameters,\nsuch as OpenLLaMA (Geng & Liu, 2023) and TinyLLaMA (Zhang et al., 2024a).\n\u2217Equal contribution.\n1\narXiv:2404.07413v1  [cs.CL]  11 Apr 2024",
        "score": 0.7057,
        "rank": 6,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 1,
          "paragraph_index": 1,
          "section_id": "shen2024:sec1",
          "section_index": 1
        }
      },
      {
        "node_id": "shen2024:sec10:p14:s209",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "8 Conclusion\nWe introduce JetMoE-8B, an open-source MoE model that achieves state-of-the-art perfor-\nmance among open-source models while maintaining high efficiency.",
        "score": 0.7039,
        "rank": 7,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec10:p14",
          "section_id": "shen2024:sec10",
          "sentence_index": 209
        }
      },
      {
        "node_id": "shen2024:sec10:p14",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\nModel MT-Bench Score\nGPT-4 9.014\nGPT-3.5-turbo 7.995\nClaude-v1 7.923\nJetMoE-8B-chat 6.681\nLlama-2-13b-chat 6.650\nVicuna-13b-v1.3 6.413\nWizardlm-13b 6.353\nLlama-2-7b-chat 6.269\nTable 4: MT-Bench score comparison of various models\nFigure 3: MT-Bench radar figure\nhave a temperature of 0.0, suggesting preciseness; and \u201dSTEM\u201d and \u201dHumanities\u201d have a\ntemperature of 0.1, implying slightly more variability than 0.0 tasks.\nJetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment,\ndemonstrating its superior performance. However, as shown in Figure 3, JetMoE-8B-chat is\nrelatively weak in coding and extraction compared to GPT-3.5-turbo. This might be due to\nthe smaller model size leading to suboptimal reasoning capability in these tasks. Despite\nthis limitation, JetMoE-8B-chat exhibits strong performance across various other dimensions,\nmaking it a competitive model in the open-source LLM landscape.\n7 Limitation and Future Works\nDue to the limited $100k budget, we can not afford any ablation study for the model\narchitecture. The hyperparameters and data mixtures are also handpicked based on the\nempirical results from previous works (Shen et al., 2023; Zoph et al., 2022; Hu et al., 2024).\nIn the future, it would be interesting to further study the actual contribution of different\ncomponents to the final results.\n8 Conclusion\nWe introduce JetMoE-8B, an open-source MoE model that achieves state-of-the-art perfor-\nmance among open-source models while maintaining high efficiency. By leveraging sparse\n10",
        "score": 0.7039,
        "rank": 7,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 10,
          "paragraph_index": 14,
          "section_id": "shen2024:sec10",
          "section_index": 10
        }
      },
      {
        "node_id": "shen2024:sec7:p9:s144",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "However, it serves as a good starting point for training JetMoE-8B and can be further\noptimized in future iterations.",
        "score": 0.7024,
        "rank": 8,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec7:p9",
          "section_id": "shen2024:sec7",
          "sentence_index": 144
        }
      },
      {
        "node_id": "shen2024:sec7:p9",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\n\u2022 Phase 2 (decay learning rate): We include additional high-quality data to further\nimprove the model\u2019s performance.\nThe detailed data mixture can be found in Figure 2 and Table 2. It is important to note\nthat given the limited computing budget available, our data mixture might not be ideal.\nHowever, it serves as a good starting point for training JetMoE-8B and can be further\noptimized in future iterations.\nFigure 2: Pretraining data mixture\nCategory Dataset Percentage\nNL pretraining data\nRefinedweb 39.8%\nPile Wikipedia 6.7%\nPile StackExchange 4.8%\nPile arXiv 1.0%\nPile remaining 5.1%\nDolma peS2o 1.0%\nNL SFT data xP3x, OpenAssistant, OpenHermes 7.3%UltraChat, Oasst-octopack\nTextbook UltraTextbooks 4.8%\nCode pretraining data Starcoder Github 19.6%\nCode SFT data\nMagicoder-OSS, Magicoder-Evol\n3.8% Code-290k-ShareGPT, CommitPackFT\nEvol-Code Alpaca\nMath data Open-web-math, algebraic-stack 5.8%TemplateGSM, StackMathQA\nTable 2: Detailed data mixture for Phase 2\n5 Model Alignment\n5.1 Distilled Supervised Fine-Tuning (dSFT)\nThe dSFT process involves training a student language model for replying to user prompts,\nwith data generated by a teacher model (such as GPT-4 or Claude) (Wang et al., 2022; Taori\net al., 2023; Chiang et al., 2023; Tunstall et al., 2023b). The key steps are as follows:\n7",
        "score": 0.7024,
        "rank": 8,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 7,
          "paragraph_index": 9,
          "section_id": "shen2024:sec7",
          "section_index": 7
        }
      },
      {
        "node_id": "shen2024:sec11:p16:s213",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "Trained using a two-phase approach and a carefully curated mixture of open-source datasets,\nJetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leader-\nboard.",
        "score": 0.7012,
        "rank": 9,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec11:p16",
          "section_id": "shen2024:sec11",
          "sentence_index": 213
        }
      },
      {
        "node_id": "shen2024:sec11:p16",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\nactivation in both the attention and feed-forward layers, JetMoE-8B reduces computational\ncosts while maintaining strong performance across a wide range of tasks.\nTrained using a two-phase approach and a carefully curated mixture of open-source datasets,\nJetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leader-\nboard. In addition, JetMoE-8B-Chat demonstrates competitive performance compared to\nother open-source chatbots.\nWe provide detailed training parameters and data mixture information to encourage repro-\nducibility and enable researchers to build upon our work. JetMoE-8B represents a significant\nstep forward in the development of open-source, efficient, and high-performing language\nmodels, contributing to the democratization of advanced language technologies.\nAcknowledgments\nWe express our gratitude to Shengding Hu for his valuable advice on the Phase 2 data\nmixture. We also express our gratitude to Exabits for their assistance in setting up the GPU\nclusters, and to Lepton AI for their support in setting up the chat demo.\nReferences\nabacusai. Systemchat, 2024. URL https://huggingface.co/datasets/abacusai/\nSystemChat.\najibawa 2023. Code-290k-sharegpt, 2024. URL https://huggingface.co/datasets/\najibawa-2023/Code-290k-ShareGPT .\nJason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Vozne-\nsensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine\nlearning through dynamic python bytecode transformation and graph compilation, 2024.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with\nlarge language models. arXiv preprint arXiv:2108.07732, 2021.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language\nmodel for mathematics, 2023.\nLoubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro\nvon Werra. A framework for the evaluation of code generation models. https://github.\ncom/bigcode-project/bigcode-evaluation-harness , 2022.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\nRaff, et al. Pythia: A suite for analyzing large language models across training and scaling.\narXiv preprint arXiv:2304.01373, 2023.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\ncommonsense in natural language. In Proceedings of the AAAI conference on artificial\nintelligence, volume 34, pp. 7432\u20137439, 2020.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neural information processing systems,\n33:1877\u20131901, 2020.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\n11",
        "score": 0.7012,
        "rank": 9,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 11,
          "paragraph_index": 16,
          "section_id": "shen2024:sec11",
          "section_index": 11
        }
      },
      {
        "node_id": "shen2024:sec9:p13:s188",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "We compare JetMoE-8B models to several external\nopen-source (OSS) LLMs, including Gemma, LLaMA2, DeepseekMoE.",
        "score": 0.6984,
        "rank": 10,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec9:p13",
          "section_id": "shen2024:sec9",
          "sentence_index": 188
        }
      },
      {
        "node_id": "shen2024:sec9:p13",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\n2023), Code-Feedback (Zheng et al., 2024b), Orca-math-word-problems-200k (Mitra et al.,\n2024), SystemChat (abacusai, 2024), and Capybara (Daniele & Suphavadeeprasit, 2023).\nChat template is the same as Zephyr-7b-beta. The key hyperparameters for dSFT are a\nlearning rate of 2e-5 with an Adam optimizer, a batch size of 128, and 3 epochs.\nWe further finetune the JetMoE-8B-SFT model using dDPO on the UltraFeedback\ndataset (Cui et al., 2023), which contains binary preference labels indicating the preferred\nresponse between two options. The key hyperparameters for dDPO are a learning rate of\n5e-7 with AdamW, a batch size of 128, and 1 epoch. This fine-tuning process results in the\nJetMoE-8B-Chat model. The entire alignment process takes 60 H100 GPU hours.\n6 Evaluation\nLLaMA2 DeepseekMoE Gemma JetMoE\n# Total Params 7B 16B 2B 8B\n# Activate Params 7B 2.8B 2B 2.2B\n# Training tokens 2T 2T 2T 1.25T\nARC-challenge 53.1 53.2 48.4 48.7\nHellaswag 78.6 79.8 71.8 80.5\nMMLU 46.9 46.3 41.8 49.2\nTruthfulQA 38.8 36.1 33.1 41.7\nWinoGrande 74.0 73.7 66.3 70.2\nGSM8k 14.5 17.3 16.9 27.8\nOpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0\nMBPP (Pass@1) 20.8 34.0 28.0 34.2\nHumanEval (Pass@1) 12.8 25.0 24.4 14.6\nAll Avg. 45.5 47.3 43.2 47.6\nTable 3: OpenLLM leaderboard and code benchmarks results from four different models.\nWe measure JetMoE-8B\u2019s performance on tasks included in OpenLLM leaderboard2 and\nfrom other domains, including physical reasoning (Bisk et al., 2020), social reasoning (Sap\net al., 2019), question answering (Clark et al., 2019; Kwiatkowski et al., 2019), mathemat-\nics (Cobbe et al., 2021), commonsense reasoning (Sakaguchi et al., 2021), language model-\ning (Paperno et al., 2016), reading comprehension (Joshi et al., 2017), and more. For most\nbenchmarks, we use the same evaluation methodology as in the OpenLLM leaderboard\nto be comparable to other models.. We compare JetMoE-8B models to several external\nopen-source (OSS) LLMs, including Gemma, LLaMA2, DeepseekMoE.\nIn addition, we include HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021)\nto evaluate the code generation of the models. Utilizing the BigCode Evaluation Har-\nness (Ben Allal et al., 2022), we follow recent work on Code LLMs (Rozi`ere et al., 2024; Guo\net al., 2024a) with greedy decoding, and report the mean pass@1 (mean success rate) for the\ntwo benchmarks.\nTable 3 shows the OpenLLM leaderboard and code benchmarks results from four different\nmodels. JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM\nleaderboard, achieving the best scores in all tasks except ARC-challenge and WinoGrande.\nAdditionally, JetMoE-8B obtains the highest MBPP scores in Python programming.\nWe also evaluated our model on MT-Bench (Zheng et al., 2023) with a strong LLM judge\n(gpt-4-0613 checkpoint). The temperature configuration, following the official FastChat\nimplementation, is defined as follows: \u201dWriting\u201d and \u201dRoleplay\u201d tasks have a temperature\nof 0.7, indicating higher creativity; \u201dExtraction\u201d, \u201dMath\u201d, \u201dCoding\u201d, and \u201dReasoning\u201d tasks\n2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n9",
        "score": 0.6984,
        "rank": 10,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 9,
          "paragraph_index": 13,
          "section_id": "shen2024:sec9",
          "section_index": 9
        }
      },
      {
        "node_id": "shen2024:sec11:p16:s214",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "In addition, JetMoE-8B-Chat demonstrates competitive performance compared to\nother open-source chatbots.",
        "score": 0.6972,
        "rank": 11,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec11:p16",
          "section_id": "shen2024:sec11",
          "sentence_index": 214
        }
      },
      {
        "node_id": "shen2024:sec11:p16",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\nactivation in both the attention and feed-forward layers, JetMoE-8B reduces computational\ncosts while maintaining strong performance across a wide range of tasks.\nTrained using a two-phase approach and a carefully curated mixture of open-source datasets,\nJetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leader-\nboard. In addition, JetMoE-8B-Chat demonstrates competitive performance compared to\nother open-source chatbots.\nWe provide detailed training parameters and data mixture information to encourage repro-\nducibility and enable researchers to build upon our work. JetMoE-8B represents a significant\nstep forward in the development of open-source, efficient, and high-performing language\nmodels, contributing to the democratization of advanced language technologies.\nAcknowledgments\nWe express our gratitude to Shengding Hu for his valuable advice on the Phase 2 data\nmixture. We also express our gratitude to Exabits for their assistance in setting up the GPU\nclusters, and to Lepton AI for their support in setting up the chat demo.\nReferences\nabacusai. Systemchat, 2024. URL https://huggingface.co/datasets/abacusai/\nSystemChat.\najibawa 2023. Code-290k-sharegpt, 2024. URL https://huggingface.co/datasets/\najibawa-2023/Code-290k-ShareGPT .\nJason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Vozne-\nsensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine\nlearning through dynamic python bytecode transformation and graph compilation, 2024.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with\nlarge language models. arXiv preprint arXiv:2108.07732, 2021.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language\nmodel for mathematics, 2023.\nLoubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro\nvon Werra. A framework for the evaluation of code generation models. https://github.\ncom/bigcode-project/bigcode-evaluation-harness , 2022.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\nRaff, et al. Pythia: A suite for analyzing large language models across training and scaling.\narXiv preprint arXiv:2304.01373, 2023.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\ncommonsense in natural language. In Proceedings of the AAAI conference on artificial\nintelligence, volume 34, pp. 7432\u20137439, 2020.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neural information processing systems,\n33:1877\u20131901, 2020.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\n11",
        "score": 0.6972,
        "rank": 11,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 11,
          "paragraph_index": 16,
          "section_id": "shen2024:sec11",
          "section_index": 11
        }
      },
      {
        "node_id": "shen2024:sec1:p1:s3",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "Despite its low cost, the JetMoE-8B demonstrates impressive\nperformance, with JetMoE-8B outperforming the Llama2-7B model and\nJetMoE-8B-Chat surpassing the Llama2-13B-Chat model.",
        "score": 0.6932,
        "rank": 12,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec1:p1",
          "section_id": "shen2024:sec1",
          "sentence_index": 3
        }
      },
      {
        "node_id": "shen2024:sec1:p1",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\nJetMoE: Reaching Llama2 Performance with 0.1M Dollars\nYikang Shen \u2217\nMIT-IBM Watson AI Lab\nyikang.shn@gmail.com\nZhen Guo\u2217\nMIT EECS\nzguo0525@mit.edu\nTianle Cai\nPrinceton University\ntianle.cai@princeton.edu\nZengyi Qin\nMyShell.ai & MIT\nqinzy@mit.edu\nAbstract\nLarge Language Models (LLMs) have achieved remarkable results, but\ntheir increasing resource demand has become a major obstacle to the devel-\nopment of powerful and accessible super-human intelligence. This report\nintroduces JetMoE-8B, a new LLM trained with less than$0.1 million, using\n1.25T tokens from carefully mixed open-source corpora and 30,000 H100\nGPU hours. Despite its low cost, the JetMoE-8B demonstrates impressive\nperformance, with JetMoE-8B outperforming the Llama2-7B model and\nJetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results\nsuggest that LLM training can be much more cost-effective than gener-\nally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-\nof-Experts (SMoE) architecture, composed of attention and feedforward\nexperts. Both layers are sparsely activated, allowing JetMoE-8B to have\n8B parameters while only activating 2B for each input token, reducing\ninference computation by about 70% compared to Llama2-7B. Moreover,\nJetMoE-8B is highly open and academia-friendly, using only public datasets\nand training code. All training parameters and data mixtures have been\ndetailed in this report to facilitate future efforts in the development of open\nfoundation models. This transparency aims to encourage collaboration and\nfurther advancements in the field of accessible and efficient LLMs. The mod-\nels are publicly available at https://github.com/myshell-ai/JetMoE.\n1 Introduction\nLarge Language Models (LLMs) have achieved remarkable results, but their increasing\nresource demand has become a major obstacle to developing powerful and accessible AI.\nAlthough modern LLMs have surpassed human performance on some tasks, they remain\ninefficient and inflexible. Most LLMs (e.g., Llama, Touvron et al. 2023; Pythia, Biderman\net al. 2023; GPT-3, Brown et al. 2020; Mistral, Jiang et al. 2023) use all of their parameters\nduring inference and training, which are referred to as dense models. Considering the\nsubstantial costs, the Mixture-of-Experts (MoE) architecture (Yuksel et al., 2012; Shazeer\net al., 2017; Du et al., 2022; Pan et al., 2024) has emerged as a popular solution, enabling\nparameter scaling while keeping computational costs modest. Recent applications of MoE\narchitectures in Transformers (Vaswani et al., 2017) have yielded successful attempts at\nscaling language models to a substantial size, accompanied by remarkable performance,\nsuch as Deepseek MoE (Dai et al., 2024), Mixtral 8x7B (Jiang et al., 2024), Grok-1 (xai-org,\n2024), and DBRX (Databricks, 2024). However, even though these models achieve excellent\nperformance, they are not truly open-sourced as the training recipes are not published and\nmay contain proprietary datasets inaccessible outside of large corporations. The open-source\ncommunity has also attempted to train MoE models, such as OpenMoE (Xue et al., 2024), but\nits performance is only on par with weak dense models with similar activation parameters,\nsuch as OpenLLaMA (Geng & Liu, 2023) and TinyLLaMA (Zhang et al., 2024a).\n\u2217Equal contribution.\n1\narXiv:2404.07413v1  [cs.CL]  11 Apr 2024",
        "score": 0.6932,
        "rank": 12,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 1,
          "paragraph_index": 1,
          "section_id": "shen2024:sec1",
          "section_index": 1
        }
      },
      {
        "node_id": "shen2024:sec6:p8:s137",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "4.3 Training Data Mixture\nJetMoE-8B is trained on 1.25T tokens of primarily English data from web documents,\nmathematics, and code.",
        "score": 0.6924,
        "rank": 13,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec6:p8",
          "section_id": "shen2024:sec6",
          "sentence_index": 137
        }
      },
      {
        "node_id": "shen2024:sec6:p8",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\n4 Model Pretraining\n4.1 Infrastructures\nWe use Megatron (Shoeybi et al., 2019) as the training framework and integrate\nMegablock (Gale et al., 2023) for MoE support. We further modified the training framework\nto support MoA (Section 2.3) and z-loss (Section 2.4). Against the common practice, we\nchoose the Pipeline parallelism introduced in (Narayanan et al., 2021) instead of the expert\nparallelism for model parallel during training. This is mainly due to two reasons. First,\nSparse MoE models usually have a narrower hidden state compared to standard transformer\nmodels. Thus, the communication cost for pipeline parallelism is smaller. Second, we use\nthe dropless MoE schema introduced in Gale et al. (2023); Shen et al. (2023), which could\ncause load unbalance across experts. Thus, using expert parallel will cause an unbalanced\nload across devices and result in inefficient training. Pipeline parallelism could avoid this\nslowdown because it computes all the experts inside a layer on the same device. We con-\nduct training on a cluster containing 12 nodes and 96 H100s. Inside each node, gpus are\nconnected via NVLinks. Infiniband is used for fast communication between nodes.\n4.2 Hyper-parameters\nPtotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp\n8B 2B 24 2048 8 2 16 128 5632\nTable 1: JetMoE-8B hyperparameters.\nThe hyperparameters of JetMoE-8B are selected based on the common practice for the 1B\ntransformer language model. We replace all self-attention and MLP layers in the transformer\nwith MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every\nlayer. Such that the model has approximately two times the computation compared to a 1B\nmodel. Following ST-MoE (Zoph et al., 2022), the weight for load balancing loss and z-loss\nis set to 0.01 and 0.001, respectively. Table 1 shows the key hyperparameters in JetMoE-8B.\nJetMoE-8B is trained with the AdamW optimizer (Loshchilov & Hutter, 2017) with a maxi-\nmum learning rate of 5e-4 and a batch size of 4M tokens with sequence length of 4096. We\nemploy the Warmup-Stable-Decay (WSD) learning rate schedule introduced in Hu et al.\n(2024). This learning rate scheduler is divided into three stages: the warmup stage (denoted\nby W, representing the number of steps at the end of the warmup stage), the stable training\nstage (denoted by S), and the annealing stage (denoted by D):\nlr(s) =\n\uf8f1\n\uf8f2\n\uf8f3\ns\nW \u2217 \u03b7, s < W\n\u03b7, W < s < S\nf (s \u2212 S) \u2217 \u03b7, S < s < S + D\n(13)\nwhere 0 < f (s \u2212 S) \u2264 1 is a decreasing function of s, and \u03b7 is the maximum learning rate.\nIn our settings, the warmup stage lasts for 10 billion tokens, and the decay stage spans 250\nbillion tokens. The initial and final learning rates are set to 10% of the maximum learning\nrate. A weight decay of 0.1 and gradient clipping of 1.0 are applied during training.\n4.3 Training Data Mixture\nJetMoE-8B is trained on 1.25T tokens of primarily English data from web documents,\nmathematics, and code. Similar to the approach advocated in miniCPM (Hu et al., 2024) and\nGemma (Team et al., 2024), we increase the weight of high-quality data during the learning\nrate decay phase. The training process is divided into two phases:\n\u2022 Phase 1 (warmup and stable learning rate): The dataset includes RefinedWeb, Star-\ncoder, The Pile, peS2o from Dolma, and OpenWebMath.\n6",
        "score": 0.6924,
        "rank": 13,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 6,
          "paragraph_index": 8,
          "section_id": "shen2024:sec6",
          "section_index": 6
        }
      },
      {
        "node_id": "shen2024:sec6:p8:s124",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "The hyperparameters of JetMoE-8B are selected based on the common practice for the 1B\ntransformer language model.",
        "score": 0.6913,
        "rank": 14,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec6:p8",
          "section_id": "shen2024:sec6",
          "sentence_index": 124
        }
      },
      {
        "node_id": "shen2024:sec6:p8",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\n4 Model Pretraining\n4.1 Infrastructures\nWe use Megatron (Shoeybi et al., 2019) as the training framework and integrate\nMegablock (Gale et al., 2023) for MoE support. We further modified the training framework\nto support MoA (Section 2.3) and z-loss (Section 2.4). Against the common practice, we\nchoose the Pipeline parallelism introduced in (Narayanan et al., 2021) instead of the expert\nparallelism for model parallel during training. This is mainly due to two reasons. First,\nSparse MoE models usually have a narrower hidden state compared to standard transformer\nmodels. Thus, the communication cost for pipeline parallelism is smaller. Second, we use\nthe dropless MoE schema introduced in Gale et al. (2023); Shen et al. (2023), which could\ncause load unbalance across experts. Thus, using expert parallel will cause an unbalanced\nload across devices and result in inefficient training. Pipeline parallelism could avoid this\nslowdown because it computes all the experts inside a layer on the same device. We con-\nduct training on a cluster containing 12 nodes and 96 H100s. Inside each node, gpus are\nconnected via NVLinks. Infiniband is used for fast communication between nodes.\n4.2 Hyper-parameters\nPtotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp\n8B 2B 24 2048 8 2 16 128 5632\nTable 1: JetMoE-8B hyperparameters.\nThe hyperparameters of JetMoE-8B are selected based on the common practice for the 1B\ntransformer language model. We replace all self-attention and MLP layers in the transformer\nwith MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every\nlayer. Such that the model has approximately two times the computation compared to a 1B\nmodel. Following ST-MoE (Zoph et al., 2022), the weight for load balancing loss and z-loss\nis set to 0.01 and 0.001, respectively. Table 1 shows the key hyperparameters in JetMoE-8B.\nJetMoE-8B is trained with the AdamW optimizer (Loshchilov & Hutter, 2017) with a maxi-\nmum learning rate of 5e-4 and a batch size of 4M tokens with sequence length of 4096. We\nemploy the Warmup-Stable-Decay (WSD) learning rate schedule introduced in Hu et al.\n(2024). This learning rate scheduler is divided into three stages: the warmup stage (denoted\nby W, representing the number of steps at the end of the warmup stage), the stable training\nstage (denoted by S), and the annealing stage (denoted by D):\nlr(s) =\n\uf8f1\n\uf8f2\n\uf8f3\ns\nW \u2217 \u03b7, s < W\n\u03b7, W < s < S\nf (s \u2212 S) \u2217 \u03b7, S < s < S + D\n(13)\nwhere 0 < f (s \u2212 S) \u2264 1 is a decreasing function of s, and \u03b7 is the maximum learning rate.\nIn our settings, the warmup stage lasts for 10 billion tokens, and the decay stage spans 250\nbillion tokens. The initial and final learning rates are set to 10% of the maximum learning\nrate. A weight decay of 0.1 and gradient clipping of 1.0 are applied during training.\n4.3 Training Data Mixture\nJetMoE-8B is trained on 1.25T tokens of primarily English data from web documents,\nmathematics, and code. Similar to the approach advocated in miniCPM (Hu et al., 2024) and\nGemma (Team et al., 2024), we increase the weight of high-quality data during the learning\nrate decay phase. The training process is divided into two phases:\n\u2022 Phase 1 (warmup and stable learning rate): The dataset includes RefinedWeb, Star-\ncoder, The Pile, peS2o from Dolma, and OpenWebMath.\n6",
        "score": 0.6913,
        "rank": 14,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 6,
          "paragraph_index": 8,
          "section_id": "shen2024:sec6",
          "section_index": 6
        }
      },
      {
        "node_id": "samsi2024:sec6:p13:s183",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Interestingly, there appears to be an exception for\nthe GSM8K math problem dataset;",
        "score": 0.6897,
        "rank": 15,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "paragraph_id": "samsi2024:sec6:p13",
          "section_id": "samsi2024:sec6",
          "sentence_index": 183
        }
      },
      {
        "node_id": "samsi2024:sec6:p13",
        "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
        "text": "Fig. 5: Energy per second (Watts) estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards for\nmax generation length 1024 : inference energy estimates on\nAlpaca and GSM8K on log-scale. Color indicates batch size.\nincreases. However, we see little change in the average energy\nper token between max generation length 512 and 1024. For\ninstance, with length 512, we see that it takes about 3-4 Joules\nfor a output token, which is approximately the same amount\nfor length 512. As with energy per second, max generation\nlength seems to have a negligible effect on energy costs from\n512 to 1024. Interestingly, there appears to be an exception for\nthe GSM8K math problem dataset; there exists a \u201csweet spot\u201d\nat 16 shards where continuously increasing the batch size can\nactually reduce the energy per token at max generation length\n512. However, this disappears under max generation length\n1024 where increasing the batch size increases the energy per\ntoken. The definitive existence of this sweet spot for datasets\nof differing styles/complexities, or others like it, will require\nmore experimentation and benchmarking to establish.\nD. Energy per Response: LLaMA 65B\nFigures 8 and 9 show energy metrics in terms of responses\nfrom the 65B model. Like before, we see that increasing\nthe number of shards still tends to increase the energy costs\nof inference per response most overall while increasing the\nmaximum generation length from 512 (Figure 8) to 1024\n(Figure 9) does not induce a clear or significant effect in\ninference energy costs. Also like before, while we see slight\nincreases in energy costs per response generated within a\nshard configuration as batch size increases, but not consistently\nor significantly. Again, we see that for GSM8K, at max\nFig. 6: Energy per output token estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards\nfor max generation length 512 : inference energy estimates\non Alpaca and GSM8K on log-scale. Color indicates batch\nsize.\nFig. 7: Energy per output token estimates of LLaMA 65B\nacross batch sizes of 64/128/256/512 and 8/16/32 shards for\nmax generation length 1024 : inference energy estimates on\nAlpaca and GSM8K on log-scale. Color indicates batch size.",
        "score": 0.6897,
        "rank": 15,
        "metadata": {
          "document_id": "samsi2024",
          "document_title": "From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference",
          "page": 6,
          "paragraph_index": 13,
          "section_id": "samsi2024:sec6",
          "section_index": 6
        }
      },
      {
        "node_id": "shen2024:sec11:p16:s216",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE-8B represents a significant\nstep forward in the development of open-source, efficient, and high-performing language\nmodels, contributing to the democratization of advanced language technologies.",
        "score": 0.689,
        "rank": 16,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec11:p16",
          "section_id": "shen2024:sec11",
          "sentence_index": 216
        }
      },
      {
        "node_id": "shen2024:sec11:p16",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\nactivation in both the attention and feed-forward layers, JetMoE-8B reduces computational\ncosts while maintaining strong performance across a wide range of tasks.\nTrained using a two-phase approach and a carefully curated mixture of open-source datasets,\nJetMoE-8B outperforms larger and more resource-intensive models on the OpenLLM Leader-\nboard. In addition, JetMoE-8B-Chat demonstrates competitive performance compared to\nother open-source chatbots.\nWe provide detailed training parameters and data mixture information to encourage repro-\nducibility and enable researchers to build upon our work. JetMoE-8B represents a significant\nstep forward in the development of open-source, efficient, and high-performing language\nmodels, contributing to the democratization of advanced language technologies.\nAcknowledgments\nWe express our gratitude to Shengding Hu for his valuable advice on the Phase 2 data\nmixture. We also express our gratitude to Exabits for their assistance in setting up the GPU\nclusters, and to Lepton AI for their support in setting up the chat demo.\nReferences\nabacusai. Systemchat, 2024. URL https://huggingface.co/datasets/abacusai/\nSystemChat.\najibawa 2023. Code-290k-sharegpt, 2024. URL https://huggingface.co/datasets/\najibawa-2023/Code-290k-ShareGPT .\nJason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Vozne-\nsensky, Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. Pytorch 2: Faster machine\nlearning through dynamic python bytecode transformation and graph compilation, 2024.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David\nDohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with\nlarge language models. arXiv preprint arXiv:2108.07732, 2021.\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer,\nAlbert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language\nmodel for mathematics, 2023.\nLoubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro\nvon Werra. A framework for the evaluation of code generation models. https://github.\ncom/bigcode-project/bigcode-evaluation-harness , 2022.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric\nHallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward\nRaff, et al. Pythia: A suite for analyzing large language models across training and scaling.\narXiv preprint arXiv:2304.01373, 2023.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\ncommonsense in natural language. In Proceedings of the AAAI conference on artificial\nintelligence, volume 34, pp. 7432\u20137439, 2020.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in neural information processing systems,\n33:1877\u20131901, 2020.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,\nJared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray,\nRaul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin,\n11",
        "score": 0.689,
        "rank": 16,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 11,
          "paragraph_index": 16,
          "section_id": "shen2024:sec11",
          "section_index": 11
        }
      },
      {
        "node_id": "shen2024:sec1:p1:s6",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "Both layers are sparsely activated, allowing JetMoE-8B to have\n8B parameters while only activating 2B for each input token, reducing\ninference computation by about 70% compared to Llama2-7B.",
        "score": 0.6886,
        "rank": 17,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec1:p1",
          "section_id": "shen2024:sec1",
          "sentence_index": 6
        }
      },
      {
        "node_id": "shen2024:sec1:p1",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\nJetMoE: Reaching Llama2 Performance with 0.1M Dollars\nYikang Shen \u2217\nMIT-IBM Watson AI Lab\nyikang.shn@gmail.com\nZhen Guo\u2217\nMIT EECS\nzguo0525@mit.edu\nTianle Cai\nPrinceton University\ntianle.cai@princeton.edu\nZengyi Qin\nMyShell.ai & MIT\nqinzy@mit.edu\nAbstract\nLarge Language Models (LLMs) have achieved remarkable results, but\ntheir increasing resource demand has become a major obstacle to the devel-\nopment of powerful and accessible super-human intelligence. This report\nintroduces JetMoE-8B, a new LLM trained with less than$0.1 million, using\n1.25T tokens from carefully mixed open-source corpora and 30,000 H100\nGPU hours. Despite its low cost, the JetMoE-8B demonstrates impressive\nperformance, with JetMoE-8B outperforming the Llama2-7B model and\nJetMoE-8B-Chat surpassing the Llama2-13B-Chat model. These results\nsuggest that LLM training can be much more cost-effective than gener-\nally thought. JetMoE-8B is based on an efficient Sparsely-gated Mixture-\nof-Experts (SMoE) architecture, composed of attention and feedforward\nexperts. Both layers are sparsely activated, allowing JetMoE-8B to have\n8B parameters while only activating 2B for each input token, reducing\ninference computation by about 70% compared to Llama2-7B. Moreover,\nJetMoE-8B is highly open and academia-friendly, using only public datasets\nand training code. All training parameters and data mixtures have been\ndetailed in this report to facilitate future efforts in the development of open\nfoundation models. This transparency aims to encourage collaboration and\nfurther advancements in the field of accessible and efficient LLMs. The mod-\nels are publicly available at https://github.com/myshell-ai/JetMoE.\n1 Introduction\nLarge Language Models (LLMs) have achieved remarkable results, but their increasing\nresource demand has become a major obstacle to developing powerful and accessible AI.\nAlthough modern LLMs have surpassed human performance on some tasks, they remain\ninefficient and inflexible. Most LLMs (e.g., Llama, Touvron et al. 2023; Pythia, Biderman\net al. 2023; GPT-3, Brown et al. 2020; Mistral, Jiang et al. 2023) use all of their parameters\nduring inference and training, which are referred to as dense models. Considering the\nsubstantial costs, the Mixture-of-Experts (MoE) architecture (Yuksel et al., 2012; Shazeer\net al., 2017; Du et al., 2022; Pan et al., 2024) has emerged as a popular solution, enabling\nparameter scaling while keeping computational costs modest. Recent applications of MoE\narchitectures in Transformers (Vaswani et al., 2017) have yielded successful attempts at\nscaling language models to a substantial size, accompanied by remarkable performance,\nsuch as Deepseek MoE (Dai et al., 2024), Mixtral 8x7B (Jiang et al., 2024), Grok-1 (xai-org,\n2024), and DBRX (Databricks, 2024). However, even though these models achieve excellent\nperformance, they are not truly open-sourced as the training recipes are not published and\nmay contain proprietary datasets inaccessible outside of large corporations. The open-source\ncommunity has also attempted to train MoE models, such as OpenMoE (Xue et al., 2024), but\nits performance is only on par with weak dense models with similar activation parameters,\nsuch as OpenLLaMA (Geng & Liu, 2023) and TinyLLaMA (Zhang et al., 2024a).\n\u2217Equal contribution.\n1\narXiv:2404.07413v1  [cs.CL]  11 Apr 2024",
        "score": 0.6886,
        "rank": 17,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 1,
          "paragraph_index": 1,
          "section_id": "shen2024:sec1",
          "section_index": 1
        }
      },
      {
        "node_id": "shen2024:sec9:p13:s185",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "We measure JetMoE-8B\u2019s performance on tasks included in OpenLLM leaderboard2 and\nfrom other domains, including physical reasoning (Bisk et al., 2020), social reasoning (Sap\net al., 2019), question answering (Clark et al., 2019;",
        "score": 0.6883,
        "rank": 18,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec9:p13",
          "section_id": "shen2024:sec9",
          "sentence_index": 185
        }
      },
      {
        "node_id": "shen2024:sec9:p13",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\n2023), Code-Feedback (Zheng et al., 2024b), Orca-math-word-problems-200k (Mitra et al.,\n2024), SystemChat (abacusai, 2024), and Capybara (Daniele & Suphavadeeprasit, 2023).\nChat template is the same as Zephyr-7b-beta. The key hyperparameters for dSFT are a\nlearning rate of 2e-5 with an Adam optimizer, a batch size of 128, and 3 epochs.\nWe further finetune the JetMoE-8B-SFT model using dDPO on the UltraFeedback\ndataset (Cui et al., 2023), which contains binary preference labels indicating the preferred\nresponse between two options. The key hyperparameters for dDPO are a learning rate of\n5e-7 with AdamW, a batch size of 128, and 1 epoch. This fine-tuning process results in the\nJetMoE-8B-Chat model. The entire alignment process takes 60 H100 GPU hours.\n6 Evaluation\nLLaMA2 DeepseekMoE Gemma JetMoE\n# Total Params 7B 16B 2B 8B\n# Activate Params 7B 2.8B 2B 2.2B\n# Training tokens 2T 2T 2T 1.25T\nARC-challenge 53.1 53.2 48.4 48.7\nHellaswag 78.6 79.8 71.8 80.5\nMMLU 46.9 46.3 41.8 49.2\nTruthfulQA 38.8 36.1 33.1 41.7\nWinoGrande 74.0 73.7 66.3 70.2\nGSM8k 14.5 17.3 16.9 27.8\nOpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0\nMBPP (Pass@1) 20.8 34.0 28.0 34.2\nHumanEval (Pass@1) 12.8 25.0 24.4 14.6\nAll Avg. 45.5 47.3 43.2 47.6\nTable 3: OpenLLM leaderboard and code benchmarks results from four different models.\nWe measure JetMoE-8B\u2019s performance on tasks included in OpenLLM leaderboard2 and\nfrom other domains, including physical reasoning (Bisk et al., 2020), social reasoning (Sap\net al., 2019), question answering (Clark et al., 2019; Kwiatkowski et al., 2019), mathemat-\nics (Cobbe et al., 2021), commonsense reasoning (Sakaguchi et al., 2021), language model-\ning (Paperno et al., 2016), reading comprehension (Joshi et al., 2017), and more. For most\nbenchmarks, we use the same evaluation methodology as in the OpenLLM leaderboard\nto be comparable to other models.. We compare JetMoE-8B models to several external\nopen-source (OSS) LLMs, including Gemma, LLaMA2, DeepseekMoE.\nIn addition, we include HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021)\nto evaluate the code generation of the models. Utilizing the BigCode Evaluation Har-\nness (Ben Allal et al., 2022), we follow recent work on Code LLMs (Rozi`ere et al., 2024; Guo\net al., 2024a) with greedy decoding, and report the mean pass@1 (mean success rate) for the\ntwo benchmarks.\nTable 3 shows the OpenLLM leaderboard and code benchmarks results from four different\nmodels. JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM\nleaderboard, achieving the best scores in all tasks except ARC-challenge and WinoGrande.\nAdditionally, JetMoE-8B obtains the highest MBPP scores in Python programming.\nWe also evaluated our model on MT-Bench (Zheng et al., 2023) with a strong LLM judge\n(gpt-4-0613 checkpoint). The temperature configuration, following the official FastChat\nimplementation, is defined as follows: \u201dWriting\u201d and \u201dRoleplay\u201d tasks have a temperature\nof 0.7, indicating higher creativity; \u201dExtraction\u201d, \u201dMath\u201d, \u201dCoding\u201d, and \u201dReasoning\u201d tasks\n2https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard\n9",
        "score": 0.6883,
        "rank": 18,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 9,
          "paragraph_index": 13,
          "section_id": "shen2024:sec9",
          "section_index": 9
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s34",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "In addition, JetMoE-8B has 8B parameters while only activating 2B for each input\ntoken, reducing inference computation by about 70% compared to Llama2-7B.",
        "score": 0.6845,
        "rank": 19,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 34
        }
      },
      {
        "node_id": "shen2024:sec2:p2",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\nFigure 1: JetMoE architecture\nTo facilitate future efforts on open foundation models, particularly MoE models, we intro-\nduce JetMoE-8B, an innovative MoE architecture inspired by ModuleFormer (Shen et al.,\n2023) that extends the concept of sparse activation to both the attention and feed-forward\nlayers. Unlike prior works that only apply sparse activation to the feed-forward layer,\nJetMoE-8B leverages sparse activation in both components to further reduce computational\ncosts while maintaining performance.\nImpressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from\nmixed open-source datasets and 30,000 H100 GPU hours. Despite its low cost, JetMoE-8B\noutperforms the Llama2-7B model, and JetMoE-8B-Chat outperforms the Llama2-13B-Chat\nmodel, demonstrating that LLM training can be much more cost-effective than generally\nthought. In addition, JetMoE-8B has 8B parameters while only activating 2B for each input\ntoken, reducing inference computation by about 70% compared to Llama2-7B.\nThe key advantages of JetMoE-8B include:\n\u2022 Openness and academia-friendly: JetMoE-8B is trained using only public datasets and\nopen-source training code, making it accessible to many academia research settings.\nThe model can also be finetuned with limited compute budgets (e.g., consumer-grade\nGPUs).\n\u2022 Sparse activation on both attention and feed-forward layers , which significantly\nreduces training and inference costs. We also propose to share the kv projection in\nattention experts to improve training stability.\n\u2022 Comprehensive open-source data mixture, which ensures high-quality training using\nonly open-source datasets.\nThese innovations in JetMoE-8B pave the way for more accessible and efficient LLMs, bene-\nfiting the broader AI research community. To foster collaboration and further advancements,\nwe have detailed all the training parameters and data mixture in this report.\n2 Model Architecture\n2.1 Mixture of Experts\nA Mixture of Experts (MoE) layer comprises N modules f1, . . ., fN and a router g(e | x).\nGiven an input x to the MoE layer, the router predicts a probability distribution over the N\n2",
        "score": 0.6845,
        "rank": 19,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 2,
          "paragraph_index": 2,
          "section_id": "shen2024:sec2",
          "section_index": 2
        }
      },
      {
        "node_id": "shen2024:sec2:p2",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\nFigure 1: JetMoE architecture\nTo facilitate future efforts on open foundation models, particularly MoE models, we intro-\nduce JetMoE-8B, an innovative MoE architecture inspired by ModuleFormer (Shen et al.,\n2023) that extends the concept of sparse activation to both the attention and feed-forward\nlayers. Unlike prior works that only apply sparse activation to the feed-forward layer,\nJetMoE-8B leverages sparse activation in both components to further reduce computational\ncosts while maintaining performance.\nImpressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from\nmixed open-source datasets and 30,000 H100 GPU hours. Despite its low cost, JetMoE-8B\noutperforms the Llama2-7B model, and JetMoE-8B-Chat outperforms the Llama2-13B-Chat\nmodel, demonstrating that LLM training can be much more cost-effective than generally\nthought. In addition, JetMoE-8B has 8B parameters while only activating 2B for each input\ntoken, reducing inference computation by about 70% compared to Llama2-7B.\nThe key advantages of JetMoE-8B include:\n\u2022 Openness and academia-friendly: JetMoE-8B is trained using only public datasets and\nopen-source training code, making it accessible to many academia research settings.\nThe model can also be finetuned with limited compute budgets (e.g., consumer-grade\nGPUs).\n\u2022 Sparse activation on both attention and feed-forward layers , which significantly\nreduces training and inference costs. We also propose to share the kv projection in\nattention experts to improve training stability.\n\u2022 Comprehensive open-source data mixture, which ensures high-quality training using\nonly open-source datasets.\nThese innovations in JetMoE-8B pave the way for more accessible and efficient LLMs, bene-\nfiting the broader AI research community. To foster collaboration and further advancements,\nwe have detailed all the training parameters and data mixture in this report.\n2 Model Architecture\n2.1 Mixture of Experts\nA Mixture of Experts (MoE) layer comprises N modules f1, . . ., fN and a router g(e | x).\nGiven an input x to the MoE layer, the router predicts a probability distribution over the N\n2",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 2,
          "paragraph_index": 2,
          "section_id": "shen2024:sec2",
          "section_index": 2
        }
      },
      {
        "node_id": "shen2024:sec2",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "Page 2",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "page": 2,
          "section_index": 2
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s30",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "JetMoE\nFigure 1: JetMoE architecture\nTo facilitate future efforts on open foundation models, particularly MoE models, we intro-\nduce JetMoE-8B, an innovative MoE architecture inspired by ModuleFormer (Shen et al.,\n2023) that extends the concept of sparse activation to both the attention and feed-forward\nlayers.",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 30
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s31",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "Unlike prior works that only apply sparse activation to the feed-forward layer,\nJetMoE-8B leverages sparse activation in both components to further reduce computational\ncosts while maintaining performance.",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 31
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s32",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from\nmixed open-source datasets and 30,000 H100 GPU hours.",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 32
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s33",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "Despite its low cost, JetMoE-8B\noutperforms the Llama2-7B model, and JetMoE-8B-Chat outperforms the Llama2-13B-Chat\nmodel, demonstrating that LLM training can be much more cost-effective than generally\nthought.",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 33
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s34",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "In addition, JetMoE-8B has 8B parameters while only activating 2B for each input\ntoken, reducing inference computation by about 70% compared to Llama2-7B.",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 34
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s35",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "The key advantages of JetMoE-8B include:\n\u2022 Openness and academia-friendly: JetMoE-8B is trained using only public datasets and\nopen-source training code, making it accessible to many academia research settings.",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 35
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s36",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "The model can also be finetuned with limited compute budgets (e.g., consumer-grade\nGPUs).",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 36
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s37",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "\u2022 Sparse activation on both attention and feed-forward layers , which significantly\nreduces training and inference costs.",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 37
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s38",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "We also propose to share the kv projection in\nattention experts to improve training stability.",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 38
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s39",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "\u2022 Comprehensive open-source data mixture, which ensures high-quality training using\nonly open-source datasets.",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 39
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s40",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "These innovations in JetMoE-8B pave the way for more accessible and efficient LLMs, bene-\nfiting the broader AI research community.",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 40
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s41",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "To foster collaboration and further advancements,\nwe have detailed all the training parameters and data mixture in this report.",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 41
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s42",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "2 Model Architecture\n2.1 Mixture of Experts\nA Mixture of Experts (MoE) layer comprises N modules f1, .",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 42
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s43",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": ".",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 43
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s44",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "., fN and a router g(e | x).",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 44
        }
      },
      {
        "node_id": "shen2024:sec2:p2:s45",
        "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
        "text": "Given an input x to the MoE layer, the router predicts a probability distribution over the N\n2",
        "score": 0.6826,
        "rank": 20,
        "metadata": {
          "document_id": "shen2024",
          "document_title": "JetMoE: Reaching Llama2 Performance with 0.1M Dollars",
          "paragraph_id": "shen2024:sec2:p2",
          "section_id": "shen2024:sec2",
          "sentence_index": 45
        }
      }
    ],
    "num_snippets": 56,
    "retry_count": 1
  }
]