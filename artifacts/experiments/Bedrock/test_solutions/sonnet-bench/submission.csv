"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","According to the context, the total energy consumption of US data centers increased by about 4% from 2010-2014, as stated directly in document wu2021b.","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","According to the context, the total energy consumption of US data centers increased by about 4% from 2010-2014, as stated directly in document wu2021b."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","According to the context, the Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is explicitly stated to be equivalent to taking more than 13,900 cars off the road.","13900","cars","[""amazon2023""]","is_blank","Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","According to the context, the Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is explicitly stated to be equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","According to the context, AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023, meaning they added 4 new data centers with recycled water cooling that year. The text specifically identifies these as two data centers in Virginia, one in California, and one in Singapore.","4","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","According to the context, AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023, meaning they added 4 new data centers with recycled water cooling that year. The text specifically identifies these as two data centers in Virginia, one in California, and one in Singapore."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","According to morrison2025, they assume the same embodied carbon emissions as Luccioni et al. (2023), which is 3700 kg CO2eq per 8x server node, equal to 463 kg per GPU.","463","kg/GPU","[""morrison2025""]","is_blank","NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU.","According to morrison2025, they assume the same embodied carbon emissions as Luccioni et al. (2023), which is 3700 kg CO2eq per 8x server node, equal to 463 kg per GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The context shows that GPT-4's estimated amortized hardware and energy cost was $40M (from cottier2024), while FLM-101B was trained with a total budget of $100K (from li2025a). The ratio between these values is 40M/100K = 400.","400","ratio","[""cottier2024"", ""li2025a""]","is_blank","['For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.', 'In this paper, we introduce FLM-101B, an open-sourced LLM that is successfully trained from scratch within a $100,000 budget.']","The context shows that GPT-4's estimated amortized hardware and energy cost was $40M (from cottier2024), while FLM-101B was trained with a total budget of $100K (from li2025a). The ratio between these values is 40M/100K = 400."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","According to the context, Google Flights calculates that for a Boeing 767 flight between SFO and JFK with 84.5% average seat occupancy, the CO2e per passenger round trip is 1.2t.","1.2","tCO2e","[""patterson2021""]","is_blank","Google Flights uses the historical average of 84.5% seat occupancy, yielding 1.2t of CO2e per passenger round trip.","According to the context, Google Flights calculates that for a Boeing 767 flight between SFO and JFK with 84.5% average seat occupancy, the CO2e per passenger round trip is 1.2t."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","According to Table 4 in document li2025a, FLM-101B achieved an average score of 43.94 on the Open LLM Leaderboard evaluation across ARC, HellaSwag, MMLU, and TruthfulQA tasks.","43.94","score","[""li2025a""]","is_blank","Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B [...] Model Cost (zettaFLOPs) Average ARC HellaSwag MMLU TruthfulQA [...] FLM-101B 28.22 43.94 39.76 66.23 28.30 ∗ 41.47","According to Table 4 in document li2025a, FLM-101B achieved an average score of 43.94 on the Open LLM Leaderboard evaluation across ARC, HellaSwag, MMLU, and TruthfulQA tasks."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context directly states there was a 6,750 fold improvement in processor clock speed between the Intel 4004 manufactured in 1971 and typical microprocessors in 2021.","6750","fold","[""wu2021b""]","is_blank","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context directly states there was a 6,750 fold improvement in processor clock speed between the Intel 4004 manufactured in 1971 and typical microprocessors in 2021."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","According to Patterson 2021, it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute the total floating point operations needed to train GPT-3. This is explicitly stated in their calculation of GPT-3's training time.","14.8","days","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20]. OpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec [Sut21]. It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","According to Patterson 2021, it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute the total floating point operations needed to train GPT-3. This is explicitly stated in their calculation of GPT-3's training time."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","From Table 3 in the Morrison 2025 paper, for Llama 3.2 1B model at 8 requests/s frequency, the GPU Power Usage is explicitly stated as 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","Table 3: [...] Model: Llama 3.2 1B, Request freq. (req/s): 8, GPU Power Usage (kWh): 0.036","From Table 3 in the Morrison 2025 paper, for Llama 3.2 1B model at 8 requests/s frequency, the GPU Power Usage is explicitly stated as 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","According to the context, the total permitted annual emission limits for diesel generators in Northern Virginia were approximately 13,000 tons of NOx, as stated multiple times in the text.","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","According to the context, the total permitted annual emission limits for diesel generators in Northern Virginia were approximately 13,000 tons of NOx, as stated multiple times in the text."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","According to the paper, using the growth strategy for FLM-101B achieved a time-saving of 72% compared to training a 101B model from scratch. This is explicitly stated as '72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated)'.","72","percent","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","According to the paper, using the growth strategy for FLM-101B achieved a time-saving of 72% compared to training a 101B model from scratch. This is explicitly stated as '72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated)'."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context does not provide specific data about premature deaths from data center pollutants in 2030. The closest projection mentioned is for 2028, when U.S. data centers' scope-1 and scope-2 pollutants combined could cause approximately 1,300 premature deaths.","is_blank","deaths","[""is_blank""]","is_blank","is_blank","The context does not provide specific data about premature deaths from data center pollutants in 2030. The closest projection mentioned is for 2028, when U.S. data centers' scope-1 and scope-2 pollutants combined could cause approximately 1,300 premature deaths."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","According to the context, while the monitored training run was only 8 days (13% completion), the authors estimate a full training run would take 60 days to complete.","60","days","[""dodge2022""]","is_blank","This model was not trained to completion, but only until 13%; a full training run would take 60 days.","According to the context, while the monitored training run was only 8 days (13% completion), the authors estimate a full training run would take 60 days to complete."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","According to Table 2 in the luccioni2025c document, the Command-R Plus model by Cohere has the highest GPU energy consumption for 1,000 queries at 3,426.38 Wh among all models listed.","Command-R Plus","is_blank","[""luccioni2025c""]","is_blank","Table 2. Range of Inference Energy Use21 (Representative Models Displayed)
Model Organization GPU Energy for 1k Queries (Wh) Task
[...]
Command-R Plus Cohere 3,426.38 Text Generation","According to Table 2 in the luccioni2025c document, the Command-R Plus model by Cohere has the highest GPU energy consumption for 1,000 queries at 3,426.38 Wh among all models listed."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context explicitly states that 'The One Hundred Year Study on Artificial Intelligence' was 'launched in the fall of 2014'.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.","The context explicitly states that 'The One Hundred Year Study on Artificial Intelligence' was 'launched in the fall of 2014'."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","According to the UN's Global E-Waste Monitor 2024, 22% of e-waste has been formally collected and recycled. This is stated directly in multiple quotes from the context, and matches the requested percentage unit format.","22","percent","[""luccioni2025a""]","is_blank","The UN's Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","According to the UN's Global E-Waste Monitor 2024, 22% of e-waste has been formally collected and recycled. This is stated directly in multiple quotes from the context, and matches the requested percentage unit format."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","According to the context, the final training run for BLOOM used 37.24% of the total energy of the BLOOM project. Using Meta's 2023 PUE of 1.08 and excluding non-GPU overhead, the total training energy consumption was approximately 30 GWh or 30,000 MWh.","30000","MWh","[""han2024""]","is_blank","Considering Meta's 2023 PUE of 1.08 [37] and excluding the non-GPU overhead for servers, we estimate the total training energy consumption as approximately 30 GWh.","According to the context, the final training run for BLOOM used 37.24% of the total energy of the BLOOM project. Using Meta's 2023 PUE of 1.08 and excluding non-GPU overhead, the total training energy consumption was approximately 30 GWh or 30,000 MWh."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","According to the context, the Switch Transformer has 1500 billion parameters but only 0.1% are activated per token, as explicitly stated in Patterson et al.'s documentation.","0.1","percent","[""patterson2021""]","is_blank","Switch Transformer simplifies the Mixture of Expert (MoE) routing algorithm to design intuitive improved models with reduced communication and computational costs [Fed21]. The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources.","According to the context, the Switch Transformer has 1500 billion parameters but only 0.1% are activated per token, as explicitly stated in Patterson et al.'s documentation."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","According to the context, JetMoE-8B uses 8 experts in each MoE layer. This is explicitly shown in Table 1 of the paper where Nexperts=8, and is consistent across both attention and feed-forward layers.","8","experts","[""shen2024""]","is_blank","Table 1: JetMoE-8B hyperparameters.
Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp
8B 2B 24 2048 8 2 16 128 5632","According to the context, JetMoE-8B uses 8 experts in each MoE layer. This is explicitly shown in Table 1 of the paper where Nexperts=8, and is consistent across both attention and feed-forward layers."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","Looking at Figure 4 in the context, which shows execution time breakdowns, a dense BlackMamba model with batch size of 30 shows total execution time (sum of Forward, Backward, and Optimizer stages) of approximately 1.5 seconds.","1.5","second","[""xia2024""]","is_blank","Figure 4 from xia2024 shows execution time breakdown for Dense(bsz=30) configuration for Mamba (BlackMamba) model","Looking at Figure 4 in the context, which shows execution time breakdowns, a dense BlackMamba model with batch size of 30 shows total execution time (sum of Forward, Backward, and Optimizer stages) of approximately 1.5 seconds."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","According to the context, FLM-101B's English portion required 28.22 zettaFLOPs for training. This is explicitly stated in multiple tables and discussions showing the computational costs, particularly in Table 4 and other sections that break down the model's training costs by language.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","According to the context, FLM-101B's English portion required 28.22 zettaFLOPs for training. This is explicitly stated in multiple tables and discussions showing the computational costs, particularly in Table 4 and other sections that break down the model's training costs by language."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","According to the experimental setup described in Khan et al. 2025, the hardware used included an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz with a base frequency of 1.69 GHz.","Intel(R) Core(TM) i7-1165G7","is_blank","[""khan2025""]","is_blank","The hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable).","According to the experimental setup described in Khan et al. 2025, the hardware used included an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz with a base frequency of 1.69 GHz."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","According to the study by Luccioni et al. (2024), they analyzed 80 finetuned models across ten different tasks. This is explicitly shown in Table 6 which lists all the task-specific models tested, with 8 models for each of the 10 tasks.","80","models","[""luccioni2024""]","is_blank","Table 6. The full list of the 80 finetuned models that were tested for the ten tasks we analyzed.","According to the study by Luccioni et al. (2024), they analyzed 80 finetuned models across ten different tasks. This is explicitly shown in Table 6 which lists all the task-specific models tested, with 8 models for each of the 10 tasks."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","According to the context, increasing GPU utilization up to 80% decreases the overall carbon footprint by 3×. This is explicitly stated in the text.","3","multiplier","[""wu2021a""]","is_blank","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","According to the context, increasing GPU utilization up to 80% decreases the overall carbon footprint by 3×. This is explicitly stated in the text."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","According to the context, the authors estimate that total compute for model development ranges from 1.2x to 4x larger than final training run compute, with a median of 2.2x.","[1.2, 4.0]","multiplier","[""cottier2024""]","is_blank","we estimate that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x.","According to the context, the authors estimate that total compute for model development ranges from 1.2x to 4x larger than final training run compute, with a median of 2.2x."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","From the context, the 6 billion parameter transformer used 13,812.4 kWh for 13% of training, and they estimate a full training run would consume approximately 103,593 kWh. Converting to MWh: 103,593 kWh = 103.593 MWh.","103.593","MWh","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","From the context, the 6 billion parameter transformer used 13,812.4 kWh for 13% of training, and they estimate a full training run would consume approximately 103,593 kWh. Converting to MWh: 103,593 kWh = 103.593 MWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The 2025 paper by Luccioni et al. specifically examines how Jevons' Paradox applies to AI, arguing that efficiency gains may paradoxically lead to increased resource consumption rather than net environmental benefits. This is directly stated in the paper's description and further elaborated through discussions of rebound effects.","Jevons' Paradox","is_blank","[""luccioni2025a""]","is_blank","This paper examines how the problem of Jevons' Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption. [...] Similarly to Jevons' Paradox, just because an AI model becomes more efficient, that does not imply that overall AI resource consumption will decrease, and in fact the inverse effect is highly plausible.","The 2025 paper by Luccioni et al. specifically examines how Jevons' Paradox applies to AI, arguing that efficiency gains may paradoxically lead to increased resource consumption rather than net environmental benefits. This is directly stated in the paper's description and further elaborated through discussions of rebound effects."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","According to multiple mentions in the context, global AI demand is projected to account for 4.2-6.6 billion cubic meters of water withdrawal in 2027. This range represents the combined scope-1 and scope-2 water withdrawal as detailed in the appendix calculations.","[4.2, 6.6]","billion cubic meters","[""li2025b""]","is_blank","More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","According to multiple mentions in the context, global AI demand is projected to account for 4.2-6.6 billion cubic meters of water withdrawal in 2027. This range represents the combined scope-1 and scope-2 water withdrawal as detailed in the appendix calculations."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context directly states 'Red AI is on the rise despite the well-known diminishing returns of increased cost.' This clearly indicates that Red AI is not declining, but rather increasing, even though there are known diminishing returns.","0","is_blank","[""schwartz2019""]","is_blank","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The context directly states 'Red AI is on the rise despite the well-known diminishing returns of increased cost.' This clearly indicates that Red AI is not declining, but rather increasing, even though there are known diminishing returns."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","According to the context, under the growth schedule used to train FLM-101B, the total time cost for training was 21.54 days, which achieved a 72% time savings compared to training a 101B model from scratch (which would have taken 76.74 days).","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","According to the context, under the growth schedule used to train FLM-101B, the total time cost for training was 21.54 days, which achieved a 72% time savings compared to training a 101B model from scratch (which would have taken 76.74 days)."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","According to multiple context snippets, a significant portion/majority of Facebook's model experimentation workloads utilize GPUs at only 30-50% capacity, which directly contradicts the claim about >80% utilization.","0","is_blank","[""wu2021a""]","is_blank","A vast majority of model experimentation (over tens of thousands of training workﬂows) utilizes GPUs at only 30-50%, leaving room for utilization and efﬁciency improvements.","According to multiple context snippets, a significant portion/majority of Facebook's model experimentation workloads utilize GPUs at only 30-50% capacity, which directly contradicts the claim about >80% utilization."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","Multiple context snippets state that GPT-3 training consumed 1,287 megawatt-hours (MWh) of electricity. This appears in both the jegham2025 and li2025b documents, providing consistent confirmation of this value.","1287","MWh","[""jegham2025"", ""li2025b""]","is_blank","Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric tons of CO 2 equivalent (CO2e) [12]","Multiple context snippets state that GPT-3 training consumed 1,287 megawatt-hours (MWh) of electricity. This appears in both the jegham2025 and li2025b documents, providing consistent confirmation of this value."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","According to the context, the AI Energy Score project aims to establish a unified approach for comparing the inference efficiency of AI models. This is mentioned explicitly in multiple sources including Luccioni 2025c.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.","According to the context, the AI Energy Score project aims to establish a unified approach for comparing the inference efficiency of AI models. This is mentioned explicitly in multiple sources including Luccioni 2025c."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","Looking at Fig. 6 in the context showing execution time breakdown for different kernels, for BlackMamba with Dense(bsz=30), the matmul(w1) kernel has the longest execution time at approximately 1600 microseconds.","1600","microseconds","[""xia2024""]","is_blank","Fig. 6. Execution breakdown of the MoE layer for different kernels.","Looking at Fig. 6 in the context showing execution time breakdown for different kernels, for BlackMamba with Dense(bsz=30), the matmul(w1) kernel has the longest execution time at approximately 1600 microseconds."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","According to the context in xia2024, 'for sparse fine-tuning, only the top two experts are selected for each token.' This is specifically stated in the experimental setup section discussing how the model activates experts.","2","experts","[""xia2024""]","is_blank","For dense fine-tuning, all experts are activated, whereas for sparse fine-tuning, only the top two experts are selected for each token.","According to the context in xia2024, 'for sparse fine-tuning, only the top two experts are selected for each token.' This is specifically stated in the experimental setup section discussing how the model activates experts."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","According to the Schwartz 2019 paper, there was a 300,000x (not 200,000x) increase in compute used to train deep learning models from 2012 to 2018. Since the statement is incorrect about the magnitude of increase, it is false.","0","is_blank","[""schwartz2019""]","is_blank","The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2].","According to the Schwartz 2019 paper, there was a 300,000x (not 200,000x) increase in compute used to train deep learning models from 2012 to 2018. Since the statement is incorrect about the magnitude of increase, it is false."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","According to the context, global carbon emissions dropped by 6.4% in 2020, with vehicle transportation in the US accounting for a portion of this reduction.","6.4","percent","[""wu2021b""]","is_blank","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","According to the context, global carbon emissions dropped by 6.4% in 2020, with vehicle transportation in the US accounting for a portion of this reduction."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","According to the context, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023, which was an increase from 19 regions in 2022.","22","data centers","[""amazon2023""]","is_blank","Amazon's energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.‡","According to the context, 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023, which was an increase from 19 regions in 2022."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","Based on the context, AI was officially established at the 1956 Dartmouth workshop, making the field approximately 69 years old by 2025. Multiple context snippets confirm 1956 as the birth year of the field, including statements like 'The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop' and references to the field being 'sixty years ago' from a report written in the 2010s.","69","years","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","Based on the context, AI was officially established at the 1956 Dartmouth workshop, making the field approximately 69 years old by 2025. Multiple context snippets confirm 1956 as the birth year of the field, including statements like 'The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop' and references to the field being 'sixty years ago' from a report written in the 2010s."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","According to the context, the 'five cars' estimate from Strubell et al.'s 2019 study was based on Neural Architecture Search (NAS), which is described as a large-scale procedure for identifying the best model architecture that is performed much less frequently than average AI model training.","Neural Architecture Search","is_blank","[""luccioni2025c""]","is_blank","The authors quantified the costs of model development through both a case study of the energy required for them to develop a model published in the previous year, and by estimating the energy required to automate that process using an approach called neural architecture search (NAS) based on figures reported in a recent Google study using NAS to identify an optimized variant of the Transformer architecture. 31 In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.","According to the context, the 'five cars' estimate from Strubell et al.'s 2019 study was based on Neural Architecture Search (NAS), which is described as a large-scale procedure for identifying the best model architecture that is performed much less frequently than average AI model training."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","According to the context, when targeting an average TPOT of 100ms for Llama 3.1 8B, this lands on the Pareto frontier at 77ms TPOT and reduces energy consumption by 44% compared to the configuration that minimizes latency.","44","percent","[""chung2025""]","is_blank","In this context, a chatbot provider can target an average TPOT of 100 ms (equivalent to 10 tokens per second or about 7.5 words per second [55]), which is sufficient for most reading or listening speeds. This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","According to the context, when targeting an average TPOT of 100ms for Llama 3.1 8B, this lands on the Pareto frontier at 77ms TPOT and reduces energy consumption by 44% compared to the configuration that minimizes latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","According to Table III in the context, which shows maximum batch sizes supported by LLM fine-tuning, BlackMamba with sparse setup (BlackMamba-S) on the MATH dataset supports a batch size of 8. Since the paper refers to GSM8K as 'GS' or 'MATH' dataset interchangeably, this value represents the maximum batch size for GSM8K.","8","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE
AND S:SPARSE .
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20
MATH 1 3 2 8","According to Table III in the context, which shows maximum batch sizes supported by LLM fine-tuning, BlackMamba with sparse setup (BlackMamba-S) on the MATH dataset supports a batch size of 8. Since the paper refers to GSM8K as 'GS' or 'MATH' dataset interchangeably, this value represents the maximum batch size for GSM8K."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","According to the context, Amazon held 1.3 GW of storage capacity in 2023, up from 445 MW in 2022. This information is directly stated in the context.","1.3","GW","[""amazon2023""]","is_blank","We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.","According to the context, Amazon held 1.3 GW of storage capacity in 2023, up from 445 MW in 2022. This information is directly stated in the context."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","According to the context, GPT-4o's projected annual emissions of 138,125-163,441 tons of CO2e are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","According to the context, GPT-4o's projected annual emissions of 138,125-163,441 tons of CO2e are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","According to the context, the average global data center PUE in 2023 was 1.58. This value is directly stated in the text, though it refers to data centers in general rather than specifically AI-dedicated data centers.","1.58","PUE","[""ebert2024""]","is_blank","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","According to the context, the average global data center PUE in 2023 was 1.58. This value is directly stated in the text, though it refers to data centers in general rather than specifically AI-dedicated data centers."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context clearly states multiple times that JetMoE-8B has 8B total parameters but only activates 2B parameters for each input token. This is one of the key features of its sparse activation architecture.","2B","parameters","[""shen2024""]","is_blank","Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context clearly states multiple times that JetMoE-8B has 8B total parameters but only activates 2B parameters for each input token. This is one of the key features of its sparse activation architecture."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","According to Table 1 in the Appendix of Luccioni 2025c, the Llama 7B model produced 14 tCO2e in greenhouse gas emissions during pre-training.","14","tCO2e","[""luccioni2025c""]","is_blank","Model Organization Energy Consumption (MWh) GHG Emissions (tCO2e)
Llama 7B 63 Meta 356 14","According to Table 1 in the Appendix of Luccioni 2025c, the Llama 7B model produced 14 tCO2e in greenhouse gas emissions during pre-training."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","According to the context, in 2023 Amazon had 19,000 electric delivery vans globally, up from 9,000+ in 2022, representing a total addition of around 10,000 electric delivery vans across those two years.","10000","electric delivery vans","[""amazon2023""]","is_blank","In 2023, we delivered more than 680 million packages globally using more than 24,000 electric delivery vehicles, including 19,000 electric delivery vans, around the world.","According to the context, in 2023 Amazon had 19,000 electric delivery vans globally, up from 9,000+ in 2022, representing a total addition of around 10,000 electric delivery vans across those two years."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","According to morrison2025, operational environmental impacts of LLMs explicitly include 'GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.' Therefore, the statement is false.","0","is_blank","[""morrison2025""]","is_blank","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","According to morrison2025, operational environmental impacts of LLMs explicitly include 'GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.' Therefore, the statement is false."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","According to Table 4 in document jegham2025, the o3 model's energy consumption for long prompts (10k input-1.5k output) is 12.222 ± 1.082 Wh.","12.222","Wh","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). [...] o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082","According to Table 4 in document jegham2025, the o3 model's energy consumption for long prompts (10k input-1.5k output) is 12.222 ± 1.082 Wh."
"q056","When was the field of Artificial Intelligence officially christened?","According to multiple context snippets, the field of Artificial Intelligence was officially born and christened at a workshop organized by John McCarthy at Dartmouth in 1956.","1956","year","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","According to multiple context snippets, the field of Artificial Intelligence was officially born and christened at a workshop organized by John McCarthy at Dartmouth in 1956."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context does not specify Google's water use effectiveness specifically for AI-dedicated data centers in 2024. While various WUE values are mentioned, none are specifically tied to Google's AI data centers for that year.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The context does not specify Google's water use effectiveness specifically for AI-dedicated data centers in 2024. While various WUE values are mentioned, none are specifically tied to Google's AI data centers for that year."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","According to the context from document wu2021b, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity, as cited from the International Energy Agency.","1","is_blank","[""wu2021b""]","is_blank","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency]","According to the context from document wu2021b, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity, as cited from the International Energy Agency."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","According to the context, for LLaMA-65B at length 512, it takes about 3-4 Joules per output token, as directly stated in the text when discussing energy per decoded token.","[3, 4]","joules per token","[""samsi2024""]","is_blank","For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","According to the context, for LLaMA-65B at length 512, it takes about 3-4 Joules per output token, as directly stated in the text when discussing energy per decoded token."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context explicitly states that 'By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%'.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context explicitly states that 'By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%'."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","According to the context, the 5-10% reduction estimate from BCG reports lacks clear underlying calculations beyond vague references to client experience, and applying such observations from individual projects to global emissions 'lacks any scientific grounding.' The context explicitly states the reasoning is unclear and calculations are not detailed.","0","is_blank","[""luccioni2025c""]","is_blank","The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG's experience in dealing with their clients and using AI to optimize and improve existing processes... Applying observations made from individual projects to the entire planet's GHG emissions lacks any scientific grounding","According to the context, the 5-10% reduction estimate from BCG reports lacks clear underlying calculations beyond vague references to client experience, and applying such observations from individual projects to global emissions 'lacks any scientific grounding.' The context explicitly states the reasoning is unclear and calculations are not detailed."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","According to the context, specifically in Patterson 2021's abstract and key highlights, large but sparsely activated DNNs can consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy. For True/False questions, 1 represents True and 0 represents False.","1","is_blank","[""patterson2021""]","is_blank","Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","According to the context, specifically in Patterson 2021's abstract and key highlights, large but sparsely activated DNNs can consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy. For True/False questions, 1 represents True and 0 represents False."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context directly states that 'Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000'.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context directly states that 'Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000'."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","According to the context, when conducting sparse fine-tuning with batch size = 1, the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1)","According to the context, when conducting sparse fine-tuning with batch size = 1, the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","Using 1 billion queries per day and an energy consumption of 0.083 kWh per 1000 queries, the calculation is: (1 billion queries * 0.083 kWh/1000 queries) = 83,000 kWh = 83 MWh per day.","83","MWh","[""luccioni2024""]","is_blank","Google reported translating more than 100 billion words per day in 2016, assuming an average query length of 100 words yields an estimate of 1 billion queries to the model per day.","Using 1 billion queries per day and an energy consumption of 0.083 kWh per 1000 queries, the calculation is: (1 billion queries * 0.083 kWh/1000 queries) = 83,000 kWh = 83 MWh per day."
"q067","What was the average global data center PUE in 2023?","According to the context from ebert2024, the average data center PUE globally in 2023 was 1.58. This value represents the ratio of total energy needed by a data center to the energy used solely by computational devices.","1.58","PUE","[""ebert2024""]","is_blank","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","According to the context from ebert2024, the average data center PUE globally in 2023 was 1.58. This value represents the ratio of total energy needed by a data center to the energy used solely by computational devices."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","While the context discusses Microsoft's renewable energy initiatives and partnerships, it does not specify the exact number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","While the context discusses Microsoft's renewable energy initiatives and partnerships, it does not specify the exact number of wind turbines directly contracted by Microsoft to power Azure AI clusters in 2023."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","According to the context, Gemini Ultra had the highest R&D staff cost fraction at 49% of total amortized model development costs when equity is included. This is explicitly stated multiple times in the document.","49","percent","[""cottier2024""]","is_blank","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","According to the context, Gemini Ultra had the highest R&D staff cost fraction at 49% of total amortized model development costs when equity is included. This is explicitly stated multiple times in the document."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context explicitly states that 'The seventeen-member Study Panel' was launched in mid-fall 2015, and this is confirmed by the complete list of Study Panel members shown in the document.","17","people","[""stone2022""]","is_blank","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The context explicitly states that 'The seventeen-member Study Panel' was launched in mid-fall 2015, and this is confirmed by the complete list of Study Panel members shown in the document."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","According to the context, manufacturing carbon cost accounts for 74% of the total carbon footprint of client devices.","74","percent","[""wu2021a""]","is_blank","manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices","According to the context, manufacturing carbon cost accounts for 74% of the total carbon footprint of client devices."
"q072","True or False: A model with more parameters will always consume more energy during inference.","According to the Zschache2025 study, while larger LLMs generally consume more energy in their empirical analysis of text classification, they also found significant variability in inference energy consumption influenced by multiple factors including model type, model size, and hardware specifications. This indicates that model size alone does not determine energy consumption.","0","is_blank","[""zschache2025""]","[""arXiv:2508.14170v1""]","We observe substantial variability in inference energy consumption (<mWh to >kWh), influenced by model type, model size, and hardware specifications.","According to the Zschache2025 study, while larger LLMs generally consume more energy in their empirical analysis of text classification, they also found significant variability in inference energy consumption influenced by multiple factors including model type, model size, and hardware specifications. This indicates that model size alone does not determine energy consumption."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context explicitly states 'Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.' Therefore, this statement is false (0).","0","is_blank","[""stone2022""]","is_blank","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context explicitly states 'Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.' Therefore, this statement is false (0)."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","is_blank","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","is_blank"
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","According to the context, Meta reported that their Llama 3 family of models emitted 11,390 tons CO2e, which is over 40x the 'five cars' estimate (which was 284 metric tons CO2e according to the Strubell et al. study).","11390","tCO2e","[""luccioni2025c""]","is_blank","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the 'five cars' estimate [...] the 'five cars' number has since been misinterpreted as a proxy for the carbon footprint of training AI models at large, which is misleading given the diversity of architectures, training approaches and electricity sources used for powering AI model training","According to the context, Meta reported that their Llama 3 family of models emitted 11,390 tons CO2e, which is over 40x the 'five cars' estimate (which was 284 metric tons CO2e according to the Strubell et al. study)."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","According to Figure 2(d) and multiple text mentions in the context, Facebook's AI training infrastructure capacity increased by 2.9× over a 1.5 year period. This represents the scale-out of training infrastructure driven by explosive growth in AI use cases.","2.9","multiplier","[""wu2021a""]","is_blank","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","According to Figure 2(d) and multiple text mentions in the context, Facebook's AI training infrastructure capacity increased by 2.9× over a 1.5 year period. This represents the scale-out of training infrastructure driven by explosive growth in AI use cases."
"q079","How many miles is the Earth from the Sun?","is_blank","is_blank","miles","[""is_blank""]","is_blank","is_blank","is_blank"
"q080","True or False: The AlphaGo program defeated the human Go champion.","According to the context, AlphaGo, developed by Google DeepMind, defeated the human Go champion in a five-game match. This is mentioned multiple times in the text, including when discussing the program's use of reinforcement learning and machine learning algorithms.","1","is_blank","[""stone2022""]","is_blank","The recent success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to reinforcement learning.","According to the context, AlphaGo, developed by Google DeepMind, defeated the human Go champion in a five-game match. This is mentioned multiple times in the text, including when discussing the program's use of reinforcement learning and machine learning algorithms."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","Multiple documents refer to 'continuous batching' as the strategy that dynamically replaces completed requests with new ones to improve GPU utilization and reduce idle time when generation lengths vary.","continuous batching","is_blank","[""fernandez2025""]","is_blank","Continuous batching miti-gates this by dynamically replacing completed re-quests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","Multiple documents refer to 'continuous batching' as the strategy that dynamically replaces completed requests with new ones to improve GPU utilization and reduce idle time when generation lengths vary."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context explicitly states that 'The entire alignment process takes 60 H100 GPU hours'.","60","H100 GPU hours","[""shen2024""]","is_blank","The entire alignment process takes 60 H100 GPU hours.","The context explicitly states that 'The entire alignment process takes 60 H100 GPU hours'."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","According to the context, with a 100 TPS SLO requirement, InferSave selected g4dn.xlarge with a cost of $2.13, while Max-Performance selected g6e.xlarge with a cost of $2.699. This represents a 26.7% increase in cost, as explicitly stated in the text.","26.7","percent","[""kim2025""]","is_blank","Given a SLO requirement of 100 TPS, InferSave selected g4dn.xlarge as its top choice, providing a throughput of about 160 TPS with the lowest total processing cost of $2.13. On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.","According to the context, with a 100 TPS SLO requirement, InferSave selected g4dn.xlarge with a cost of $2.13, while Max-Performance selected g6e.xlarge with a cost of $2.699. This represents a 26.7% increase in cost, as explicitly stated in the text."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","According to the 2024 study described in the context, the stable-diffusion-xl-base-1.0 model generates 1,594 grams of CO2eq per 1,000 inferences, making it the most carbon-intensive image generation model examined.","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of 𝐶𝑂2𝑒𝑞 for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle","According to the 2024 study described in the context, the stable-diffusion-xl-base-1.0 model generates 1,594 grams of CO2eq per 1,000 inferences, making it the most carbon-intensive image generation model examined."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","According to Table 2 in the Luccioni 2025 study, GPU energy usage for 1,000 queries ranges from 0.06 Wh (bert-tiny-finetuned-squadv2) to 3,426.38 Wh (Command-R Plus), showing a dramatic variation across different model architectures and tasks.","[0.06, 3426.38]","Wh","[""luccioni2025c""]","is_blank","task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","According to Table 2 in the Luccioni 2025 study, GPU energy usage for 1,000 queries ranges from 0.06 Wh (bert-tiny-finetuned-squadv2) to 3,426.38 Wh (Command-R Plus), showing a dramatic variation across different model architectures and tasks."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context explicitly states 'There is no one-size-ﬁts-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.' This directly contradicts the idea that researchers believe a universal approach can be developed.","0","is_blank","[""luccioni2025b""]","is_blank","There is no one-size-ﬁts-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.","The context explicitly states 'There is no one-size-ﬁts-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.' This directly contradicts the idea that researchers believe a universal approach can be developed."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","According to Patterson 2021, the gross carbon intensity of energy for the U.S. average mix was 0.429 kg of CO2e/KWh, as cited from [USE21].","0.429","kg of CO2e/KWh","[""patterson2021""]","[""https://www.eia.gov/tools/faqs/faq.php?id=74&t=11""]","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","According to Patterson 2021, the gross carbon intensity of energy for the U.S. average mix was 0.429 kg of CO2e/KWh, as cited from [USE21]."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context clearly states that Hivemind was used as the PyTorch-based framework to enable collaborative distributed deep learning training across clouds and continents. It is described as running in a decentralized fashion and capable of handling peers dropping out at any stage of training.","Hivemind","is_blank","[""erben2023""]","is_blank","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion. Its main difference to other state-of-the-art distributed training frameworks, such as PyTorch DDP [26] and DeepSpeed [35], is that it runs in a decentralized fashion and can handle peers that drop out at any stage of the training.","The context clearly states that Hivemind was used as the PyTorch-based framework to enable collaborative distributed deep learning training across clouds and continents. It is described as running in a decentralized fashion and capable of handling peers dropping out at any stage of training."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","According to the context, Ehsan et al. proposed expanding transparency in AI to encompass 'social transparency', which involves integrating socio-technical aspects in describing and understanding AI systems, including their societal impacts, ethical considerations, and environmental footprint.","social transparency","is_blank","[""luccioni2025b""]","is_blank","as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems [56]. Social transparency involves a portrayal of an AI system's societal impacts, ethical considerations, and eventually its environmental footprint.","According to the context, Ehsan et al. proposed expanding transparency in AI to encompass 'social transparency', which involves integrating socio-technical aspects in describing and understanding AI systems, including their societal impacts, ethical considerations, and environmental footprint."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context states that 'The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings' in experiments on the FKTG dataset from German public administration that involved categorizing public submissions about radioactive waste repository site selection.","traditional linear model using pre-trained sentence embeddings","is_blank","[""zschache2025""]","[""arxiv:2508.14170v1""]","The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.","The context states that 'The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings' in experiments on the FKTG dataset from German public administration that involved categorizing public submissions about radioactive waste repository site selection."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context clearly states that they developed and deployed 'Lamina', an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","Lamina","is_blank","[""chen2024""]","is_blank","To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The context clearly states that they developed and deployed 'Lamina', an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster."
"q093","How many parameters does the largest T5 model have?","According to Patterson 2021's Table 4, T5 (NAS T5) has 11 billion parameters. This is supported by the 'Number of Parameters (B)' row which shows 11B for the T5 model.","11000000000","parameters","[""patterson2021""]","is_blank","Table 4: Number of Parameters (B): NAS T5: 11","According to Patterson 2021's Table 4, T5 (NAS T5) has 11 billion parameters. This is supported by the 'Number of Parameters (B)' row which shows 11B for the T5 model."
"q094","What is the total number of parameters in the JetMoE-8B model?","According to the context, JetMoE-8B has 8B total parameters while only activating 2B for each input token. This is shown in both the model's name and explicitly stated in the hyperparameters table and description.","8000000000","parameters","[""shen2024""]","is_blank","Table 1: JetMoE-8B hyperparameters.
Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp
8B 2B 24 2048 8 2 16 128 5632","According to the context, JetMoE-8B has 8B total parameters while only activating 2B for each input token. This is shown in both the model's name and explicitly stated in the hyperparameters table and description."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","According to multiple sources in the context, Google's data center water consumption increased by 20% from 2021 to 2022. This is mentioned consistently across documents li2025b and luccioni2025a.","20","percent","[""li2025b"", ""luccioni2025a""]","is_blank","the company's data center water consumption increased by∼20% from 2021 to 2022 and by ∼17% from 2022 to 2023 [4]"" AND ""Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period","According to multiple sources in the context, Google's data center water consumption increased by 20% from 2021 to 2022. This is mentioned consistently across documents li2025b and luccioni2025a."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","According to Table I in khan2025, 'Carbon Intensity' is explicitly defined as 'CO2 emissions per unit of electricity consumed' with units of gCO2/kWh.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","TABLE I
COMMON CARBON EMISSION METRICS IN GREEN AI
Metric Unit Definition Reference
Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed","According to Table I in khan2025, 'Carbon Intensity' is explicitly defined as 'CO2 emissions per unit of electricity consumed' with units of gCO2/kWh."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","According to Table 2 in the context, for the 101B parameter model (final growth stage), the FLOPs utilization was 52.88%.","52.88","percent","[""li2025a""]","is_blank","Table 2: [...] Params Tensor Pipeline Data Number Batch teraFLOP/s FLOPs (billion) Parallel Size Parallel Size Parallel Size of GPUs Size per GPU Utilization [...] 101 4 4 12 192 2160 165 52.88%","According to Table 2 in the context, for the 101B parameter model (final growth stage), the FLOPs utilization was 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","According to Cottier 2024, OpenAI's GPT-4 had amortized training costs of $40M, which was noted as the most expensive publicly-announced training run to date.","40000000","USD","[""cottier2024""]","[""arXiv:2405.21015v2""]","We find that the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at $40M and Google's Gemini Ultra at $30M.","According to Cottier 2024, OpenAI's GPT-4 had amortized training costs of $40M, which was noted as the most expensive publicly-announced training run to date."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","According to Figure 7 and the analysis in the context, the Transformer-based language translation model (LM) achieved a total 810x reduction in operational power footprint through combined optimizations including platform-level caching (6.7x), GPU acceleration (10.1x), and algorithmic optimizations (12x) compared to the CPU baseline.","810","multiplier","[""wu2021a""]","is_blank","For the cross-lingual ML task (LM), the operational energy footprint can be signiﬁcantly reduced by more than 800× using platform-level caching, GPUs, low precision data format, and additional algorithmic optimization","According to Figure 7 and the analysis in the context, the Transformer-based language translation model (LM) achieved a total 810x reduction in operational power footprint through combined optimizations including platform-level caching (6.7x), GPU acceleration (10.1x), and algorithmic optimizations (12x) compared to the CPU baseline."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","For the 4-continent experiment (C-4), NLP throughput was 36% slower compared to local runs (A-4), meaning it achieved 64% of the local throughput. This is stated directly in the context when discussing the intercontinental setup with 4 regions.","0.64","multiplier","[""erben2023""]","is_blank","As we know from the transatlantic experiments(B) that an additional continent has a detrimental effect on throughput, which, for the four continents experiment, C-4, results in a 9% slower throughput for CV and 36% slower for NLP compared to the A-4 runs (Figure 7a).","For the 4-continent experiment (C-4), NLP throughput was 36% slower compared to local runs (A-4), meaning it achieved 64% of the local throughput. This is stated directly in the context when discussing the intercontinental setup with 4 regions."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","According to the context, AWS's water replenishment portfolio returned 3.5 billion (3.5B) liters of water to local communities in 2023 through its replenishment projects.","3500000000","liters","[""amazon2023""]","is_blank","In 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities.","According to the context, AWS's water replenishment portfolio returned 3.5 billion (3.5B) liters of water to local communities in 2023 through its replenishment projects."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","According to the research findings, custom tags reduced energy consumption across all three prompt engineering techniques (zero-shot, one-shot, and few-shots). Specifically, zero-shot showed 7% reduction, one-shot showed 99% reduction, and few-shots showed 83% reduction in energy consumption when using custom tags.","1","is_blank","[""rubei2025""]","[""https://github.com/riccardoRubei/Greens-2025-Replication-Package""]","Our findings reveal that the energy consumption of LLMs for the inference phase can be reduced by using the introduced custom tags. [...] In particular, with the best configuration, zero-shot reduced the consumption of about 7%, whereas one-shot and few-shots decreased their consumption of about 99% and 83%, respectively.","According to the research findings, custom tags reduced energy consumption across all three prompt engineering techniques (zero-shot, one-shot, and few-shots). Specifically, zero-shot showed 7% reduction, one-shot showed 99% reduction, and few-shots showed 83% reduction in energy consumption when using custom tags."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","According to the context, NVIDIA shipped 3.7 million GPUs in 2024, which was noted as being more than a million more units than in 2023 (when they shipped 3.76 million units).","3700000","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","While efficiency improvements are being made to the hardware used for training and deploying AI models [ 9, 82, 89], NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].","According to the context, NVIDIA shipped 3.7 million GPUs in 2024, which was noted as being more than a million more units than in 2023 (when they shipped 3.76 million units)."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","According to the context, when breaking down the components of amortized hardware CapEx + energy costs, 44% goes toward AI accelerator chips on average. This is directly stated in multiple instances in the document cottier2024.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","According to the context, when breaking down the components of amortized hardware CapEx + energy costs, 44% goes toward AI accelerator chips on average. This is directly stated in multiple instances in the document cottier2024."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","Multiple sources consistently state that Facebook's data centers have a PUE of 1.10. This indicates that Facebook's data centers use only 10% additional energy beyond what directly powers the computing equipment.","1.10","PUE","[""wu2021a"", ""wu2021b""]","is_blank","Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook's data centers are about 40% more efficient than small-scale, typical data centers.","Multiple sources consistently state that Facebook's data centers have a PUE of 1.10. This indicates that Facebook's data centers use only 10% additional energy beyond what directly powers the computing equipment."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context mentions that the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed integrating ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainabilit y, design and foresight for inter-disciplinary governance of AI systems [133]","The context mentions that the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed integrating ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","According to the context, Gemini Ultra was estimated to cost $30M in training costs. This comes from direct reporting of publicly-announced training costs.","30000000","USD","[""cottier2024""]","[""arXiv:2405.21015v2""]","We find that the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at $40M and Google's Gemini Ultra at $30M.","According to the context, Gemini Ultra was estimated to cost $30M in training costs. This comes from direct reporting of publicly-announced training costs."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context shows that providers of GPAI models with systemic risk must conduct risk assessments under Art. 55(1)(b), and these assessments must include environmental risks because environmental protection is included as a fundamental right that must be considered under the AI Act according to Article 1 and Recitals 1, 2 and 8.","1","is_blank","[""ebert2024""]","is_blank","For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8. Crucially, both provisions relate to risks of the AI model or system for fundamental rights which, within the AI Act, must be interpreted as including environmental risks [5].","The context shows that providers of GPAI models with systemic risk must conduct risk assessments under Art. 55(1)(b), and these assessments must include environmental risks because environmental protection is included as a fundamental right that must be considered under the AI Act according to Article 1 and Recitals 1, 2 and 8."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","According to multiple mentions in the context, the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9 µg/m³. This is explicitly stated and consistently referenced throughout the document.","9","µg/m³","[""han2024""]","is_blank","In fact, the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m 3, considerably higher than the WHO's recommended level of 5µg/m3 [48, 52].","According to multiple mentions in the context, the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9 µg/m³. This is explicitly stated and consistently referenced throughout the document."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","According to the context, a life cycle assessment (LCA) comparing print books to e-readers found that 115 books would produce the same amount of CO2 as a single Amazon Kindle device.","115","books","[""luccioni2025a""]","[""https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/""]","For instance, a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","According to the context, a life cycle assessment (LCA) comparing print books to e-readers found that 115 books would produce the same amount of CO2 as a single Amazon Kindle device."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context states that some low-income counties experience significantly greater health costs, with per-household burdens exceeding those in other counties by more than 200-fold (i.e., a multiplier of 200).","200","multiplier","[""han2024""]","is_blank","In particular, some low-income counties experience significantly greater health costs, with per-household burdens exceeding those in other counties by more than 200-fold.","The context states that some low-income counties experience significantly greater health costs, with per-household burdens exceeding those in other counties by more than 200-fold (i.e., a multiplier of 200)."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","According to Table B1 in the context, the DS Llama 70B model consumed 702.06 Wh of energy during inference on the FKTG dataset using 2 GPUs, measured as an average over 10 runs.","702.06","Wh","[""zschache2025""]","is_blank","Table B1 from context shows: DS Llama 70B 2 702.06 0.46 2543.47 993.68","According to Table B1 in the context, the DS Llama 70B model consumed 702.06 Wh of energy during inference on the FKTG dataset using 2 GPUs, measured as an average over 10 runs."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","According to the context, Dodge et al. 2022 analyzed a '6 Billion Parameter Transformer' model that was trained for approximately 13% of the total training time required. This is stated clearly in Table 2 and discussed in the text.","6000000000","parameters","[""dodge2022""]","is_blank","6 Billion Parameter Transformer. We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s.","According to the context, Dodge et al. 2022 analyzed a '6 Billion Parameter Transformer' model that was trained for approximately 13% of the total training time required. This is stated clearly in Table 2 and discussed in the text."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context clearly describes Jevons' Paradox as a phenomenon where improved efficiency leads to increased consumption and resource usage rather than decreased usage. This is mentioned multiple times across the documents, particularly in reference to AI systems and their environmental impacts.","Jevons' Paradox","is_blank","[""luccioni2025a"", ""jegham2025""]","is_blank","As per-task efficiency improves, total AI usage expands far more rapidly, amplifying net resource consumption, a phenomenon aligned with the Jevons Paradox [76], where increased efficiency drives systemic demand.","The context clearly describes Jevons' Paradox as a phenomenon where improved efficiency leads to increased consumption and resource usage rather than decreased usage. This is mentioned multiple times across the documents, particularly in reference to AI systems and their environmental impacts."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","From context, GPT-3's training used 1,287 MWh of energy, while Meena used 232 MWh. Dividing 1,287 by 232 shows that 5.55 Meena training runs would equal one GPT-3 training run in terms of energy consumption.","5.55","multiplier","[""patterson2021"", ""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh [29]... Meena...used 232 MWh","From context, GPT-3's training used 1,287 MWh of energy, while Meena used 232 MWh. Dividing 1,287 by 232 shows that 5.55 Meena training runs would equal one GPT-3 training run in terms of energy consumption."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","According to Table 2 in Luccioni et al.'s 2024 study, the mean energy consumption for image generation is 2.907 kWh per 1,000 inferences, with a standard deviation of 3.31 kWh.","2.907","kWh","[""luccioni2024""]","is_blank","inference energy (kWh)
task mean std
[...]
image generation 2.907 3.31
Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis.","According to Table 2 in Luccioni et al.'s 2024 study, the mean energy consumption for image generation is 2.907 kWh per 1,000 inferences, with a standard deviation of 3.31 kWh."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","According to the context from dodge2022, the average US home emits 8.3 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil. Converting 8.3 metric tons to pounds (1 metric ton = 2204.62 lbs) gives us approximately 18,298 pounds CO2e per year.","18298","lbs","[""dodge2022""]","is_blank","Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).","According to the context from dodge2022, the average US home emits 8.3 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil. Converting 8.3 metric tons to pounds (1 metric ton = 2204.62 lbs) gives us approximately 18,298 pounds CO2e per year."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","While the context discusses per-household health costs in West Virginia counties (with Marion County having the highest at $1,218.3 per household), these figures are from 2019-2023 data, not 2030 projections. The context does not contain any 2030 projections for county-level health costs.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","While the context discusses per-household health costs in West Virginia counties (with Marion County having the highest at $1,218.3 per household), these figures are from 2019-2023 data, not 2030 projections. The context does not contain any 2030 projections for county-level health costs."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","From Table III in khan2025, Mistral-small's CO2 emissions changed from 0.020 kg to 0.015 kg per inference task after optimization. Calculating the multiplier: 0.015/0.020 = 0.75, meaning emissions were reduced to 75% of original or a 0.75x multiplier.","0.75","multiplier","[""khan2025""]","is_blank","TABLE III
COMPARISON OF PERFORMANCE METRICS AND CARBON EMISSIONS FOR
FIVE LLM S BEFORE AND AFTER OPTIMIZATION . CARBON EMISSIONS ARE
CALCULATED PER INFERENCE TASK .
[...]
Mistral-small 0.70 0.67 0.65 0.67 0.020
[...]
Mistral-small 0.73 0.70 0.69 0.70 0.015","From Table III in khan2025, Mistral-small's CO2 emissions changed from 0.020 kg to 0.015 kg per inference task after optimization. Calculating the multiplier: 0.015/0.020 = 0.75, meaning emissions were reduced to 75% of original or a 0.75x multiplier."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","According to Table 5 in the study, BLOOMz-7B had training energy costs of 51,686 kWh and fine-tuning energy costs of 7,571 kWh. Adding these together gives a total of 59,257 kWh for combined training and fine-tuning energy costs.","59257","kWh","[""luccioni2024""]","is_blank","Table 5. BLOOMz-7B
Training energy (kWh) 51,686
Finetuning energy (kWh) 7,571","According to Table 5 in the study, BLOOMz-7B had training energy costs of 51,686 kWh and fine-tuning energy costs of 7,571 kWh. Adding these together gives a total of 59,257 kWh for combined training and fine-tuning energy costs."
"q125","What is the total number of parameters in the final FLM-101B model?","From the FLM-101B configurations section in document li2025a, the model is explicitly described with specific architecture parameters, including 10,240 hidden dimension, 80 layers, 80 attention heads, and vocabulary size of 100,256. The total parameter count is 101 billion, as can be confirmed from multiple references to 'FLM-101B' throughout the document.","101000000000","parameters","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","From the FLM-101B configurations section in document li2025a, the model is explicitly described with specific architecture parameters, including 10,240 hidden dimension, 80 layers, 80 attention heads, and vocabulary size of 100,256. The total parameter count is 101 billion, as can be confirmed from multiple references to 'FLM-101B' throughout the document."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","According to the context, a 6.1B parameter model used 103,593 kWh for a full training run. Using BLOOMz-7B's inference energy of 1.0 × 10^-4 kWh per inference, it would take approximately 1,035,930,000 inferences to match this energy cost (103,593/0.0001).","1035930000","inferences","[""dodge2022"", ""luccioni2024""]","is_blank","we estimate a full training run would consume approximately 103,593 kWh [dodge2022]; inference energy (kWh) 1.0 × 10^-4 for BLOOMz-7B [luccioni2024 Table 5]","According to the context, a 6.1B parameter model used 103,593 kWh for a full training run. Using BLOOMz-7B's inference energy of 1.0 × 10^-4 kWh per inference, it would take approximately 1,035,930,000 inferences to match this energy cost (103,593/0.0001)."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","According to the study, they ran each of the 88 models on 3 datasets 10 times, using a total of 754.66 kWh of energy for all model experimentation and evaluation.","754.66","kWh","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","According to the study, they ran each of the 88 models on 3 datasets 10 times, using a total of 754.66 kWh of energy for all model experimentation and evaluation."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","According to Table 5 in the context, the BLOOMz-7B model requires 592,570,000 inferences for the deployment energy cost to equal the combined training (51,686 kWh) and fine-tuning (7,571 kWh) energy costs, based on its inference energy usage of 1.0 × 10^-4 kWh per inference.","592570000","inferences","[""luccioni2024""]","is_blank","Table 5. BLOOMz-7B Cost parity (# inferences): 592,570,000","According to Table 5 in the context, the BLOOMz-7B model requires 592,570,000 inferences for the deployment energy cost to equal the combined training (51,686 kWh) and fine-tuning (7,571 kWh) energy costs, based on its inference energy usage of 1.0 × 10^-4 kWh per inference."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The dataset used is called FKTG-dataset (Fachkonferenz Teilgebiete), which was published at beteiligung.bge.de/index.php and contained the categorized statements from the population regarding the nuclear waste repository site selection process in Germany.","FKTG-dataset","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","During the first phase, sub-areas were identified and discussed in a process called FKTG (Fachkonferenz Teilgebiete). The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).","The dataset used is called FKTG-dataset (Fachkonferenz Teilgebiete), which was published at beteiligung.bge.de/index.php and contained the categorized statements from the population regarding the nuclear waste repository site selection process in Germany."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide specific information about Meta's Llama 3 inference serving clusters' freshwater consumption in 2024. While there are mentions of Llama 3 and water consumption by data centers in general, no specific water consumption data is given for Llama 3 inference clusters.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about Meta's Llama 3 inference serving clusters' freshwater consumption in 2024. While there are mentions of Llama 3 and water consumption by data centers in general, no specific water consumption data is given for Llama 3 inference clusters."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context does not provide any specific information about what percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals. While we know from morrison2025 that H100s contain rare earth metals (0.1% by mass), there is no data about recycled content.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide any specific information about what percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals. While we know from morrison2025 that H100s contain rare earth metals (0.1% by mass), there is no data about recycled content."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","According to the context, the CO2e equivalent of NAS (3.2 net tCO2e) is explicitly stated to be '~3 passengers taking a round trip between San Francisco and New York'.","3","passengers","[""patterson2021""]","is_blank","Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","According to the context, the CO2e equivalent of NAS (3.2 net tCO2e) is explicitly stated to be '~3 passengers taking a round trip between San Francisco and New York'."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","According to OpenRouter's May 2025 data shown in Figure 2, 84% of LLM token usage was through models with no environmental impact disclosure.","84","percent","[""luccioni2025c""]","is_blank","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","According to OpenRouter's May 2025 data shown in Figure 2, 84% of LLM token usage was through models with no environmental impact disclosure."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","According to Table II in the samsi2024 paper, LLaMA-13B requires 1 A100 80GB GPU as the bare minimum hardware configuration for inference with a maximum batch size of 64.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64
13B 2 64 1 64
65B 8 64 4 128","According to Table II in the samsi2024 paper, LLaMA-13B requires 1 A100 80GB GPU as the bare minimum hardware configuration for inference with a maximum batch size of 64."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","From the context in dodge2022, we can calculate that a complete training run of the 6.1B parameter model would consume 103,593 kWh. Using the standard conversion factor implied in the context snippets (not explicitly given), this energy consumption would translate to approximately 46.6 metric tons of CO2 emissions.","46.6","metric tons","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","From the context in dodge2022, we can calculate that a complete training run of the 6.1B parameter model would consume 103,593 kWh. Using the standard conversion factor implied in the context snippets (not explicitly given), this energy consumption would translate to approximately 46.6 metric tons of CO2 emissions."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context does not provide any specific data about total carbon emissions avoided through pruning and quantization of large language models in 2023. While some studies discuss energy efficiency improvements through various optimization techniques, none explicitly quantify the total emissions avoided through pruning and quantization in that year.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide any specific data about total carbon emissions avoided through pruning and quantization of large language models in 2023. While some studies discuss energy efficiency improvements through various optimization techniques, none explicitly quantify the total emissions avoided through pruning and quantization in that year."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context explicitly states that using 2 A100s and 1 A10G resulted in a 24% cost saving compared to using A100-only GPUs for serving Llama2-7b.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context explicitly states that using 2 A100s and 1 A10G resulted in a 24% cost saving compared to using A100-only GPUs for serving Llama2-7b."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","According to the specifications table in Chen et al., the H20's price per hour is listed as $4.63/hr, though it's noted with an asterisk that this price is estimated using relative complete system cost against H100 since H20 is not readily available on cloud service providers.","4.63","USD per hour","[""chen2024""]","is_blank","Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr
*: As H20 is not readily available on cloud service providers, the listed price is estimated using the relative complete system cost against H100.","According to the specifications table in Chen et al., the H20's price per hour is listed as $4.63/hr, though it's noted with an asterisk that this price is estimated using relative complete system cost against H100 since H20 is not readily available on cloud service providers."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","According to multiple sources, most carbon footprint analyses must gather information manually by contacting authors directly. For example, Luccioni and Hernandez-Garcia had to manually reach out to over 500 authors to get the necessary information, with only 95 responding.","0","is_blank","[""luccioni2025b""]","is_blank","For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","According to multiple sources, most carbon footprint analyses must gather information manually by contacting authors directly. For example, Luccioni and Hernandez-Garcia had to manually reach out to over 500 authors to get the necessary information, with only 95 responding."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","According to the context, the U.S. data centers' public health cost in 2023 was equivalent to approximately 44% of their total electricity cost, as directly stated in multiple passages.","44","percent","[""han2024""]","is_blank","This is equivalent to approximately 44% of the data centers' total electricity cost.","According to the context, the U.S. data centers' public health cost in 2023 was equivalent to approximately 44% of their total electricity cost, as directly stated in multiple passages."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","According to Table II in the context, LLaMA-7B requires only 1 A100 80GB GPU for inference with a maximum batch size of 64, representing the bare minimum hardware requirement without compression or quantization.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: [...] Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64","According to Table II in the context, LLaMA-7B requires only 1 A100 80GB GPU for inference with a maximum batch size of 64, representing the bare minimum hardware requirement without compression or quantization."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context from khan2025 explicitly states that 'Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization', which directly supports this claim being true.","1","is_blank","[""khan2025""]","is_blank","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.","The context from khan2025 explicitly states that 'Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization', which directly supports this claim being true."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","According to multiple context snippets, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers and were able to collect 95 answers to help estimate carbon footprints of ML models.","95","answers","[""luccioni2025b"", ""luccioni2023""]","is_blank","For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers","According to multiple context snippets, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers and were able to collect 95 answers to help estimate carbon footprints of ML models."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","JetMoE-8B was trained with a $100k budget using 30,000 H100 GPU hours, so dividing the total budget by total GPU hours gives us approximately $3.33 per H100 GPU-hour.","3.33","USD per hour","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","JetMoE-8B was trained with a $100k budget using 30,000 H100 GPU hours, so dividing the total budget by total GPU hours gives us approximately $3.33 per H100 GPU-hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","According to Table 2 in han2024, when training a Llama-3.1 scale model in Altoona, IA, the health cost was 2.51 million USD compared to electricity cost of 2.1 million USD, which is 122% of the electricity cost.","122","percent","[""han2024""]","is_blank","Table 2: [...] Altoona, IA 6.91 2.1 2.51(1.84, 3.17) 122% 1.52 (34000) 11.78 (10600) 14.76","According to Table 2 in han2024, when training a Llama-3.1 scale model in Altoona, IA, the health cost was 2.51 million USD compared to electricity cost of 2.1 million USD, which is 122% of the electricity cost."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","According to the context, JetMoE-8B was trained using 1.25T (1.25 trillion) tokens from mixed open-source datasets. This is explicitly stated multiple times in the document.","1250000000000","tokens","[""shen2024""]","is_blank","JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","According to the context, JetMoE-8B was trained using 1.25T (1.25 trillion) tokens from mixed open-source datasets. This is explicitly stated multiple times in the document."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","According to the table 'Amazon Renewable Energy Projects' in the context, which shows projects announced as of January 2024, the United Kingdom had 36 renewable energy projects with a total capacity of 901 MW.","36","projects","[""amazon2023""]","is_blank","Project Location
Number 
of Projects
Total MW 
Capacity†
United Kingdom 36 901","According to the table 'Amazon Renewable Energy Projects' in the context, which shows projects announced as of January 2024, the United Kingdom had 36 renewable energy projects with a total capacity of 901 MW."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","According to the gender data shown in the workforce representation table, 53.2% of Amazon's total U.S. workforce identified as men in 2023.","53.2","percent","[""amazon2023""]","is_blank","Gender—U.S. ... 2023 ... 53.2% Men","According to the gender data shown in the workforce representation table, 53.2% of Amazon's total U.S. workforce identified as men in 2023."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","According to the context, Apple reports that its supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","is_blank","For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].","According to the context, Apple reports that its supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","From Figure 4 in the paper, for BlackMamba with sparse fine-tuning and batch size=84, the execution time breakdown shows approximately 2.0 seconds total across the forward, backward, and optimizer stages.","2.0","seconds","[""xia2024""]","is_blank","Fig. 4. Execution time breakdown. [Shows Dense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84) with ~2.0 seconds total execution time for Mamba Sparse(bsz=84)]","From Figure 4 in the paper, for BlackMamba with sparse fine-tuning and batch size=84, the execution time breakdown shows approximately 2.0 seconds total across the forward, backward, and optimizer stages."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The paper introduced the 'granularity' metric to evaluate scalability, which is defined as the ratio between calculation and communication time. This metric is used to compare model suitability for distributed training and estimate performance with additional resources.","granularity","is_blank","[""erben2023""]","is_blank","For comparison of the models' scalability and to show which of them can be trained in a distributed fashion, we introduce thegranularity metric, the ratio of calculation to communication time, and show how it can be used for predicting performance with different hardware setups.","The paper introduced the 'granularity' metric to evaluate scalability, which is defined as the ratio between calculation and communication time. This metric is used to compare model suitability for distributed training and estimate performance with additional resources."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","According to the context, Microsoft employees estimated that a single deal with Exxon Mobil to expand oil and gas production in Texas and New Mexico could add up to 640 percent (6.4 times) more carbon emissions compared to the company's carbon removal targets for the year.","6.4","times","[""luccioni2025a""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/""]","For instance, a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year","According to the context, Microsoft employees estimated that a single deal with Exxon Mobil to expand oil and gas production in Texas and New Mexico could add up to 640 percent (6.4 times) more carbon emissions compared to the company's carbon removal targets for the year."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context explicitly defines this term as 'water withdrawal', stating that it 'refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses'.","Water withdrawal","is_blank","[""li2025b""]","is_blank","• Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","The context explicitly defines this term as 'water withdrawal', stating that it 'refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses'."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context clearly states that 'As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.'","5","years","[""stone2022""]","is_blank","As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","The context clearly states that 'As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.'"
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","According to multiple mentions in document wu2021b, every US household had an average of 25 connected devices in 2021, including devices like cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines.","25","devices","[""wu2021b""]","is_blank","At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, ﬁtness trackers, and connected ﬁtness machines [Deloitte, 2021].","According to multiple mentions in document wu2021b, every US household had an average of 25 connected devices in 2021, including devices like cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","According to dodge2022, training a 6.1 billion parameter transformer model for 8 days on 256 NVIDIA A100s consumed 13.8 MWh, but this was only 13% of the full training time - they estimate a full training run would consume approximately 103.5 MWh. Therefore the range is [13.8,103.5] MWh.","[13.8,103.5]","MWh","[""dodge2022""]","is_blank","6 Billion Parameter Transformer. We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh","According to dodge2022, training a 6.1 billion parameter transformer model for 8 days on 256 NVIDIA A100s consumed 13.8 MWh, but this was only 13% of the full training time - they estimate a full training run would consume approximately 103.5 MWh. Therefore the range is [13.8,103.5] MWh."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","According to multiple references in the context, IBM's Watson program did beat human contenders in the Jeopardy challenge in 2011. This is clearly stated in multiple sections that discuss how Watson was based on efficient knowledge representation and retrieval systems that enabled it to win the competition.","0","is_blank","[""stone2022""]","is_blank","IBM's Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","According to multiple references in the context, IBM's Watson program did beat human contenders in the Jeopardy challenge in 2011. This is clearly stated in multiple sections that discuss how Watson was based on efficient knowledge representation and retrieval systems that enabled it to win the competition."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","According to the context, one paper found that 10-50 queries on GPT-3 consumes around half a liter of water.","[10, 50]","queries","[""luccioni2025a""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","According to the context, one paper found that 10-50 queries on GPT-3 consumes around half a liter of water."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","According to Table 4 in the context, JetMoE-8B-chat achieved an MT-Bench score of 6.681, which was higher than Llama-2-13b-chat's score of 6.650. The text explicitly states that 'JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment.'","6.681","score","[""shen2024""]","is_blank","Table 4: MT-Bench score comparison of various models
GPT-4 9.014
GPT-3.5-turbo 7.995
Claude-v1 7.923
JetMoE-8B-chat 6.681
Llama-2-13b-chat 6.650","According to Table 4 in the context, JetMoE-8B-chat achieved an MT-Bench score of 6.681, which was higher than Llama-2-13b-chat's score of 6.650. The text explicitly states that 'JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment.'"
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","According to Table 1 in li2025b, GPT-3's average water consumption in U.S. data centers is 16.904 mL per request. Therefore, 500 mL would allow for approximately 29.6 requests.","29.6","responses","[""li2025b""]","is_blank","Table 1: [...] U.S. Average [...] Total Water [...] 16.904 [...] # of Requests for 500ml Water [...] 29.6","According to Table 1 in li2025b, GPT-3's average water consumption in U.S. data centers is 16.904 mL per request. Therefore, 500 mL would allow for approximately 29.6 requests."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The paper shows that for short-context tasks (interactive chats) using the Arena dataset, Mélange achieves up to 77% cost reduction with a 120ms SLO compared to single-GPU-type baselines.","77","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO)","The paper shows that for short-context tasks (interactive chats) using the Arena dataset, Mélange achieves up to 77% cost reduction with a 120ms SLO compared to single-GPU-type baselines."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","Multiple documents confirm that 4 A100 80GB GPUs are required as the bare minimum for running LLaMA-65B inference. This is stated explicitly in multiple sources and shown in hardware requirement tables.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","Multiple documents confirm that 4 A100 80GB GPUs are required as the bare minimum for running LLaMA-65B inference. This is stated explicitly in multiple sources and shown in hardware requirement tables."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","Multiple sections across the han2024 document consistently state that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips between Los Angeles and New York City (LA-NYC) by car.","10000","round trips","[""han2024""]","is_blank","Moreover, depending on the location, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to driving a car for more than 10,000 round trips between Los Angeles and New York City (LA-NYC), resulting in a health cost that even exceeds 120% of the training electricity cost.","Multiple sections across the han2024 document consistently state that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips between Los Angeles and New York City (LA-NYC) by car."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","According to multiple sources citing NVIDIA's 2019 estimate, 80-90% of ML workload is inference processing.","[80, 90]","percent","[""patterson2021"", ""chung2025"", ""fernandez2025""]","is_blank","For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].","According to multiple sources citing NVIDIA's 2019 estimate, 80-90% of ML workload is inference processing."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The paper 'Power Hungry Processing' (2024) by Luccioni et al. shows in Figure 1 the emissions for tasks examined in their study measured in gCO2eq per 1,000 queries, but the total CO2 emissions for the entire study are not explicitly stated in the provided context.","is_blank","kg CO2eq","[""is_blank""]","is_blank","is_blank","The paper 'Power Hungry Processing' (2024) by Luccioni et al. shows in Figure 1 the emissions for tasks examined in their study measured in gCO2eq per 1,000 queries, but the total CO2 emissions for the entire study are not explicitly stated in the provided context."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context explicitly states that TDP-based estimations are 'nearly always an overestimation' since GPUs rarely draw maximum power continuously. Studies show TDP estimates can overestimate actual energy consumption by up to 4.1x (for CodeGemma 2B on H100 GPUs).","0","is_blank","[""chung2025""]","is_blank","Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time. In fact, such an estimation can lead to a worst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).","The context explicitly states that TDP-based estimations are 'nearly always an overestimation' since GPUs rarely draw maximum power continuously. Studies show TDP estimates can overestimate actual energy consumption by up to 4.1x (for CodeGemma 2B on H100 GPUs)."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","According to the context, GPT-4o mini actually consumes more energy than GPT-4o, specifically for long queries it consumes 20% more energy due to its reliance on older A100 GPU hardware instead of H100s. For instance, GPT-4o consumes around 2.875 Wh while GPT-4o mini's consumption is slightly higher at 3.098 Wh.","0","is_blank","[""jegham2025""]","is_blank","For instance GPT-4o consumes around 2.875 Wh while GPT-4o mini's consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.","According to the context, GPT-4o mini actually consumes more energy than GPT-4o, specifically for long queries it consumes 20% more energy due to its reliance on older A100 GPU hardware instead of H100s. For instance, GPT-4o consumes around 2.875 Wh while GPT-4o mini's consumption is slightly higher at 3.098 Wh."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","Looking at Figure 15 in the context which shows throughput data for Mixtral-CS-A100-40GB, we can see that for batch size 1 with dense setup, the throughput is approximately 0.3 queries/sec.","0.3","queries/sec","[""xia2024""]","is_blank","Fig. 15. Estimation and validation of fine-tuning throughput for Mixtral GS for different GPUs: A100 and H100.","Looking at Figure 15 in the context which shows throughput data for Mixtral-CS-A100-40GB, we can see that for batch size 1 with dense setup, the throughput is approximately 0.3 queries/sec."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","According to the context, after peaking in 2022 with 10% of notable models releasing environmental information, there was actually a reversal in this trend due to increasingly commercial and proprietary models. By the first quarter of 2025, the majority of models fell under the 'no disclosure' category.","0","is_blank","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category","According to the context, after peaking in 2022 with 10% of notable models releasing environmental information, there was actually a reversal in this trend due to increasingly commercial and proprietary models. By the first quarter of 2025, the majority of models fell under the 'no disclosure' category."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","According to the paper by Griggs et al. (2024), in their experiments they set a normalized H100 price of $7.516 per hour for on-demand cloud GPUs. This was calculated by comparing RunPod's H100 cost ($4.69) to RunPod's A100-80G cost ($2.29), then adjusting relative to major cloud providers' A100 price ($3.67).","7.516","USD per hour","[""griggs2024""]","is_blank","Type L4 A10G (PCIe) A100-80G (SXM) H100 (SXM)
On-demand Price ($/h) 0.7 1.01 3.67 7.5164","According to the paper by Griggs et al. (2024), in their experiments they set a normalized H100 price of $7.516 per hour for on-demand cloud GPUs. This was calculated by comparing RunPod's H100 cost ($4.69) to RunPod's A100-80G cost ($2.29), then adjusting relative to major cloud providers' A100 price ($3.67)."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context does not specify the exact water usage for GPT-4's training run. While there are estimates for GPT-3's training water usage (700,000 liters) and projections for GPT-4o's inference water usage in 2025, there is no explicit mention of GPT-4's training water consumption.","is_blank","liters of water","[""is_blank""]","is_blank","is_blank","The context does not specify the exact water usage for GPT-4's training run. While there are estimates for GPT-3's training water usage (700,000 liters) and projections for GPT-4o's inference water usage in 2025, there is no explicit mention of GPT-4's training water consumption."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","According to the context, Llama2-70b at BF16 precision costs over $5,200 per month on major cloud platforms using 2 NVIDIA A100-80GB GPUs. Converting this monthly cost to hourly: $5,200/(30*24) ≈ $7.22 per hour.","7.22","USD per hour","[""griggs2024""]","is_blank","serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","According to the context, Llama2-70b at BF16 precision costs over $5,200 per month on major cloud platforms using 2 NVIDIA A100-80GB GPUs. Converting this monthly cost to hourly: $5,200/(30*24) ≈ $7.22 per hour."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","According to Figure 2(a) and the accompanying text in wu2021a, to increase the GPT-3 model quality BLEU score from 5 to 40 requires a model 1,000 times larger in size. This is explicitly stated in multiple places in the document.","1000","multiplier","[""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.","According to Figure 2(a) and the accompanying text in wu2021a, to increase the GPT-3 model quality BLEU score from 5 to 40 requires a model 1,000 times larger in size. This is explicitly stated in multiple places in the document."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","According to Strubell et al. 2019, training a Transformer model with neural architecture search produced 626,155 lbs of CO2. Wu et al. 2021 indicates that training one large ML model produced emissions equivalent to 242,231 miles driven by an average passenger vehicle. Using this ratio from the more recent study, we can derive the equivalent driving distance.","242231","miles","[""strubell2019"", ""wu2021a""]","is_blank","Although recent work shows the carbon footprint of training one large ML model, such as Meena [21], is equivalent to 242,231 miles driven by an average passenger vehicle [ 22], this is only one aspect; [...] Training one model (GPU) w/ neural architecture search 626,155 [lbs CO2]","According to Strubell et al. 2019, training a Transformer model with neural architecture search produced 626,155 lbs of CO2. Wu et al. 2021 indicates that training one large ML model produced emissions equivalent to 242,231 miles driven by an average passenger vehicle. Using this ratio from the more recent study, we can derive the equivalent driving distance."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","According to the context, BLOOMz-7B has an inference energy cost of 1.0 × 10^-4 kWh per inference. For 606,096 downloads with 1 million inferences each, the total would be: 606,096 * 1,000,000 * (1.0 × 10^-4) kWh = 60,609.6 MWh.","60609.6","MWh","[""luccioni2024""]","is_blank","Inference energy (kWh) 1.0 × 10−4 [from Table 5]","According to the context, BLOOMz-7B has an inference energy cost of 1.0 × 10^-4 kWh per inference. For 606,096 downloads with 1 million inferences each, the total would be: 606,096 * 1,000,000 * (1.0 × 10^-4) kWh = 60,609.6 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","According to the context, JetMoE-8B was trained using 30,000 H100 GPU hours, as explicitly stated in the document.","30000","H100 GPU hours","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","According to the context, JetMoE-8B was trained using 30,000 H100 GPU hours, as explicitly stated in the document."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","According to multiple quotes from the Cottier 2024 paper, the trend suggests that the most expensive publicly announced model will cost one billion dollars to train by the start of 2027, based on the current growth rate of 2.4× per year in training costs.","1000000000","USD","[""cottier2024""]","is_blank","The trend suggests that the most expensive publicly announced model will cost one billion dollars to train by the start of 2027","According to multiple quotes from the Cottier 2024 paper, the trend suggests that the most expensive publicly announced model will cost one billion dollars to train by the start of 2027, based on the current growth rate of 2.4× per year in training costs."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","According to the context, OpenAI published the total number of floating point operations to train GPT-3 as 3.14E23 FLOPS, as mentioned in [patterson2021].","3.14E23","FLOPS","[""patterson2021""]","is_blank","OpenAI   published   the   total   number   of   floating   point   operations   to   train   their   model:   3.14E+23   [Bro20].","According to the context, OpenAI published the total number of floating point operations to train GPT-3 as 3.14E23 FLOPS, as mentioned in [patterson2021]."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","Multiple snippets clearly state that 8 V100 GPUs each with 32 GB of RAM are required at minimum for meaningful inference with the 65B LLaMA model without compression or quantization.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","Multiple snippets clearly state that 8 V100 GPUs each with 32 GB of RAM are required at minimum for meaningful inference with the 65B LLaMA model without compression or quantization."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","From the context, for the 101B stage the per-GPU throughput is 165 teraFLOPs/sec with 192 GPUs running for 26.54B tokens. Converting teraFLOPs/s to zettaFLOPs by multiplying throughput × GPUs × training time: 165e12 × 192 × (26.54e9/2160) × 3600 × 24 = 28.22 zettaFLOPs.","28.22","zettaFLOPs","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages...101 4 4 12 192 2160 165 52.88%... Based on the speed test results and total budget, we train the 16B model with 245.37B tokens, the 51B model with 39.64B tokens, and the 101B model with 26.54B tokens.","From the context, for the 101B stage the per-GPU throughput is 165 teraFLOPs/sec with 192 GPUs running for 26.54B tokens. Converting teraFLOPs/s to zettaFLOPs by multiplying throughput × GPUs × training time: 165e12 × 192 × (26.54e9/2160) × 3600 × 24 = 28.22 zettaFLOPs."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The provided context does not directly state AlexNet's top-1 accuracy on ImageNet in 2012. While AlexNet is mentioned in several contexts, none specify its ImageNet accuracy for that year.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context does not directly state AlexNet's top-1 accuracy on ImageNet in 2012. While AlexNet is mentioned in several contexts, none specify its ImageNet accuracy for that year."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","According to the context, FLM-101B was trained on a cluster of 24 DGX-A800 GPU servers, with each server having 8 x 80G GPUs. Therefore, the total number of A800 GPUs used was 24 servers × 8 GPUs per server = 192 GPUs.","192","GPUs","[""li2025a""]","is_blank","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","According to the context, FLM-101B was trained on a cluster of 24 DGX-A800 GPU servers, with each server having 8 x 80G GPUs. Therefore, the total number of A800 GPUs used was 24 servers × 8 GPUs per server = 192 GPUs."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","According to Strubell et al.'s estimate cited in multiple sources, training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, which was stated to be similar to the lifetime emissions of five US cars. Therefore, this would be equivalent to 5 average American car lifetimes.","5","lifetimes","[""luccioni2023"", ""dodge2022""]","is_blank","The first paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars.","According to Strubell et al.'s estimate cited in multiple sources, training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, which was stated to be similar to the lifetime emissions of five US cars. Therefore, this would be equivalent to 5 average American car lifetimes."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","According to the context, FAIR's RoBERTa was trained on 160GB of text and required around 25,000 GPU hours to train.","25000","hours","[""schwartz2019""]","is_blank","FAIR's
RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours
to train.","According to the context, FAIR's RoBERTa was trained on 160GB of text and required around 25,000 GPU hours to train."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","According to the context, Amazon's rooftop solar projects avoid roughly 17 million metric tons of CO2e each year. This number represents the total avoidance from all 270 rooftop solar projects across Amazon and Whole Foods Market facilities.","17000000","metric tons","[""amazon2023""]","is_blank","These projects bring the total number of rooftop solar projects across Amazon and Whole Foods Market facilities to 270, with a total capacity of 344 MW. Altogether, these projects avoid roughly 17 million metric tons of CO2e each year.","According to the context, Amazon's rooftop solar projects avoid roughly 17 million metric tons of CO2e each year. This number represents the total avoidance from all 270 rooftop solar projects across Amazon and Whole Foods Market facilities."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","From the context, Hivemind is mentioned as a PyTorch-based framework used for distributed DL training that enables training on heterogeneous hardware in a decentralized fashion and can handle peers dropping out.","Hivemind","is_blank","[""erben2023""]","is_blank","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion. Its main difference to other state-of-the-art distributed training frameworks, such as PyTorch DDP [26] and DeepSpeed [35], is that it runs in a decentralized fashion and can handle peers that drop out at any stage of the training.","From the context, Hivemind is mentioned as a PyTorch-based framework used for distributed DL training that enables training on heterogeneous hardware in a decentralized fashion and can handle peers dropping out."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","According to Table B2 in the Zschache2025 paper, for the Llama 3.1 70B model, the energy consumption ratio between double node and single node deployment was 1.95, meaning energy consumption increased by a factor of 1.95 when using two nodes.","1.95","multiplier","[""zschache2025""]","is_blank","Model Duration (s) Energy consumed (Wh)
single double ratio single double ratio
Llama 3.1 70B 161.59 304.77 1.89 48.60 94.88 1.95","According to Table B2 in the Zschache2025 paper, for the Llama 3.1 70B model, the energy consumption ratio between double node and single node deployment was 1.95, meaning energy consumption increased by a factor of 1.95 when using two nodes."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context does not provide specific information about water consumption per ChatGPT user session in 2023. While there are discussions of water usage by AI systems and some metrics for GPT-4o in 2025, there is no data specifically about ChatGPT's per-user water consumption in 2023.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about water consumption per ChatGPT user session in 2023. While there are discussions of water usage by AI systems and some metrics for GPT-4o in 2025, there is no data specifically about ChatGPT's per-user water consumption in 2023."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","According to the context, when scaling 0.42 Wh short GPT-4o queries to 700M queries per day, the annual electricity consumption would be comparable to 35,000 U.S. homes, as directly stated in the abstract of the Jegham paper.","35000","homes","[""jegham2025""]","[""arXiv:2505.09598v6""]","Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes","According to the context, when scaling 0.42 Wh short GPT-4o queries to 700M queries per day, the annual electricity consumption would be comparable to 35,000 U.S. homes, as directly stated in the abstract of the Jegham paper."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","According to the context, Microsoft reported a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons. This information appears in a 2025 paper by Luccioni et al.","34","percent","[""luccioni2025a""]","is_blank","Corporate reports have revealed the scale of water demand in-creases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [ 42, 78].","According to the context, Microsoft reported a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons. This information appears in a 2025 paper by Luccioni et al."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","According to the context, on the Yelp dataset, traditional models perform 'considerably worse' than LLMs, which justifies the higher energy costs of LLM deployment. This indicates they are not comparable in accuracy.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","According to the context, on the Yelp dataset, traditional models perform 'considerably worse' than LLMs, which justifies the higher energy costs of LLM deployment. This indicates they are not comparable in accuracy."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","According to the context, the PUE for the Iowa datacenter where the Evolved Transformer was run was explicitly stated as 1.11. This value is referenced multiple times in the text and appears in Table 1.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","According to the context, the PUE for the Iowa datacenter where the Evolved Transformer was run was explicitly stated as 1.11. This value is referenced multiple times in the text and appears in Table 1."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","According to the analysis, GPT-4o usage will grow by 20% monthly from January to May 2025, followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion queries in 2025.","772000000000","queries","[""jegham2025""]","is_blank","This is followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].","According to the analysis, GPT-4o usage will grow by 20% monthly from January to May 2025, followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion queries in 2025."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","According to Table 3 in the context, JetMoE-8B achieved an OpenLLM Leaderboard Average score of 53.0, outperforming Gemma (46.4), LLaMA2 (51.0), and DeepseekMoE (51.1).","53.0","score","[""shen2024""]","is_blank","Table 3: OpenLLM leaderboard and code benchmarks results from four different models [...] OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0","According to Table 3 in the context, JetMoE-8B achieved an OpenLLM Leaderboard Average score of 53.0, outperforming Gemma (46.4), LLaMA2 (51.0), and DeepseekMoE (51.1)."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","While the context discusses renewable energy and AI training extensively, none of the documents provide specific data about the total number of AI training runs conducted globally on renewable-only power in 2022. The sources discuss trends and strategies around renewable energy use for AI, but do not quantify completed training runs.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","While the context discusses renewable energy and AI training extensively, none of the documents provide specific data about the total number of AI training runs conducted globally on renewable-only power in 2022. The sources discuss trends and strategies around renewable energy use for AI, but do not quantify completed training runs."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","According to the context, the open-source exemption from reporting obligations should be removed as 'making parts of a model public does not justify exclusion from environmental accountability.' This indicates open-source models are not fully exempt, and the policy proposal explicitly calls for eliminating this exemption.","0","is_blank","[""ebert2024""]","is_blank","• Elimination of the Open-Source Exemption : The open-source exemption from reporting obligations should be re-moved, as making parts of a model public does not justify ex-clusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.","According to the context, the open-source exemption from reporting obligations should be removed as 'making parts of a model public does not justify exclusion from environmental accountability.' This indicates open-source models are not fully exempt, and the policy proposal explicitly calls for eliminating this exemption."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","According to Patterson 2021, the US national datacenter average PUE in 2020 was 1.59.","1.59","PUE","[""patterson2021""]","is_blank","In   2020,   it   was   1.59","According to Patterson 2021, the US national datacenter average PUE in 2020 was 1.59."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","According to the context, when running the OPT-2.7B model on a g4dn.xlarge instance with batch size 32, the KV Cache expands to 5.312GB.","5.312","GB","[""kim2025""]","is_blank","When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","According to the context, when running the OPT-2.7B model on a g4dn.xlarge instance with batch size 32, the KV Cache expands to 5.312GB."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","Multiple sections of the Cottier paper explicitly state that for the four models studied (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs including equity made up between 29% and 49% of total amortized model development costs.","[29, 49]","percent","[""cottier2024""]","is_blank","We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model.","Multiple sections of the Cottier paper explicitly state that for the four models studied (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs including equity made up between 29% and 49% of total amortized model development costs."
"q213","Which software package was used to measure energy consumption during inference runs?","Multiple documents mention that CodeCarbon was used to measure energy consumption during inference runs. Specifically, both Zschache2025 and Morrison2025 explicitly state they used CodeCarbon for tracking energy consumption during their inference experiments.","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025""]","is_blank","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon). [from zschache2025] and 'In our inference experiments, we measure cumulative energy consumption using CodeCarbon (Courty et al., 2024) tracking' [from morrison2025]","Multiple documents mention that CodeCarbon was used to measure energy consumption during inference runs. Specifically, both Zschache2025 and Morrison2025 explicitly state they used CodeCarbon for tracking energy consumption during their inference experiments."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","According to the analysis of 100 news articles about ChatGPT energy consumption published as of April 11, 2025, 53% of articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search.","53","percent","[""luccioni2025c""]","is_blank","Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search 42","According to the analysis of 100 news articles about ChatGPT energy consumption published as of April 11, 2025, 53% of articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context clearly states that the Compute Time Calibration Function (CTCF) is designed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance in LLM inference workloads.","Compute Time Calibration Function","is_blank","[""kim2025""]","is_blank","Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","The context clearly states that the Compute Time Calibration Function (CTCF) is designed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance in LLM inference workloads."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context from samsi2024 repeatedly states that increasing the number of shards tends to increase the energy costs of inference per response overall, as mentioned in multiple sections of the paper including the discussion of Figures 8 and 9.","1","is_blank","[""samsi2024""]","is_blank","Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall","The context from samsi2024 repeatedly states that increasing the number of shards tends to increase the energy costs of inference per response overall, as mentioned in multiple sections of the paper including the discussion of Figures 8 and 9."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","From the context, an H100 GPU is 0.1% rare earth metal by mass, and mining 1 kg of rare earth materials consumes 11 kL of water. Therefore, for one H100 that contains 0.1% rare earth metals, the water consumption would be 11 kL * 0.001 = 0.011 kL.","0.011","kL","[""morrison2025""]","is_blank","We additionally estimate the environmental impact from mining rare earth metals used during manufacturing, assuming an H100 is 0.1% rare earth metal by mass. Mining 1 kg of rare earth materials consumes about 11 kL of water","From the context, an H100 GPU is 0.1% rare earth metal by mass, and mining 1 kg of rare earth materials consumes 11 kL of water. Therefore, for one H100 that contains 0.1% rare earth metals, the water consumption would be 11 kL * 0.001 = 0.011 kL."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","According to the context, open-source general-purpose AI models are currently exempt from reporting obligations unless they present systemic risk (Articles 2(12) and 53(2)). This is presented as a current limitation that policy proposals aim to change.","0","is_blank","[""ebert2024""]","is_blank","Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2)","According to the context, open-source general-purpose AI models are currently exempt from reporting obligations unless they present systemic risk (Articles 2(12) and 53(2)). This is presented as a current limitation that policy proposals aim to change."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","According to multiple sources in the context, in 2020, Amazon, Microsoft, Meta (Facebook), and Google together accounted for 30% of all PPAs purchased by corporations worldwide.","30","percent","[""luccioni2025a"", ""wu2021b""]","is_blank","['In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131]', 'In 2020, Amazon, Google, Facebook, and Microsoft were the top four technology companies that purchased signiﬁcant renewable energy capacities, accounting for 30% of the cumulative total from corporations globally [Schechner, 2021].']","According to multiple sources in the context, in 2020, Amazon, Microsoft, Meta (Facebook), and Google together accounted for 30% of all PPAs purchased by corporations worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","According to the context, U.S. data centers resulted in a total public health cost of about $6.7 billion in 2023, which is equivalent to $47.5 per household.","6700000000","USD","[""han2024""]","is_blank","Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about$6.7 billion, or$47.5 per household, in 2023.","According to the context, U.S. data centers resulted in a total public health cost of about $6.7 billion in 2023, which is equivalent to $47.5 per household."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","From the data table, for prompts of size '10k input-1.5k output', o3 uses 12.222 Wh while GPT-4.1 nano uses 0.827 Wh. Dividing 12.222 by 0.827 gives us approximately 14.78, meaning o3 consumes about 14.78 times more energy.","14.78","multiplier","[""jegham2025""]","is_blank","Model Energy Consumption(100 input-300 output)(Wh)
Energy Consumption(1k input-1k output)(Wh)
Energy Consumption(10k input-1.5k output)(Wh)GPT-4.1 0.871 ± 0.302 3.161 ± 0515 4.833 ± 0.650GPT-4.1 mini 0.450 ± 0.081 1.545 ± 0.211 2.122 ± 0.348GPT-4.1 nano 0.207 ± 0.047 0.575 ± 0.108 0.827 ± 0.094o4-mini (high) 3.649 ± 1.468 7.380 ± 2.177 7.237 ± 1.674o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082","From the data table, for prompts of size '10k input-1.5k output', o3 uses 12.222 Wh while GPT-4.1 nano uses 0.827 Wh. Dividing 12.222 by 0.827 gives us approximately 14.78, meaning o3 consumes about 14.78 times more energy."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","According to the evaluation results for the Arena dataset with 120ms SLO, Mélange achieved cost reductions ranging from 15.35% (compared to A10G-only) to 77.25% (compared to H100-only), as shown in Table 3 of the document.","[15.35, 77.25]","percent","[""griggs2024""]","is_blank","Rate (req/s) Solver L4 A10G A100 H100 Norm. Cost
($/hr) Savings
1 Mélange 1 1 1.71 N/A
H100-only 1 7.516 77.25%
A100-only 1 3.67 53.41%
A10G-only 2 2.02 15.35%
L4-only 3 2.1 18.57%","According to the evaluation results for the Arena dataset with 120ms SLO, Mélange achieved cost reductions ranging from 15.35% (compared to A10G-only) to 77.25% (compared to H100-only), as shown in Table 3 of the document."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","According to Table 3 referenced in the context, FLM-101B had a net carbon footprint of 26 metric tons CO2e (tCO2e) from pre-training.","26","tCO2e","[""li2025a""]","is_blank","Table 3... For our model yields only 1/10 pre-training carbon footprint of a typical LLM. [...] net tCO2e 26","According to Table 3 referenced in the context, FLM-101B had a net carbon footprint of 26 metric tons CO2e (tCO2e) from pre-training."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","From looking at Figure 4 in the document showing execution time breakdown, for Mixtral with sparse(bsz=1) configuration, the total execution time adding up forward, backward and optimizer stages is approximately 0.3 seconds.","0.3","seconds","[""xia2024""]","is_blank","Fig. 4. Execution time breakdown.","From looking at Figure 4 in the document showing execution time breakdown, for Mixtral with sparse(bsz=1) configuration, the total execution time adding up forward, backward and optimizer stages is approximately 0.3 seconds."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context explicitly states multiple times that health costs are NOT evenly distributed - disadvantaged and low-income communities bear a disproportionate share, with per-household impacts up to 200 times higher in the most affected areas compared to less affected areas.","0","is_blank","[""han2024""]","is_blank","Importantly, these health costs are not evenly distributed: disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas.","The context explicitly states multiple times that health costs are NOT evenly distributed - disadvantaged and low-income communities bear a disproportionate share, with per-household impacts up to 200 times higher in the most affected areas compared to less affected areas."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","According to the Wu 2021 paper, which directly states that 'GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years', this statement is true.","1","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore's law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","According to the Wu 2021 paper, which directly states that 'GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years', this statement is true."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions that 4-bit quantization was applied through Ollama, described as 'an open-source platform known for its support of edge computing principles and privacy-centric deployments'.","Ollama","is_blank","[""khan2025""]","is_blank","We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The context mentions that 4-bit quantization was applied through Ollama, described as 'an open-source platform known for its support of edge computing principles and privacy-centric deployments'."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","According to the context, Backblaze (B2) was chosen as an independent S3 storage provider to simulate real-world deployment with non-public datasets, with replicated data centers that could better serve requests worldwide.","Backblaze","is_blank","[""erben2023""]","is_blank","To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4]. Backblaze has replicated data centers that can better serve requests from anywhere worldwide, guaranteeing a reasonable ingress rate from every continent.","According to the context, Backblaze (B2) was chosen as an independent S3 storage provider to simulate real-world deployment with non-public datasets, with replicated data centers that could better serve requests worldwide."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","Yes - according to the context, Zschache et al. (2025) states that 'the energy consumption during inference is shown to highly correlate with the model's runtime' and notes that runtime can serve as a good proxy for energy usage. Multiple sources confirm this strong correlation.","1","is_blank","[""zschache2025""]","is_blank","Additionally, the energy consumption during inference is shown to highly correlate with the model's runtime. This makes the duration of computations a valuable proxy measure for energy consumption in settings where the latter cannot be traced.","Yes - according to the context, Zschache et al. (2025) states that 'the energy consumption during inference is shown to highly correlate with the model's runtime' and notes that runtime can serve as a good proxy for energy usage. Multiple sources confirm this strong correlation."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","According to the context, Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act bill in the U.S. Senate on February 1, 2024.","Edward J. Markey","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","Relating to AI more specifically, although not limited to data centers, is a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024","According to the context, Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act bill in the U.S. Senate on February 1, 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","According to Chen et al., the price per hour for an NVIDIA H100 is $11.06/hr, as shown in Table 1 of their paper which compares specifications and pricing of different accelerators.","11.06","USD per hour","[""chen2024""]","is_blank","Table 1: H100, H20, and TPU v6e specifications.
H100 H20 TPU v6e [7]
Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","According to Chen et al., the price per hour for an NVIDIA H100 is $11.06/hr, as shown in Table 1 of their paper which compares specifications and pricing of different accelerators."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","Based on context, while GPUs can theoretically last 5 years, companies like Facebook assume a 4-year lifespan internally for their GPUs. Facebook's internal assumption provides the most concrete operational estimate.","4","years","[""morrison2025"", ""luccioni2025a""]","is_blank","Internally, we assume a 4 year lifespan for our GPUs, which leads to an embodied emissions of 0.013 kg of CO2eq and 0.003 liters of water consumed per GPU hour when the estimated embodied impacts is amortized over the assumed lifetime of the GPU.","Based on context, while GPUs can theoretically last 5 years, companies like Facebook assume a 4-year lifespan internally for their GPUs. Facebook's internal assumption provides the most concrete operational estimate."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","According to Table II in the samsi2024 paper, LLaMA-13B requires 2 V100 32GB GPUs as the bare minimum hardware configuration for inference with a maximum batch size of 64, assuming no model compression, optimization, quantization, or distillation.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: [...] Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size [...] 13B 2 64 1 64","According to Table II in the samsi2024 paper, LLaMA-13B requires 2 V100 32GB GPUs as the bare minimum hardware configuration for inference with a maximum batch size of 64, assuming no model compression, optimization, quantization, or distillation."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","According to the context, Google reported that training their Gemma family of language models emitted 1247.61 tons CO2e, which is over 4 times larger than the 'five cars' estimate (which was 284 metric tons CO2e from the Strubell et al. study).","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the 'five cars' number","According to the context, Google reported that training their Gemma family of language models emitted 1247.61 tons CO2e, which is over 4 times larger than the 'five cars' estimate (which was 284 metric tons CO2e from the Strubell et al. study)."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","According to the context, Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which equals 336 hours.","336","hours","[""strubell2019""]","is_blank","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","According to the context, Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which equals 336 hours."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","Multiple context snippets indicate that the U.S. national average water consumption for electricity generation is 3.1-3.14 L/kWh. The most precise value given is 3.14 L/kWh, cited as the U.S. average electricity water consumption intensity factor.","3.14","L/kWh","[""li2025b""]","is_blank","Thus, in our estimate, we use the U.S. average electricity water withdrawal intensity factor 43.83 L/kWh [35], and electricity water consumption intensity factor 3.14 L/kWh [8], respectively.","Multiple context snippets indicate that the U.S. national average water consumption for electricity generation is 3.1-3.14 L/kWh. The most precise value given is 3.14 L/kWh, cited as the U.S. average electricity water consumption intensity factor."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","According to Wu 2021 and supported by multiple other sources, Google's PUE improved to 1.10 by 2021. This is directly stated in Wu's paper and shown in Figure 1.","1.10","PUE","[""wu2021b""]","is_blank","Figure 1: PUE of hyperscalar datacenters, such as Google's, has improved from 1.21 (2008) to 1.10 (2021) [Google, a]","According to Wu 2021 and supported by multiple other sources, Google's PUE improved to 1.10 by 2021. This is directly stated in Wu's paper and shown in Figure 1."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","According to the context, AWS can lower customers' workload carbon footprints by up to 96% compared to on-premises computing workloads in North America when AWS's electricity is matched with 100% renewable energy, which AWS achieved in 2023.","96","percent","[""amazon2023""]","is_blank","Research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","According to the context, AWS can lower customers' workload carbon footprints by up to 96% compared to on-premises computing workloads in North America when AWS's electricity is matched with 100% renewable energy, which AWS achieved in 2023."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","According to the context, when using OpenOrca dataset with 2M queries, the model predicts that fine-tuning a sparse Mixtral model using NVIDIA H100 GPU would cost $3460 based on CUDO compute GPU rental rates.","3460","USD","[""xia2024""]","is_blank","For OpenOrca, by scaling the cost by number of queries, our model predicts that the most cost-effective option to rent GPU resources on CUDO compute is NVIDIA H100 with a net cost of $3460.","According to the context, when using OpenOrca dataset with 2M queries, the model predicts that fine-tuning a sparse Mixtral model using NVIDIA H100 GPU would cost $3460 based on CUDO compute GPU rental rates."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","According to the context, GPUs account for 50-70% of the total provisioned power in datacenters running ML services, as stated directly in the Chung 2025 paper.","[50, 70]","percent","[""chung2025""]","is_blank","GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter [52–54, 58].","According to the context, GPUs account for 50-70% of the total provisioned power in datacenters running ML services, as stated directly in the Chung 2025 paper."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context explicitly states that training was conducted on a cluster containing 12 nodes and 96 H100s total. Since we are asked about the total number of H100 GPUs used, the answer is 96.","96","H100 GPUs","[""shen2024""]","is_blank","We conduct training on a cluster containing 12 nodes and 96 H100s.","The context explicitly states that training was conducted on a cluster containing 12 nodes and 96 H100s total. Since we are asked about the total number of H100 GPUs used, the answer is 96."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","According to the context from morrison2025, during active training (not checkpointing), the average GPU power consumption is over 600W, which is stated to be over 85% of an H100's maximum power draw of 700W.","600","Watts","[""morrison2025""]","is_blank","When actively training, the average GPU power is over 600W, over 85% of an H100's maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.","According to the context from morrison2025, during active training (not checkpointing), the average GPU power consumption is over 600W, which is stated to be over 85% of an H100's maximum power draw of 700W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","According to the context, the average US home's total emissions (including electricity generation, natural gas, liquid petroleum gas, and fuel oil) is 8.3 metric tons CO2 per year. However, this is specifically for US homes, not the global average human life, which is not mentioned in the context.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","According to the context, the average US home's total emissions (including electricity generation, natural gas, liquid petroleum gas, and fuel oil) is 8.3 metric tons CO2 per year. However, this is specifically for US homes, not the global average human life, which is not mentioned in the context."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","According to the context, for LLaMA-13B, there was a 1.25 times increase in inference latency on the A100 compared to the V100 across words per second, tokens per second, and responses per second.","1.25","multiplier","[""samsi2024""]","is_blank","particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","According to the context, for LLaMA-13B, there was a 1.25 times increase in inference latency on the A100 compared to the V100 across words per second, tokens per second, and responses per second."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context clearly states that a single short GPT-4o query consumes 0.42 Wh with a margin of error of ±0.13 Wh. This value is consistently mentioned multiple times in the document and is directly compared to a Google search's energy consumption.","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The context clearly states that a single short GPT-4o query consumes 0.42 Wh with a margin of error of ±0.13 Wh. This value is consistently mentioned multiple times in the document and is directly compared to a Google search's energy consumption."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","According to the context, with a 400 TPS SLO, Max-Performance selected g6e.xlarge at $2.699, which was about 280% more expensive than InferSave's top choice (g4dn.xlarge at $0.71).","280","percent","[""kim2025""]","is_blank","With an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of $0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave's top choice.","According to the context, with a 400 TPS SLO, Max-Performance selected g6e.xlarge at $2.699, which was about 280% more expensive than InferSave's top choice (g4dn.xlarge at $0.71)."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","According to the context from zschache2025, a V100 or even an A30 GPU was found to be more efficient for models generating a single token per inference.","V100","is_blank","[""zschache2025""]","is_blank","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.","According to the context from zschache2025, a V100 or even an A30 GPU was found to be more efficient for models generating a single token per inference."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","According to the context from schwartz2019, reporting the computational price tag of finding, training, and running models is explicitly stated as a key Green AI practice.","1","is_blank","[""schwartz2019""]","is_blank","Reporting the computational price tag of ﬁnding, training, and running models is a key Green AI practice (see Equation 1).","According to the context from schwartz2019, reporting the computational price tag of finding, training, and running models is explicitly stated as a key Green AI practice."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","According to the 2025 paper, electronic waste reached 62 million tonnes worldwide in 2022. Since the question asks for metric tons and 1 tonne = 1 metric ton, the value is 62 million metric tons.","62000000","metric tons","[""luccioni2025a""]","is_blank","AI's expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","According to the 2025 paper, electronic waste reached 62 million tonnes worldwide in 2022. Since the question asks for metric tons and 1 tonne = 1 metric ton, the value is 62 million metric tons."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","From Table 3 in Patterson 2021, the average system power for TPU v2 is 221 Watts and for V100 GPU is 325 Watts. The difference between these values is 325 - 221 = 104 Watts.","104","Watts","[""patterson2021""]","is_blank","Table 3. Average system power per processor...
TPU v2   221   5%   Transformer (Big), Evolved Transformer (Medium), Neural Architecture Search [So19]
V100 GPU   325   2%   Transformer (Big), GPT-3 [Sut21]","From Table 3 in Patterson 2021, the average system power for TPU v2 is 221 Watts and for V100 GPU is 325 Watts. The difference between these values is 325 - 221 = 104 Watts."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","According to the context, training GPT-3 in Microsoft's U.S. data centers directly evaporates 700,000 liters of clean freshwater. This figure is explicitly stated in Li et al.'s paper as scope-1 on-site water consumption for training GPT-3.","700000","liters","[""li2025b""]","is_blank","By taking the GPT-3 model with 175 billion parameters as an example [11], we show that training GPT-3 in Microsoft's U.S. data centers can consume a total of 5.4 million liters of water, including700,000 liters of scope-1 on-site water consumption.","According to the context, training GPT-3 in Microsoft's U.S. data centers directly evaporates 700,000 liters of clean freshwater. This figure is explicitly stated in Li et al.'s paper as scope-1 on-site water consumption for training GPT-3."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","Multiple excerpts from wu2021a explicitly state that Facebook's recommendation model sizes increased by 20 times (20x) between 2019 and 2021. This is consistently mentioned across different sections and shown in Figure 2(c) of the paper.","20","multiplier","[""wu2021a""]","is_blank","Facebook's recommendation and ranking model sizes have increased by 20 times during the same time period [11]. (Figure 2(c))","Multiple excerpts from wu2021a explicitly state that Facebook's recommendation model sizes increased by 20 times (20x) between 2019 and 2021. This is consistently mentioned across different sections and shown in Figure 2(c) of the paper."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","According to Figure 8 and its description in the context, o3-mini achieved the highest cross-efficiency DEA score of 0.884, leading all other models in eco-efficiency analysis.","o3-mini","is_blank","[""jegham2025""]","is_blank","As shown in Figure 8, OpenAI's reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic's Claude 3.7 Sonnet (0.825)","According to Figure 8 and its description in the context, o3-mini achieved the highest cross-efficiency DEA score of 0.884, leading all other models in eco-efficiency analysis."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","According to Wu2021b, cell phones currently have average lifetimes of less than 3 years, making this statement true. This is mentioned in the context of discussing the need to develop hardware and software that facilitate longer lifetimes.","1","is_blank","[""wu2021b""]","is_blank","develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020]","According to Wu2021b, cell phones currently have average lifetimes of less than 3 years, making this statement true. This is mentioned in the context of discussing the need to develop hardware and software that facilitate longer lifetimes."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context indicates that per-GPU speedup actually decreased with more GPUs for CV models. Specifically, it mentions 'The more GPUs we add, the lower the contribution, e.g., RN18 goes from 0.7 to 0.4 with two to eight GPUs, respectively.'","0","is_blank","[""erben2023""]","is_blank","The more GPUs we add, the lower the contribution, e.g., RN18 goes from 0.7 to 0.4 with two to eight GPUs, respectively. This decrease is likely to continue due to a granularity of 1.0 at 8 GPUs (Figure 6a), as doubling the GPUs would, at best, increase the throughput by 33% by halving the calculation time.","The context indicates that per-GPU speedup actually decreased with more GPUs for CV models. Specifically, it mentions 'The more GPUs we add, the lower the contribution, e.g., RN18 goes from 0.7 to 0.4 with two to eight GPUs, respectively.'"
"q264","What is the context window size, in tokens, for the FLM-101B model?","According to the context, the FLM-101B model has a context window of 2,048 tokens, as explicitly stated in the model configuration details.","2048","tokens","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","According to the context, the FLM-101B model has a context window of 2,048 tokens, as explicitly stated in the model configuration details."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context from chung2025 explicitly states that LLMs consume significantly less power than GPU's TDP because LLM decoding is memory-intensive and does not fully utilize GPU compute resources, while diffusion models consume nearly maximum GPU power due to being more compute-intensive.","1","is_blank","[""chung2025""]","is_blank","Generally, LLMs and VLMs consume significantly less power than the GPU's TDP because LLM decoding, the dominant operation for LLM serving, is memory-intensive and does not fully utilize the GPU's compute resources. Diffusion models, on the other hand, consume nearly the maximum power of the GPU when batch size is not small. This is because Diffusion models are significantly more compute-intensive compared to LLM decoding.","The context from chung2025 explicitly states that LLMs consume significantly less power than GPU's TDP because LLM decoding is memory-intensive and does not fully utilize GPU compute resources, while diffusion models consume nearly maximum GPU power due to being more compute-intensive."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","According to Amazon's 2023 representation data shown in the table, 33.3% of People Managers globally identified as women in 2023.","33.3","percent","[""amazon2023""]","is_blank","People Managers
2021
2022
2023
32.7%67.2%
33.3%66.6%
33.3%66.5%","According to Amazon's 2023 representation data shown in the table, 33.3% of People Managers globally identified as women in 2023."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","According to the context, when excluding equity, computing hardware costs made up 61-76% of total amortized costs across the four key models analyzed (GPT-3, OPT-175B, GPT-4, and Gemini Ultra).","[61, 76]","percent","[""cottier2024""]","is_blank","However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.","According to the context, when excluding equity, computing hardware costs made up 61-76% of total amortized costs across the four key models analyzed (GPT-3, OPT-175B, GPT-4, and Gemini Ultra)."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","According to multiple statements in khan2025, metrics like accuracy and F1 score showed slight decreases after optimization, not improvements. The paper explicitly states that 'metrics like accuracy and F1 score are slightly lower after optimization' and mentions this trade-off between energy efficiency and performance multiple times.","0","is_blank","[""khan2025""]","is_blank","On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","According to multiple statements in khan2025, metrics like accuracy and F1 score showed slight decreases after optimization, not improvements. The paper explicitly states that 'metrics like accuracy and F1 score are slightly lower after optimization' and mentions this trade-off between energy efficiency and performance multiple times."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","According to the Strubell 2019 paper, the EPA's CO2 conversion factor is 0.954 pounds per kilowatt-hour, which takes into account the relative proportions of different energy sources (natural gas, coal, nuclear and renewable) used to produce energy in the United States.","0.954","lbs/kWh","[""strubell2019""]","is_blank","The U.S. Environmental Protection Agency (EPA) provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO2 emissions: CO2e = 0.954pt","According to the Strubell 2019 paper, the EPA's CO2 conversion factor is 0.954 pounds per kilowatt-hour, which takes into account the relative proportions of different energy sources (natural gas, coal, nuclear and renewable) used to produce energy in the United States."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context from li2025b clearly states that 'a recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027' based on GPU shipment projections.","[85, 134]","TWh","[""li2025b""]","is_blank","For example, a recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 [7], whereas a more aggressive projection by the recent U.S.","The context from li2025b clearly states that 'a recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027' based on GPU shipment projections."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","According to the context, under the Europe section it explicitly states 'We delivered 150 million packages via EVs' in 2023.","150","packages","[""amazon2023""]","is_blank","Europe
• We deployed more than 300 electric delivery vans from 
Rivian on the road in Europe as part of our broader fleet 
of more than 3,000 electric delivery vehicles.
• We delivered 150 million packages via EVs.","According to the context, under the Europe section it explicitly states 'We delivered 150 million packages via EVs' in 2023."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","From the context, the online inference workload used 128 input tokens and 512 output tokens per request, with a total of 3000 requests. Therefore, the total tokens processed was (128 + 512) * 3000 = 1,920,000 tokens.","1920000","tokens","[""kim2025""]","is_blank","• Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. [...] The workload evaluates a total of 3000 requests.","From the context, the online inference workload used 128 input tokens and 512 output tokens per request, with a total of 3000 requests. Therefore, the total tokens processed was (128 + 512) * 3000 = 1,920,000 tokens."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","According to the context, this is false - it is specifically mentioned as a gap that 'The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration'.","0","is_blank","[""ebert2024""]","is_blank","5) The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37].","According to the context, this is false - it is specifically mentioned as a gap that 'The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration'."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","According to Figure 3(a) in Dodge et al. 2022, for very short experiments like DenseNet 201 which ran for less than half an hour, they found significant reductions 'up to 80% in West US' when using the Flexible Start optimization approach.","80","percent","[""dodge2022""]","is_blank","For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US","According to Figure 3(a) in Dodge et al. 2022, for very short experiments like DenseNet 201 which ran for less than half an hour, they found significant reductions 'up to 80% in West US' when using the Flexible Start optimization approach."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","According to the study, image generation (2.9 kWh) vs text classification (0.002 kWh) vary by a factor of 1450 for the same number of inferences, as explicitly stated in the context.","1450","times","[""luccioni2024""]","is_blank","This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","According to the study, image generation (2.9 kWh) vs text classification (0.002 kWh) vary by a factor of 1450 for the same number of inferences, as explicitly stated in the context."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","According to the context, in Yelp sentiment analysis, traditional models performed 'considerably worse' than LLMs, with the paper explicitly stating that this performance gap justifies the higher energy costs of LLM deployment.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","According to the context, in Yelp sentiment analysis, traditional models performed 'considerably worse' than LLMs, with the paper explicitly stating that this performance gap justifies the higher energy costs of LLM deployment."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","According to the table 'Amazon Renewable Energy Projects' in the context, the United States had 244 renewable energy projects announced as of January 2024. This information comes directly from the table showing project locations and their corresponding number of projects.","244","projects","[""amazon2023""]","is_blank","Project Location    Number of Projects   Total MW Capacity†
United States       244                 17,706","According to the table 'Amazon Renewable Energy Projects' in the context, the United States had 244 renewable energy projects announced as of January 2024. This information comes directly from the table showing project locations and their corresponding number of projects."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context does not provide information about AWS's renewable energy usage specifically for 2018. While there is data showing Amazon's overall renewable energy percentage from 2019-2023, no data is given for 2018.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not provide information about AWS's renewable energy usage specifically for 2018. While there is data showing Amazon's overall renewable energy percentage from 2019-2023, no data is given for 2018."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","Based on the context in ebert2024, the authors recommend measuring and reporting energy consumption at the 'cumulative server level' for AI applications, as mentioned in their policy proposals table.","cumulative server level","is_blank","[""ebert2024""]","is_blank","Cumulative server energy reporting : Require energy consumption to be measured and reported at the cumulative server level.","Based on the context in ebert2024, the authors recommend measuring and reporting energy consumption at the 'cumulative server level' for AI applications, as mentioned in their policy proposals table."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","According to Table 1 in the context, when training BERT base on a single NVIDIA TITAN X GPU, the GPU consumed 187.1 watts out of a total 251.6 watts, which is explicitly stated to be 74% of the total energy consumption.","74","percent","[""dodge2022""]","is_blank","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB)... The GPU alone accounts for 74% of the total energy consumption due to these components.","According to Table 1 in the context, when training BERT base on a single NVIDIA TITAN X GPU, the GPU consumed 187.1 watts out of a total 251.6 watts, which is explicitly stated to be 74% of the total energy consumption."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","According to the context, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, as explicitly stated in the Griggs document.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms","According to the context, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, as explicitly stated in the Griggs document."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","According to Figure 8 and the text, Facebook achieved a 28.5% operational energy footprint reduction over a two-year period through iterative optimization processes, despite continued AI infrastructure scaling.","28.5","percent","[""wu2021a""]","is_blank","The net effect, with Jevon's Paradox, is a 28.5% operational power footprint reduction over two years (Figure 8).","According to Figure 8 and the text, Facebook achieved a 28.5% operational energy footprint reduction over a two-year period through iterative optimization processes, despite continued AI infrastructure scaling."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","is_blank","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","is_blank"
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","According to the context, the hardware acquisition cost to train GPT-4 was $800M, as explicitly stated by Cottier et al. This represents the upfront cost of purchasing the hardware outright, which is one to two orders of magnitude larger than the amortized hardware costs.","800000000","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","According to the context, the hardware acquisition cost to train GPT-4 was $800M, as explicitly stated by Cottier et al. This represents the upfront cost of purchasing the hardware outright, which is one to two orders of magnitude larger than the amortized hardware costs."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","According to the context, 'Sustainable AI' was proposed by van Wynsberghe as a field that encompasses BOTH using AI in climate-positive applications AND improving the environmental sustainability of AI approaches themselves. Therefore, it was not proposed to only encompass climate-positive applications.","0","is_blank","[""luccioni2025b""]","is_blank","The umbrella term 'Sustainable AI' was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves","According to the context, 'Sustainable AI' was proposed by van Wynsberghe as a field that encompasses BOTH using AI in climate-positive applications AND improving the environmental sustainability of AI approaches themselves. Therefore, it was not proposed to only encompass climate-positive applications."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","Looking at Figure 13 in xia2024, which shows the ground truth projection for different GPUs, the A100-40GB GPU point shows a maximum batch size of approximately 10 samples for fine-tuning Mixtral.","10","samples","[""xia2024""]","is_blank","Fig. 13. Projected maximum batch size of Mixtral for different GPUs. [Shows A100-40GB data point at approximately 10 on y-axis]","Looking at Figure 13 in xia2024, which shows the ground truth projection for different GPUs, the A100-40GB GPU point shows a maximum batch size of approximately 10 samples for fine-tuning Mixtral."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","According to the context, when the server is overloaded, Swapping consistently consumes less energy than Recomputation because Swapping only copies data without running computation, while Recomputation performs extra computation when restoring requests, and computation consumes more energy than memory operations.","Swapping","is_blank","[""chung2025""]","is_blank","It can be seen that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations","According to the context, when the server is overloaded, Swapping consistently consumes less energy than Recomputation because Swapping only copies data without running computation, while Recomputation performs extra computation when restoring requests, and computation consumes more energy than memory operations."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","According to multiple context snippets, Google's 2024 environmental sustainability report (ESG) reported a 48% increase in GHG emissions since 2019, which they attribute primarily to 'increases in data center energy consumption'.","48","percent","[""luccioni2025a""]","is_blank","For example, in their 2024 annual environmental sustainability report (ESG), Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to 'increases in data center energy consumption' [42]","According to multiple context snippets, Google's 2024 environmental sustainability report (ESG) reported a 48% increase in GHG emissions since 2019, which they attribute primarily to 'increases in data center energy consumption'."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","According to the context from fernandez2025, projections estimate that data centers will consume between 9.1% and 11.7% of the total US energy demand by 2030.","[9.1, 11.7]","percent","[""fernandez2025""]","is_blank","Primarily motivated by the increased demands from LLM and AI workloads, projections estimate that that data centers consume between 9.1% and 11.7% of the total US energy demand by 2030 (Aljbour et al., 2024;","According to the context from fernandez2025, projections estimate that data centers will consume between 9.1% and 11.7% of the total US energy demand by 2030."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","According to Figure 4(b) in dodge2022, the Pause and Resume optimization for the 6B parameter Transformer model shows potential emissions reductions up to about 25% across different regions when allowing duration to increase up to 100% of the original run time.","25","percent","[""dodge2022""]","is_blank","For very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.","According to Figure 4(b) in dodge2022, the Pause and Resume optimization for the 6B parameter Transformer model shows potential emissions reductions up to about 25% across different regions when allowing duration to increase up to 100% of the original run time."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context clearly and consistently states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. This is mentioned multiple times and is attributed to JetMoE-8B's ability to use only 2B parameters for each input token while having 8B total parameters.","70","percent","[""shen2024""]","[""https://github.com/myshell-ai/JetMoE""]","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context clearly and consistently states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. This is mentioned multiple times and is attributed to JetMoE-8B's ability to use only 2B parameters for each input token while having 8B total parameters."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","According to Strubell et al.'s 2019 study referenced in the context, training BERT emitted 626,155 pounds of CO2 emissions.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","which quantiﬁed the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].","According to Strubell et al.'s 2019 study referenced in the context, training BERT emitted 626,155 pounds of CO2 emissions."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","Multiple sources in the context confirm that GPT-3's training energy consumption was 1,287 MWh. This is explicitly stated in both li2025b and jegham2025 documents.","1287","MWh","[""li2025b"", ""jegham2025""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh [29].","Multiple sources in the context confirm that GPT-3's training energy consumption was 1,287 MWh. This is explicitly stated in both li2025b and jegham2025 documents."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","Multiple sections of the paper explicitly state that the MoE layer is crucial for performance improvement in LLM fine-tuning. The paper identifies that MoE layer consumes the highest fraction of execution time and states that optimizing MoE layer performance is key to improving overall cost and performance.","1","is_blank","[""xia2024""]","is_blank","['Our study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fine-tuning.', 'MoE layer consumes the highest fraction of execution time in LLM fine-tuning; optimizing MoE layer performance is key to improving the overall cost of LLM fine-tuning.']","Multiple sections of the paper explicitly state that the MoE layer is crucial for performance improvement in LLM fine-tuning. The paper identifies that MoE layer consumes the highest fraction of execution time and states that optimizing MoE layer performance is key to improving overall cost and performance."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","According to Table III in the paper, for Mixtral with dense setup (Mixtral-D) on the CS (Hellaswag) dataset, the maximum batch size supported is 2 samples when using an NVIDIA A40 GPU with 48GB memory.","2","samples","[""xia2024""]","is_blank","TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20 MATH 1 3 2 8","According to Table III in the paper, for Mixtral with dense setup (Mixtral-D) on the CS (Hellaswag) dataset, the maximum batch size supported is 2 samples when using an NVIDIA A40 GPU with 48GB memory."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","According to the context, for CV models with high granularity (granularity of 3.33 on C-8), distributing VMs over four continents only slowed down performance by 7% compared to fully local experiments. This is specifically mentioned when discussing C-8 versus A-8 performance.","1","is_blank","[""erben2023""]","is_blank","However, as the CV task is still at a granularity of 3.33 on C-8, it reaches a speedup of 3.02x, only 7% slower than the fully local A-8 experiment.","According to the context, for CV models with high granularity (granularity of 3.33 on C-8), distributing VMs over four continents only slowed down performance by 7% compared to fully local experiments. This is specifically mentioned when discussing C-8 versus A-8 performance."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","is_blank","is_blank","hectares","[""is_blank""]","is_blank","is_blank","is_blank"
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","According to the context, the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries for text classification tasks.","0.32","g CO2eq","[""luccioni2024""]","is_blank","bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries","According to the context, the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 queries for text classification tasks."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","According to Figure 1 in the dodge2022 paper, when training BERT (language modeling on 8 V100s for 36 hours) across 16 different regions, the emissions ranged from 7k grams in the most efficient region to 26k grams in the least efficient region, representing a difference of 19k grams.","19000","grams","[""dodge2022""]","is_blank","There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful ( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","According to Figure 1 in the dodge2022 paper, when training BERT (language modeling on 8 V100s for 36 hours) across 16 different regions, the emissions ranged from 7k grams in the most efficient region to 26k grams in the least efficient region, representing a difference of 19k grams."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","According to the context, the direct release of environmental information peaked in 2022 with 10% of notable models releasing some degree of information that year, before declining due to the introduction of increasingly commercial and proprietary models after 2022.","2022","year","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures.","According to the context, the direct release of environmental information peaked in 2022 with 10% of notable models releasing some degree of information that year, before declining due to the introduction of increasingly commercial and proprietary models after 2022."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","From Table 2 in morrison2025, the OLMo 60M model's water consumption was equivalent to 5 days of water usage for 1 person in the US.","5","days","[""morrison2025""]","is_blank","Table 2: [...] OLMo 60M† 1.2 0.4 1 month 1.6 5 days","From Table 2 in morrison2025, the OLMo 60M model's water consumption was equivalent to 5 days of water usage for 1 person in the US."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context does not provide any specific information about Google's DeepMind AlphaFold servers' water consumption in 2023. While the context mentions Google's data center water consumption trends and includes a figure showing AI models like AlphaGo, it does not give specific consumption data for AlphaFold.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","The context does not provide any specific information about Google's DeepMind AlphaFold servers' water consumption in 2023. While the context mentions Google's data center water consumption trends and includes a figure showing AI models like AlphaGo, it does not give specific consumption data for AlphaFold."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","According to the context, adding compute resources to accelerate MoE layers will reduce costs, not increase them. This is explicitly stated in the conclusion section where it says 'A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.'","0","is_blank","[""xia2024""]","is_blank","A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.","According to the context, adding compute resources to accelerate MoE layers will reduce costs, not increase them. This is explicitly stated in the conclusion section where it says 'A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.'"
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","According to Table 3 in document li2025a, FLM-101B's energy consumption during training was 40 MkWh (megakilowatt hours), which converts to 40,000 kWh.","40000","kWh","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs... Energy (MkWh) ... FLM-101B ... 40","According to Table 3 in document li2025a, FLM-101B's energy consumption during training was 40 MkWh (megakilowatt hours), which converts to 40,000 kWh."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context projects data center health impacts for 2028, not 2030. For 2028, it projects the total public health burden could reach more than $20 billion. Since the question asks specifically about 2030 projections, which are not mentioned in the context, I cannot provide that specific information.","is_blank","USD","[""is_blank""]","is_blank","is_blank","The context projects data center health impacts for 2028, not 2030. For 2028, it projects the total public health burden could reach more than $20 billion. Since the question asks specifically about 2030 projections, which are not mentioned in the context, I cannot provide that specific information."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","According to Table IV in the paper, fine-tuning Mixtral on GS (Generative Sampling dataset) with sparse MoE using an NVIDIA A40 48GB GPU costs $32.7.","32.7","USD","[""xia2024""]","is_blank","TABLE IV
ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE
BASED ON OUR ANALYTICAL MODEL
GPU Mem MBS Throughput Cost ($/hr) Cost ($)
A40 48GB 4 1.01 0.79 32.7","According to Table IV in the paper, fine-tuning Mixtral on GS (Generative Sampling dataset) with sparse MoE using an NVIDIA A40 48GB GPU costs $32.7."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","From Figure 9 in the paper, which shows GPU SM utilization for the MoE layer, we can see that for Mixtral sparse fine-tuning, the maximum batch size tested was 32 samples. This represents the longest-running MoE layer configuration for sparse Mixtral on the A40 GPU.","32","samples","[""xia2024""]","is_blank","Fig. 9 shows the kernel-level breakdown of GPU Streaming Multi-processor (SM) utilization for the MoE layer... Dense(bsz=1), Dense(bsz=10), Sparse(bsz=1), Sparse(bsz=10), Sparse(bsz=32)","From Figure 9 in the paper, which shows GPU SM utilization for the MoE layer, we can see that for Mixtral sparse fine-tuning, the maximum batch size tested was 32 samples. This represents the longest-running MoE layer configuration for sparse Mixtral on the A40 GPU."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","Looking at Figure 4 in the paper (xia2024), which shows execution time breakdown, for Mixtral with sparse batch size=10 configuration, the total execution time is approximately 2.0 seconds when summing the forward, backward and optimizer stages.","2.0","seconds","[""xia2024""]","is_blank","Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)
0.0
2.0
4.0
6.0
8.0
Forward Backward Optimizer
Dense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)
0.0
0.5
1.0
1.5
2.0
    Execution Time
Breakdown (seconds)
Mixtral
Mamba","Looking at Figure 4 in the paper (xia2024), which shows execution time breakdown, for Mixtral with sparse batch size=10 configuration, the total execution time is approximately 2.0 seconds when summing the forward, backward and optimizer stages."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","Based on the context, while GPUs are the dominant energy consumer (50-70% of total power) and most easily measurable component, focusing only on GPU energy provides an underestimate since it excludes other datacenter components and shared resources. The context specifically notes this limitation.","0","is_blank","[""dodge2022"", ""chung2025""]","is_blank","In this work we focus on the former, a single cloud instance; because of this, a reader should understand that our estimates of the electricity consumption and emissions are underestimates.","Based on the context, while GPUs are the dominant energy consumer (50-70% of total power) and most easily measurable component, focusing only on GPU energy provides an underestimate since it excludes other datacenter components and shared resources. The context specifically notes this limitation."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","According to Luccioni et al.'s study, training accounted for only half (50%) of the model's overall emissions, meaning similar studies that only considered training were potentially underestimating emissions by half.","50","percent","[""luccioni2025b""]","is_blank","In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, since it takes into account diﬀerent stages of the model life cycle including the manufacturing of computing hardware, idle energy usage, and model deployment, ﬁnding that training accounted for only half of the model's overall emissions [121], meaning that similar studies that only took training into account were potentially underestimating their emissions by half.","According to Luccioni et al.'s study, training accounted for only half (50%) of the model's overall emissions, meaning similar studies that only considered training were potentially underestimating emissions by half."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","According to Table II in the context, which shows baseline configurations for LLaMA models, LLaMA-7B requires 1 V100 32GB GPU as the bare minimum hardware for inference with a maximum batch size of 64 and response length of 256, without any compression or optimization.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256... Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64","According to Table II in the context, which shows baseline configurations for LLaMA models, LLaMA-7B requires 1 V100 32GB GPU as the bare minimum hardware for inference with a maximum batch size of 64 and response length of 256, without any compression or optimization."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","According to Table 1 in li2025b, for Arizona data centers, it takes 16.7 requests to consume 500ml of water during GPT-3 inference. This can be read directly from the rightmost column '# of Requests for 500ml Water' in the table.","16.7","requests","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3's operational water consumption footprint [...] Arizona 1.180 1.630 4.959 2.098 7.531 9.629 6.520 23.406 29.926 16.7","According to Table 1 in li2025b, for Arizona data centers, it takes 16.7 requests to consume 500ml of water during GPT-3 inference. This can be read directly from the rightmost column '# of Requests for 500ml Water' in the table."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","According to the context, the average US home emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil total 8.3 metric tons CO2 per year.","8.3","metric tons","[""dodge2022""]","is_blank","Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).","According to the context, the average US home emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil total 8.3 metric tons CO2 per year."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","According to the evaluation results table in the context, JetMoE-8B achieved a score of 27.8 on the GSM8k benchmark, which was higher than LLaMA2 (14.5), DeepseekMoE (17.3), and Gemma (16.9).","27.8","score","[""shen2024""]","is_blank","GSM8k 14.5 17.3 16.9 27.8","According to the evaluation results table in the context, JetMoE-8B achieved a score of 27.8 on the GSM8k benchmark, which was higher than LLaMA2 (14.5), DeepseekMoE (17.3), and Gemma (16.9)."
