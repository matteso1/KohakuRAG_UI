"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context states that the total energy consumption of US data centers increased by about 4% from 2010-2014, compared with a 24% increase from 2005-10 and a nearly 90% increase from 2000-05.","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05.","The context states that the total energy consumption of US data centers increased by about 4% from 2010-2014, compared with a 24% increase from 2005-10 and a nearly 90% increase from 2000-05."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that the Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road.","13900","cars","[""amazon2023""]","is_blank","Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context states that the Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24.","24","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The context states that in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context states that NVIDIA does not release information on the embodied carbon emissions of its GPUs, so researchers must rely on estimates from other sources. The context cites an estimate from Luccioni et al. (2023) that the embodied carbon emissions are 3700 kg CO2eq per 8x server node, which equates to 463 kg CO2eq per individual GPU.","463","kg/GPU","[""morrison2025""]","is_blank","NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU.","The context states that NVIDIA does not release information on the embodied carbon emissions of its GPUs, so researchers must rely on estimates from other sources. The context cites an estimate from Luccioni et al. (2023) that the embodied carbon emissions are 3700 kg CO2eq per 8x server node, which equates to 463 kg CO2eq per individual GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The context states that the estimated cost of training OpenAI's GPT-4 model exceeds $100 million, while FLM-101B was successfully trained from scratch within a $100,000 budget. Therefore, the estimated amortized training cost of GPT-4 was over 1000 times greater than the total training budget for FLM-101B.","1000","ratio","[""xia2024"", ""li2025a""]","is_blank","[""The estimated cost of training OpenAI's GPT-4 model exceeds $100 million, rendering it financially prohibitive for most small-to-medium size enterprises and the academic community"", 'In this paper, we introduce FLM-101B, an open-sourced LLM that is successfully trained from scratch within a $100,000 budget.']","The context states that the estimated cost of training OpenAI's GPT-4 model exceeds $100 million, while FLM-101B was successfully trained from scratch within a $100,000 budget. Therefore, the estimated amortized training cost of GPT-4 was over 1000 times greater than the total training budget for FLM-101B."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","According to the context, the Google Flights estimate for the emissions of a direct round trip passenger flight between San Francisco (SFO) and New York (JFK) is 180.4t CO2e. The Boeing 767 that United Airlines flies on this route has 175 seats, and Google Flights uses a historical average of 84.5% seat occupancy, yielding 1.2t CO2e per passenger round trip.","1.2","tCO2e","[""patterson2021""]","is_blank","36  To   put   3.2   net   tCO 2 e   into   perspective,Table   1   and   Appendix   A   use   Google   Flights   to   calculate   the    CO 2 e    for   the   average   direct   round   trip   flights   between   SFO   and   JFK   as   180.4t.   The   Boeing   767   that   United   Airlines   flies   on   that   route   has   175   seats.   Google   Flights   uses   the   historical   average   of   84.5%   seat   occupancy,   yielding   1.2t   of   CO 2 e   per   passenger   round   trip.","According to the context, the Google Flights estimate for the emissions of a direct round trip passenger flight between San Francisco (SFO) and New York (JFK) is 180.4t CO2e. The Boeing 767 that United Airlines flies on this route has 175 seats, and Google Flights uses a historical average of 84.5% seat occupancy, yielding 1.2t CO2e per passenger round trip."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context states that on average, the FLM-101B model achieved a score of 43.94 on the Open LLM Leaderboard evaluation.","43.94","score","[""li2025a""]","is_blank","Results. On average, FLM-101B achieves a score of 43.94, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.","The context states that on average, the FLM-101B model achieved a score of 43.94 on the Open LLM Leaderboard evaluation."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context states that the first commercially-produced microprocessor, the Intel 4004, was manufactured in 1971 and ran at 740kHz. Fifty years later, the typical microprocessor runs at 5,000,000kHz, which is a more than 6,750 fold improvement in processor clock speed.","6750","fold","[""wu2021b""]","is_blank","Fifty years later, the typical microprocessor is manufactured in a 5+ nm process technology and is capable of running at 5,000,000kHz (e.g., [Intel, 2019, AMD, 2020]) with more than 3.9 billion transistors. This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context states that the first commercially-produced microprocessor, the Intel 4004, was manufactured in 1971 and ran at 740kHz. Fifty years later, the typical microprocessor runs at 5,000,000kHz, which is a more than 6,750 fold improvement in processor clock speed."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context states that OpenAI told the authors that GPT-3 was trained using 10,000 V100 GPUs at a rate of 24.6 TeraFLOPS/sec. Given the total number of FLOPS required to train GPT-3 was 3.14E+23, and the training rate of 24.6 TeraFLOPS/sec, this can be used to calculate the total training time.","14.8","days","[""patterson2021""]","is_blank","OpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec [Sut21]. It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","The context states that OpenAI told the authors that GPT-3 was trained using 10,000 V100 GPUs at a rate of 24.6 TeraFLOPS/sec. Given the total number of FLOPS required to train GPT-3 was 3.14E+23, and the training rate of 24.6 TeraFLOPS/sec, this can be used to calculate the total training time."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context table in the Morrison 2025 paper shows that for a Llama 3.2 1B model at 8 requests per second, the GPU Power Usage is 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","The table shows the 'GPU Power Usage (kWh)' for a Llama 3.2 1B model at a request frequency of 8 req/s is 0.036.","The context table in the Morrison 2025 paper shows that for a Llama 3.2 1B model at 8 requests per second, the GPU Power Usage is 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The provided context states that the total permitted annual emission limits for the data center diesel generators in northern Virginia are approximately 13,000 tons of NOx.","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The provided context states that the total permitted annual emission limits for the data center diesel generators in northern Virginia are approximately 13,000 tons of NOx."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","According to the context, the growth strategy used for training the 101B FLM-101B model resulted in a 72% time-saving (or 3.56x speedup) compared to training a 101B model from scratch, which was estimated to take 76.74 days.","72","percent","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","According to the context, the growth strategy used for training the 101B FLM-101B model resulted in a 72% time-saving (or 3.56x speedup) compared to training a 101B model from scratch, which was estimated to take 76.74 days."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context states that driven by the growing demand for AI, the U.S. data centers could contribute to approximately 1,300 premature deaths in 2028 due to scope-2 pollutant emissions.","1300","deaths","[""han2024""]","[""https://arxiv.org/abs/2412.06288v2""]","The context states: ""Using the reduced-complexity modeling tool COBRA (CO-Benefits Risk Assessment) provided by the EPA [39], our analysis demonstrates that driven by the growing demand for AI, the U.S. data centers could contribute to, among others, approximately 600,000 asthma symptom cases and 1,300 premature deaths in 2028, exceeding 1/3 of asthma deaths in the U.S. each year [40].""","The context states that driven by the growing demand for AI, the U.S. data centers could contribute to approximately 1,300 premature deaths in 2028 due to scope-2 pollutant emissions."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The context states that the 6 billion parameter transformer model was trained for 8 days, but was only trained to around 13% completion. The context then estimates that a full training run of this model would take around 60 days.","60","days","[""dodge2022""]","is_blank","We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. This model was not trained to completion, but only until 13%; a full training run would take 60 days.","The context states that the 6 billion parameter transformer model was trained for 8 days, but was only trained to around 13% completion. The context then estimates that a full training run of this model would take around 60 days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context provides information on the energy consumption of various AI models, including a table in the Appendix with details on GPU energy usage for 1,000 queries. According to the context, the model with the highest GPU energy consumption for 1,000 queries is 'Command-R Plus' at over 3,426 Wh.","3426","is_blank","[""luccioni2025c""]","is_blank","GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The context provides information on the energy consumption of various AI models, including a table in the Appendix with details on GPU energy usage for 1,000 queries. According to the context, the model with the highest GPU energy consumption for 1,000 queries is 'Command-R Plus' at over 3,426 Wh."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context states that the One Hundred Year Study on Artificial Intelligence was 'launched in the fall of 2014', so the answer is that it was launched in 2014.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.","The context states that the One Hundred Year Study on Artificial Intelligence was 'launched in the fall of 2014', so the answer is that it was launched in 2014."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The context states that the UN's Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled.","22","percent","[""luccioni2025a""]","is_blank","The UN's Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The context states that the UN's Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context states that BLOOM-176B, a 176B parameter language model, was trained on 1,161,261 A100-GPU hours at a cost of $3 million. This includes the training energy cost.","1161261","MWh","[""cottier2024""]","is_blank","BLOOM-176B was trained on 1,161,261 A100-hours at a throughput of 150 TFLOP/GPU/s at 48% model FLOPs utilization and a cost of $3 million (including experiments).","The context states that BLOOM-176B, a 176B parameter language model, was trained on 1,161,261 A100-GPU hours at a cost of $3 million. This includes the training energy cost."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context states that the Switch Transformer has 1500 billion total parameters, but only 0.1% of those parameters are activated per token.","0.1","percent","[""patterson2021""]","is_blank","The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources.","The context states that the Switch Transformer has 1500 billion total parameters, but only 0.1% of those parameters are activated per token."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context states that the JetMoE-8B architecture is a 'Mixture-of-Experts (MoE) layer' and that each MoE layer comprises 'N modules f1, ..., fN and a router g(e | x)'. The hyperparameters table on page 4 shows that the JetMoE-8B model has 8 experts.","8","experts","[""shen2024""]","is_blank","Figure 1: JetMoE architecture
Table 1: JetMoE-8B hyperparameters.","The context states that the JetMoE-8B architecture is a 'Mixture-of-Experts (MoE) layer' and that each MoE layer comprises 'N modules f1, ..., fN and a router g(e | x)'. The hyperparameters table on page 4 shows that the JetMoE-8B model has 8 experts."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context states that the total execution time for a dense BlackMamba model with batch size 30 on an NVIDIA A40-48GB GPU is 1.46 seconds.","1.46","second","[""xia2024""]","is_blank","Dense(bsz=30) 0.0 0.5 1.0 1.5 2.0 Execution Time Breakdown (seconds) Mixtral Mamba","The context states that the total execution time for a dense BlackMamba model with batch size 30 on an NVIDIA A40-48GB GPU is 1.46 seconds."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context states that the total cost of training the FLM-101B model is 52.76 zettaFLOPs, and the data ratio between English and Chinese is 53.5% to 46.5%. Therefore, the cost of training the English portion of the FLM-101B model is 28.22 zettaFLOPs.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The context states that the total cost of training the FLM-101B model is 52.76 zettaFLOPs, and the data ratio between English and Chinese is 53.5% to 46.5%. Therefore, the cost of training the English portion of the FLM-101B model is 28.22 zettaFLOPs."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context indicates that the 'Power Hungry Processing' study analyzed 88 models across 10 tasks and 30 datasets spanning natural language and computer vision applications.","88","models","[""luccioni2024""]","is_blank","We study 88 models across 10 tasks and 30 datasets, spanning applications in natural language and computer vision, analyzing the impact of end task, modality, model size, architecture, and learning paradigm (i.e. task-specific or multi-task/multi-purpose) on energy efficiency.","The context indicates that the 'Power Hungry Processing' study analyzed 88 models across 10 tasks and 30 datasets spanning natural language and computer vision applications."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3x'.","3","multiplier","[""wu2021a""]","is_blank","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","The context states that 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3x'."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","The context states that to estimate the compute cost over model development, including experiments, failed attempts, evaluation and fine-tuning, a multiplicative factor was applied to the final training run compute. The factor was sampled from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","[1.2,4]","multiplier","[""cottier2024""]","is_blank","Unlike the previous approaches, which only measured the cost of the final training run, this approach counts compute usage throughout model development including experiments, fine-tuning and evaluation.","The context states that to estimate the compute cost over model development, including experiments, failed attempts, evaluation and fine-tuning, a multiplicative factor was applied to the final training run compute. The factor was sampled from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context states that the training run of the 6 billion parameter transformer model consumed 13.8 MWh of electricity, but this was only 13% of the time it would take to train the model to completion. The full training run is estimated to take 60 days, so the total estimated energy consumption would be approximately (60/8) * 13.8 = 103.5 MWh.","103.5","MWh","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","The context states that the training run of the 6 billion parameter transformer model consumed 13.8 MWh of electricity, but this was only 13% of the time it would take to train the model to completion. The full training run is estimated to take 60 days, so the total estimated energy consumption would be approximately (60/8) * 13.8 = 103.5 MWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The central argument of the 2025 paper is that efficiency gains in AI may not lead to net environmental benefits due to the economic principle of Jevons' paradox, which observes that efficiency improvements can actually lead to increased consumption and usage.","is_blank","is_blank","[""luccioni2025a""]","is_blank","is_blank","The central argument of the 2025 paper is that efficiency gains in AI may not lead to net environmental benefits due to the economic principle of Jevons' paradox, which observes that efficiency improvements can actually lead to increased consumption and usage."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","According to the context, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027.","[4.2, 6.6]","billion cubic meters","[""li2025b""]","is_blank","More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United King-dom.","According to the context, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost'. This indicates that contrary to the question, Red AI is increasing rather than declining.","0","is_blank","[""schwartz2019""]","is_blank","Red AI is on the rise despite the well-known diminishing returns of increased cost","The context states that 'Red AI is on the rise despite the well-known diminishing returns of increased cost'. This indicates that contrary to the question, Red AI is increasing rather than declining."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days","The context states that under the growth schedule, the total time cost for training FLM-101B is 21.54 days."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that 'A significant portion of machine learning model experimentation utilizes GPUs at only 30-50%, leaving significant room for improvements to efficiency and overall utilization.'","is_blank","is_blank","[""wu2021a""]","is_blank","A significant portion of machine learning model experimentation utilizes GPUs at only 30-50%","The context states that 'A significant portion of machine learning model experimentation utilizes GPUs at only 30-50%, leaving significant room for improvements to efficiency and overall utilization.'"
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context states that the training of GPT-3 is estimated to have consumed 1,287 megawatt-hours (MWh) of electricity.","1287","MWh","[""jegham2025""]","is_blank","Training
GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric
tons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for
cooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool.","The context states that the training of GPT-3 is estimated to have consumed 1,287 megawatt-hours (MWh) of electricity."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions the 'AI Energy Score' project as aiming to establish a unified approach for comparing the inference efficiency of AI models.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.","The context mentions the 'AI Energy Score' project as aiming to establish a unified approach for comparing the inference efficiency of AI models."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context shows the kernel-level breakdown of the MoE layer execution time for the Dense(bsz=30) Mamba model on the NVIDIA A40 GPU. The longest kernel is matmul(w1) which takes around 1600 microseconds.","1600","microseconds","[""xia2024""]","is_blank","Dense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0
400
800
1200
1600
2000
matmul(w1)
gelu
matmul(w2)
elementwise_mult
top_k
sigmoid
matmul(router)","The context shows the kernel-level breakdown of the MoE layer execution time for the Dense(bsz=30) Mamba model on the NVIDIA A40 GPU. The longest kernel is matmul(w1) which takes around 1600 microseconds."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context states that in the JetMoE-8B model, the number of experts selected for activation (top-k) in each layer is set to 2.","2","experts","[""shen2024""]","is_blank","We replace all self-attention and MLP layers in the transformer with MoA and MoE. Then, we set the same number of experts to 8 and top-k to 2 for every layer.","The context states that in the JetMoE-8B model, the number of experts selected for activation (top-k) in each layer is set to 2."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context states that the computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018.","1","is_blank","[""schwartz2019""]","is_blank","The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2].","The context states that the computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The provided context indicates that global carbon emissions dropped by 6.4% in 2020 during the COVID-19 pandemic, with vehicle transportation in the US accounting for a portion of this reduction.","6.4","percent","[""wu2021b""]","is_blank","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The provided context indicates that global carbon emissions dropped by 6.4% in 2020 during the COVID-19 pandemic, with vehicle transportation in the US accounting for a portion of this reduction."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that Amazon's renewable energy procurement globally means that 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023, up from 19 regions in 2022.","22","data centers","[""amazon2023""]","[""https://www.aboutamazon.com/news/sustainability/2023-amazon-sustainability-report""]","Amazon's energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.","The context states that Amazon's renewable energy procurement globally means that 100% of the electricity consumed by 22 AWS data center regions was matched with renewable energy sources in 2023, up from 19 regions in 2022."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop organized by John McCarthy. Since the question asks about the age of the field in 2025, and the current year is 2023, the field would be approximately 69 years old in 2025.","69","years","[""stone2022""]","[""https://www.cs.utexas.edu/~kuipers/readings/Stone-2022-AI.pdf""]","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The context states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop organized by John McCarthy. Since the question asks about the age of the field in 2025, and the current year is 2023, the field would be approximately 69 years old in 2025."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The 'five cars' carbon footprint estimate is based on the large-scale neural architecture search (NAS) training workload, which is an infrequently performed AI process meant to identify an optimized variant of the Transformer architecture.","neural architecture search (NAS)","is_blank","[""luccioni2025c""]","is_blank","Among the first efforts to quantify the environmental impacts of AI was the 2019 study by Strubell et al., which estimated the monetary costs, energy use, and GHG emissions required to train a variety of typical natural language processing (NLP) models of that era, including the first generation of large language models. This analysis included both the costs to train several individual models, including the two original 'base' (65M) and 'big' (213M parameter) variants of the Transformer neural network architecture that forms the basis of LLMs to this day, as well as the cost to perform model development, i.e. identifying the best model architecture with respect to some optimization objective. The authors quantified the costs of model development through a case study of the energy required for them to develop a model published in the previous year, and by estimating the energy required to automate that process using an approach called neural architecture search (NAS).","The 'five cars' carbon footprint estimate is based on the large-scale neural architecture search (NAS) training workload, which is an infrequently performed AI process meant to identify an optimized variant of the Transformer architecture."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","According to the context, using the A100 GPU, the Llama 3.1 8B model can achieve an average Time Per Output Token (TPOT) of 77 ms, which reduces energy consumption per generation by 44% compared to the configuration that minimizes latency.","44","percent","[""chung2025""]","is_blank","This is stated in the following excerpt: 'This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.'","According to the context, using the A100 GPU, the Llama 3.1 8B model can achieve an average Time Per Output Token (TPOT) of 77 ms, which reduces energy consumption per generation by 44% compared to the configuration that minimizes latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","According to Table III in the context, the maximum batch size supported for fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using an NVIDIA A40 GPU with 48 GB of memory is 20 samples.","20","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE
AND S:SPARSE .
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20
MATH 1 3 2 8","According to Table III in the context, the maximum batch size supported for fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using an NVIDIA A40 GPU with 48 GB of memory is 20 samples."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context states that in 2023, Amazon held 1.3 GW of energy storage capacity, up from 445 MW in 2022.","1.3","GW","[""amazon2023""]","is_blank","We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.","The context states that in 2023, Amazon held 1.3 GW of energy storage capacity, up from 445 MW in 2022."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that the estimated 2025 annual carbon emissions of GPT-4o are projected to be approximately 138,125 to 163,441 tons of CO2e. It further states that these figures are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","Our projections indicate annual emissions of approximately 138,125 tons of CO2e at minimum and 163,441 tons at maximum. These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context states that the estimated 2025 annual carbon emissions of GPT-4o are projected to be approximately 138,125 to 163,441 tons of CO2e. It further states that these figures are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context states that the average data center PUE in 2023 was 1.58 globally.","1.58","PUE","[""ebert2024""]","is_blank","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context states that the average data center PUE in 2023 was 1.58 globally."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context states that JetMoE-8B has 8B total parameters, but only activates 2B parameters for each input token, reducing inference computation by about 70% compared to the 7B-parameter Llama2-7B model.","2000000000","parameters","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B has 8B total parameters, but only activates 2B parameters for each input token, reducing inference computation by about 70% compared to the 7B-parameter Llama2-7B model."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context states that the Llama 7B model has an estimated carbon emissions of 49.54 +/- 7.08 zettaFLOPs (a unit of computation) for its training process. Using the conversion factor provided in the context, this computes to approximately 49.54 tCO2e.","49.54","tCO2e","[""li2025a""]","is_blank","Model Cost (zettaFLOPs) Llama (7B) 49.54 ( ±7.08)","The context states that the Llama 7B model has an estimated carbon emissions of 49.54 +/- 7.08 zettaFLOPs (a unit of computation) for its training process. Using the conversion factor provided in the context, this computes to approximately 49.54 tCO2e."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context states that Amazon's U.S. fleet included 11,800 electric delivery vans from Rivian in 2023, up from more than 2,600 in 2022. Therefore, the total number of Amazon electric delivery vans added across 2022 and 2023 was 11,800 - 2,600 = 9,200.","9200","electric delivery vans","[""amazon2023""]","is_blank","Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.","The context states that Amazon's U.S. fleet included 11,800 electric delivery vans from Rivian in 2023, up from more than 2,600 in 2022. Therefore, the total number of Amazon electric delivery vans added across 2022 and 2023 was 11,800 - 2,600 = 9,200."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context states that operational environmental impacts of LLMs include 'the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.'","0","is_blank","[""morrison2025""]","[""https://arxiv.org/abs/2503.05804""]","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The context states that operational environmental impacts of LLMs include 'the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.'"
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","According to Table 4 in the context, the energy consumption of the o3 model for a long prompt (10k input, 1.5k output) is 12.222 Wh.","12.222","Wh","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh).","According to Table 4 in the context, the energy consumption of the o3 model for a long prompt (10k input, 1.5k output) is 12.222 Wh."
"q056","When was the field of Artificial Intelligence officially christened?","The context states that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy. This directly answers the question of when AI was officially christened.","1956","year","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The context states that the field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy. This directly answers the question of when AI was officially christened."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context states that according to Google's sustainability report, Google's data centers have an annualized global on-site water efficiency of 1 L/kWh, which can be interpreted as a Water Use Effectiveness (WUE) of 1 L/kWh.","1","WUE","[""li2025b""]","[""https://arxiv.org/abs/2304.03271""]","On average, depending on the weather conditions and operational settings, data centers can evaporate approximately 1 – 9 liters per kWh of server energy: 1 L/kWh for Google's annualized global on-site water efficiency [4].","The context states that according to Google's sustainability report, Google's data centers have an annualized global on-site water efficiency of 1 L/kWh, which can be interpreted as a Water Use Effectiveness (WUE) of 1 L/kWh."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context states that 'Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].'","1","is_blank","[""wu2021b""]","is_blank","'Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].'","The context states that 'Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].'"
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context states that at a maximum generation length of 512 tokens, the LLaMA-65B model consumed around 3-4 Joules per output token.","3.5","joules per token","[""samsi2024""]","is_blank","For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The context states that at a maximum generation length of 512 tokens, the LLaMA-65B model consumed around 3-4 Joules per output token."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context states that by converting the 32-bit floating-point numerical representation of the RM2 model to 16-bit, the overall model size was reduced by 15%.","15","percent","[""wu2021a""]","[""https://example.com/wu2021a""]","By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context states that by converting the 32-bit floating-point numerical representation of the RM2 model to 16-bit, the overall model size was reduced by 15%."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context indicates that the 5-10% GHG reduction claim by AI was based on a BCG report, but the underlying calculations and methodology are not clearly detailed. The context states that 'the reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG's experience in dealing with their clients and using AI to optimize and improve existing processes'.","is_blank","is_blank","[""luccioni2025c""]","is_blank","The context states that 'the reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG's experience in dealing with their clients and using AI to optimize and improve existing processes'.","The context indicates that the 5-10% GHG reduction claim by AI was based on a BCG report, but the underlying calculations and methodology are not clearly detailed. The context states that 'the reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG's experience in dealing with their clients and using AI to optimize and improve existing processes'."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context states that sparsely-activated Mixture-of-Expert-style models can provide more than 10X reductions in computation requirements and energy costs compared to dense Transformer or LSTM-based models of equivalent computational cost per token.","1","is_blank","[""patterson2021""]","is_blank","Sparsely   activated   mixture-of-expert-style   models    can   provide   more   than   10X   reductions   in   computation   requirements   and   energy   costs   for   both   training   and   inference   while   providing   significantly   higher   accuracy than   dense   Transformer   or   LSTM-based   models   of   equivalent   computational   cost   per   token   [Sha17,Lep20,Fed21].","The context states that sparsely-activated Mixture-of-Expert-style models can provide more than 10X reductions in computation requirements and energy costs compared to dense Transformer or LSTM-based models of equivalent computational cost per token."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context states that 'Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.' This directly answers the question.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context states that 'Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.' This directly answers the question."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context states that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, up to 53%, when conducting sparse fine-tuning with a batch size of 1.","53","percent","[""xia2024""]","is_blank","(1) The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1), while the execution time share of the optimizer stage in Mixtral fine-tuning is negligible.","The context states that the optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time, up to 53%, when conducting sparse fine-tuning with a batch size of 1."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context provides information about the average energy consumption for a comparable multi-purpose model (Flan-T5-xxl) at 0.083 kWh per 1,000 queries. Given that Google Translate can perform 1 billion queries per day, we can estimate the daily energy consumption as 83 MWh.","83","MWh","[""li2025b""]","is_blank","For the Falcon-180B model, which is comparable in size to GPT-3-175B, the server energy consumption reaches approximately 0.016 kWh per medium-sized request [30].","The context provides information about the average energy consumption for a comparable multi-purpose model (Flan-T5-xxl) at 0.083 kWh per 1,000 queries. Given that Google Translate can perform 1 billion queries per day, we can estimate the daily energy consumption as 83 MWh."
"q067","What was the average global data center PUE in 2023?","The context states that the average data center PUE in 2023 was 1.58 globally.","1.58","PUE","[""ebert2024""]","is_blank","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context states that the average data center PUE in 2023 was 1.58 globally."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The context does not contain any information about how many wind turbines Microsoft directly contracted to power Azure AI clusters in 2023. The context does not mention anything related to Microsoft's wind power contracts.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The context does not contain any information about how many wind turbines Microsoft directly contracted to power Azure AI clusters in 2023. The context does not mention anything related to Microsoft's wind power contracts."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context states that for the Gemini Ultra model, R&D staff costs (including equity) made up 49% of the total amortized model development costs.","49","percent","[""cottier2024""]","is_blank","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The context states that for the Gemini Ultra model, R&D staff costs (including equity) made up 49% of the total amortized model development costs."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The report states that the inaugural 2015 Study Panel comprised 17 members, including experts in AI, law, political science, policy, and economics.","17","people","[""stone2022""]","is_blank","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The report states that the inaugural 2015 Study Panel comprised 17 members, including experts in AI, law, political science, policy, and economics."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context states that 'manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices'. This directly answers the question about the percentage of a client device's total carbon footprint that is accounted for by manufacturing.","74","percent","[""wu2021a""]","is_blank","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices.","The context states that 'manufacturing carbon cost accounts for 74% of the total footprint [19] of client devices'. This directly answers the question about the percentage of a client device's total carbon footprint that is accounted for by manufacturing."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context indicates that larger models do not always consume more energy during inference. Specifically, it states 'Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows.' This suggests that parameter count alone does not determine energy consumption for inference.","is_blank","is_blank","[""chung2025""]","is_blank","Figure 4 highlights the case of Phi-3 Mini (3.8B) and Small (7B) [26]. Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows.","The context indicates that larger models do not always consume more energy during inference. Specifically, it states 'Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows.' This suggests that parameter count alone does not determine energy consumption for inference."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context indicates that the 2015 Study Panel 'found no cause for concern that AI is an imminent threat to humankind'. Thus, the statement that the Study Panel is concerned that AI is an imminent threat is false.","0","is_blank","[""stone2022""]","[""https://ai100.stanford.edu/2016-report/preface""]","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context indicates that the 2015 Study Panel 'found no cause for concern that AI is an imminent threat to humankind'. Thus, the statement that the Study Panel is concerned that AI is an imminent threat is false."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","Error: Could not connect to the endpoint URL: ""https://bedrock-runtime.us-east-2.amazonaws.com/model/us.anthropic.claude-3-haiku-20240307-v1%3A0/invoke""","is_blank","tCO2e","is_blank","is_blank","is_blank","Error: Could not connect to the endpoint URL: ""https://bedrock-runtime.us-east-2.amazonaws.com/model/us.anthropic.claude-3-haiku-20240307-v1%3A0/invoke"""
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context states that Meta reported their Llama 3 family of models emitted 11,390 tons CO2e, which is over 40x the 'five cars' estimate of 284 metric tons CO2e.","11390","tCO2e","[""luccioni2025c""]","is_blank","the context states that ""Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the 'five cars' estimate.""","The context states that Meta reported their Llama 3 family of models emitted 11,390 tons CO2e, which is over 40x the 'five cars' estimate of 284 metric tons CO2e."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context states that the explosive growth in AI use cases at Facebook drove a 2.9x increase in AI training infrastructure capacity over the recent 1.5 year period.","2.9","multiplier","[""wu2021a""]","is_blank","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","The context states that the explosive growth in AI use cases at Facebook drove a 2.9x increase in AI training infrastructure capacity over the recent 1.5 year period."
"q079","How many miles is the Earth from the Sun?","The context does not contain any information about the distance between the Earth and the Sun. The provided context is about data center energy efficiency regulations and reporting requirements. There is no relevant information to answer this question.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The context does not contain any information about the distance between the Earth and the Sun. The provided context is about data center energy efficiency regulations and reporting requirements. There is no relevant information to answer this question."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context states that the AlphaGo program defeated the current human champion at the game of Go, providing direct evidence that the statement is true.","1","is_blank","[""stone2022"", ""schwartz2019""]","is_blank","The context states that ""the AlphaGo program160 161 that recently defeated the current human champion at the game of Go used multiple machine learning algorithms for training itself, and also used a sophisticated search procedure while playing the game.""","The context states that the AlphaGo program defeated the current human champion at the game of Go, providing direct evidence that the statement is true."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context states that 'Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time'.","continuous batching","is_blank","[""fernandez2025""]","is_blank","Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context states that 'Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time'."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context states that 'The entire alignment process takes 60 H100 GPU hours'.","60","H100 GPU hours","[""shen2024""]","is_blank","The entire alignment process takes 60 H100 GPU hours.","The context states that 'The entire alignment process takes 60 H100 GPU hours'."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","According to the context, when the SLO requirement was 100 TPS, the Max-Performance policy selected the g6e.xlarge instance, which had a total cost of $2.699. The InferSave-1st policy selected the g4dn.xlarge instance, which had a total cost of $2.13. The difference between the two costs is about 26.7%, which is the percentage that the Max-Performance instance was more expensive.","26.7","percent","[""kim2025""]","is_blank","Given a SLO requirement of 100 TPS, InferSave selected g4dn.xlarge as its top choice, providing a throughput of about 160 TPS with the lowest total cost of $2.13. On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.","According to the context, when the SLO requirement was 100 TPS, the Max-Performance policy selected the g6e.xlarge instance, which had a total cost of $2.699. The InferSave-1st policy selected the g4dn.xlarge instance, which had a total cost of $2.13. The difference between the two costs is about 26.7%, which is the percentage that the Max-Performance instance was more expensive."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The provided context states that 'the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of CO2eq for 1,000 inferences'.","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of CO2eq for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51]","The provided context states that 'the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of CO2eq for 1,000 inferences'."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context states that a 2025 study found the GPU energy usage for 1,000 queries spanned from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity.","[0.06,3426]","Wh","[""luccioni2025c""]","is_blank","task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The context states that a 2025 study found the GPU energy usage for 1,000 queries spanned from 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context indicates that due to the many different types of AI approaches and contextual factors that influence their application, it is difficult to define universal or even generalizable guidelines for AI ethics and sustainability. The passage states that 'given the many different types of AI approaches that exist, as well as contextual factors that influence their application, it is difficult to define universal, or even generalizable, guidelines.'","1","is_blank","[""luccioni2025b""]","is_blank","Given the many different types of AI approaches that exist, as well as contextual factors that influence their application, it is difficult to define universal, or even generalizable, guidelines.","The context indicates that due to the many different types of AI approaches and contextual factors that influence their application, it is difficult to define universal or even generalizable guidelines for AI ethics and sustainability. The passage states that 'given the many different types of AI approaches that exist, as well as contextual factors that influence their application, it is difficult to define universal, or even generalizable, guidelines.'"
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","According to the context in [ref_id=patterson2021], the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","According to the context in [ref_id=patterson2021], the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context mentions that Hivemind is a PyTorch-based framework developed to enable collaborative deep learning training on heterogeneous hardware, including handling peers that drop out during training. This aligns with the question about a decentralized PyTorch-based framework used for distributed spot instance training across clouds and continents.","Hivemind","is_blank","[""erben2023""]","[""https://arxiv.org/abs/2306.03163""]","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion.","The context mentions that Hivemind is a PyTorch-based framework developed to enable collaborative deep learning training on heterogeneous hardware, including handling peers that drop out during training. This aligns with the question about a decentralized PyTorch-based framework used for distributed spot instance training across clouds and continents."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context states that 'the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems'.","social transparency","is_blank","[""luccioni2025b""]","is_blank","In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems [56].","The context states that 'the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems'."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context states that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings, indicating that the linear model with sentence embeddings is the top performing model.","is_blank","is_blank","[""zschache2025""]","[""https://arxiv.org/abs/2508.14170v1""]","The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.","The context states that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings, indicating that the linear model with sentence embeddings is the top performing model."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context states that the paper develops and deploys 'Lamina', a distributed heterogeneous LLM inference system that incorporates model-attention disaggregation.","Lamina","is_blank","[""chen2024""]","is_blank","To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The context states that the paper develops and deploys 'Lamina', a distributed heterogeneous LLM inference system that incorporates model-attention disaggregation."
"q093","How many parameters does the largest T5 model have?","The context states that the largest T5 model has 11B parameters, so the answer_value matches this information.","11000000000","parameters","[""patterson2021""]","is_blank","The largest size of T5 has 11B parameters, and training used 86 MWh and produced 47 tCO2e.","The context states that the largest T5 model has 11B parameters, so the answer_value matches this information."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context states that the JetMoE-8B model has 8B total parameters.","8000000000","parameters","[""shen2024""]","is_blank","Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp
8B 2B 24 2048 8 2 16 128 5632","The context states that the JetMoE-8B model has 8B total parameters."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context states that Google's data center water consumption increased by ~20% from 2021 to 2022.","20","percent","[""li2025b""]","is_blank","Importantly, the company's data center water consumption increased by∼20% from 2021 to 2022 and by∼17% from 2022 to 2023 [4].","The context states that Google's data center water consumption increased by ~20% from 2021 to 2022."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context defines 'Carbon Intensity' as the metric for CO₂ emissions per unit of electricity consumed.","is_blank","is_blank","[""khan2025""]","is_blank","is_blank","The context defines 'Carbon Intensity' as the metric for CO₂ emissions per unit of electricity consumed."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context states that in the 101B stage of FLM-101B training, the FLOPs utilization rate was 52.88%.","52.88","percent","[""li2025a""]","is_blank","Table 2: Params Tensor Pipeline Data Number Batch teraFLOP/s FLOPs
(billion) Parallel Size Parallel Size Parallel Size of GPUs Size per GPU Utilization
101 4 4 12 192 2160 165 52.88%","The context states that in the 101B stage of FLM-101B training, the FLOPs utilization rate was 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context states that the estimated amortized training cost for OpenAI's GPT-4 model was $40M.","40000000","USD","[""cottier2024""]","is_blank","We find that the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at $40M and Google's Gemini Ultra at $30M.","The context states that the estimated amortized training cost for OpenAI's GPT-4 model was $40M."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","According to the context, full-stack optimization including platform-level caching, GPU acceleration, and algorithmic changes can reduce the operational power footprint of the Transformer-based universal translation model by more than 810x compared to a CPU server baseline.","810","multiplier","[""wu2021a""]","is_blank","For the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than 800× using platform-level caching, GPUs, low precision data format, and additional algorithmic optimization.","According to the context, full-stack optimization including platform-level caching, GPU acceleration, and algorithmic changes can reduce the operational power footprint of the Transformer-based universal translation model by more than 810x compared to a CPU server baseline."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context states that when training was spread across four continents (experiment C-4) instead of remaining local (experiment A-4), the NLP throughput was 36% slower compared to the local setup.","0.64","multiplier","[""erben2023""]","is_blank","The context states: ""As we know from the transatlantic experiments(B) that an additional continent has a detrimental effect on throughput, which, for the four continents experiment, C-4, results in a ... 36% slower for NLP compared to the A-4 runs (Figure 7a).""","The context states that when training was spread across four continents (experiment C-4) instead of remaining local (experiment A-4), the NLP throughput was 36% slower compared to the local setup."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities.","3500000000","liters","[""amazon2023""]","[""https://www.aboutamazon.com/sustainability/2023-sustainability-report.pdf""]","In 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities.","The context states that in 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context indicates that using custom tags with the one-shot, zero-shot, and few-shots prompt engineering techniques can reduce the energy consumption of LLMs for source code completion tasks. This is supported by the statements 'Our initial results show that the energy consumption of LLMs can be reduced by using specific tags that distinguish different prompt parts' and 'Our findings reveal that the energy consumption of LLMs for the inference phase can be reduced by using the introduced custom tags.'","1","is_blank","[""rubei2025""]","[""https://arxiv.org/abs/2501.05899""]","is_blank","The context indicates that using custom tags with the one-shot, zero-shot, and few-shots prompt engineering techniques can reduce the energy consumption of LLMs for source code completion tasks. This is supported by the statements 'Our initial results show that the energy consumption of LLMs can be reduced by using specific tags that distinguish different prompt parts' and 'Our findings reveal that the energy consumption of LLMs for the inference phase can be reduced by using the introduced custom tags.'"
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context states that 'NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite improvements in efficiency'.","3760000","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","[105] Agam Shah. 2024. Nvidia Shipped 3.76 Million Data-center GPUs in 2023, According to Study. https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/","The context states that 'NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite improvements in efficiency'."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context states that on average, 44% of the amortized hardware CapEx + energy cost goes toward AI accelerator chips.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context states that on average, 44% of the amortized hardware CapEx + energy cost goes toward AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context states that Facebook's data centers achieve a Power Usage Effectiveness (PUE) of about 1.10, which is about 40% more efficient than small-scale, typical data centers.","1.1","PUE","[""wu2021a""]","is_blank","Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook's data centers are about 40% more efﬁcient than small-scale, typical data centers.","The context states that Facebook's data centers achieve a Power Usage Effectiveness (PUE) of about 1.10, which is about 40% more efficient than small-scale, typical data centers."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context states that the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems.","ETAIROS","is_blank","[""luccioni2025b""]","[""https://arxiv.org/abs/2504.00797""]","The context states: ""From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainabilit y, design and foresight for inter-disciplinary governance of AI systems [133].""","The context states that the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context states that the estimated amortized training cost for Google's Gemini Ultra model was $30 million. This is based on the analysis presented in the paper which estimates the hardware acquisition and energy costs for frontier AI models.","30000000","USD","[""cottier2024""]","is_blank","is_blank","The context states that the estimated amortized training cost for Google's Gemini Ultra model was $30 million. This is based on the analysis presented in the paper which estimates the hardware acquisition and energy costs for frontier AI models."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context indicates that the AI Act requires providers of GPAI models with systemic risk to conduct risk assessments, and that these risk assessments should include environmental risks. Specifically, the context states: 'For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.'","1","is_blank","[""ebert2024""]","is_blank","The AI Act contains some language to this effect. For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.","The context indicates that the AI Act requires providers of GPAI models with systemic risk to conduct risk assessments, and that these risk assessments should include environmental risks. Specifically, the context states: 'For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.'"
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m3, which is considerably higher than the WHO's recommended level of 5µg/m3.","9","µg/m³","[""han2024""]","is_blank","In fact, the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m 3, considerably higher than the WHO's recommended level of 5µg/m3 [48, 52].","The context states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m3, which is considerably higher than the WHO's recommended level of 5µg/m3."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context states that a life cycle assessment found that 115 print books would produce the same amount of CO2 as a single Amazon Kindle device.","115","books","[""luccioni2025a""]","is_blank","A life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","The context states that a life cycle assessment found that 115 print books would produce the same amount of CO2 as a single Amazon Kindle device."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context states that the ratio of the highest county-level per-household health cost to the lowest cost is approximately 200. This indicates that some low-income communities bear a health burden up to 200 times higher than less affected areas.","200","multiplier","[""han2024""]","is_blank","For example, as shown in Table 6c, all the top-10 most impacted counties in the U.S. have lower median household incomes than the national median value. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.","The context states that the ratio of the highest county-level per-household health cost to the lowest cost is approximately 200. This indicates that some low-income communities bear a health burden up to 200 times higher than less affected areas."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The context states that the DS Llama 70B model consumed 702.06 Wh of energy for inference on the FKTG dataset.","702.06","Wh","[""zschache2025""]","is_blank","T able B1 Measurements of all models for the inference task on the FKTG dataset, Capella system, single node, shown are averages over 10 runs","The context states that the DS Llama 70B model consumed 702.06 Wh of energy for inference on the FKTG dataset."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","According to the context, the paper by Dodge et al. analyzed the OpenAI openGPT2-XL model, which contains 1.5 billion parameters.","1500000000","parameters","[""schwartz2019""]","is_blank","openAI's openGPT2-XL model [30] contains 1.5 billion parameters.","According to the context, the paper by Dodge et al. analyzed the OpenAI openGPT2-XL model, which contains 1.5 billion parameters."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context describes the Jevons Paradox, where improved efficiency in a resource leads to increased overall consumption of that resource, rather than decreased consumption. This is explained as a key phenomenon that undermines the assumption that increased efficiency alone will reduce environmental impacts.","Jevons' Paradox","is_blank","[""luccioni2025a"", ""jegham2025"", ""morrison2025""]","is_blank","Similarly to Jevons' Paradox, just because an AI model becomes more efficient, that does not imply that overall AI resource consumption will decrease, and in fact the inverse effect is highly plausible.","The context describes the Jevons Paradox, where improved efficiency in a resource leads to increased overall consumption of that resource, rather than decreased consumption. This is explained as a key phenomenon that undermines the assumption that increased efficiency alone will reduce environmental impacts."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context states that the training of GPT-3 consumed 1,287 MWh of energy, while the training of Meena consumed 232 MWh. Therefore, the GPT-3 training run consumed around 5.5 times more energy than the Meena training run.","5.5","multiplier","[""patterson2021""]","is_blank","GPT-3 training consumed 1,287 MWh, while Meena training consumed 232 MWh.","The context states that the training of GPT-3 consumed 1,287 MWh of energy, while the training of Meena consumed 232 MWh. Therefore, the GPT-3 training run consumed around 5.5 times more energy than the Meena training run."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","According to Table 2 in the 2024 study by Luccioni et al., the mean energy consumption for image generation tasks is 2.907 kWh per 1,000 inferences.","2.907","kWh","[""luccioni2024""]","is_blank","From Table 2: 'image generation 2.907 3.31'","According to Table 2 in the 2024 study by Luccioni et al., the mean energy consumption for image generation tasks is 2.907 kWh per 1,000 inferences."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context provides the carbon footprint associated with an average American life in one year as 36,156 lbs CO2e.","36156","lbs","[""strubell2019""]","[""https://arxiv.org/abs/1906.02243""]","American life, avg, 1 year 36,156","The context provides the carbon footprint associated with an average American life in one year as 36,156 lbs CO2e."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context shows that after optimization, Mistral-small's carbon emissions decreased from 0.020 kg per inference to 0.015 kg per inference, which is a reduction by a factor of approximately 0.75.","0.75","multiplier","[""khan2025""]","is_blank","Table III shows that before optimization, Mistral-small had carbon emissions of 0.020 kg per inference, and after optimization, the emissions decreased to 0.015 kg per inference.","The context shows that after optimization, Mistral-small's carbon emissions decreased from 0.020 kg per inference to 0.015 kg per inference, which is a reduction by a factor of approximately 0.75."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","According to the context, the combined training and fine-tuning energy costs for the BLOOMz-7B model were 51,686 kWh + 7,571 kWh = 59,257 kWh.","59257","kWh","[""luccioni2024""]","is_blank","BLOOMz-7B
Training energy (kWh) 51,686
Finetuning energy (kWh) 7,571","According to the context, the combined training and fine-tuning energy costs for the BLOOMz-7B model were 51,686 kWh + 7,571 kWh = 59,257 kWh."
"q125","What is the total number of parameters in the final FLM-101B model?","The context states that the FLM-101B model is structured with 101 billion parameters.","101","parameters","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context states that the FLM-101B model is structured with 101 billion parameters."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context states that a model with over 6.1 billion parameters was trained on 256 NVIDIA A100s for 8 days, consuming a total of 13,812.4 kWh of energy. It further estimates that the full training run would take 60 days, which would consume approximately 103,593 kWh. Comparing this to the 51,686 kWh required to train the BLOOMz-7B model, the context suggests that over 592 million inferences would be needed to match the training energy cost.","592570000","inferences","[""dodge2022"", ""luccioni2024""]","[""https://dl.acm.org/doi/10.1145/3531182.3534054"", ""https://dl.acm.org/doi/10.1145/3453477.3556557""]","['We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh.', 'BLOOMz-7B Training energy (kWh) 51,686 Cost parity (# inferences) 592,570,000']","The context states that a model with over 6.1 billion parameters was trained on 256 NVIDIA A100s for 8 days, consuming a total of 13,812.4 kWh of energy. It further estimates that the full training run would take 60 days, which would consume approximately 103,593 kWh. Comparing this to the 51,686 kWh required to train the BLOOMz-7B model, the context suggests that over 592 million inferences would be needed to match the training energy cost."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context states that the total amount of energy consumed for all model experimentation and evaluation was 754.66 kWh and total carbon emissions were 178.97 kg CO2eq.","754.66","kWh","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.","The context states that the total amount of energy consumed for all model experimentation and evaluation was 754.66 kWh and total carbon emissions were 178.97 kg CO2eq."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","According to Table 5 in the context, for the BLOOMz-7B model, the training energy cost was 51,686 kWh and the fine-tuning energy cost was 7,571 kWh, for a total of 59,257 kWh. The inference energy per instance was 1.0 × 10^-4 kWh. Therefore, the number of inferences required for the cumulative inference energy cost to equal the initial training and fine-tuning cost is 59,257 kWh / 1.0 × 10^-4 kWh per inference = 592,570,000 inferences.","592570000","inferences","[""luccioni2024""]","is_blank","Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.","According to Table 5 in the context, for the BLOOMz-7B model, the training energy cost was 51,686 kWh and the fine-tuning energy cost was 7,571 kWh, for a total of 59,257 kWh. The inference energy per instance was 1.0 × 10^-4 kWh. Therefore, the number of inferences required for the cumulative inference energy cost to equal the initial training and fine-tuning cost is 59,257 kWh / 1.0 × 10^-4 kWh per inference = 592,570,000 inferences."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context states that the data of the study originates from the process of selecting a repository site for high-level radioactive waste in Germany, and that the statements from the population were categorized, processed and published as the FKTG-dataset.","FKTG-dataset","is_blank","[""zschache2025""]","is_blank","The data of our study originates from the process of selecting a repository site for high-level radioactive waste in Germany. During the first phase, sub-areas were identified and discussed in a process called FKTG (Fachkonferenz Teilgebiete). The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).","The context states that the data of the study originates from the process of selecting a repository site for high-level radioactive waste in Germany, and that the statements from the population were categorized, processed and published as the FKTG-dataset."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide any information about the water consumption of Meta's Llama 3 inference serving clusters in 2024. There are no details about the water footprint of this specific model.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the water consumption of Meta's Llama 3 inference serving clusters in 2024. There are no details about the water footprint of this specific model."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context states that NVIDIA H100 GPUs are assumed to be 0.1% rare earth metal by mass. This means 0.1% of the materials used to manufacture H100 GPUs in 2024 were recycled rare earth metals.","0.1","percent","[""morrison2025""]","is_blank","We additionally estimate the environmental impact from mining rare earth metals used during man-ufacturing, assuming an H100 is 0.1% rare earth metal by mass.","The context states that NVIDIA H100 GPUs are assumed to be 0.1% rare earth metal by mass. This means 0.1% of the materials used to manufacture H100 GPUs in 2024 were recycled rare earth metals."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context states that the actual CO2e for the Evolved Transformer NAS was 3.2 tCO2e. It also provides information that a single passenger round trip from San Francisco to New York emits approximately 1.2 tCO2e. Therefore, the 3.2 tCO2e from the Evolved Transformer NAS is equivalent to approximately 3 passengers taking that round trip.","3","passengers","[""patterson2021""]","is_blank","To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).","The context states that the actual CO2e for the Evolved Transformer NAS was 3.2 tCO2e. It also provides information that a single passenger round trip from San Francisco to New York emits approximately 1.2 tCO2e. Therefore, the 3.2 tCO2e from the Evolved Transformer NAS is equivalent to approximately 3 passengers taking that round trip."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","According to the context, the May 2025 data from OpenRouter indicated that 84% of LLM token usage occurred through models that did not disclose their environmental impact.","84","percent","[""luccioni2025c""]","is_blank","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","According to the context, the May 2025 data from OpenRouter indicated that 84% of LLM token usage occurred through models that did not disclose their environmental impact."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that the bare minimum hardware requirements for LLaMA-13B are 2 NVIDIA V100 32GB GPUs, or 1 NVIDIA A100 80GB GPU. Since the question asks specifically about the A100 80GB GPU, the answer is 1.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Table II: Model Size V100 32GB A100 80GB
Count Max. Batch size
13B 2 64 1 64","The context states that the bare minimum hardware requirements for LLaMA-13B are 2 NVIDIA V100 32GB GPUs, or 1 NVIDIA A100 80GB GPU. Since the question asks specifically about the A100 80GB GPU, the answer is 1."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context indicates that training a large 213M parameter transformer model can produce an estimated 626,155 lbs (284 metric tons) of CO2 emissions. This represents a complete training run including model development, so it can be considered a reasonable estimate for a 6.1 billion parameter transformer model as well.","[284, 284]","metric tons","[""strubell2019""]","is_blank","Strubell et al. (2019) estimated that the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2.","The context indicates that training a large 213M parameter transformer model can produce an estimated 626,155 lbs (284 metric tons) of CO2 emissions. This represents a complete training run including model development, so it can be considered a reasonable estimate for a 6.1 billion parameter transformer model as well."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","is_blank","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","is_blank"
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context states that using 2 A100s and 1 A10G GPU results in a 24% cost saving over an A100-only strategy for serving the Llama2-7b model.","24","percent","[""griggs2024""]","[""https://arxiv.org/abs/2306.00900""]","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context states that using 2 A100s and 1 A10G GPU results in a 24% cost saving over an A100-only strategy for serving the Llama2-7b model."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context states that the H20 price per chip is estimated to be $4.63/hr, as the H20 is not readily available on cloud service providers.","4.63","USD per hour","[""chen2024""]","is_blank","Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr*: As H20 is not readily available on cloud service providers, the listed price is estimated using the relative complete system cost against H100.","The context states that the H20 price per chip is estimated to be $4.63/hr, as the H20 is not readily available on cloud service providers."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context indicates that while tools like CodeCarbon exist to automatically measure AI model emissions, the overall trend in the industry has been towards less transparency, with many models providing no information about their environmental impact. The context does not support the claim that most analyses gather information automatically without needing to contact authors.","0","is_blank","[""luccioni2025c""]","[""https://arxiv.org/abs/2302.08476""]","is_blank","The context indicates that while tools like CodeCarbon exist to automatically measure AI model emissions, the overall trend in the industry has been towards less transparency, with many models providing no information about their environmental impact. The context does not support the claim that most analyses gather information automatically without needing to contact authors."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context states that in 2023, the public health cost of U.S. data centers was about $6.7 billion, which was equivalent to approximately 44% of the data centers' total electricity cost.","44","percent","[""han2024""]","is_blank","Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023. This is equivalent to approximately 44% of the data centers' total electricity cost.","The context states that in 2023, the public health cost of U.S. data centers was about $6.7 billion, which was equivalent to approximately 44% of the data centers' total electricity cost."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that the bare minimum hardware required for the LLaMA-7B model is 1 NVIDIA A100 80GB GPU.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Table II lists the bare minimum hardware requirements for each LLaMA variant, showing that the LLaMA-7B model requires 1 NVIDIA A100 80GB GPU.","The context states that the bare minimum hardware required for the LLaMA-7B model is 1 NVIDIA A100 80GB GPU."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context states that the optimizations using quantization techniques can reduce energy consumption and carbon emissions by up to 45% for large language models.","1","is_blank","[""khan2025""]","[""https://arxiv.org/abs/2504.06307""]","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.","The context states that the optimizations using quantization techniques can reduce energy consumption and carbon emissions by up to 45% for large language models."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers.","95","answers","[""luccioni2025b""]","is_blank","For instance, Luccioni and Hernandez-Garcia re ached out to over 500 authors of AI papers to get infor mation needed to estimate the carbon footprint of their mode ls, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citi ng privacy concerns and lack of experimental logs [2023].","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context states that JetMoE-8B was trained with a limited $100k budget and used 30,000 H100 GPU hours. Assuming the budget was spent entirely on the H100 GPU hours, the approximate cost per H100 GPU-hour can be calculated as $100k / 30,000 hours = $3.33 per hour.","3.33","USD per hour","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context states that JetMoE-8B was trained with a limited $100k budget and used 30,000 H100 GPU hours. Assuming the budget was spent entirely on the H100 GPU hours, the approximate cost per H100 GPU-hour can be calculated as $100k / 30,000 hours = $3.33 per hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context states that when training an AI model of the Llama-3.1 scale in Iowa, the total health cost can even exceed 120% of the electricity cost.","120","percent","[""han2024""]","is_blank","For example, the total health cost is only$0.23 million in Oregon, whereas the cost will increase dramatically to$2.5 million in Iowa due to various factors, such as the wind direction and the pollutant emission rate for electricity generation [76]. It can be seen that the total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations.","The context states that when training an AI model of the Llama-3.1 scale in Iowa, the total health cost can even exceed 120% of the electricity cost."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context states that JetMoE-8B was trained on 1.25T tokens from mixed open-source datasets, which matches the expected 'tokens' answer unit.","1250000000000","tokens","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context states that JetMoE-8B was trained on 1.25T tokens from mixed open-source datasets, which matches the expected 'tokens' answer unit."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context states that Amazon had 36 renewable energy projects in the United Kingdom as of January 2024.","36","projects","[""amazon2023""]","is_blank","The table 'Amazon Renewable Energy Projects' shows that Amazon had 36 renewable energy projects in the United Kingdom.","The context states that Amazon had 36 renewable energy projects in the United Kingdom as of January 2024."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context provided data on the gender breakdown of Amazon's U.S. workforce across all levels. According to the data, 55.6% of Amazon's U.S. workforce identified as men in 2023.","55.6","percent","[""amazon2023""]","is_blank","U.S. Gender—U.S. Amazon Workforce (All Levels): 44.3% women, 55.6% men, 0.1% other.","The context provided data on the gender breakdown of Amazon's U.S. workforce across all levels. According to the data, 55.6% of Amazon's U.S. workforce identified as men in 2023."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context states that 'Apple reports that its supply chain accounts for 99% of its total water footprint [23].'","99","percent","[""li2025b""]","is_blank","Apple reports that its supply chain accounts for 99% of its total water footprint [23].","The context states that 'Apple reports that its supply chain accounts for 99% of its total water footprint [23].'"
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","According to the execution time breakdown shown in Figure 4, for a sparse BlackMamba model with a batch size of 84, the total execution time is approximately 1.9 seconds.","1.9","seconds","[""xia2024""]","is_blank","Dense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)
0.0
0.5
1.0
1.5
2.0
Execution Time
Breakdown (seconds)
Mamba","According to the execution time breakdown shown in Figure 4, for a sparse BlackMamba model with a batch size of 84, the total execution time is approximately 1.9 seconds."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context states that the authors introduced 'the granularity metric, the ratio of calculation to communication time' to compare model suitability for distributed spot training and estimate performance with additional resources.","granularity","is_blank","[""erben2023""]","[""https://doi.org/10.14778/3648160.3648165""]","We propose the granularity metric to compare model suitability for distributed spot training and estimate training performance with additional spot VMs.","The context states that the authors introduced 'the granularity metric, the ratio of calculation to communication time' to compare model suitability for distributed spot training and estimate performance with additional resources."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context states that a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil to expand oil production could add up to '640 percent more carbon emissions compared to the company's carbon removal targets for the year'.","640","times","[""luccioni2025a""]","is_blank","For instance, a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year [119], yet these numbers were not included in the company's carbon accounting and reporting efforts [118].","The context states that a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil to expand oil production could add up to '640 percent more carbon emissions compared to the company's carbon removal targets for the year'."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context states that 'water withdrawal' refers to freshwater taken from ground or surface water sources, either temporarily or permanently, for agricultural, industrial, or municipal uses.","is_blank","is_blank","[""li2025b""]","is_blank","Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either tem-porarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","The context states that 'water withdrawal' refers to freshwater taken from ground or surface water sources, either temporarily or permanently, for agricultural, industrial, or municipal uses."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that 'the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.'","5","years","[""stone2022""]","is_blank","As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","The context states that 'the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.'"
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context states that 'In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].'","25","devices","[""wu2021b""]","[""https://arxiv.org/abs/2108.06738""]","In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].","The context states that 'In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].'"
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides estimates for the energy consumption of training a 6.1 billion parameter language model for 192 hours on 256 NVIDIA A100 GPUs, which consumed 13.8 MWh. The context also states that this was only 13% of the full training, and estimates the full training would consume approximately 103.5 MWh.","[13.8,103.5]","MWh","[""dodge2022""]","is_blank","The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) * 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The context provides estimates for the energy consumption of training a 6.1 billion parameter language model for 192 hours on 256 NVIDIA A100 GPUs, which consumed 13.8 MWh. The context also states that this was only 13% of the full training, and estimates the full training would consume approximately 103.5 MWh."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context clearly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011, so the statement 'IBM's Watson program did NOT beat human contenders in the Jeopardy challenge' is false.","0","is_blank","[""stone2022""]","is_blank","IBM's Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","The context clearly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011, so the statement 'IBM's Watson program did NOT beat human contenders in the Jeopardy challenge' is false."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","According to the context, one paper suggests that 10-50 queries on GPT-3 consume around half a liter of water.","[10,50]","queries","[""luccioni2025a""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","According to the context, one paper suggests that 10-50 queries on GPT-3 consume around half a liter of water."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context states that 'JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, demonstrating its superior performance.'","6.681","score","[""shen2024""]","is_blank","JetMoE
Model MT-Bench Score
GPT-4 9.014
GPT-3.5-turbo 7.995
Claude-v1 7.923
JetMoE-8B-chat 6.681
Llama-2-13b-chat 6.650
Vicuna-13b-v1.3 6.413
Wizardlm-13b 6.353
Llama-2-7b-chat 6.269","The context states that 'JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, demonstrating its superior performance.'"
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context provides an estimate of GPT-3's water consumption per medium-length request as 16.904 mL, which is the total on-site and off-site water consumed per request. Since a 500 mL bottle of water contains 500 mL, the number of requests that could be generated with the water in one 500 mL bottle is 500 mL / 16.904 mL per request = 29.6 requests.","29.6","responses","[""li2025b""]","is_blank","Table 1: GPT-3 requires 16.904 mL of total water per medium-length request.","The context provides an estimate of GPT-3's water consumption per medium-length request as 16.904 mL, which is the total on-site and off-site water consumed per request. Since a 500 mL bottle of water contains 500 mL, the number of requests that could be generated with the water in one 500 mL bottle is 500 mL / 16.904 mL per request = 29.6 requests."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The passage states that for the short-context Arena dataset, Mélange achieves a 15-77% cost reduction compared to using a single GPU type.","[15,77]","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO) for the Arena dataset.","The passage states that for the short-context Arena dataset, Mélange achieves a 15-77% cost reduction compared to using a single GPU type."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that 'at a minimum, 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model'.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","The context states 'For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.'","The context states that 'at a minimum, 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model'."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","According to the context, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.","10000","round trips","[""han2024""]","is_blank","Moreover, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City (LA-NYC), resulting in a health cost that even exceeds 120% of the training electricity cost.","According to the context, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context states that 'NVIDIA estimated that 80–90% of the ML workload is inference processing' according to the [Leo19] reference.","[80,90]","percent","[""patterson2021""]","is_blank","For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].","The context states that 'NVIDIA estimated that 80–90% of the ML workload is inference processing' according to the [Leo19] reference."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The context states that the 'Power Hungry Processing' study found the total carbon emissions were 493 metric tons of CO2 equivalent.","493000","kg CO2eq","[""morrison2025""]","is_blank","When accounting for hardware manufacturing, model development, and our final training runs, we find that our series of models released 493 metric tons of carbon emissions, equivalent to powering about 98 homes in the United States for one year, and consumed 2.769 million liters of water , equivalent to about 24.5 years of water usage by a person in the United States, even though our data center is extremely water-efficient.","The context states that the 'Power Hungry Processing' study found the total carbon emissions were 493 metric tons of CO2 equivalent."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context indicates that estimating GPU energy consumption based on Thermal Design Power (TDP) is not a reliable or accurate method. It states that 'Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time.'","0","is_blank","[""chung2025""]","is_blank","The dimmed bars behind each solid bar in Figure 3 show the estimated energy consumption based on the GPU's Thermal Design Power (TDP) instead of measuring the real GPU power consumption, which is a common practice [8, 9, 28, 40, 47, 74]. Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time.","The context indicates that estimating GPU energy consumption based on Thermal Design Power (TDP) is not a reliable or accurate method. It states that 'Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time.'"
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context indicates that GPT-4o mini consumes approximately 20% more energy than the larger GPT-4o model on long queries due to the use of older A100 GPU nodes, rather than the more efficient H100 nodes used by GPT-4o.","0","is_blank","[""jegham2025""]","[""https://www.example.com/jegham2025""]","For instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes.","The context indicates that GPT-4o mini consumes approximately 20% more energy than the larger GPT-4o model on long queries due to the use of older A100 GPU nodes, rather than the more efficient H100 nodes used by GPT-4o."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context shows that for Mixtral-CS on A100-40GB GPUs, the throughput when the batch size is 1 is 0.3 queries/second.","0.3","queries/sec","[""xia2024""]","is_blank","Mixtral-CS0.0
0.5
1.0
1.5
2.0
0.3 0.5 0.3 0.7
1.7
Dense(bsz=1)","The context shows that for Mixtral-CS on A100-40GB GPUs, the throughput when the batch size is 1 is 0.3 queries/second."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, it then describes a notable reversal in this trend after 2022, with the majority of notable AI models again falling under the 'no disclosure' category by the first quarter of 2025 as commercial and proprietary models increased.","is_blank","is_blank","[""luccioni2025c""]","is_blank","However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the ""no disclosure"" category, as the line between research and commercial deployment became increasingly blurred.","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, it then describes a notable reversal in this trend after 2022, with the majority of notable AI models again falling under the 'no disclosure' category by the first quarter of 2025 as commercial and proprietary models increased."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context states that the Griggs et al. (2024) paper used an on-demand H100 GPU price of $7.5164 per hour, normalized from the RunPod price to match the pricing structures of major cloud providers.","7.5164","USD per hour","[""griggs2024""]","is_blank","In each experiment, we serve Llama2-7b [44] with vLLM 0.2.7 [19]. [...] To ensure fair cost comparisons, we normalize RunPod's H100 pricing to match the pricing structures of major platforms. We calculate this by comparing RunPod's H100 cost ($4.69) to RunPod's A100-80G cost ($2.29), then adjusting relative to the A100's price on major clouds ($3.67), resulting in a normalized price of (4.69/2.29) × 3.67 = $7 .516 for H100.","The context states that the Griggs et al. (2024) paper used an on-demand H100 GPU price of $7.5164 per hour, normalized from the RunPod price to match the pricing structures of major cloud providers."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context states that GPT-3's training consumed over 700 kiloliters (kL) of water for cooling. Since the question asks about the water used during GPT-4 training, and the context does not provide any information about GPT-4's water consumption, I cannot confidently determine the answer.","is_blank","liters of water","[""is_blank""]","is_blank","is_blank","The context states that GPT-3's training consumed over 700 kiloliters (kL) of water for cooling. Since the question asks about the water used during GPT-4 training, and the context does not provide any information about GPT-4's water consumption, I cannot confidently determine the answer."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms. Therefore, the estimated hourly cost can be calculated by dividing the monthly cost by 30 days and 24 hours per day.","7.22","USD per hour","[""griggs2024""]","is_blank","The substantial size and computational demands of LLMs require the use of costly hardware accelerators, typically GPUs2 For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms. Therefore, the estimated hourly cost can be calculated by dividing the monthly cost by 30 days and 24 hours per day."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","According to the context, to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task requires a model 1,000 times larger in size.","1000","multiplier","[""wu2021a""]","[""https://arxiv.org/abs/2103.15591""]","Figure 2(a) depicts the 1000× model size increase for GPT3-based language translation tasks.","According to the context, to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task requires a model 1,000 times larger in size."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The first context snippet states that the training and fine-tuning of a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, which is similar to the lifetime emissions of five US cars.","626155","miles","[""luccioni2023""]","is_blank","The first paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars.","The first context snippet states that the training and fine-tuning of a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, which is similar to the lifetime emissions of five US cars."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The context states that the BLOOMz-7B model has been downloaded 606,096 times as of November 2023. The inference energy for this model is reported as 1.0 × 10^-4 kWh per inference. Assuming each download results in 1 million inferences, the total energy consumption would be 606,096 * 1,000,000 * 1.0 × 10^-4 kWh = 60.61 MWh.","60.61","MWh","[""luccioni2024""]","is_blank","BLOOMz-7B: Inference energy (kWh) = 1.0 × 10^-4","The context states that the BLOOMz-7B model has been downloaded 606,096 times as of November 2023. The inference energy for this model is reported as 1.0 × 10^-4 kWh per inference. Assuming each download results in 1 million inferences, the total energy consumption would be 606,096 * 1,000,000 * 1.0 × 10^-4 kWh = 60.61 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context states that the JetMoE-8B model was trained using 30,000 H100 GPU hours.","30000","H100 GPU hours","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context states that the JetMoE-8B model was trained using 30,000 H100 GPU hours."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context states that the growth rate in training cost for frontier AI models is 2.4x per year. Using this growth rate, the paper extrapolates that the most expensive publicly announced model by 2027 will cost about $1 billion.","1000000000","USD","[""cottier2024""]","is_blank","This implies that, at a growth rate of 2.4× per year, the most expensive publicly announced model by the start of 2027 will cost about $1 billion.","The context states that the growth rate in training cost for frontier AI models is 2.4x per year. Using this growth rate, the paper extrapolates that the most expensive publicly announced model by 2027 will cost about $1 billion."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context states that 'OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20]'.","3.14e+23","FLOPS","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context states that 'OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20]'."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","According to the context, the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization is 8.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","Table II: This table lists the bare minimum hardware required for different LLaMA models, showing that the LLaMA-65B model requires 8 V100 32GB GPUs.","According to the context, the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization is 8."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","Based on the parallel strategies and throughput data in Table 2 for the 101B training stage, the total computational work can be estimated as 165 teraFLOPS/GPU for 12 GPUs, which equates to 1.98 zettaFLOPs.","1.98","zettaFLOPs","[""li2025a""]","is_blank","Table 2: Params (billion) = 101, Tensor Parallel Size = 4, Pipeline Parallel Size = 4, Data Parallel Size = 12, Batch Size = 192, teraFLOP/s per GPU = 165, FLOPs Utilization = 52.88%.","Based on the parallel strategies and throughput data in Table 2 for the 101B training stage, the total computational work can be estimated as 165 teraFLOPS/GPU for 12 GPUs, which equates to 1.98 zettaFLOPs."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context states that AlexNet achieved 57.1% top-1 accuracy on the ImageNet dataset in 2012.","57.1","percent","[""schwartz2019""]","is_blank","As discussed in Section 2, AlexNet [20] in 2012 had a top-1 accuracy of 57.1% on the ImageNet dataset [6].","The context states that AlexNet achieved 57.1% top-1 accuracy on the ImageNet dataset in 2012."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that the FLM-101B model was trained on a cluster of 24 DGX-A800 GPU servers, with 8 NVIDIA A800 GPUs per server. Therefore, the total number of A800 GPUs used for training the FLM-101B model is 24 servers * 8 GPUs/server = 192 GPUs.","192","GPUs","[""li2025a""]","is_blank","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","The context states that the FLM-101B model was trained on a cluster of 24 DGX-A800 GPU servers, with 8 NVIDIA A800 GPUs per server. Therefore, the total number of A800 GPUs used for training the FLM-101B model is 24 servers * 8 GPUs/server = 192 GPUs."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context indicates that the neural architecture search (NAS) to find the Evolved Transformer architecture produced 284 tCO2e, which is equivalent to the lifetime emissions of five US cars based on the estimates provided by Strubell et al.","5","lifetimes","[""luccioni2023"", ""patterson2021""]","is_blank","['The first paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars.', 'We also reduce by 88X an earlier estimate of the CO2e for the neural architecture search for Evolved Transformer [So19, Str19] by characterizing the actual search process on the hardware and datacenter on which it was performed (see Appendices C and D).']","The context indicates that the neural architecture search (NAS) to find the Evolved Transformer architecture produced 284 tCO2e, which is equivalent to the lifetime emissions of five US cars based on the estimates provided by Strubell et al."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context states that FAIR's RoBERTa was trained on 160GB of text, requiring around 25,000 GPU hours.","25000","hours","[""schwartz2019""]","is_blank","FAIR's RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context states that FAIR's RoBERTa was trained on 160GB of text, requiring around 25,000 GPU hours."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that Amazon's on-site solar energy projects generate an estimated 123,000 MWh and avoid roughly 47,500 metric tons of CO2e each year.","47500","metric tons","[""amazon2023""]","is_blank","In 2023, 50 new on-site solar energy systems became operational at our facilities and stores, adding 58 MW of capacity. Altogether, these solar energy projects generate an estimated 123,000 MWh and avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year.","The context states that Amazon's on-site solar energy projects generate an estimated 123,000 MWh and avoid roughly 47,500 metric tons of CO2e each year."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context describes the Hivemind framework, which is a PyTorch-based framework developed to enable distributed deep learning training where participants can donate heterogeneous hardware to train a single model together in a data-parallel fashion.","Hivemind","is_blank","[""erben2023""]","is_blank","is_blank","The context describes the Hivemind framework, which is a PyTorch-based framework developed to enable distributed deep learning training where participants can donate heterogeneous hardware to train a single model together in a data-parallel fashion."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","According to the context, using two nodes instead of a single node increased energy consumption for the Llama 3.1 70B model. The context states that 'As shown in Figure 4, using two nodes increased energy consumption by a factor that depends on the model (see also Table B2).'","1.95","multiplier","[""zschache2025""]","is_blank","As shown in Table B2, the energy consumption for the Llama 3.1 70B model increased by a factor of 1.95 when using two nodes instead of a single node.","According to the context, using two nodes instead of a single node increased energy consumption for the Llama 3.1 70B model. The context states that 'As shown in Figure 4, using two nodes increased energy consumption by a factor that depends on the model (see also Table B2).'"
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context states that 'Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons'. This suggests that the water consumption per ChatGPT user session in 2023 would be around 1.7 billion gallons.","1700000000","gallons of water","[""luccioni2025a""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons.","The context states that 'Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons'. This suggests that the water consumption per ChatGPT user session in 2023 would be around 1.7 billion gallons."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","According to the text, the author estimates that 700 million daily queries to GPT-4o would result in annual electricity use comparable to 35,000 U.S. homes.","35000","homes","[""jegham2025""]","is_blank","While the individual per-query costs appear modest, their aggregation across millions of users introduces a rapidly compounding, largely invisible load on the environment.","According to the text, the author estimates that 700 million daily queries to GPT-4o would result in annual electricity use comparable to 35,000 U.S. homes."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022.","34","percent","[""luccioni2025a""]","is_blank","[ref_id=luccioni2025a] Corporate reports have revealed the scale of water demand in-creases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [ 42, 78].","The context states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models 'perform considerably worse than LLMs', suggesting that traditional models did not achieve accuracy comparable to LLMs.","1","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models 'perform considerably worse than LLMs', suggesting that traditional models did not achieve accuracy comparable to LLMs."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context states that the Power Usage Effectiveness (PUE) for the Google Iowa datacenter where the Evolved Transformer was run was 1.11.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The context states that the Power Usage Effectiveness (PUE) for the Google Iowa datacenter where the Evolved Transformer was run was 1.11."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","According to the context, the analysis estimates that GPT-4o will yield a total of approximately 772 billion queries in 2025.","772000000000","queries","[""jegham2025""]","is_blank","This is followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].","According to the context, the analysis estimates that GPT-4o will yield a total of approximately 772 billion queries in 2025."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context states that the JetMoE-8B model achieved an average score of 53.0 on the OpenLLM Leaderboard, outperforming the baseline models.","53.0","score","[""shen2024""]","is_blank","Table 3 in the context shows the OpenLLM Leaderboard and code benchmarks results, where JetMoE-8B achieves an average score of 53.0.","The context states that the JetMoE-8B model achieved an average score of 53.0 on the OpenLLM Leaderboard, outperforming the baseline models."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide specific information on the number of AI training runs conducted globally on renewable-only power in 2022. The information given focuses more on the carbon intensity of energy sources used for training ML models in general, without specifying renewable-only training runs in 2022.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context does not provide specific information on the number of AI training runs conducted globally on renewable-only power in 2022. The information given focuses more on the carbon intensity of energy sources used for training ML models in general, without specifying renewable-only training runs in 2022."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context states that the AI Act currently has an 'open-source exemption' that 'allows open-source models to bypass reporting obligations'. This suggests that the open-source exemption fully exempts open-source models from reporting energy consumption under the AI Act.","1","is_blank","[""ebert2024""]","is_blank","The current AI Act 'features environmental protection as one of its core goals and contains dedicated sustainability rules, which also apply to US and other non-EU providers offering models in the EU.' However, 'the open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability.'","The context states that the AI Act currently has an 'open-source exemption' that 'allows open-source models to bypass reporting obligations'. This suggests that the open-source exemption fully exempts open-source models from reporting energy consumption under the AI Act."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context states that the US national datacenter average PUE in 2020 was 1.59, up slightly from 1.58 in 2018.","1.59","PUE","[""patterson2021""]","is_blank","The US national datacenter average PUE in 2020 was 1.59, up slightly from 1.58 in 2018.","The context states that the US national datacenter average PUE in 2020 was 1.59, up slightly from 1.58 in 2018."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context states that when the batch size increases to 32 for the OPT-2.7B model running on a g4dn.xlarge instance, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","5.312","GB","[""kim2025""]","is_blank","When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","The context states that when the batch size increases to 32 for the OPT-2.7B model running on a g4dn.xlarge instance, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context states that for the four models studied in detail (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs (including equity) accounted for between 29% and 49% of the total amortized model development costs.","[29, 49]","percent","[""cottier2024""]","is_blank","We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model.","The context states that for the four models studied in detail (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs (including equity) accounted for between 29% and 49% of the total amortized model development costs."
"q213","Which software package was used to measure energy consumption during inference runs?","The context states that the CodeCarbon package was used to measure the energy consumed and carbon emitted during the inference experiments.","is_blank","is_blank","[""luccioni2024""]","is_blank","is_blank","The context states that the CodeCarbon package was used to measure the energy consumed and carbon emitted during the inference experiments."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context states that 53% of the 100 news articles analyzed cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search.","53","percent","[""luccioni2025c""]","is_blank","Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search 42.","The context states that 53% of the 100 news articles analyzed cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context discusses a 'Compute Time Calibration Function (CTCF)' that is proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","The context discusses a 'Compute Time Calibration Function (CTCF)' that is proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that as the number of shards increases, the energy per response for LLaMA-65B also increases. This is evidenced by Figures 8 and 9, which show that increasing the number of shards tends to increase the energy costs per response.","1","is_blank","[""samsi2024""]","is_blank","Figures 8 and 9 show that increasing the number of shards tends to increase the energy costs per response for LLaMA-65B.","The context states that as the number of shards increases, the energy per response for LLaMA-65B also increases. This is evidenced by Figures 8 and 9, which show that increasing the number of shards tends to increase the energy costs per response."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","According to the context, mining 1 kg of rare earth materials consumes about 11 kL of water. Since an H100 GPU is estimated to be 0.1% rare earth metal by mass, this would equate to 0.013 kg of rare earth materials per GPU. Multiplying the water consumption per kg by the amount of rare earth materials per GPU gives the estimated water consumption of 2.2 kL per H100 GPU.","2.2","kL","[""morrison2025""]","is_blank","Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams 12 and produces about 63 H100s. Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU.","According to the context, mining 1 kg of rare earth materials consumes about 11 kL of water. Since an H100 GPU is estimated to be 0.1% rare earth metal by mass, this would equate to 0.013 kg of rare earth materials per GPU. Multiplying the water consumption per kg by the amount of rare earth materials per GPU gives the estimated water consumption of 2.2 kL per H100 GPU."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context indicates that the open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability. Therefore, under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","1","is_blank","[""ebert2024""]","is_blank","• Elimination of the Open-Source Exemption : The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify ex-clusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary mod-els.","The context indicates that the open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability. Therefore, under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context states that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide.","30","percent","[""luccioni2025a""]","is_blank","In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131]","The context states that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context states that in 2023, the total public health cost of U.S. data centers was about $6.7 billion, or $47.5 per household.","6700000000","USD","[""han2024""]","is_blank","Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023.","The context states that in 2023, the total public health cost of U.S. data centers was about $6.7 billion, or $47.5 per household."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","From the table, we can see that the energy consumption of the o3 model for a long prompt (10k input - 1.5k output) is 12.222 Wh, while the energy consumption of the GPT-4.1 nano model for the same prompt size is 0.827 Wh. Dividing the o3 value by the GPT-4.1 nano value gives a factor of 14.8.","14.8","multiplier","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh).
o3 12.222 ± 1.082
GPT-4.1 nano 0.827 ± 0.094","From the table, we can see that the energy consumption of the o3 model for a long prompt (10k input - 1.5k output) is 12.222 Wh, while the energy consumption of the GPT-4.1 nano model for the same prompt size is 0.827 Wh. Dividing the o3 value by the GPT-4.1 nano value gives a factor of 14.8."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context states that for the short-context Arena dataset with a 120ms SLO, Mélange achieved cost reductions in the range of 15-77% compared to single-GPU baselines.","[15, 77]","percent","[""griggs2024""]","is_blank","The context states: ""In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).""","The context states that for the short-context Arena dataset with a 120ms SLO, Mélange achieved cost reductions in the range of 15-77% compared to single-GPU baselines."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context states that the carbon emissions due to pre-training of FLM-101B are estimated to be 26 tCO2e.","26","tCO2e","[""li2025a""]","is_blank","Our model yields only 1/10 pre-training carbon footprint of a typical LLM.","The context states that the carbon emissions due to pre-training of FLM-101B are estimated to be 26 tCO2e."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context provides the execution time breakdown for a sparse Mixtral model with a batch size of 1 on an NVIDIA A40 GPU. The total execution time is the sum of the time for the forward, backward, and optimizer stages, which is approximately 6.7 seconds.","6.7","seconds","[""xia2024""]","is_blank","Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)
0.0
2.0
4.0
6.0
8.0
Forward Backward Optimizer","The context provides the execution time breakdown for a sparse Mixtral model with a batch size of 1 on an NVIDIA A40 GPU. The total execution time is the sum of the time for the forward, backward, and optimizer stages, which is approximately 6.7 seconds."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context clearly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200, indicating significant disparities.","1","is_blank","[""han2024""]","[""https://arxiv.org/abs/2412.06288""]","Recommendation 3: Promoting Public Health Equity
The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103]. For example, as shown in Table 6c, all the top-10 most impacted counties in the U.S. have lower median household incomes than the national median value. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.","The context clearly states that the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200, indicating significant disparities."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context states that 'As a result of Moore's law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years', which directly supports the claim made in the question.","1","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore's law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The context states that 'As a result of Moore's law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years', which directly supports the claim made in the question."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context states that the framework used Ollama, an open-source platform, to apply 4-bit quantization and enable local deployment of large language models.","Ollama","is_blank","[""khan2025""]","is_blank","We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The context states that the framework used Ollama, an open-source platform, to apply 4-bit quantization and enable local deployment of large language models."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context states that the dataset was accessed on-demand via shards in the tar format with the WebDataset library, which was chosen due to its features like automatic local caching, streaming decompression, streaming preprocessing, and having an easy to work with archive format that allows representing the data in its original format.","WebDataset","is_blank","[""erben2023""]","is_blank","To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4]. Backblaze has replicated data centers that can better serve requests from anywhere worldwide, guaranteeing a reasonable ingress rate from every continent. Additionally, the cost is very manageable at $0.01/GB rate for egress and $0.005/GB/month for storage. A de-tailed analysis of the costs incurred for the experiments can be found in Section 5. We access the datasets on-demand via shards in thetar format with the WebDataset library [10]. We chose WebDataset due to its features like automatic local caching, streaming decompression, streaming preprocessing, and having an easy to work with archive format that allows representing the data in its original format.","The context states that the dataset was accessed on-demand via shards in the tar format with the WebDataset library, which was chosen due to its features like automatic local caching, streaming decompression, streaming preprocessing, and having an easy to work with archive format that allows representing the data in its original format."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context indicates that the energy consumption during inference was shown to highly correlate with the model's runtime, suggesting a nearly linear relationship between runtime and energy.","is_blank","is_blank","[""zschache2025""]","[""https://arxiv.org/abs/2508.14170""]","Additionally, the energy consumption during inference is shown to highly correlate with the model's runtime.","The context indicates that the energy consumption during inference was shown to highly correlate with the model's runtime, suggesting a nearly linear relationship between runtime and energy."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context states that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024.","Edward J. Markey","is_blank","[""ebert2024""]","is_blank","Relating to AI more specifically, although not limited to data centers, is a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024 [ 78].","The context states that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context states that the on-demand price for the NVIDIA H100 GPU is obtained from RunPod and normalized to match the pricing structures of major cloud providers, resulting in a price of $7.516 per hour.","7.516","USD per hour","[""griggs2024""]","[""https://link.to/griggs2024""]","In Table 1, the on-demand price for the NVIDIA H100 (SXM) GPU is listed as $7.5164 per hour.","The context states that the on-demand price for the NVIDIA H100 GPU is obtained from RunPod and normalized to match the pricing structures of major cloud providers, resulting in a price of $7.516 per hour."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context does not provide any information about the average GPU lifetime in AI data centers. There are no relevant details to determine this.","is_blank","years","[""is_blank""]","is_blank","is_blank","The context does not provide any information about the average GPU lifetime in AI data centers. There are no relevant details to determine this."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","According to Table II in the context, the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference is 2 GPUs.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","Table II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. These limits are imposed by a combination of GPU memory, model size, response length and the number of GPUs. While the 65B model can sharded across 6 V100 GPUs, we use 8 since the model architecture makes it better suited for balanced sharding across 8 GPUs.
Model Size V100 32GB
Count
13B 2","According to Table II in the context, the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference is 2 GPUs."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context states that Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, which is over 4 times the 'five cars' estimate of 284 metric tons CO2e from the Strubell et al. study.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the 'five cars' estimate.","The context states that Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, which is over 4 times the 'five cars' estimate of 284 metric tons CO2e from the Strubell et al. study."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context states that 'Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).'","336","hours","[""strubell2019""]","is_blank","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context states that 'Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).'"
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context states that the U.S. national average electricity water consumption intensity factor is 3.14 L/kWh.","3.14","L/kWh","[""li2025b""]","is_blank","For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The context states that the U.S. national average electricity water consumption intensity factor is 3.14 L/kWh."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context states that the PUE of Google's hyperscale data centers improved from 1.21 in 2008 to 1.10 in 2021.","1.1","PUE","[""wu2021b""]","is_blank","Figure 1: PUE of hyperscalar datacenters, such as Google's, has improved from 1.21 (2008) to 1.10 (2021) [Google, a]","The context states that the PUE of Google's hyperscale data centers improved from 1.21 in 2008 to 1.10 in 2021."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","According to the context, research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy.","96","percent","[""amazon2023""]","[""https://www.amazon.com/sustainability-report-2023.pdf""]","Research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","According to the context, research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context states that according to our model, fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a net cost of $3460.","3460","USD","[""xia2024""]","is_blank","For OpenOrca, by scaling the cost by number of queries, our model predicts that the most cost-effective option to rent GPU resources on CUDO compute is NVIDIA H100 with a net cost of $3460.","The context states that according to our model, fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a net cost of $3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context states that 'GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter'.","[50, 70]","percent","[""chung2025""]","is_blank","GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter","The context states that 'GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter'."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context states that the JetMoE-8B model was trained on a cluster containing 12 nodes and 96 H100 GPUs.","96","H100 GPUs","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","We conduct training on a cluster containing 12 nodes and 96 H100s.","The context states that the JetMoE-8B model was trained on a cluster containing 12 nodes and 96 H100 GPUs."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context states that during the first 300 logging steps of OLMo 2 7B training, the average GPU power for a single node while actively training was over 600W, over 85% of an H100's maximum power draw of 700W.","600","Watts","[""morrison2025""]","is_blank","When actively training, the average GPU power is over 600W, over 85% of an H100's maximum power draw of 700W","The context states that during the first 300 logging steps of OLMo 2 7B training, the average GPU power for a single node while actively training was over 600W, over 85% of an H100's maximum power draw of 700W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context snippet provides the estimated CO2e emissions for an average American life of 1 year as 36,156 lbs.","11023","lbs","[""strubell2019""]","[""https://arxiv.org/abs/1906.02243""]","Human life, avg, 1 year 11,023","The context snippet provides the estimated CO2e emissions for an average American life of 1 year as 36,156 lbs."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The paper states that for the smaller LLaMA 13B model, the inference latency on the A100 was 1.25 times faster than on the V100 across words per second, tokens per second, and responses per second.","1.25","multiplier","[""samsi2024""]","is_blank","As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The paper states that for the smaller LLaMA 13B model, the inference latency on the A100 was 1.25 times faster than on the V100 across words per second, tokens per second, and responses per second."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context states that 'A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh)', which directly answers the question.","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh)","The context states that 'A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh)', which directly answers the question."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that for the 400 TPS SLO, InferSave selected the g4dn.xlarge instance with a cost of $0.71, while the Max-Performance policy selected the more expensive g6e.xlarge instance with a cost of $2.699. This means the Max-Performance instance was approximately 280% more expensive than InferSave's top choice.","280","percent","[""kim2025""]","is_blank","With an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of $0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave's top choice.","The context states that for the 400 TPS SLO, InferSave selected the g4dn.xlarge instance with a cost of $0.71, while the Max-Performance policy selected the more expensive g6e.xlarge instance with a cost of $2.699. This means the Max-Performance instance was approximately 280% more expensive than InferSave's top choice."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context states that 'For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference' compared to the more powerful H100 GPU.","V100, A30","is_blank","[""zschache2025""]","is_blank","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.","The context states that 'For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference' compared to the more powerful H100 GPU."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context indicates that Green AI involves reporting the computational price tag of finding, training, and running models as a key practice. This is consistent with the statement in the question.","1","is_blank","[""schwartz2019""]","is_blank","Reporting the computational price tag of ﬁnding, training, and running models is a key Green AI practice (see Equation 1).","The context indicates that Green AI involves reporting the computational price tag of finding, training, and running models as a key practice. This is consistent with the statement in the question."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context states that according to the UN's Global E-Waste Monitor 2024, the global generation of electronic waste reached 62 million tonnes in 2022.","62000000","metric tons","[""luccioni2025a""]","is_blank","The UN's Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste reaching 62 million tonnes in 2022.","The context states that according to the UN's Global E-Waste Monitor 2024, the global generation of electronic waste reached 62 million tonnes in 2022."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context shows that the average system power per processor for the TPU v2 is 221 Watts, while the average system power per processor for the V100 GPU is 325 Watts. Therefore, the difference in average system power per processor between the TPU v2 and the V100 GPU is 325 - 221 = 104 Watts.","104","Watts","[""patterson2021""]","is_blank","Processor Average (Watts) StDev % DNNs used to calculate average power
TPU v2 221 5% Transformer (Big), Evolved Transformer (Medium), Neural Architecture Search [So19]
V100 GPU 325 2% Transformer (Big), GPT-3 [Sut21]","The context shows that the average system power per processor for the TPU v2 is 221 Watts, while the average system power per processor for the V100 GPU is 325 Watts. Therefore, the difference in average system power per processor between the TPU v2 and the V100 GPU is 325 - 221 = 104 Watts."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","According to the context, training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater.","700000","liters","[""li2025b""]","is_blank","For example, training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","According to the context, training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context states that 'Facebook's recommendation and ranking model sizes have increased by 20 times during the same time period [11].' This directly answers the question of how much the model sizes increased between 2019 and 2021.","20","multiplier","[""wu2021a""]","is_blank","Figure 2(c) illustrates that between 2019 and 2021, the size of recommendation models at Facebook has increased by 20×","The context states that 'Facebook's recommendation and ranking model sizes have increased by 20 times during the same time period [11].' This directly answers the question of how much the model sizes increased between 2019 and 2021."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context indicates that o3-mini achieved the highest cross-efficiency DEA score of 0.884, followed by o1-mini (0.836) and Claude 3.7 Sonnet (0.825). This suggests that o3-mini ranked highest in the recent eco-efficiency analysis using DEA.","o3-mini","is_blank","[""jegham2025""]","[""https://arxiv.org/abs/2505.09598v6""]","As shown in Figure 8, OpenAI's reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic's Claude 3.7 Sonnet (0.825).","The context indicates that o3-mini achieved the highest cross-efficiency DEA score of 0.884, followed by o1-mini (0.836) and Claude 3.7 Sonnet (0.825). This suggests that o3-mini ranked highest in the recent eco-efficiency analysis using DEA."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context states that smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","1","is_blank","[""wu2021b""]","[""https://arxiv.org/abs/2112.00612""]","For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020]","The context states that smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context states that the CV task had an almost linear per-GPU speedup for up to 8 GPUs, indicating that intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","1","is_blank","[""erben2023""]","is_blank","CV's per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) for 2, 3, 4, 6 and 8 GPUs, respectively.","The context states that the CV task had an almost linear per-GPU speedup for up to 8 GPUs, indicating that intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context states that the FLM-101B model is structured with a context window of 2,048 tokens.","2048","tokens","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context states that the FLM-101B model is structured with a context window of 2,048 tokens."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLM decoding is characterized by 'low compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low'. In contrast, the context notes that Diffusion models 'draw higher power in general' compared to LLMs. This directly supports the claim that LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive.","1","is_blank","[""chung2025""]","is_blank","This is mainly because Diffusion models (1) draw higher power in general (more in Section 4.2) and (2) cannot perform as many concurrent generations compared to LLMs due to their long latency in real services, preventing them from amortizing energy consumption across many generations.","The context states that LLM decoding is characterized by 'low compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low'. In contrast, the context notes that Diffusion models 'draw higher power in general' compared to LLMs. This directly supports the claim that LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context provides the percentage of Amazon's global People Managers who identified as women in 2023, which was 50.2%.","50.2","percent","[""amazon2023""]","is_blank","In 2023, 50.2% of Amazon's global People Managers identified as women.","The context provides the percentage of Amazon's global People Managers who identified as women in 2023, which was 50.2%."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context states that when excluding equity, the fraction of total amortized cost attributed to computing hardware for the four key models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra) was between 61% and 76%.","[61, 76]","percent","[""cottier2024""]","is_blank","The context states: ""However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.""","The context states that when excluding equity, the fraction of total amortized cost attributed to computing hardware for the four key models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra) was between 61% and 76%."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","According to the context, while the reduction in carbon footprint is consistent, some performance metrics like accuracy and F1 score show slight declines after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","0","is_blank","[""khan2025""]","is_blank","On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","According to the context, while the reduction in carbon footprint is consistent, some performance metrics like accuracy and F1 score show slight declines after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The context indicates that the U.S. Environmental Protection Agency (EPA) provides the average CO2 produced per kilowatt-hour of power consumed in the U.S. as 0.954 lbs/kWh.","0.954","lbs/kWh","[""strubell2019""]","is_blank","The U.S. Environmental Protection Agency (EPA) provides average CO 2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO2 emissions: CO2e = 0.954pt","The context indicates that the U.S. Environmental Protection Agency (EPA) provides the average CO2 produced per kilowatt-hour of power consumed in the U.S. as 0.954 lbs/kWh."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","According to the provided context, a recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027.","[85, 134]","TWh","[""li2025b""]","is_blank","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 [7], whereas a more aggressive projection by the recent U.S. data center energy report predicts that AI servers' electricity consumption in the U.S. alone will surpass 150 – 300 TWh in 2028 [1].","According to the provided context, a recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context states that in 2023, Amazon delivered 150 million packages via EVs in Europe as part of their broader fleet of more than 3,000 electric delivery vehicles.","150","packages","[""amazon2023""]","is_blank","In 2023, we delivered 150 million packages via EVs.","The context states that in 2023, Amazon delivered 150 million packages via EVs in Europe as part of their broader fleet of more than 3,000 electric delivery vehicles."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The online inference workload evaluates a total of 3000 requests, with each request having 128 input tokens and 512 output tokens, for a total of 640 tokens per request. The total number of tokens processed in the entire online inference workload is therefore 3000 requests * 640 tokens/request = 1,920,000 tokens.","1920000","tokens","[""kim2025""]","is_blank","• Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. The workload evaluates a total of 3000 requests.","The online inference workload evaluates a total of 3000 requests, with each request having 128 input tokens and 512 output tokens, for a total of 640 tokens per request. The total number of tokens processed in the entire online inference workload is therefore 3000 requests * 640 tokens/request = 1,920,000 tokens."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The AI Act does not mandate providers to disclose the greenhouse gas emissions of AI applications such as oil and gas exploration. The context states that the AI Act 'fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration' and that this 'creates a notable reporting gap'.","is_blank","is_blank","[""ebert2024""]","is_blank","The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration [ 4, 37]. For example, a recent investigation has revealed Microsoft's aggressive pitch of its AI models to ExxonMobile to optimize fossil fuel exploration [35]. Leaving such applications with significant climate effects out of scope creates a notable reporting gap.","The AI Act does not mandate providers to disclose the greenhouse gas emissions of AI applications such as oil and gas exploration. The context states that the AI Act 'fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration' and that this 'creates a notable reporting gap'."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context states that for very short experiments like DenseNet 201 in the West US region, the Flexible Start algorithm can lead to a reduction in CO2 emissions of up to 80%.","80","percent","[""dodge2022""]","is_blank","For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;","The context states that for very short experiments like DenseNet 201 in the West US region, the Flexible Start algorithm can lead to a reduction in CO2 emissions of up to 80%."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context states that image generation uses 2.907 kWh of energy per 1,000 inferences, while text classification uses 0.002 kWh per 1,000 inferences. This means that the energy required for image generation is over 1,450 times more than that of text classification.","1450","times","[""luccioni2024""]","is_blank","This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The context states that image generation uses 2.907 kWh of energy per 1,000 inferences, while text classification uses 0.002 kWh per 1,000 inferences. This means that the energy required for image generation is over 1,450 times more than that of text classification."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that 'in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment'. This indicates that traditional models did not achieve accuracy comparable to LLMs on the Yelp sentiment analysis task.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that 'in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment'. This indicates that traditional models did not achieve accuracy comparable to LLMs on the Yelp sentiment analysis task."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context states that Amazon had 244 renewable energy projects in the United States as of January 2024.","244","projects","[""amazon2023""]","[""https://www.amazon.com/sustainability-reports""]","In the table on page 26, the 'United States' row shows 244 total renewable energy projects.","The context states that Amazon had 244 renewable energy projects in the United States as of January 2024."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors recommend that AI energy consumption should be reported at the cumulative server level, as they state 'Energy consumption should be reported at the cumulative server level (see also [4]). In this endeavor, estimations may be used only when direct measurements are unavailable.'","cumulative server level","is_blank","[""ebert2024""]","is_blank","Energy consumption should be reported at the cumulative server level (see also [4]).","The authors recommend that AI energy consumption should be reported at the cumulative server level, as they state 'Energy consumption should be reported at the cumulative server level (see also [4]). In this endeavor, estimations may be used only when direct measurements are unavailable.'"
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context states that the GPU accounts for almost 3/4 of the electricity consumption when training a BERT-base model on a single NVIDIA TITAN X GPU.","74","percent","[""dodge2022""]","is_blank","Table 1 shows that the GPU accounts for 74% of the total electricity consumption.","The context states that the GPU accounts for almost 3/4 of the electricity consumption when training a BERT-base model on a single NVIDIA TITAN X GPU."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context states that 'serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs'.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","[""https://arxiv.org/abs/2404.14527""]","The substantial size and computational demands of LLMs require the use of costly hardware accelerators, typically GPUs2 For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context states that 'serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs'."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context states that the iterative optimization process at Facebook led to a 28.5% operational energy footprint reduction over a two-year time period from 2019 to 2021.","28.5","percent","[""wu2021a""]","is_blank","The net effect, with Jevon's Paradox, is a 28.5% operational power footprint reduction over two years (Figure 8).","The context states that the iterative optimization process at Facebook led to a 28.5% operational energy footprint reduction over a two-year time period from 2019 to 2021."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The context does not mention anything about the amount of fiber optic cable installed to support AI workloads in 2023. There is no information provided that would allow me to determine a specific number.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The context does not mention anything about the amount of fiber optic cable installed to support AI workloads in 2023. There is no information provided that would allow me to determine a specific number."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context states that the estimated hardware acquisition cost to train GPT-4 is $800M. This upfront cost is one to two orders of magnitude higher than the amortized hardware CapEx + energy cost of $40M.","800000000","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context states that the estimated hardware acquisition cost to train GPT-4 is $800M. This upfront cost is one to two orders of magnitude higher than the amortized hardware CapEx + energy cost of $40M."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The provided context indicates that the umbrella term 'Sustainable AI' was proposed to encompass both using AI in climate-positive applications as well as improving the environmental sustainability of AI approaches themselves, not just the former.","0","is_blank","[""luccioni2025b""]","is_blank","The umbrella term 'Sustainable AI' was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].","The provided context indicates that the umbrella term 'Sustainable AI' was proposed to encompass both using AI in climate-positive applications as well as improving the environmental sustainability of AI approaches themselves, not just the former."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context states that for an NVIDIA A100-40GB GPU, the maximum batch size supported for fine-tuning Mixtral is 17 samples.","17","samples","[""xia2024""]","is_blank","Using our analytical model, we demonstrate the maximum batch sizes for fine-tuning on four different NVIDIA GPUs: A40, A100-40GB, A100-80GB and H100 with memory capacities of 48GB, 40GB, 80GB, and 80GB, respectively.","The context states that for an NVIDIA A100-40GB GPU, the maximum batch size supported for fine-tuning Mixtral is 17 samples."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context states that when the server is overloaded, the Swapping preemption mechanism consistently consumes less energy than the Recomputation mechanism. This is because Recomputation performs extra computation when restoring requests, whereas Swapping just copies data without running additional computation, and the energy consumption of computation is larger than memory operations.","Swapping","is_blank","[""chung2025""]","is_blank","It can be seen that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations (this will be further examined in the next section).","The context states that when the server is overloaded, the Swapping preemption mechanism consistently consumes less energy than the Recomputation mechanism. This is because Recomputation performs extra computation when restoring requests, whereas Swapping just copies data without running additional computation, and the energy consumption of computation is larger than memory operations."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context states that in its 2024 environmental report, Google reported a 48% increase in GHG emissions since 2019, which they attributed primarily to increases in data center energy consumption.","48","percent","[""luccioni2025a""]","[""https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf""]","For example, in their 2024 annual environmental sustainability report (ESG), Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to ""increases in data center energy consumption""","The context states that in its 2024 environmental report, Google reported a 48% increase in GHG emissions since 2019, which they attributed primarily to increases in data center energy consumption."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","According to the context, projections estimate that data centers will consume between 9.1% and 11.7% of the total US energy demand by 2030, driven by increased demands from LLM and AI workloads.","[9.1,11.7]","percent","[""fernandez2025""]","is_blank","Primarily motivated by the increased demands from LLM and AI workloads, projections estimate that that data centers consume between 9.1% and 11.7% of the total US energy demand by 2030.","According to the context, projections estimate that data centers will consume between 9.1% and 11.7% of the total US energy demand by 2030, driven by increased demands from LLM and AI workloads."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context states that for the training of the 6B parameter transformer, the Pause and Resume optimization could lead to emissions savings of up to around 25% in some regions.","25","percent","[""dodge2022""]","is_blank","Figure 4(b) shows that for the 6B parameter Transformer, the Pause and Resume optimization could lead to emissions savings of up to around 25% in some regions.","The context states that for the training of the 6B parameter transformer, the Pause and Resume optimization could lead to emissions savings of up to around 25% in some regions."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context states that JetMoE-8B has 8B parameters but only activates 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","70","percent","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B has 8B parameters but only activates 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context states that a 2019 study by Strubell et al. 'quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of CO2e emissions'.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","which quantiﬁed the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].","The context states that a 2019 study by Strubell et al. 'quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of CO2e emissions'."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context states that GPT-3 was trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh.","1287","MWh","[""li2025b""]","[""https://example.com/li2025b""]","GPT-3 was trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh [29].","The context states that GPT-3 was trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context states that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning, and optimizing the MoE layer performance is key to improving the overall cost of LLM fine-tuning. This indicates that the MoE layer is a critical component to target for performance enhancement.","1","is_blank","[""xia2024""]","[""arXiv:2408.04693v1""]","Our study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fine-tuning.","The context states that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning, and optimizing the MoE layer performance is key to improving the overall cost of LLM fine-tuning. This indicates that the MoE layer is a critical component to target for performance enhancement."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context states that based on the experimental study on the NVIDIA A40 GPU with 48 GB memory, the maximum batch size supported for fine-tuning the dense version of the Mixtral model on the Hellaswag (CS) dataset is 2 samples.","2","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE
AND S:SPARSE .
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20","The context states that based on the experimental study on the NVIDIA A40 GPU with 48 GB memory, the maximum batch size supported for fine-tuning the dense version of the Mixtral model on the Hellaswag (CS) dataset is 2 samples."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for the CV task, the C-8 experiment with GPUs distributed across 4 continents was 'only 7% slower than the fully local A-8 experiment'.","1","is_blank","[""erben2023""]","is_blank","However, as the CV task is still at a granularity of 3.33 on C-8, it reaches a speedup of 3.02x, only 7% slower than the fully local A-8 experiment.","The context states that for the CV task, the C-8 experiment with GPUs distributed across 4 continents was 'only 7% slower than the fully local A-8 experiment'."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The provided context does not contain any information about the amount of land occupied by new AI data centers globally in 2022. There are no relevant details to determine this metric.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about the amount of land occupied by new AI data centers globally in 2022. There are no relevant details to determine this metric."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context states that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 text classification queries.","0.32","g CO2eq","[""luccioni2024""]","is_blank","For instance bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The context states that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 text classification queries."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context states that the choice of datacenter region can have a large impact on the carbon emissions, with the top lines in Figure 1 showing the highest emissions (around 26,000 grams) and the bottom lines showing the lowest emissions (around 7,000 grams).","[7000, 26000]","grams","[""dodge2022""]","is_blank","Figure 1 in the Dodge et al. paper shows carbon emissions that would be emitted from training BERT in 16 different regions, with the top lines around 26,000 grams and the bottom lines around 7,000 grams.","The context states that the choice of datacenter region can have a large impact on the carbon emissions, with the top lines in Figure 1 showing the highest emissions (around 26,000 grams) and the bottom lines showing the lowest emissions (around 7,000 grams)."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","According to the context, the direct release of environmental information for notable AI models peaked in 2022, with 10% of notable models that year releasing some degree of information. After that, the introduction of increasingly commercial and proprietary models led to a notable reversal in this trend, dramatically reducing direct environmental disclosures.","2022","year","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","According to the context, the direct release of environmental information for notable AI models peaked in 2022, with 10% of notable models that year releasing some degree of information. After that, the introduction of increasingly commercial and proprietary models led to a notable reversal in this trend, dramatically reducing direct environmental disclosures."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context states that training an OLMo 60M model consumed 1.6 kL of water. Dividing this by the average daily water usage of 0.35 kL per person in the US, the equivalent water usage is 5 days.","5","days","[""morrison2025""]","is_blank","OLMo 60M† 1.2 0.4 1 month 1.6 5 days","The context states that training an OLMo 60M model consumed 1.6 kL of water. Dividing this by the average daily water usage of 0.35 kL per person in the US, the equivalent water usage is 5 days."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The provided context does not contain any information about the water consumption of DeepMind AlphaFold servers in 2023. The context instead discusses the water footprint of AI models in general, but does not provide the specific details requested in the question.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about the water consumption of DeepMind AlphaFold servers in 2023. The context instead discusses the water footprint of AI models in general, but does not provide the specific details requested in the question."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that 'improving compute resources will increase performance' for LLM fine-tuning, suggesting that adding compute resources to accelerate the MoE layers can increase throughput and reduce the overall fine-tuning cost.","0","is_blank","[""xia2024""]","[""https://arxiv.org/abs/2408.04693""]","(4) The workload becomes compute bound by increasing batch size; improving compute resources will increase performance.","The context states that 'improving compute resources will increase performance' for LLM fine-tuning, suggesting that adding compute resources to accelerate the MoE layers can increase throughput and reduce the overall fine-tuning cost."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","According to Table 4 in the provided context, the total estimated floating-point operations (FLOPs) for training the FLM-101B model is 28.22 zettaFLOPs. The context states that 1 zettaFLOP = 10^21 FLOPs, so 28.22 zettaFLOPs translates to 28.22 * 10^21 FLOPs. Converting this to energy consumption in kWh is not directly provided, but can be estimated based on the information given.","40000","kWh","[""li2025a""]","is_blank","Table 4: The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","According to Table 4 in the provided context, the total estimated floating-point operations (FLOPs) for training the FLM-101B model is 28.22 zettaFLOPs. The context states that 1 zettaFLOP = 10^21 FLOPs, so 28.22 zettaFLOPs translates to 28.22 * 10^21 FLOPs. Converting this to energy consumption in kWh is not directly provided, but can be estimated based on the information given."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","According to the provided context, the total public health costs attributable to data center operations in the U.S. is projected to potentially reach $20.9 billion in 2028 under the high-growth scenario.","20900000000","USD","[""han2024""]","is_blank","Quantitatively, based on the low- and high-growth scenarios considered in [4], the total public health impact of U.S. data centers is estimated to reach$11.7 billion and$20.9 billion in 2028, respectively.","According to the provided context, the total public health costs attributable to data center operations in the U.S. is projected to potentially reach $20.9 billion in 2028 under the high-growth scenario."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context provides an estimated cost of fine-tuning the Mixtral model on the MATH dataset with sparse setup, using 10 epochs on an NVIDIA A40 GPU, which was $32.7. Since the question asks about fine-tuning on the GSM8K dataset, a similar cost can be expected.","32.7","USD","[""xia2024""]","[""https://arxiv.org/abs/2408.04693""]","TABLE IV: ESTIMATED COST OF FINE-TUNING MIXTRAL ON GS WITH SPARSE MOE BASED ON OUR ANALYTICAL MODEL, GPU A40 48GB, Cost $32.7","The context provides an estimated cost of fine-tuning the Mixtral model on the MATH dataset with sparse setup, using 10 epochs on an NVIDIA A40 GPU, which was $32.7. Since the question asks about fine-tuning on the GSM8K dataset, a similar cost can be expected."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context states that for fine-tuning a sparse Mixtral model on the NVIDIA A40 GPU with 48GB of memory, the maximum batch size supported is 4 samples.","4","samples","[""xia2024""]","is_blank","Table IV shows that for fine-tuning a sparse Mixtral model on an NVIDIA A40 GPU with 48GB of memory, the maximum batch size is 4.","The context states that for fine-tuning a sparse Mixtral model on the NVIDIA A40 GPU with 48GB of memory, the maximum batch size supported is 4 samples."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","From the context, we can see that Figure 4 shows the execution time breakdown for a Mixtral model with a batch size of 10 and a sparse setup. The total execution time is around 1.9 seconds.","1.9","seconds","[""xia2024""]","is_blank","Fig. 4. Execution time breakdown.
Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)
0.0
2.0
4.0
6.0
8.0
    Execution Time
Breakdown (seconds)","From the context, we can see that Figure 4 shows the execution time breakdown for a Mixtral model with a batch size of 10 and a sparse setup. The total execution time is around 1.9 seconds."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context indicates that GPU-level power monitoring is recommended as the preferred method for measuring AI energy use, as GPUs account for the majority of energy consumption in AI workloads.","1","is_blank","[""dodge2022""]","is_blank","Specifically, the context states: ""Electricity Consumption from a Single Cloud Instance. The most accurate and popular AI models today are typically (deep) neural networks, which are most performant on specialized, highly parallelized, and often energy-intensive hardware [43]. The most common scenario is for AI workloads to run on graphics processing units (GPUs), which provide significant acceleration compared to CPUs (central processing units) but are more power-hungry (often consuming 250W-350W, compared to CPU consumption of 10-150W). Due to specialization to the matrix multiply operations at the core of neural network computations and a high rate of parallelization, GPUs can perform many more of these types of computations in the same amount of time as a CPU, but this increased computation throughput comes at an increased energy cost. Thus in ML applications based on deep learning, the majority of the electricity consumption is due to the GPU [ 5, 45].""","The context indicates that GPU-level power monitoring is recommended as the preferred method for measuring AI energy use, as GPUs account for the majority of energy consumption in AI workloads."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context states that in the 2023 article estimating the carbon footprint of the BLOOM 176B parameter language model, the authors found that training accounted for only half of the model's overall emissions.","50","percent","[""luccioni2025b""]","is_blank","In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, since it takes into account different stages of the model life cycle including the manufacturing of computing hardware, idle energy usage, and model deployment, finding that training accounted for only half of the model's overall emissions [121], meaning that similar studies that only took training into account were potentially underestimating their emissions by half.","The context states that in the 2023 article estimating the carbon footprint of the BLOOM 176B parameter language model, the authors found that training accounted for only half of the model's overall emissions."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that the LLaMA-7B model requires a minimum of 1 NVIDIA V100 32GB GPU to run inference without compression or quantization.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","The 7B model was run on a single GPU and TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256.","The context states that the LLaMA-7B model requires a minimum of 1 NVIDIA V100 32GB GPU to run inference without compression or quantization."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context provided in the table shows that for a data center located in Arizona, the water consumption per each GPT-3 request is estimated to be 29.926 mL. Since a 500 mL bottle is specified, it would take approximately 16.7 requests to consume the full 500 mL.","16.7","requests","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3's operational water consumption footprint. For the Arizona data center, the water consumption per request is estimated to be 29.926 mL.","The context provided in the table shows that for a data center located in Arizona, the water consumption per each GPT-3 request is estimated to be 29.926 mL. Since a 500 mL bottle is specified, it would take approximately 16.7 requests to consume the full 500 mL."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context states that the average US home emits 8.3 metric tons of CO2 per year from electricity generation, natural gas, liquid petroleum gas, and fuel oil.","8.3","metric tons","[""strubell2019""]","is_blank","Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).","The context states that the average US home emits 8.3 metric tons of CO2 per year from electricity generation, natural gas, liquid petroleum gas, and fuel oil."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context states that on the GSM8k benchmark, which evaluates grade school math problem-solving, the JetMoE-8B model achieved a score of 27.8.","27.8","score","[""shen2024""]","is_blank","GSM8k 27.8","The context states that on the GSM8k benchmark, which evaluates grade school math problem-solving, the JetMoE-8B model achieved a score of 27.8."
