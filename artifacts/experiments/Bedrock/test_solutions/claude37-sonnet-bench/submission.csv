"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","According to the context, specifically from the document with ref_id wu2021b, 'The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05.'","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","According to the context, specifically from the document with ref_id wu2021b, 'The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05.'"
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","According to the source document, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is described as equivalent to taking more than 13,900 cars off the road.","13900","cars","[""amazon2023""]","is_blank","Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","According to the source document, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is described as equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","According to the context, in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, which means they added 4 new data centers to their recycled water cooling program that year. The snippet specifically identifies two data centers in Virginia, one in California, and one in Singapore.","4","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","According to the context, in 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, which means they added 4 new data centers to their recycled water cooling program that year. The snippet specifically identifies two data centers in Virginia, one in California, and one in Singapore."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","According to morrison2025, NVIDIA does not release the embodied carbon emissions of its hardware, so the authors assume the same embodied carbon emissions as reported by Luccioni et al. (2023), which is 3700 kg of CO2eq per 8x server node. This equals 463 kg per GPU.","463","kg/GPU","[""morrison2025""]","is_blank","NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU.","According to morrison2025, NVIDIA does not release the embodied carbon emissions of its hardware, so the authors assume the same embodied carbon emissions as reported by Luccioni et al. (2023), which is 3700 kg of CO2eq per 8x server node. This equals 463 kg per GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","According to the context, the amortized hardware and energy cost for GPT-4 was $40M (from cottier2024), while FLM-101B was trained within a $100,000 budget (from li2025a). Therefore, the ratio is $40M/$100K = 400.","400","ratio","[""cottier2024"", ""li2025a""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost. (cottier2024); In this paper, we introduce FLM-101B, an open-sourced LLM that is successfully trained from scratch within a $100,000 budget. (li2025a)","According to the context, the amortized hardware and energy cost for GPT-4 was $40M (from cottier2024), while FLM-101B was trained within a $100,000 budget (from li2025a). Therefore, the ratio is $40M/$100K = 400."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","According to multiple mentions in the document, the CO2e for a single passenger round trip from San Francisco to New York is approximately 1.2 tons. This is explicitly stated in the context as '~1.2t CO2e' and is calculated based on a Boeing 767 with 175 seats at 84.5% occupancy.","1.2","tCO2e","[""patterson2021""]","is_blank","To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).","According to multiple mentions in the document, the CO2e for a single passenger round trip from San Francisco to New York is approximately 1.2 tons. This is explicitly stated in the context as '~1.2t CO2e' and is calculated based on a Boeing 767 with 175 seats at 84.5% occupancy."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","According to Table 4 in the provided context, FLM-101B achieved an average score of 43.94 on the Open LLM Leaderboard, which evaluates performance across four tasks: ARC-Challenge, HellaSwag, MMLU, and TruthfulQA. This score represents over 90% of the performance of GLM-130B despite using 7 times fewer FLOPs.","43.94","score","[""li2025a""]","is_blank","On average, FLM-101B achieves a score of 43.94, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.","According to Table 4 in the provided context, FLM-101B achieved an average score of 43.94 on the Open LLM Leaderboard, which evaluates performance across four tasks: ARC-Challenge, HellaSwag, MMLU, and TruthfulQA. This score represents over 90% of the performance of GLM-130B despite using 7 times fewer FLOPs."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","According to the text, the Intel 4004 from 1971 ran at 740kHZ while by 2021, typical microprocessors could run at 5,000,000kHz, representing 'more than 6,750 fold improvement in processor clock speed'.","6750","fold","[""wu2021b""]","is_blank","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","According to the text, the Intel 4004 from 1971 ran at 740kHZ while by 2021, typical microprocessors could run at 5,000,000kHz, representing 'more than 6,750 fold improvement in processor clock speed'."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","According to Patterson2021, OpenAI published the total floating point operations to train GPT-3 (3.14E+23) and told the authors that V100 GPUs run GPT-3 at 24.6 TeraFLOPS/sec. Using these values, it was calculated that it takes ~14.8 days for 10,000 GPUs at that performance level to compute the total FLOPS required for training GPT-3.","14.8","days","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20]. OpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec [Sut21]. It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","According to Patterson2021, OpenAI published the total floating point operations to train GPT-3 (3.14E+23) and told the authors that V100 GPUs run GPT-3 at 24.6 TeraFLOPS/sec. Using these values, it was calculated that it takes ~14.8 days for 10,000 GPUs at that performance level to compute the total FLOPS required for training GPT-3."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","According to the SGLang benchmarking results in the Morrison et al. 2025 paper, the GPU Power Usage for Llama 3.2 1B model at 8 requests per second is reported as 0.036 kWh.","0.036","kWh","[""morrison2025""]","is_blank","Llama 3.2 1B ∞ 0.003 1.0 0.004 1.38 258 bil.
8 0.036 12.0 0.054 12.64 21.5 bil.
1 0.160 53.1 0.238 100.58 4.83 bil.","According to the SGLang benchmarking results in the Morrison et al. 2025 paper, the GPU Power Usage for Llama 3.2 1B model at 8 requests per second is reported as 0.036 kWh."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context states that the total permitted annual emission limits for diesel generators in Northern Virginia are approximately 13,000 tons of NOx. This information appears in multiple places in the document, specifically discussing permits for data center diesel generators in Northern Virginia (mostly in Loudoun, Prince William, and Fairfax).","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context states that the total permitted annual emission limits for diesel generators in Northern Virginia are approximately 13,000 tons of NOx. This information appears in multiple places in the document, specifically discussing permits for data center diesel generators in Northern Virginia (mostly in Loudoun, Prince William, and Fairfax)."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","According to the context, training FLM-101B with the growth strategy took 21.54 days, which is described as '72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).'","72","percent","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","According to the context, training FLM-101B with the growth strategy took 21.54 days, which is described as '72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).'"
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context provides projections for 2028, stating that U.S. data centers' pollutants could cause approximately 1,300 premature deaths, but does not specifically mention data for 2030. Since the question asks about 2030 and no data is provided for that specific year, I cannot determine the answer from the given context.","is_blank","deaths","[""is_blank""]","is_blank","is_blank","The context provides projections for 2028, stating that U.S. data centers' pollutants could cause approximately 1,300 premature deaths, but does not specifically mention data for 2030. Since the question asks about 2030 and no data is provided for that specific year, I cannot determine the answer from the given context."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","According to Dodge et al. (2022), they refer to 'very long runs like training a 6 billion parameter language model for 8 days' and specifically mention 'training a 6 billion parameter language model for 8 days (b)' when discussing the Flexible Start optimization.","8","days","[""dodge2022""]","is_blank","for very long runs like training a 6 billion parameter language model for 8 days (b), changing the start time by up to 24 hours leads to less than 1.5% reduction at best in any region.","According to Dodge et al. (2022), they refer to 'very long runs like training a 6 billion parameter language model for 8 days' and specifically mention 'training a 6 billion parameter language model for 8 days (b)' when discussing the Flexible Start optimization."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","According to Table 2 in the luccioni2025c document, Command-R Plus by Cohere has the highest GPU energy consumption for 1,000 queries at 3,426.38 Wh among the models listed in the appendix.","Command-R Plus","is_blank","[""luccioni2025c""]","is_blank","Table 2. Range of Inference Energy Use21 (Representative Models Displayed)
Model Organization GPU Energy for 1k Queries (Wh) Task
[...]
Command-R Plus Cohere 3,426.38 Text Generation","According to Table 2 in the luccioni2025c document, Command-R Plus by Cohere has the highest GPU energy consumption for 1,000 queries at 3,426.38 Wh among the models listed in the appendix."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","According to the context, specifically from the document stone2022, 'The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.'","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.","According to the context, specifically from the document stone2022, 'The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.'"
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","According to the UN's Global E-Waste Monitor 2024 as cited in the Luccioni 2025 paper, about 22% of e-waste has been formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling.","22","percent","[""luccioni2025a""]","is_blank","The UN's Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","According to the UN's Global E-Waste Monitor 2024 as cited in the Luccioni 2025 paper, about 22% of e-waste has been formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","Based on the context, the total energy consumption for pre-training the BLOOM model can be inferred from information provided about the BLOOMz models, which are fine-tuned versions of BLOOM. According to the table in the context, the training energy for the largest BLOOMz-7B model was 51,686 kWh, which is 51.686 MWh.","51.686","MWh","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592
Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.","Based on the context, the total energy consumption for pre-training the BLOOM model can be inferred from information provided about the BLOOMz models, which are fine-tuned versions of BLOOM. According to the table in the context, the training energy for the largest BLOOMz-7B model was 51,686 kWh, which is 51.686 MWh."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","According to the context in patterson2021, the Switch Transformer model has 1500 billion parameters but only 0.1% of those parameters are activated per token. This is explicitly stated in the document's description of the model and its table of NLP models.","0.1","percent","[""patterson2021""]","is_blank","The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources.","According to the context in patterson2021, the Switch Transformer model has 1500 billion parameters but only 0.1% of those parameters are activated per token. This is explicitly stated in the document's description of the model and its table of NLP models."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","According to the context, specifically from the document with reference ID 'shen2024', JetMoE-8B has 8 experts in each MoE layer. This is explicitly stated in the hyperparameters table where Nexperts is set to 8, and also mentioned in the text: 'Then, we set the same number of experts to 8 and top-k to 2 for every layer.'","8","experts","[""shen2024""]","is_blank","Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp
8B 2B 24 2048 8 2 16 128 5632
Table 1: JetMoE-8B hyperparameters.","According to the context, specifically from the document with reference ID 'shen2024', JetMoE-8B has 8 experts in each MoE layer. This is explicitly stated in the hyperparameters table where Nexperts is set to 8, and also mentioned in the text: 'Then, we set the same number of experts to 8 and top-k to 2 for every layer.'"
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","According to Fig. 4 in the paper, which shows execution time breakdowns for different models and batch sizes, the total execution time for a dense BlackMamba model with batch size=30 is approximately 1.5 seconds.","1.5","second","[""xia2024""]","is_blank","Dense(bsz=1) Dense(bsz=10) Sparse(bsz=1) Sparse(bsz=10) Sparse(bsz=32)
0.0
2.0
4.0
6.0
8.0
Forward Backward Optimizer
Dense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)
0.0
0.5
1.0
1.5
2.0
    Execution Time
Breakdown (seconds)
Mixtral
Mamba
Fig. 4. Execution time breakdown.","According to Fig. 4 in the paper, which shows execution time breakdowns for different models and batch sizes, the total execution time for a dense BlackMamba model with batch size=30 is approximately 1.5 seconds."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","According to multiple references in the context, the total cost of FLM-101B is computed as 52.76 zettaFLOPs, with 28.22 zettaFLOPs specifically for the English portion and 24.54 for Chinese. This is explicitly stated in the document.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","According to multiple references in the context, the total cost of FLM-101B is computed as 52.76 zettaFLOPs, with 28.22 zettaFLOPs specifically for the English portion and 24.54 for Chinese. This is explicitly stated in the document."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","Based on the context, Khan et al. (2025) reported using an '11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency)' for their experimental setup on energy-efficient local inference in financial sentiment classification.","11th Gen Intel(R) Core(TM) i7-1165G7","is_blank","[""khan2025""]","is_blank","The hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable). The system type is a 64-bit operating system with an x64-based processor, running on Windows 11 Pro.","Based on the context, Khan et al. (2025) reported using an '11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency)' for their experimental setup on energy-efficient local inference in financial sentiment classification."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","According to the 'Power Hungry Processing' study by Luccioni et al. (2024), they tested 88 models across 10 tasks and 30 datasets. This information is provided in the context describing their comprehensive analysis of energy consumption during ML model inference.","88","models","[""zschache2025""]","is_blank","Finally, Luccioni et al. (2024) provide one of the most comprehensive analyses of energy consumption during ML model inference. Their study systematically compared the energy costs of 88 models across 10 tasks and 30 datasets, including both smaller task-specific and larger multi-purpose models.","According to the 'Power Hungry Processing' study by Luccioni et al. (2024), they tested 88 models across 10 tasks and 30 datasets. This information is provided in the context describing their comprehensive analysis of energy consumption during ML model inference."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","According to the context, specifically from wu2021a, 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.' This is explicitly stated in relation to LM training on GPUs.","3","multiplier","[""wu2021a""]","is_blank","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","According to the context, specifically from wu2021a, 'Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.' This is explicitly stated in relation to LM training on GPUs."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","According to Cottier et al. (2024), the ratio of total compute for model development to final training run compute ranges from 1.2x to 4x, with a median of 2.2x. This is based on evidence from the development of GPT-3, OPT-175B, BLOOM, and Meta's AI infrastructure.","2.2","multiplier","[""cottier2024""]","is_blank","This compute is not only important, but significant in scale: we estimate that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x.","According to Cottier et al. (2024), the ratio of total compute for model development to final training run compute ranges from 1.2x to 4x, with a median of 2.2x. This is based on evidence from the development of GPT-3, OPT-175B, BLOOM, and Meta's AI infrastructure."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","According to dodge2022, they estimate the total energy consumption to train a 6.1 billion parameter transformer model to completion would be approximately (60/8) * 13.8 = 103.5 MWh, or 103,500 kWh. The model was only trained to 13% completion during their study, consuming 13.8 MWh over 8 days, but would require 60 days for full training.","103.5","MWh","[""dodge2022""]","is_blank","Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) * 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","According to dodge2022, they estimate the total energy consumption to train a 6.1 billion parameter transformer model to completion would be approximately (60/8) * 13.8 = 103.5 MWh, or 103,500 kWh. The model was only trained to 13% completion during their study, consuming 13.8 MWh over 8 days, but would require 60 days for full training."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","Luccioni, Strubell, and Crawford's 2025 paper discusses Jevons' Paradox in the context of AI, arguing that efficiency gains in AI may paradoxically lead to increased consumption rather than environmental benefits. This economic principle, proposed by William Stanley Jevons in the 19th century, observed that improved efficiency in coal use led to increased consumption rather than decreased use of resources.","Jevons' Paradox","is_blank","[""luccioni2025a"", ""luccioni2025b""]","is_blank","We follow this by introducing the concept of 'rebound effects,' drawing on the economic principle of Jevons' Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60].","Luccioni, Strubell, and Crawford's 2025 paper discusses Jevons' Paradox in the context of AI, arguing that efficiency gains in AI may paradoxically lead to increased consumption rather than environmental benefits. This economic principle, proposed by William Stanley Jevons in the 19th century, observed that improved efficiency in coal use led to increased consumption rather than decreased use of resources."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","According to the document, the total water withdrawal of global AI in 2027 is projected to reach 4.2 - 6.6 billion cubic meters, which is compared to being more than the total annual water withdrawal of 4-6 Denmark or half of the United Kingdom.","[4.2, 6.6]","billion cubic meters","[""li2025b""]","is_blank","More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","According to the document, the total water withdrawal of global AI in 2027 is projected to reach 4.2 - 6.6 billion cubic meters, which is compared to being more than the total annual water withdrawal of 4-6 Denmark or half of the United Kingdom."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context directly contradicts this statement. According to the text, 'Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).' This clearly indicates that despite researchers being aware of diminishing returns, Red AI is actually increasing, not declining.","0","is_blank","[""schwartz2019""]","is_blank","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The context directly contradicts this statement. According to the text, 'Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).' This clearly indicates that despite researchers being aware of diminishing returns, Red AI is actually increasing, not declining."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","According to the context, the total time cost for training FLM-101B is explicitly stated as 21.54 days. This is mentioned in the document when discussing the growth schedule, where they train a 16B model for 9.63 days, a 51B model for 5.37 days, and the 101B model for 6.54 days, totaling 21.54 days.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","According to the context, the total time cost for training FLM-101B is explicitly stated as 21.54 days. This is mentioned in the document when discussing the growth schedule, where they train a 16B model for 9.63 days, a 51B model for 5.37 days, and the 101B model for 6.54 days, totaling 21.54 days."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","According to Figure 10 of the Wu et al. (2021a) document, a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50%, which directly contradicts the statement that they are utilized at over 80% capacity.","0","is_blank","[""wu2021a""]","is_blank","A vast majority of model experimentation (over tens of thousands of training workﬂows) utilizes GPUs at only 30-50%, leaving room for utilization and efﬁciency improvements.","According to Figure 10 of the Wu et al. (2021a) document, a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50%, which directly contradicts the statement that they are utilized at over 80% capacity."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","According to the context, GPT-3's training is estimated to have consumed 1,287 megawatt-hours (MWh) of electricity. This information appears in multiple sources, including jegham2025 which states 'GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity' in relation to its training.","1287","MWh","[""jegham2025"", ""li2025b""]","is_blank","GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity and emit over 550 metric tons of CO 2 equivalent (CO2e) [12], while requiring more than 700 kiloliters (kL) of water for cooling alone [13], enough to fill a quarter of an Olympic-sized swimming pool.","According to the context, GPT-3's training is estimated to have consumed 1,287 megawatt-hours (MWh) of electricity. This information appears in multiple sources, including jegham2025 which states 'GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity' in relation to its training."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","According to the context, the AI Energy Score is a project that aims to establish a unified approach for comparing the inference efficiency of AI models. This is mentioned in document luccioni2025c, which states 'These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.'","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.","According to the context, the AI Energy Score is a project that aims to establish a unified approach for comparing the inference efficiency of AI models. This is mentioned in document luccioni2025c, which states 'These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.'"
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","From Figure 6 in ref_id=xia2024, I can see the execution time breakdown of the MoE layer for different kernels. For the dense BlackMamba model with a batch size of 30, the longest kernel is matmul(w1) with approximately 1600 microseconds execution time.","1600","microseconds","[""xia2024""]","is_blank","Dense(bsz=1) Dense(bsz=30) Sparse(bsz=1) Sparse(bsz=30) Sparse(bsz=84)0
400
800
1200
1600
2000
matmul(w1)
gelu
matmul(w2)
elementwise_mult
top_k
sigmoid
matmul(router)
    Execution Time Breakdown (μs)
Mixtral
Mamba","From Figure 6 in ref_id=xia2024, I can see the execution time breakdown of the MoE layer for different kernels. For the dense BlackMamba model with a batch size of 30, the longest kernel is matmul(w1) with approximately 1600 microseconds execution time."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","According to the context in shen2024, JetMoE-8B uses a top-k value of 2 for every layer. This means that for each input token, 2 experts are selected for activation out of the total 8 experts per layer.","2","experts","[""shen2024""]","is_blank","Then, we set the same number of experts to 8 and top-k to 2 for every layer.","According to the context in shen2024, JetMoE-8B uses a top-k value of 2 for every layer. This means that for each input token, 2 experts are selected for activation out of the total 8 experts per layer."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The statement is partially true but has the number wrong. According to Schwartz 2019, the amount of compute used to train deep learning models increased 300,000x (not 200,000x) in 6 years. The context states 'The amount of compute used to train deep learning models has increased 300,000x in 6 years' and specifies this period as '2012 to 2018'.","0","is_blank","[""schwartz2019""]","is_blank","Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.","The statement is partially true but has the number wrong. According to Schwartz 2019, the amount of compute used to train deep learning models increased 300,000x (not 200,000x) in 6 years. The context states 'The amount of compute used to train deep learning models has increased 300,000x in 6 years' and specifies this period as '2012 to 2018'."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","According to the context snippets, the global carbon emissions for 2020 dropped by 6.4% due to the COVID-19 pandemic, with vehicle transportation in the US accounting for a portion of this global reduction.","6.4","percent","[""wu2021b""]","is_blank","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","According to the context snippets, the global carbon emissions for 2020 dropped by 6.4% due to the COVID-19 pandemic, with vehicle transportation in the US accounting for a portion of this global reduction."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","According to the Amazon 2023 Sustainability Report, 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources, which represents an increase from 19 regions in 2022.","22","data centers","[""amazon2023""]","is_blank","Amazon's energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.‡","According to the Amazon 2023 Sustainability Report, 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources, which represents an increase from 19 regions in 2022."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","According to the context, the field of Artificial Intelligence (AI) was officially born and christened at a workshop in 1956 organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. Calculating from this founding date to 2025 gives approximately 69 years.","69","years","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","According to the context, the field of Artificial Intelligence (AI) was officially born and christened at a workshop in 1956 organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence. Calculating from this founding date to 2025 gives approximately 69 years."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","According to the context in luccioni2025c, the 'five cars' carbon footprint estimate from Strubell et al.'s 2019 study is specifically based on neural architecture search (NAS), which is described as 'a large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload.' The estimate of 284 metric tons CO2e came from this specific approach to model development.","Neural Architecture Search","is_blank","[""luccioni2025c""]","is_blank","by estimating the energy required to automate that process using an approach called neural architecture search (NAS) based on figures reported in a recent Google study using NAS to identify an optimized variant of the Transformer architecture. 31 In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.","According to the context in luccioni2025c, the 'five cars' carbon footprint estimate from Strubell et al.'s 2019 study is specifically based on neural architecture search (NAS), which is described as 'a large-scale procedure that is meant to be and is in practice performed much less frequently than the average AI model training workload.' The estimate of 284 metric tons CO2e came from this specific approach to model development."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","According to the context, when targeting an average TPOT of 100ms (which is sufficient for most reading speeds), this will land on the Pareto frontier at the point where average TPOT is 77ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","44","percent","[""chung2025""]","is_blank","This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","According to the context, when targeting an average TPOT of 100ms (which is sufficient for most reading speeds), this will land on the Pareto frontier at the point where average TPOT is 77ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","According to Table III in the Xia et al. paper, the maximum batch size supported by BlackMamba with sparse setup (BlackMamba-S) on the CS dataset is 20, and on the MATH dataset is 8. Since GSM8K is categorized as a math dataset in the paper, the maximum batch size for BlackMamba-S on GSM8K would be 8 samples.","8","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE
AND S:SPARSE .
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20
MATH 1 3 2 8","According to Table III in the Xia et al. paper, the maximum batch size supported by BlackMamba with sparse setup (BlackMamba-S) on the CS dataset is 20, and on the MATH dataset is 8. Since GSM8K is categorized as a math dataset in the paper, the maximum batch size for BlackMamba-S on GSM8K would be 8 samples."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","According to the document with ref_id amazon2023, 'We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.' This is also confirmed in multiple places in the document, including in a highlights section that states '1.3 GW Energy storage capacity, up from 445 megawatts (MW) in 2022'.","1.3","GW","[""amazon2023""]","is_blank","We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.","According to the document with ref_id amazon2023, 'We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.' This is also confirmed in multiple places in the document, including in a highlights section that states '1.3 GW Energy storage capacity, up from 445 megawatts (MW) in 2022'."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","According to document jegham2025, the annual carbon emissions of GPT-4o are projected to be approximately 138,125 to 163,441 tons of CO2e, which are specifically stated to be comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","Our projections indicate annual emissions of approximately 138,125 tons of CO2e at minimum and 163,441 tons at maximum. These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","According to document jegham2025, the annual carbon emissions of GPT-4o are projected to be approximately 138,125 to 163,441 tons of CO2e, which are specifically stated to be comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","According to the context, the average data center PUE in 2023 was 1.58 globally. This value is explicitly stated in [ref_id=ebert2024].","1.58","PUE","[""ebert2024""]","is_blank","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","According to the context, the average data center PUE in 2023 was 1.58 globally. This value is explicitly stated in [ref_id=ebert2024]."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","According to the context, JetMoE-8B has 8 billion total parameters but only activates 2 billion parameters for each input token. This sparse activation approach is applied to both the attention and feed-forward layers, which significantly reduces computational costs during inference.","2B","parameters","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","According to the context, JetMoE-8B has 8 billion total parameters but only activates 2 billion parameters for each input token. This sparse activation approach is applied to both the attention and feed-forward layers, which significantly reduces computational costs during inference."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","According to Table 1 in the context from document luccioni2025c, which shows the range of pre-training environmental impacts for representative models, the Llama 7B model from Meta produced 14 tCO2e of GHG emissions.","14","tCO2e","[""luccioni2025c""]","is_blank","Table 1. Range of Pre-Training Environmental Impacts (Representative Models Displayed)
Model Organization Energy Consumption (MWh) GHG Emissions (tCO2e)
OLMo 20M 26 Ai2 0.8 0.3
CodeGen 350M 62 Salesforce 71 6
Llama 7B 63 Meta 356 14
BLOOM 11 Big Science 520 30
T5 16 Google 85.7 47
OLMo 2 13B 26 Ai2 157 101
Gemma 2B + 9B 64 Google ? 131","According to Table 1 in the context from document luccioni2025c, which shows the range of pre-training environmental impacts for representative models, the Llama 7B model from Meta produced 14 tCO2e of GHG emissions."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","According to the context, Amazon had 19,000+ electric delivery vans in 2023. In 2022, specific regional numbers show 2,600 in the U.S., 1,220 in Europe, and 3,800 in India, totaling 7,620 electric delivery vans for 2022. Therefore, the total added across both years would be 19,000.","19000","electric delivery vans","[""amazon2023""]","is_blank","In 2023, we delivered more than 680 million packages globally using more than 24,000 electric delivery vehicles, including 19,000 electric delivery vans, around the world.","According to the context, Amazon had 19,000+ electric delivery vans in 2023. In 2022, specific regional numbers show 2,600 in the U.S., 1,220 in Europe, and 3,800 in India, totaling 7,620 electric delivery vans for 2022. Therefore, the total added across both years would be 19,000."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context from morrison2025 clearly states that 'Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.' This directly contradicts the statement in the question.","0","is_blank","[""morrison2025""]","is_blank","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The context from morrison2025 clearly states that 'Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.' This directly contradicts the statement in the question."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","According to Table 4 in the jegham2025 paper, the o3 model consumes 12.222 ± 1.082 Wh for a long prompt with 10k input and 1.5k output tokens.","12.222","Wh","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh). [...] o3 [...] Energy Consumption(10k input-1.5k output)(Wh) [...] 12.222 ± 1.082","According to Table 4 in the jegham2025 paper, the o3 model consumes 12.222 ± 1.082 Wh for a long prompt with 10k input and 1.5k output tokens."
"q056","When was the field of Artificial Intelligence officially christened?","Multiple snippets from stone2022 clearly state that the field of Artificial Intelligence was 'officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.' This event marks the official beginning and naming of the field.","1956","year","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","Multiple snippets from stone2022 clearly state that the field of Artificial Intelligence was 'officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.' This event marks the official beginning and naming of the field."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","Based on the provided context, I don't find a specific value for Google's AI-dedicated data centers' WUE in 2024. Li et al. mention Google's 'annualized global on-site water efficiency' as 1 L/kWh, but this appears to be their general data center efficiency rather than specifically for AI-dedicated facilities in 2024. The context doesn't provide this specific information.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","Based on the provided context, I don't find a specific value for Google's AI-dedicated data centers' WUE in 2024. Li et al. mention Google's 'annualized global on-site water efficiency' as 1 L/kWh, but this appears to be their general data center efficiency rather than specifically for AI-dedicated facilities in 2024. The context doesn't provide this specific information."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","According to the context from document wu2021b, ""approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency]"".","1","is_blank","[""wu2021b""]","is_blank","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","According to the context from document wu2021b, ""approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency]""."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","According to the context, for LLaMA 65B with max generation length 512, it takes about 3-4 Joules per output token. This is explicitly stated in the text when discussing energy consumption.","[3, 4]","joules per token","[""samsi2024""]","is_blank","For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","According to the context, for LLaMA 65B with max generation length 512, it takes about 3-4 Joules per output token. This is explicitly stated in the text when discussing energy consumption."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","According to the context, specifically from Wu et al. (2021), 'By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.' This directly states the percentage reduction in the model size for RM2 when quantized from 32-bit to 16-bit.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","According to the context, specifically from Wu et al. (2021), 'By converting 32-bit floating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.' This directly states the percentage reduction in the model size for RM2 when quantized from 32-bit to 16-bit."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","According to the context, the 5-10% reduction claim comes from a 2021 Boston Consulting Group report, but the reasoning behind this estimate is described as 'unclear' with 'underlying calculations not detailed' beyond being based on BCG's client experience. The text explicitly states that 'applying observations made from individual projects to the entire planet's GHG emissions lacks any scientific grounding' and the claim fails to account for necessary methodological requirements like defining counterfactual scenarios and accounting for rebound effects.","0","is_blank","[""luccioni2025c""]","is_blank","The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG's experience in dealing with their clients and using AI to optimize and improve existing processes. The second, Google-commissioned BCG study provides slightly more detail in terms of the kinds of projects AI can be used for, but does not offer specific calculations translating individual project numbers to a global scale. Applying observations made from individual projects to the entire planet's GHG emissions lacks any scientific grounding—in fact, many of the emissions reductions on a global scale require individual, societal and political shifts.","According to the context, the 5-10% reduction claim comes from a 2021 Boston Consulting Group report, but the reasoning behind this estimate is described as 'unclear' with 'underlying calculations not detailed' beyond being based on BCG's client experience. The text explicitly states that 'applying observations made from individual projects to the entire planet's GHG emissions lacks any scientific grounding' and the claim fails to account for necessary methodological requirements like defining counterfactual scenarios and accounting for rebound effects."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","According to Patterson et al. (2021), 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.' This is explicitly stated in both the abstract and in the opportunities section to improve energy efficiency.","1","is_blank","[""patterson2021""]","is_blank","We highlight the following opportunities to improve energy efficiency and CO2 equivalent emissions (CO2e): 
● Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","According to Patterson et al. (2021), 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.' This is explicitly stated in both the abstract and in the opportunities section to improve energy efficiency."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","According to the Schwartz2019 document, 'Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.' This provides a direct statement of the cost in USD.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","According to the Schwartz2019 document, 'Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.' This provides a direct statement of the cost in USD."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","According to Figure 4 and the text on page 4, the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with batch size = 1.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1), while the execution time share of the optimizer stage in Mixtral fine-tuning is negligible.","According to Figure 4 and the text on page 4, the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with batch size = 1."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","To calculate the daily energy consumption for 1 billion queries, I need to convert the energy consumption of Flan-T5-xxl from kWh per 1,000 queries to MWh for 1 billion queries. The given energy consumption is 0.083 kWh per 1,000 queries. For 1 billion queries, this would be (0.083 kWh/1,000 queries) × 1,000,000,000 queries = 83,000 kWh = 83 MWh.","83","MWh","[""luccioni2024""]","is_blank","for BLOOM-176B, which required, on average, 0.004 kWh of energy per query, or 40 times more than BLOOMz-7B, being roughly 25 times bigger [ 31] - although this included API deployment of the model, which is not the case for the models in our study.","To calculate the daily energy consumption for 1 billion queries, I need to convert the energy consumption of Flan-T5-xxl from kWh per 1,000 queries to MWh for 1 billion queries. The given energy consumption is 0.083 kWh per 1,000 queries. For 1 billion queries, this would be (0.083 kWh/1,000 queries) × 1,000,000,000 queries = 83,000 kWh = 83 MWh."
"q067","What was the average global data center PUE in 2023?","According to Ebert2024, 'The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].' This clearly states the global average PUE value for data centers in 2023 was 1.58.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","According to Ebert2024, 'The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].' This clearly states the global average PUE value for data centers in 2023 was 1.58."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","is_blank","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","is_blank"
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","According to the context, Cottier et al. found that 'when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs' and specifically states that 'Gemini Ultra has the highest fraction of R&D staff cost at 49%'.","49","percent","[""cottier2024""]","is_blank","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","According to the context, Cottier et al. found that 'when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs' and specifically states that 'Gemini Ultra has the highest fraction of R&D staff cost at 49%'."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context clearly states that the inaugural Study Panel of the One Hundred Year Study on AI had seventeen members. This is mentioned explicitly in the document: 'The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.'","17","people","[""stone2022""]","[""https://ai100.stanford.edu""]","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The context clearly states that the inaugural Study Panel of the One Hundred Year Study on AI had seventeen members. This is mentioned explicitly in the document: 'The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.'"
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","According to the context from [ref_id=wu2021a], manufacturing carbon cost accounts for 74% of the total footprint of client devices.","74","percent","[""wu2021a""]","is_blank","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices.","According to the context from [ref_id=wu2021a], manufacturing carbon cost accounts for 74% of the total footprint of client devices."
"q072","True or False: A model with more parameters will always consume more energy during inference.","According to the context, the relationship between model size and energy consumption is not always straightforward. In particular, the Fernandez2025 paper states 'Generally, models with more parameters consume more energy, but this is not always the case.' This is supported by a specific example in Chung2025 showing that Phi-3 Small (7B) can consume less energy than Phi-3 Mini (3.8B) at larger batch sizes, despite having nearly twice the parameters, due to differences in attention mechanisms (MHA vs GQA).","0","is_blank","[""chung2025"", ""fernandez2025""]","is_blank","Memory consumption of operations and energy amortization. Generally, models with more parameters consume more energy, but this is not always the case. Figure 4 highlights the case of Phi-3 Mini (3.8B) and Small (7B) [26]. Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows. This happens because Mini uses Multi-Head Attention (MHA) [76], whereas Small uses Grouped Query Attention (GQA) [10]. Due to this, Mini's KV cache uses 3 × more memory than Small, which prevents it from scaling to larger batch sizes and amortizing energy consumption across more generations.","According to the context, the relationship between model size and energy consumption is not always straightforward. In particular, the Fernandez2025 paper states 'Generally, models with more parameters consume more energy, but this is not always the case.' This is supported by a specific example in Chung2025 showing that Phi-3 Small (7B) can consume less energy than Phi-3 Mini (3.8B) at larger batch sizes, despite having nearly twice the parameters, due to differences in attention mechanisms (MHA vs GQA)."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","According to the context, specifically from document stone2022, the Study Panel explicitly states 'Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.' This directly contradicts the statement in the question.","0","is_blank","[""stone2022""]","is_blank","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind. No machines with self-sustaining long-term goals and intent have been developed, nor are they likely to be developed in the near future.","According to the context, specifically from document stone2022, the Study Panel explicitly states 'Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.' This directly contradicts the statement in the question."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","is_blank","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","is_blank"
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","According to the context, Meta reported that their Llama 3 family of models emitted 11,390 tons CO2e, which is over 40 times the 'five cars' estimate. The context specifically mentions that the 'five cars' estimate from Strubell et al. is equivalent to 284 metric tons CO2e.","11390","tCO2e","[""luccioni2025c""]","is_blank","recent first-hand reports of the estimated GHG emissions arising from language model pretraining typically exceed the 'five cars' estimate: Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the 'five cars' number, and Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the 'five cars' estimate.","According to the context, Meta reported that their Llama 3 family of models emitted 11,390 tons CO2e, which is over 40 times the 'five cars' estimate. The context specifically mentions that the 'five cars' estimate from Strubell et al. is equivalent to 284 metric tons CO2e."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","According to Figure 2(d) and multiple parts of the text, the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years period from Yr1-Q1 to Yr2-Q2.","2.9","multiplier","[""wu2021a""]","is_blank","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","According to Figure 2(d) and multiple parts of the text, the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years period from Yr1-Q1 to Yr2-Q2."
"q079","How many miles is the Earth from the Sun?","is_blank","is_blank","miles","[""is_blank""]","is_blank","is_blank","is_blank"
"q080","True or False: The AlphaGo program defeated the human Go champion.","According to multiple context snippets, AlphaGo did defeat the human Go champion. Stone2022 states that 'the AlphaGo program that recently defeated the current human champion at the game of Go' and specifically mentions 'AlphaGo beats human Go champ in milestone for artificial intelligence' from the Los Angeles Times.","1","is_blank","[""stone2022""]","is_blank","For example, the AlphaGo program160 161 that recently defeated the current human champion at the game of Go used multiple machine learning algorithms for training itself, and also used a sophisticated search procedure while playing the game.","According to multiple context snippets, AlphaGo did defeat the human Go champion. Stone2022 states that 'the AlphaGo program that recently defeated the current human champion at the game of Go' and specifically mentions 'AlphaGo beats human Go champ in milestone for artificial intelligence' from the Los Angeles Times."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","According to the context, continuous batching is the strategy that mitigates GPU under-utilization by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time. This is explicitly stated in multiple references.","continuous batching","is_blank","[""fernandez2025""]","is_blank","Continuous batching miti-gates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","According to the context, continuous batching is the strategy that mitigates GPU under-utilization by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time. This is explicitly stated in multiple references."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","According to the context from shen2024, it explicitly states 'The entire alignment process takes 60 H100 GPU hours.' This refers to the complete alignment process of JetMoE-8B-Chat which includes both the dSFT and dDPO fine-tuning steps.","60","H100 GPU hours","[""shen2024""]","is_blank","The entire alignment process takes 60 H100 GPU hours.","According to the context from shen2024, it explicitly states 'The entire alignment process takes 60 H100 GPU hours.' This refers to the complete alignment process of JetMoE-8B-Chat which includes both the dSFT and dDPO fine-tuning steps."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","According to Figure 6 and Table VI, for the offline workload with a 100 TPS SLO, the Max-Performance policy selected g6e.xlarge at a cost of $2.699, while InferSave selected g4dn.xlarge at a cost of $2.13. The percentage increase is calculated as: ($2.699-$2.13)/$2.13×100 = 26.7%.","26.7","percent","[""kim2025""]","is_blank","Given a SLO requirement of 100 TPS, InferSave selected g4dn.xlarge as its top choice, providing a throughput of about 160 TPS with the lowest total processing cost of $2.13. On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.","According to Figure 6 and Table VI, for the offline workload with a 100 TPS SLO, the Max-Performance policy selected g6e.xlarge at a cost of $2.699, while InferSave selected g4dn.xlarge at a cost of $2.13. The percentage increase is calculated as: ($2.699-$2.13)/$2.13×100 = 26.7%."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","According to the context from luccioni2024, the stable-diffusion-xl-base-1.0 model generates 1,594 grams of CO₂eq per 1,000 inferences, which is equivalent to driving 4.1 miles in an average gasoline-powered passenger vehicle.","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of 𝐶𝑂2𝑒𝑞 for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51]","According to the context from luccioni2024, the stable-diffusion-xl-base-1.0 model generates 1,594 grams of CO₂eq per 1,000 inferences, which is equivalent to driving 4.1 miles in an average gasoline-powered passenger vehicle."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","According to Table 2 from Luccioni et al. 2025, the range of GPU energy usage for performing 1,000 queries spans from 0.06 Wh (bert-tiny-finetuned-squadv2) to 3,426.38 Wh (Command-R Plus), showing a significant variance in energy consumption across different models and tasks.","[0.06, 3426.38]","Wh","[""luccioni2025c""]","is_blank","Table 2. Range of Inference Energy Use21 (Representative Models Displayed)
Model Organization GPU Energy for 1k Queries (Wh) Task
bert-tiny-finetuned-squadv2 mrm8488 0.06 Extractive QA
GIST-all-MiniLM-L6-v2 avsolatorio 0.11 Sentence Similarity
dynamic_tinybert Intel 0.21 Extractive QA
distilbert-imdb lvwerra 0.22 Text Classification
question_answering_v2 Falconsai 0.23 Extractive QA
Resnet 18 Microsoft 0.30 Image Classification
yolos-tiny hustvl 1.00 Object Detection
Vision Perceiver Conv Google 2.64 Image Classification
SFR-Embedding-Mistral Salesforce 5.22 Sentence Similarity
yolos-base hustvl 7.98 Object Detection
Gemma 7B Google 18.90 Text Generation
T5 11b Google 27.79 Text Classification
phi-4 Microsoft 28.74 Text Generation
T5 11b Google 178.13 Extractive QA
Mitsua Diffusion One Mitsua 186.81 Image Generation
Mixtral 8x7B Mistral 615.39 Text Generation
Stable Diffusion XL Base Stability AI 1,639.85 Image Generation
Llama 3 70B Meta 1,719.66 Text Generation
Qwen2.5 72B Qwen 1,869.55 Text Generation
Command-R Plus Cohere 3,426.38 Text Generation
Max/Min Variance: 57,106","According to Table 2 from Luccioni et al. 2025, the range of GPU energy usage for performing 1,000 queries spans from 0.06 Wh (bert-tiny-finetuned-squadv2) to 3,426.38 Wh (Command-R Plus), showing a significant variance in energy consumption across different models and tasks."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","According to Luccioni et al. (2025), 'There is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.' The authors explicitly state that a universal approach is not possible for addressing both ethics and sustainability in AI.","0","is_blank","[""luccioni2025b""]","is_blank","There is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.","According to Luccioni et al. (2025), 'There is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.' The authors explicitly state that a universal approach is not possible for addressing both ethics and sustainability in AI."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","According to the Patterson2021 document, section 2.4 on Energy mix improvement clearly states that the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","According to the Patterson2021 document, section 2.4 on Energy mix improvement clearly states that the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context clearly mentions Hivemind as the PyTorch-based framework used for decentralized training across clouds and continents. Section 2.1 specifically describes Hivemind as 'a PyTorch-based framework developed initially to enable collaborative DL training' that 'runs in a decentralized fashion and can handle peers that drop out at any stage of the training'.","Hivemind","is_blank","[""erben2023""]","is_blank","2.1 Hivemind
Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion. Its main difference to other state-of-the-art distributed training frameworks, such as PyTorch DDP [26] and DeepSpeed [35], is that it runs in a decentralized fashion and can handle peers that drop out at any stage of the training.","The context clearly mentions Hivemind as the PyTorch-based framework used for decentralized training across clouds and continents. Section 2.1 specifically describes Hivemind as 'a PyTorch-based framework developed initially to enable collaborative DL training' that 'runs in a decentralized fashion and can handle peers that drop out at any stage of the training'."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","According to the context, Ehsan et al. propose expanding the notion of transparency in AI to encompass 'social transparency,' which involves integrating socio-technical aspects in the description and understanding of AI systems. Social transparency involves portraying an AI system's societal impacts, ethical considerations, and environmental footprint.","social transparency","is_blank","[""luccioni2025b""]","is_blank","In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems [56]. Social transparency involves a portrayal of an AI system's societal impacts, ethical considerations, and eventually its environmental footprint.","According to the context, Ehsan et al. propose expanding the notion of transparency in AI to encompass 'social transparency,' which involves integrating socio-technical aspects in the description and understanding of AI systems. Social transparency involves portraying an AI system's societal impacts, ethical considerations, and environmental footprint."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","According to the context, the linear model using pre-trained sentence embeddings achieved the highest accuracy in the text classification experiments. This is explicitly stated in zschache2025: 'The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.' The model used multilingual sentence embeddings from 'https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2'.","linear model using pre-trained sentence embeddings","is_blank","[""zschache2025""]","is_blank","The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.","According to the context, the linear model using pre-trained sentence embeddings achieved the highest accuracy in the text classification experiments. This is explicitly stated in zschache2025: 'The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.' The model used multilingual sentence embeddings from 'https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2'."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","According to the paper, the authors developed and deployed 'Lamina', an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. This is explicitly mentioned multiple times in the paper.","Lamina","is_blank","[""chen2024""]","is_blank","To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","According to the paper, the authors developed and deployed 'Lamina', an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster. This is explicitly mentioned multiple times in the paper."
"q093","How many parameters does the largest T5 model have?","According to the context from Patterson 2021, the T5 model is described as having 11 billion parameters: 'T5 is a pre-trained language model that casts all NLP problems in a unified text-to-text format to enable application of transfer learning techniques to reduce the cost of training [Raf19]. The largest size has 11B parameters, and training used 86 MWh and produced 47 tCO2e.'","11000000000","parameters","[""patterson2021""]","is_blank","T5 is a pre-trained language model that casts all NLP problems in a unified text-to-text format to enable application of transfer learning techniques to reduce the cost of training [Raf19]. The largest size has 11B parameters, and training used 86 MWh and produced 47 tCO2e.","According to the context from Patterson 2021, the T5 model is described as having 11 billion parameters: 'T5 is a pre-trained language model that casts all NLP problems in a unified text-to-text format to enable application of transfer learning techniques to reduce the cost of training [Raf19]. The largest size has 11B parameters, and training used 86 MWh and produced 47 tCO2e.'"
"q094","What is the total number of parameters in the JetMoE-8B model?","According to Table 1 in the context, JetMoE-8B has a total parameter count of 8B (billion) parameters, as indicated by the 'Ptotal' value of 8B in the hyperparameters table. This is further confirmed by the model name itself 'JetMoE-8B' which indicates 8 billion parameters.","8000000000","parameters","[""shen2024""]","is_blank","Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp
8B 2B 24 2048 8 2 16 128 5632
Table 1: JetMoE-8B hyperparameters.","According to Table 1 in the context, JetMoE-8B has a total parameter count of 8B (billion) parameters, as indicated by the 'Ptotal' value of 8B in the hyperparameters table. This is further confirmed by the model name itself 'JetMoE-8B' which indicates 8 billion parameters."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","According to the provided context, Google's data center water consumption increased by approximately 20% from 2021 to 2022. This is mentioned in reference li2025b which states 'the company's data center water consumption increased by∼20% from 2021 to 2022 and by∼17% from 2022 to 2023'.","20","percent","[""li2025b""]","is_blank","Importantly, the company's data center water consumption increased by∼20% from 2021 to 2022 and by∼17% from 2022 to 2023 [4], and another technology company's data center water consumption saw∼34% and ∼22% increases over the same periods, respectively [6].","According to the provided context, Google's data center water consumption increased by approximately 20% from 2021 to 2022. This is mentioned in reference li2025b which states 'the company's data center water consumption increased by∼20% from 2021 to 2022 and by∼17% from 2022 to 2023'."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","According to Table I in document khan2025, the metric defined as 'CO2 emissions per unit of electricity consumed' is Carbon Intensity, measured in gCO2/kWh. This is explicitly stated in the table under the 'Definition' column for Carbon Intensity.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","TABLE I COMMON CARBON EMISSION METRICS IN GREEN AI
Metric Unit Definition Reference
Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed International Energy Agency","According to Table I in document khan2025, the metric defined as 'CO2 emissions per unit of electricity consumed' is Carbon Intensity, measured in gCO2/kWh. This is explicitly stated in the table under the 'Definition' column for Carbon Intensity."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","According to Table 2 in the context, which shows the parallel strategies and throughput for different growth stages of FLM-101B training, the FLOPs utilization for the 101B parameter model (the final growth stage) was 52.88%.","52.88","percent","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages. For NVIDIA A800 GPUs, the peak theoretical FLOPs per second is 312 teraFLOPs/sec. Gradient accumulation is applied for the large global batch size. Params Tensor Pipeline Data Number Batch teraFLOP/s FLOPs (billion) Parallel Size Parallel Size Parallel Size of GPUs Size per GPU Utilization [...] 101 4 4 12 192 2160 165 52.88%","According to Table 2 in the context, which shows the parallel strategies and throughput for different growth stages of FLM-101B training, the FLOPs utilization for the 101B parameter model (the final growth stage) was 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","According to the provided context in cottier2024, OpenAI's GPT-4 has 'the largest amortized hardware and energy cost, at $40M' as stated in section 3.2. This is explicitly mentioned when discussing the trend of increasing training costs.","40000000","USD","[""cottier2024""]","is_blank","Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.","According to the provided context in cottier2024, OpenAI's GPT-4 has 'the largest amortized hardware and energy cost, at $40M' as stated in section 3.2. This is explicitly mentioned when discussing the trend of increasing training costs."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","According to Figure 7 in the provided context, the operational power footprint of a Transformer-based language model (LM) for cross-lingual tasks can be reduced by 810× compared to a CPU baseline through full-stack optimization, including platform-level caching (6.7×), GPU acceleration (10.1×), and algorithmic optimization (12×) which includes halving precision and using custom operators.","810","multiplier","[""wu2021a""]","is_blank","Fig. 7. For the cross-lingual ML task (LM), the operational energy footprint can be signiﬁcantly reduced by more than 800× using platform-level caching, GPUs, low precision data format, and additional algorithmic optimization. In aggregate the optimizations reduce the infrastructure resources required to serve LM at scale by over 800×.","According to Figure 7 in the provided context, the operational power footprint of a Transformer-based language model (LM) for cross-lingual tasks can be reduced by 810× compared to a CPU baseline through full-stack optimization, including platform-level caching (6.7×), GPU acceleration (10.1×), and algorithmic optimization (12×) which includes halving precision and using custom operators."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","According to the context, when comparing the C-8 experiment (distributed across four continents) with the A-8 experiment (fully local), NLP experienced a performance drop of 41% compared to the local experiment. This means the throughput achieved was 59% of the local throughput, or 0.59 as a multiplier.","0.59","multiplier","[""erben2023""]","is_blank","However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).","According to the context, when comparing the C-8 experiment (distributed across four continents) with the A-8 experiment (fully local), NLP experienced a performance drop of 41% compared to the local experiment. This means the throughput achieved was 59% of the local throughput, or 0.59 as a multiplier."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","According to the context, AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023. This is clearly stated in multiple sections of the document.","3500000000","liters","[""amazon2023""]","is_blank","In 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities.","According to the context, AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023. This is clearly stated in multiple sections of the document."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","Multiple context snippets from the paper clearly state that custom tags can reduce energy consumption for all three prompt engineering techniques (zero-shot, one-shot, and few-shots). The paper reports specific percentage reductions for each technique when using custom tags, particularly with configuration C2.","1","is_blank","[""rubei2025""]","is_blank","Answer to RQ 1: Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.","Multiple context snippets from the paper clearly state that custom tags can reduce energy consumption for all three prompt engineering techniques (zero-shot, one-shot, and few-shots). The paper reports specific percentage reductions for each technique when using custom tags, particularly with configuration C2."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","According to the paper by Luccioni et al. (2025), NVIDIA shipped 4.7 million GPUs in 2024, which was more than a million units compared to 2023 when they shipped 3.76 million GPUs.","4700000","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","While efficiency improvements are being made to the hardware used for training and deploying AI models [ 9, 82, 89], NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].","According to the paper by Luccioni et al. (2025), NVIDIA shipped 4.7 million GPUs in 2024, which was more than a million units compared to 2023 when they shipped 3.76 million GPUs."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","According to the Cottier 2024 study, when breaking down the components of amortized hardware CapEx + energy, they find that on average, 44% goes toward AI accelerator chips.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","According to the Cottier 2024 study, when breaking down the components of amortized hardware CapEx + energy, they find that on average, 44% goes toward AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","According to multiple sources, Facebook's data centers have a Power Usage Effectiveness (PUE) of about 1.10. This is stated directly in ref_id=wu2021a which mentions that 'Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook's data centers are about 40% more efficient than small-scale, typical data centers.' This is also confirmed in ref_id=wu2021b.","1.10","PUE","[""wu2021a"", ""wu2021b""]","is_blank","Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook's data centers are about 40% more efﬁcient than small-scale, typical data centers.","According to multiple sources, Facebook's data centers have a Power Usage Effectiveness (PUE) of about 1.10. This is stated directly in ref_id=wu2021a which mentions that 'Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook's data centers are about 40% more efficient than small-scale, typical data centers.' This is also confirmed in ref_id=wu2021b."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","According to the provided context, the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems is called ETAIROS. This information appears multiple times in the document, specifically mentioning 'the Finnish ETAIROS (Ethical AI for the Governance of the Society) project'.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainabilit y, design and foresight for inter-disciplinary governance of AI systems [133]","According to the provided context, the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems is called ETAIROS. This information appears multiple times in the document, specifically mentioning 'the Finnish ETAIROS (Ethical AI for the Governance of the Society) project'."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","According to the context, Google's Gemini Ultra has an estimated amortized hardware and energy cost of $30M. This is explicitly stated in the document as one of the most expensive publicly-announced training runs to date.","30000000","USD","[""cottier2024""]","is_blank","We find that the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at $40M and Google's Gemini Ultra at $30M.","According to the context, Google's Gemini Ultra has an estimated amortized hardware and energy cost of $30M. This is explicitly stated in the document as one of the most expensive publicly-announced training runs to date."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","According to the context, the AI Act requires providers of GPAI models with systemic risk to conduct risk assessments (Art. 55(1)(b)), and the authors argue that these assessments should be interpreted to include environmental risks. The text states that 'both provisions relate to risks of the AI model or system for fundamental rights which, within the AI Act, must be interpreted as including environmental risks.' The authors make a case that environmental protection should be included in the AI Act's consideration of fundamental rights.","1","is_blank","[""ebert2024""]","is_blank","For providers of GPAI models with systemic risk and providers of HRAI systems, the Act mandates risk assessment and mitigation (Art. 55(1)(b) and Art. 9). We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8. Crucially, both provisions relate to risks of the AI model or system for fundamental rights which, within the AI Act, must be interpreted as including environmental risks [5].","According to the context, the AI Act requires providers of GPAI models with systemic risk to conduct risk assessments (Art. 55(1)(b)), and the authors argue that these assessments should be interpreted to include environmental risks. The text states that 'both provisions relate to risks of the AI model or system for fundamental rights which, within the AI Act, must be interpreted as including environmental risks.' The authors make a case that environmental protection should be included in the AI Act's consideration of fundamental rights."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","According to the context, the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³. This is mentioned multiple times in the document han2024, which also notes that this level is considerably higher than the WHO's recommended level of 5µg/m³.","9","µg/m³","[""han2024""]","is_blank","In fact, the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, considerably higher than the WHO's recommended level of 5µg/m³","According to the context, the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³. This is mentioned multiple times in the document han2024, which also notes that this level is considerably higher than the WHO's recommended level of 5µg/m³."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","According to the context from Luccioni2025a, a life cycle assessment (LCA) comparing print books to e-readers found that 115 books would produce the same amount of CO2 as a single Amazon Kindle device. This information is provided in a specific reference to environmental impacts of e-readers versus print books.","115","books","[""luccioni2025a""]","is_blank","For instance, a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","According to the context from Luccioni2025a, a life cycle assessment (LCA) comparing print books to e-readers found that 115 books would produce the same amount of CO2 as a single Amazon Kindle device. This information is provided in a specific reference to environmental impacts of e-readers versus print books."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","According to the context from 'han2024', some low-income counties experience significantly greater health costs from data center-related air pollutants, with per-household burdens exceeding those in other counties by more than 200-fold. This is specifically mentioned in multiple sections of the document, including both the introduction and conclusion.","200","multiplier","[""han2024""]","is_blank","In particular, some low-income counties experience significantly greater health costs, with per-household burdens exceeding those in other counties by more than 200-fold.","According to the context from 'han2024', some low-income counties experience significantly greater health costs from data center-related air pollutants, with per-household burdens exceeding those in other counties by more than 200-fold. This is specifically mentioned in multiple sections of the document, including both the introduction and conclusion."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","According to Table B1 in the Appendix of the referenced document, the DS Llama 70B model consumed 702.06 Wh of energy for the inference task on the FKTG dataset. This value is clearly specified in the 'Energy (Wh)' column for the 'DS Llama 70B' model row.","702.06","Wh","[""zschache2025""]","is_blank","Table B1 Measurements of all models for the inference task on the FKTG dataset, Capella system, single node, shown are averages over 10 runs
DS Llama 70B 2 702.06 0.46 2543.47 993.68","According to Table B1 in the Appendix of the referenced document, the DS Llama 70B model consumed 702.06 Wh of energy for the inference task on the FKTG dataset. This value is clearly specified in the 'Energy (Wh)' column for the 'DS Llama 70B' model row."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","is_blank","is_blank","parameters","[""is_blank""]","is_blank","is_blank","is_blank"
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The phenomenon described is Jevons' Paradox, which was proposed by economist William Stanley Jevons in the 19th century. It refers to when improved efficiency of a resource paradoxically leads to increased consumption of that resource rather than decreased consumption, as might be expected.","Jevons' Paradox","is_blank","[""luccioni2025a"", ""morrison2025""]","is_blank","Economists refer to such transformations as Jevons' Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries.","The phenomenon described is Jevons' Paradox, which was proposed by economist William Stanley Jevons in the 19th century. It refers to when improved efficiency of a resource paradoxically leads to increased consumption of that resource rather than decreased consumption, as might be expected."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","According to the context, GPT-3's training energy consumption was 1287 MWh as stated in li2025b and patterson2021. Meena's training energy consumption was 232 MWh as shown in patterson2021. Therefore, the number of Meena training runs that would use the same energy as a single GPT-3 training run is 1287 ÷ 232 = 5.55.","5.55","multiplier","[""li2025b"", ""patterson2021""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh [29]. (from li2025b); Energy Consumption (MWh) 7.5 85.7 232 24.1 179 1,287 (from patterson2021 table showing Meena as 232 MWh and GPT-3 as 1287 MWh)","According to the context, GPT-3's training energy consumption was 1287 MWh as stated in li2025b and patterson2021. Meena's training energy consumption was 232 MWh as shown in patterson2021. Therefore, the number of Meena training runs that would use the same energy as a single GPT-3 training run is 1287 ÷ 232 = 5.55."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","According to Table 2 from the Luccioni et al. 2024 study, the mean energy consumption for image generation is 2.907 kWh per 1,000 inferences, with a standard deviation of 3.31 kWh.","2.907","kWh","[""luccioni2024""]","is_blank","inference energy (kWh)
task mean std
text classification 0.002 0.001
extractive QA 0.003 0.001
masked language modeling 0.003 0.001
token classification 0.004 0.002
image classification 0.007 0.001
object detection 0.038 0.02
text generation 0.047 0.03
summarization 0.049 0.01
image captioning 0.063 0.02
image generation 2.907 3.31
Table 2.","According to Table 2 from the Luccioni et al. 2024 study, the mean energy consumption for image generation is 2.907 kWh per 1,000 inferences, with a standard deviation of 3.31 kWh."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","According to the context snippet from Strubell 2019, the average American life is associated with 36,156 pounds of CO2e emissions per year.","36156","lbs","[""strubell2019""]","is_blank","American life, avg, 1 year 36,156","According to the context snippet from Strubell 2019, the average American life is associated with 36,156 pounds of CO2e emissions per year."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","is_blank","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","is_blank"
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","According to Table III in the Khan 2025 paper, the CO2 emissions for Mistral-small before optimization were 0.020 kg per inference task, and after optimization, they were reduced to 0.015 kg. This represents a reduction to 75% of the original emissions, or a multiplier of 0.75.","0.75","multiplier","[""khan2025""]","is_blank","TABLE III
COMPARISON OF PERFORMANCE METRICS AND CARBON EMISSIONS FOR
FIVE LLM S BEFORE AND AFTER OPTIMIZATION . CARBON EMISSIONS ARE
CALCULATED PER INFERENCE TASK .

Model Name Precision Recall F1 Accuracy CO2
(kg)
Before
Optimization
Baseline metrics for comparison
Llama 3.2 0.55 0.45 0.44 0.45 0.012
Phi 3.2 0.97 0.82 0.88 0.82 0.012
Qwen 0.77 0.79 0.76 0.79 0.009
Mistral-small 0.70 0.67 0.65 0.67 0.020
Llava-Llama 3 0.58 0.50 0.48 0.50 0.014
After
Optimization
Metrics following quantization and local
inference techniques
Llama 3.2 0.57 0.48 0.47 0.48 0.005
Phi 3.2 1.00 0.84 0.91 0.84 0.007
Qwen 0.80 0.81 0.80 0.81 0.004
Mistral-small 0.73 0.70 0.69 0.70 0.015
Llava-Llama 3 0.61 0.54 0.51 0.54 0.006","According to Table III in the Khan 2025 paper, the CO2 emissions for Mistral-small before optimization were 0.020 kg per inference task, and after optimization, they were reduced to 0.015 kg. This represents a reduction to 75% of the original emissions, or a multiplier of 0.75."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","According to Table 5 in the Luccioni et al. study, the BLOOMz-7B model required 51,686 kWh for training and 7,571 kWh for fine-tuning, which adds up to a total of 59,257 kWh.","59257","kWh","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10^-4 7.3 × 10^-5 6.2 × 10^-5 5.4 × 10^-5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592
Table 5.","According to Table 5 in the Luccioni et al. study, the BLOOMz-7B model required 51,686 kWh for training and 7,571 kWh for fine-tuning, which adds up to a total of 59,257 kWh."
"q125","What is the total number of parameters in the final FLM-101B model?","According to the context, the FLM-101B model is described as having a hidden state dimension of 10,240, 80 layers, 80 attention heads, and a vocabulary size of 100,256. The document explicitly states that this is a 101 billion parameter model.","101000000000","parameters","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","According to the context, the FLM-101B model is described as having a hidden state dimension of 10,240, 80 layers, 80 attention heads, and a vocabulary size of 100,256. The document explicitly states that this is a 101 billion parameter model."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","According to ref_id=dodge2022, a full training run for a 6.1B parameter model would consume approximately 103,593 kWh. Using the inference energy cost of BLOOMz-7B (1.0 × 10^-4 kWh per inference) from ref_id=luccioni2024, we can calculate that approximately 1,035,930,000 inferences would be needed to match this training energy cost.","1035930000","inferences","[""dodge2022"", ""luccioni2024""]","is_blank","We estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","According to ref_id=dodge2022, a full training run for a 6.1B parameter model would consume approximately 103,593 kWh. Using the inference energy cost of BLOOMz-7B (1.0 × 10^-4 kWh per inference) from ref_id=luccioni2024, we can calculate that approximately 1,035,930,000 inferences would be needed to match this training energy cost."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","According to Luccioni et al. (2024), they report that 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.' This clearly states the total energy consumption for their study.","754.66","kWh","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.","According to Luccioni et al. (2024), they report that 'In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of CO2eq.' This clearly states the total energy consumption for their study."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","According to Table 5 in the context, the cost parity (number of inferences required to sum to the training cost) for BLOOMz-7B is 592,570,000. This represents the point where the cumulative energy used for inference equals the combined energy costs of training (51,686 kWh) and fine-tuning (7,571 kWh).","592570000","inferences","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592
Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.","According to Table 5 in the context, the cost parity (number of inferences required to sum to the training cost) for BLOOMz-7B is 592,570,000. This represents the point where the cumulative energy used for inference equals the combined energy costs of training (51,686 kWh) and fine-tuning (7,571 kWh)."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","According to the context, the dataset used for the German nuclear waste site objection texts is called the 'FKTG-dataset' (from 'Fachkonferenz Teilgebiete'). This dataset was published at beteiligung.bge.de/index.php and contained categorized statements from the population regarding the selection of a repository site for high-level radioactive waste in Germany.","FKTG-dataset","is_blank","[""zschache2025""]","is_blank","The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php). The text of the submission is given by the column 'Beitrag' (input). The column 'Themenkomplex' (topic) contains the category of the text.","According to the context, the dataset used for the German nuclear waste site objection texts is called the 'FKTG-dataset' (from 'Fachkonferenz Teilgebiete'). This dataset was published at beteiligung.bge.de/index.php and contained categorized statements from the population regarding the selection of a repository site for high-level radioactive waste in Germany."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context provides information about Meta's scope-2 water consumption for its global data center fleet in 2023, which was 3.7 L/kWh (calculated as 55,475 megaliters divided by 14,975,435 MWh). However, there is no specific information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context provides information about Meta's scope-2 water consumption for its global data center fleet in 2023, which was 3.7 L/kWh (calculated as 55,475 megaliters divided by 14,975,435 MWh). However, there is no specific information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","is_blank","is_blank","percent","[""is_blank""]","is_blank","is_blank","is_blank"
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","According to Patterson et al., the CO2e equivalent of NAS (Neural Architecture Search) is approximately 3 passengers taking a round trip between San Francisco and New York. This is explicitly stated in the context where they put the 3.2 net tCO2e into perspective by comparing it with passenger flights.","3","passengers","[""patterson2021""]","is_blank","Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","According to Patterson et al., the CO2e equivalent of NAS (Neural Architecture Search) is approximately 3 passengers taking a round trip between San Francisco and New York. This is explicitly stated in the context where they put the 3.2 net tCO2e into perspective by comparing it with passenger flights."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","According to the May 2025 data from OpenRouter presented in Figure 2 of the Luccioni et al. 2025 paper, 84% of LLM token usage was through models with no disclosure of environmental impact information.","84","percent","[""luccioni2025c""]","is_blank","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","According to the May 2025 data from OpenRouter presented in Figure 2 of the Luccioni et al. 2025 paper, 84% of LLM token usage was through models with no disclosure of environmental impact information."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","According to Table II in the context, which lists the bare minimum hardware required for different LLaMA model variants, LLaMA-13B requires 1 A100 80GB GPU for inference. This is explicitly stated in the table showing baseline configurations for LLaMA 7B, 13B, and 65B models.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. [...] Model Size V100 32GB A100 80GB Count Max. [...] Batch size Count Max. [...] Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128","According to Table II in the context, which lists the bare minimum hardware required for different LLaMA model variants, LLaMA-13B requires 1 A100 80GB GPU for inference. This is explicitly stated in the table showing baseline configurations for LLaMA 7B, 13B, and 65B models."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","According to the context from ref_id=dodge2022, the 6 billion parameter transformer model described was only partially trained (about 13% of a full run). The document states that if trained to completion, it would have emitted between 21 to 78 metric tons of CO2 depending on the region it was run in.","[21, 78]","metric tons","[""dodge2022""]","is_blank","If this had been trained to completion, we estimate it would have emitted 21 to 78 metric tons of CO2 (depending on the region it was run in).","According to the context from ref_id=dodge2022, the 6 billion parameter transformer model described was only partially trained (about 13% of a full run). The document states that if trained to completion, it would have emitted between 21 to 78 metric tons of CO2 depending on the region it was run in."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","is_blank","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","is_blank"
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","According to the context in griggs2024, a scenario using 2 A100s and 1 A10G resulted in a 24% cost saving over the A100-only strategy. This is explicitly stated in Figure 9's description.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","According to the context in griggs2024, a scenario using 2 A100s and 1 A10G resulted in a 24% cost saving over the A100-only strategy. This is explicitly stated in Figure 9's description."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","According to the context from Chen et al. (2024), the price per hour for an NVIDIA H20 is $4.63/hr. This price is marked with an asterisk because it's estimated using the relative complete system cost against H100, as H20 is not readily available on cloud service providers.","4.63","USD per hour","[""chen2024""]","is_blank","Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr *: As H20 is not readily available on cloud service providers, the listed price is estimated using the relative complete system cost against H100.","According to the context from Chen et al. (2024), the price per hour for an NVIDIA H20 is $4.63/hr. This price is marked with an asterisk because it's estimated using the relative complete system cost against H100, as H20 is not readily available on cloud service providers."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","According to the context from [ref_id=luccioni2025b], 'most carbon footprint analyses gather the information manually by writing to authors.' This indicates that the manual collection process is more common than automated collection, requiring direct outreach to authors.","0","is_blank","[""luccioni2025b""]","is_blank","In fact, most carbon foot print analyses gather the information manually by writing to authors. For instance, Luccioni and Hernandez-Garcia re ached out to over 500 authors of AI papers to get infor-mation needed to estimate the carbon footprint of their mode ls, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citi ng privacy concerns and lack of experimental logs [2023].","According to the context from [ref_id=luccioni2025b], 'most carbon footprint analyses gather the information manually by writing to authors.' This indicates that the manual collection process is more common than automated collection, requiring direct outreach to authors."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","According to the reference [han2024], the public health cost of U.S. data centers in 2023 was $6.7 billion, which is explicitly stated to be 'equivalent to approximately 44% of the data centers' total electricity cost.'","44","percent","[""han2024""]","is_blank","Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about$6.7 billion, or$47.5 per household, in 2023. This is equivalent to approximately 44% of the data centers' total electricity cost.","According to the reference [han2024], the public health cost of U.S. data centers in 2023 was $6.7 billion, which is explicitly stated to be 'equivalent to approximately 44% of the data centers' total electricity cost.'"
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","According to Table II in the context, which lists the bare minimum hardware requirements for different LLaMA model sizes, LLaMA-7B requires 1 A100 80GB GPU for inference with a maximum batch size of 64.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64
13B 2 64 1 64
65B 8 64 4 128","According to Table II in the context, which lists the bare minimum hardware requirements for different LLaMA model sizes, LLaMA-7B requires 1 A100 80GB GPU for inference with a maximum batch size of 64."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","According to Khan et al. (2025), their experimental results reveal that quantization and local inference techniques can reduce energy consumption and carbon emissions by up to 45% post quantization, making these methods particularly suitable for resource-constrained environments.","1","is_blank","[""khan2025""]","is_blank","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.","According to Khan et al. (2025), their experimental results reveal that quantization and local inference techniques can reduce energy consumption and carbon emissions by up to 45% post quantization, making these methods particularly suitable for resource-constrained environments."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","According to the context, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers.","95","answers","[""luccioni2025b"", ""luccioni2023""]","is_blank","We were able to collect information for a total of 95 models from 77 papers (since some of the papers trained more than one model), which represents an author response rate of 15.4 %.","According to the context, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","According to Shen et al. (2024), JetMoE-8B was trained with a limited $100,000 budget and used 30,000 H100 GPU hours. Dividing the total budget by the total GPU hours gives us $100,000/30,000 = $3.33 per H100 GPU-hour.","3.33","USD per hour","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","According to Shen et al. (2024), JetMoE-8B was trained with a limited $100,000 budget and used 30,000 H100 GPU hours. Dividing the total budget by the total GPU hours gives us $100,000/30,000 = $3.33 per H100 GPU-hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","According to han2024, the health cost of training a model of the Llama-3.1 scale in Iowa is dramatically higher compared to other locations and 'can even exceed 120% of the electricity cost.' While the document doesn't provide the exact percentage for Iowa specifically, it states that the health cost 'will increase dramatically to $2.5 million in Iowa' compared to only $0.23 million in Oregon.","120","percent","[""han2024""]","is_blank","We show the results in Table 2. It can be seen that the total health cost can even exceed 120% of the electricity cost and vary widely depending on the training data center locations. For example, the total health cost is only$0.23 million in Oregon, whereas the cost will increase dramatically to$2.5 million in Iowa due to various factors, such as the wind direction and the pollutant emission rate for electricity generation [76].","According to han2024, the health cost of training a model of the Llama-3.1 scale in Iowa is dramatically higher compared to other locations and 'can even exceed 120% of the electricity cost.' While the document doesn't provide the exact percentage for Iowa specifically, it states that the health cost 'will increase dramatically to $2.5 million in Iowa' compared to only $0.23 million in Oregon."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","According to the context, JetMoE-8B was trained on 1.25T tokens (1.25 trillion tokens) from mixed open-source datasets. This information appears consistently in multiple sections of the document.","1250000000000","tokens","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","According to the context, JetMoE-8B was trained on 1.25T tokens (1.25 trillion tokens) from mixed open-source datasets. This information appears consistently in multiple sections of the document."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","According to the Amazon Renewable Energy Projects table on page 26 of the 2023 Amazon Sustainability Report, as of January 2024, Amazon had announced 36 renewable energy projects in the United Kingdom.","36","projects","[""amazon2023""]","is_blank","Project Location
Number 
of Projects
Total MW 
Capacity†
...
United Kingdom 36 901","According to the Amazon Renewable Energy Projects table on page 26 of the 2023 Amazon Sustainability Report, as of January 2024, Amazon had announced 36 renewable energy projects in the United Kingdom."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","According to the Amazon Representation by the Numbers table on page 89 of the 2023 Amazon Sustainability Report, the gender breakdown for Amazon Workforce (All Levels) in the U.S. in 2023 shows that 53.2% identified as men.","53.2","percent","[""amazon2023""]","is_blank","Amazon Workforce (All Levels) Gender—U.S. 2023: 53.2% Men, 46.6% Women, 0.2% Other Gender","According to the Amazon Representation by the Numbers table on page 89 of the 2023 Amazon Sustainability Report, the gender breakdown for Amazon Workforce (All Levels) in the U.S. in 2023 shows that 53.2% identified as men."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","According to the context snippet from [ref_id=li2025b], Apple reports that its supply chain accounts for 99% of its total water footprint, as stated in reference [23] of that document.","99","percent","[""li2025b""]","is_blank","Although largely obscure, scope-3 water usage is likely significant [21]. For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].","According to the context snippet from [ref_id=li2025b], Apple reports that its supply chain accounts for 99% of its total water footprint, as stated in reference [23] of that document."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","Looking at Figure 4 in the document with ref_id=xia2024, I can see the execution time breakdown for BlackMamba with sparse fine-tuning at batch size 84. The graph shows that the total execution time for this configuration is approximately 2.0 seconds.","2.0","seconds","[""xia2024""]","is_blank","Fig. 4. Execution time breakdown.","Looking at Figure 4 in the document with ref_id=xia2024, I can see the execution time breakdown for BlackMamba with sparse fine-tuning at batch size 84. The graph shows that the total execution time for this configuration is approximately 2.0 seconds."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","According to the context, the authors introduced the 'granularity' metric to assess the ratio of calculation to communication time when deciding on distributed training suitability. This metric helps compare the scalability potential between different models on the same hardware and estimate training performance with additional resources.","granularity","is_blank","[""erben2023""]","is_blank","Granularity is important to evaluate scalability.We found that the ratio between calculation and communication time, granularity, is the most important metric to track when deciding on distributed training suitability. It enables us to compare the scalability potential between different models on the same hardware due to summarizing their model size and throughput ratio.","According to the context, the authors introduced the 'granularity' metric to assess the ratio of calculation to communication time when deciding on distributed training suitability. This metric helps compare the scalability potential between different models on the same hardware and estimate training performance with additional resources."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","According to the context, a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil to expand oil and gas production could add up to 640 percent (or 6.4 times) more carbon emissions compared to the company's carbon removal targets for the year.","6.4","times","[""luccioni2025a""]","is_blank","For instance, a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year [119], yet these numbers were not included in the company's carbon accounting and reporting efforts [118].","According to the context, a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil to expand oil and gas production could add up to 640 percent (or 6.4 times) more carbon emissions compared to the company's carbon removal targets for the year."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","Based on the context, the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses is 'water withdrawal'. The context explicitly states this definition in document li2025b.","Water withdrawal","is_blank","[""li2025b""]","is_blank","• Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either tem-porarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","Based on the context, the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses is 'water withdrawal'. The context explicitly states this definition in document li2025b."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","According to the context, specifically in the document [ref_id=stone2022], it states: 'As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.'","5","years","[""stone2022""]","is_blank","As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","According to the context, specifically in the document [ref_id=stone2022], it states: 'As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.'"
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","According to the Deloitte study cited in the context, every US household had an average of 25 connected devices in 2021. These include cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines.","25","devices","[""wu2021b""]","is_blank","At the personal level, every US household has an average of 25 connected devices such as cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, ﬁtness trackers, and connected ﬁtness machines [Deloitte, 2021].","According to the Deloitte study cited in the context, every US household had an average of 25 connected devices in 2021. These include cell phones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","According to the context, the energy required to pre-train an LLM ranges from as little as 0.8 MWh for OLMo 20M to 3,500 MWh for LLaMa 4 Scout. This information is directly stated in the document with ref_id=luccioni2025c.","[0.8, 3500]","MWh","[""luccioni2025c""]","is_blank","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).","According to the context, the energy required to pre-train an LLM ranges from as little as 0.8 MWh for OLMo 20M to 3,500 MWh for LLaMa 4 Scout. This information is directly stated in the document with ref_id=luccioni2025c."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","According to the text, IBM's Watson program 'beat human contenders to win the Jeopardy challenge in 2011.' This is mentioned multiple times in the context, specifically in the stone2022 document where it states that Watson 'was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.'","0","is_blank","[""stone2022""]","is_blank","IBM's Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","According to the text, IBM's Watson program 'beat human contenders to win the Jeopardy challenge in 2011.' This is mentioned multiple times in the context, specifically in the stone2022 document where it states that Watson 'was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.'"
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","According to Luccioni et al., one paper suggests that 10-50 queries on GPT-3 consumes around half a liter of water.","[10,50]","queries","[""luccioni2025a""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","According to Luccioni et al., one paper suggests that 10-50 queries on GPT-3 consumes around half a liter of water."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","According to Table 4 in the provided context, JetMoE-8B-chat achieved an MT-Bench score of 6.681, which is higher than Llama-2-13b-chat's score of 6.650. This information is explicitly stated in the context from reference shen2024.","6.681","score","[""shen2024""]","is_blank","Model MT-Bench Score
GPT-4 9.014
GPT-3.5-turbo 7.995
Claude-v1 7.923
JetMoE-8B-chat 6.681
Llama-2-13b-chat 6.650
Vicuna-13b-v1.3 6.413
Wizardlm-13b 6.353
Llama-2-7b-chat 6.269
Table 4: MT-Bench score comparison of various models","According to Table 4 in the provided context, JetMoE-8B-chat achieved an MT-Bench score of 6.681, which is higher than Llama-2-13b-chat's score of 6.650. This information is explicitly stated in the context from reference shen2024."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","According to Table 1 in Li et al. (2025), GPT-3 needs to consume a 500ml bottle of water for roughly 10-50 medium-length responses, depending on where it is deployed. The exact numbers vary by data center location, with values ranging from 10.5 responses in Washington to 70.4 responses in Ireland (shown in the rightmost column of Table 1).","[10.5,70.4]","responses","[""li2025b""]","is_blank","Additionally, GPT-3 needs to ""drink"" (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","According to Table 1 in Li et al. (2025), GPT-3 needs to consume a 500ml bottle of water for roughly 10-50 medium-length responses, depending on where it is deployed. The exact numbers vary by data center location, with values ranging from 10.5 responses in Washington to 70.4 responses in Ireland (shown in the rightmost column of Table 1)."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The Griggs et al. 2024 paper explicitly states that Mélange reduces costs by 9-77% for short-context tasks (interactive chats). This information appears in multiple places in the document, including the abstract and the results section where they specifically evaluate the Arena dataset as their short-context/conversational dataset.","77","percent","[""griggs2024""]","is_blank","Mélange reduces costs by 9-77% for short-context tasks (interactive chats), 2-33% for long-context tasks (document-based), and 4-51% in mixed-context workloads (§ 6).","The Griggs et al. 2024 paper explicitly states that Mélange reduces costs by 9-77% for short-context tasks (interactive chats). This information appears in multiple places in the document, including the abstract and the results section where they specifically evaluate the Arena dataset as their short-context/conversational dataset."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","According to multiple parts of the context, particularly in Table II and the discussion section, the bare minimum hardware requirement for LLaMA 65B inference is 4 A100 GPUs each with 80GB of memory, assuming no compression, optimization, quantization, or distillation.","4","A100_80GB_GPUs","[""samsi2024""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","According to multiple parts of the context, particularly in Table II and the discussion section, the bare minimum hardware requirement for LLaMA 65B inference is 4 A100 GPUs each with 80GB of memory, assuming no compression, optimization, quantization, or distillation."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","According to the han2024 document, training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. This is stated multiple times in the document, consistently using the figure of 'more than 10,000' round trips.","10000","round trips","[""han2024""]","is_blank","Our findings reveal that training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City.","According to the han2024 document, training a large AI model comparable to the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. This is stated multiple times in the document, consistently using the figure of 'more than 10,000' round trips."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","According to the context from patterson2021, NVIDIA estimated that 80-90% of the ML workload is inference processing, which is cited as [Leo19]. The range 80-90% directly answers the question about what percentage of the ML workload NVIDIA estimated to be inference processing in 2019.","85","percent","[""patterson2021""]","is_blank","For   example,   NVIDIA   estimated   that   80–90%   of   the   ML   workload   is   inference   processing   [Leo19].","According to the context from patterson2021, NVIDIA estimated that 80-90% of the ML workload is inference processing, which is cited as [Leo19]. The range 80-90% directly answers the question about what percentage of the ML workload NVIDIA estimated to be inference processing in 2019."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The 'Power Hungry Processing' (2024) study analyzed different ML models and tasks and reported the carbon emissions they produced for 1,000 queries as shown in Figure 1, but the context does not explicitly state the total amount of CO2eq emissions generated throughout the entire study. While it mentions that task-specific models emit 4-10g of CO2eq for 1,000 inferences and multi-purpose models emit 20-30g for the same task, this is per task rather than a study-wide total.","is_blank","kg CO2eq","[""luccioni2024""]","is_blank","The tasks examined in our study and the average quantity of carbon emissions they produced (in g of 𝐶𝑂 2𝑒𝑞) for 1,000 queries.","The 'Power Hungry Processing' (2024) study analyzed different ML models and tasks and reported the carbon emissions they produced for 1,000 queries as shown in Figure 1, but the context does not explicitly state the total amount of CO2eq emissions generated throughout the entire study. While it mentions that task-specific models emit 4-10g of CO2eq for 1,000 inferences and multi-purpose models emit 20-30g for the same task, this is per task rather than a study-wide total."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","According to multiple sources, estimating GPU energy consumption based on TDP consistently leads to overestimation. Chung2025 shows TDP-based estimates overestimate actual energy consumption by factors ranging from 1.0x to 4.1x compared to real measurements. This demonstrates TDP is not a reliable or accurate method for GPU energy estimation.","0","is_blank","[""chung2025"", ""fernandez2025"", ""luccioni2023""]","is_blank","Importance of measuring. The dimmed bars behind each solid bar in Figure 3 show the estimated energy consumption based on the GPU's Thermal Design Power (TDP) instead of measuring the real GPU power consumption, which is a common practice [8, 9, 28, 40, 47, 74]. Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time. In fact, such an estimation can lead to a worst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).","According to multiple sources, estimating GPU energy consumption based on TDP consistently leads to overestimation. Chung2025 shows TDP-based estimates overestimate actual energy consumption by factors ranging from 1.0x to 4.1x compared to real measurements. This demonstrates TDP is not a reliable or accurate method for GPU energy estimation."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","According to the context, GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries because it relies on older A100 GPU nodes instead of H100s. For long queries, GPT-4o consumes around 2.875 Wh while GPT-4o mini's consumption is slightly higher at 3.098 Wh.","0","is_blank","[""jegham2025""]","is_blank","For instance, GPT-4o mini, despite its smaller architecture, consumes approximately 20% more energy than GPT-4o on long queries due to reliance on older A100 GPU nodes. [...] For instance GPT-4o consumes around 2.875 Wh while GPT-4o mini's consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.","According to the context, GPT-4o mini consumes approximately 20% more energy than GPT-4o on long queries because it relies on older A100 GPU nodes instead of H100s. For long queries, GPT-4o consumes around 2.875 Wh while GPT-4o mini's consumption is slightly higher at 3.098 Wh."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","According to Figure 15 in the context, which shows the throughput estimation and validation for Mixtral-CS on different GPUs, we can see that the ground truth throughput (represented by dots) for Dense Mixtral-CS-A100-40GB at batch size 1 is approximately 0.3 queries/sec.","0.3","queries/sec","[""xia2024""]","is_blank","0 1 2 3 4 50.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
RMSE=0.03
Mixtral-CS-A100-40GB
Dense Sparse","According to Figure 15 in the context, which shows the throughput estimation and validation for Mixtral-CS on different GPUs, we can see that the ground truth throughput (represented by dots) for Dense Mixtral-CS-A100-40GB at batch size 1 is approximately 0.3 queries/sec."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","According to the paper, direct release of environmental information peaked in 2022 at 10% of notable models, but then the introduction of increasingly commercial and proprietary models after 2022 triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By Q1 2025, the majority of notable AI models again fell under the 'no disclosure' category.","0","is_blank","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category, as the line between research and commercial deployment became increasingly blurred.","According to the paper, direct release of environmental information peaked in 2022 at 10% of notable models, but then the introduction of increasingly commercial and proprietary models after 2022 triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By Q1 2025, the majority of notable AI models again fell under the 'no disclosure' category."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","According to the Griggs et al. (2024) paper, they normalized the H100 price to match pricing structures of major platforms. They calculated this by comparing RunPod's H100 cost ($4.69) to RunPod's A100-80G cost ($2.29), then adjusting relative to the A100's price on major clouds ($3.67), resulting in a normalized price of $7.516 for H100.","7.516","USD per hour","[""griggs2024""]","is_blank","Since on-demand H100 is not offered by these major providers, we defer to the pricing from RunPod [39] due to its popularity and availability. To ensure fair cost comparisons, we normalize RunPod's H100 pricing to match the pricing structures of major platforms. We calculate this by comparing RunPod's H100 cost ($4.69) to RunPod's A100-80G cost ($2.29), then adjusting relative to the A100's price on major clouds ($3.67), resulting in a normalized price of (4.69/2.29) × 3.67 = $7.516 for H100.","According to the Griggs et al. (2024) paper, they normalized the H100 price to match pricing structures of major platforms. They calculated this by comparing RunPod's H100 cost ($4.69) to RunPod's A100-80G cost ($2.29), then adjusting relative to the A100's price on major clouds ($3.67), resulting in a normalized price of $7.516 for H100."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","is_blank","is_blank","liters of water","[""is_blank""]","is_blank","is_blank","is_blank"
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","According to the context from reference 'griggs2024', serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs. To calculate the hourly cost, I divide $5,200 by 30 days and then by 24 hours per day.","7.22","USD per hour","[""griggs2024""]","is_blank","For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","According to the context from reference 'griggs2024', serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs. To calculate the hourly cost, I divide $5,200 by 30 days and then by 24 hours per day."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","According to the context from Wu et al. 2021, to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task requires a model that is 1,000× larger in size. This is explicitly stated in multiple places in the document.","1000","multiplier","[""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.","According to the context from Wu et al. 2021, to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task requires a model that is 1,000× larger in size. This is explicitly stated in multiple places in the document."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","According to the context, Strubell et al. estimated that training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 626,155 lbs of CO2. In the context from [ref_id=wu2021a], it's mentioned that the emissions of training one large ML model (Meena) is equivalent to 242,231 miles driven by an average passenger vehicle.","242231","miles","[""strubell2019"", ""wu2021a""]","is_blank","Training one model (GPU)
NLP pipeline (parsing, SRL) 39
w/ tuning & experimentation 78,468
Transformer (big) 192
w/ neural architecture search 626,155
Table 1: Estimated CO 2 emissions from training com-
mon NLP models, compared to familiar consumption.","According to the context, Strubell et al. estimated that training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 626,155 lbs of CO2. In the context from [ref_id=wu2021a], it's mentioned that the emissions of training one large ML model (Meena) is equivalent to 242,231 miles driven by an average passenger vehicle."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","According to the context, BLOOMz-7B requires 1.0 × 10^-4 kWh of energy per inference. If each of the 606,096 downloads resulted in 1 million inferences, the total energy would be: 606,096 downloads × 1,000,000 inferences per download × 1.0 × 10^-4 kWh per inference = 60,609.6 MWh.","60609.6","MWh","[""luccioni2024""]","is_blank","We can see that the amount of energy required per inference varies from 5.4× 10−5 for the smallest model, BLOOMz-560M to 1.0 × 10−4 kWh for the biggest one, BLOOMz-7B.","According to the context, BLOOMz-7B requires 1.0 × 10^-4 kWh of energy per inference. If each of the 606,096 downloads resulted in 1 million inferences, the total energy would be: 606,096 downloads × 1,000,000 inferences per download × 1.0 × 10^-4 kWh per inference = 60,609.6 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","According to the context, JetMoE-8B was trained using 30,000 H100 GPU hours. This information is explicitly stated in the document with ref_id=shen2024, which mentions that JetMoE-8B was 'trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.'","30000","H100 GPU hours","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","According to the context, JetMoE-8B was trained using 30,000 H100 GPU hours. This information is explicitly stated in the document with ref_id=shen2024, which mentions that JetMoE-8B was 'trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.'"
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","According to the paper by Cottier et al., if the current trend of growing development costs continues at a rate of 2.4× per year, the largest training runs will cost more than one billion dollars by 2027. This is based on extrapolating from GPT-4's cost of $40M in 2023 with the observed growth rate.","1000000000","USD","[""cottier2024""]","is_blank","If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.","According to the paper by Cottier et al., if the current trend of growing development costs continues at a rate of 2.4× per year, the largest training runs will cost more than one billion dollars by 2027. This is based on extrapolating from GPT-4's cost of $40M in 2023 with the observed growth rate."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","According to the context snippet from document patterson2021, OpenAI published the total number of floating point operations to train their GPT-3 model as 3.14E+23 FLOPS (i.e., 3.14 × 10^23).","3.14E+23","FLOPS","[""patterson2021""]","is_blank","OpenAI   published   the   total   number   of   floating   point   operations   to   train   their   model:   3.14E+23   [Bro20].","According to the context snippet from document patterson2021, OpenAI published the total number of floating point operations to train their GPT-3 model as 3.14E+23 FLOPS (i.e., 3.14 × 10^23)."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","According to multiple sources in the context, including Table II, 8 V100 GPUs each with 32 GB of RAM are required at minimum for any meaningful inferences with the 65B LLaMA model. This is explicitly stated in both the samsi2024 and rubei2025 references.","8","V100_32GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","According to multiple sources in the context, including Table II, 8 V100 GPUs each with 32 GB of RAM are required at minimum for any meaningful inferences with the 65B LLaMA model. This is explicitly stated in both the samsi2024 and rubei2025 references."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","According to the context, the 101B model was trained with 26.54B tokens, and Table 2 shows that in the 101B stage the throughput was 165 teraFLOPs/sec per GPU with 192 GPUs. To calculate the total zettaFLOPs, I need to multiply the teraFLOPs/sec × number of GPUs × training time required for 26.54B tokens.","4.4","zettaFLOPs","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages. For NVIDIA A800 GPUs, the peak theoretical FLOPs per second is 312 teraFLOPs/sec. Gradient accumulation is applied for the large global batch size. Params Tensor Pipeline Data Number Batch teraFLOP/s FLOPs (billion) Parallel Size Parallel Size Parallel Size of GPUs Size per GPU Utilization 101 4 4 12 192 2160 165 52.88%... Based on the speed test results and total budget, we train the 16B model with 245.37B tokens, the 51B model with 39.64B tokens, and the 101B model with 26.54B tokens.","According to the context, the 101B model was trained with 26.54B tokens, and Table 2 shows that in the 101B stage the throughput was 165 teraFLOPs/sec per GPU with 192 GPUs. To calculate the total zettaFLOPs, I need to multiply the teraFLOPs/sec × number of GPUs × training time required for 26.54B tokens."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","While there are multiple references to AlexNet in the provided context, none of them specifically mention the top-1 accuracy value of AlexNet on ImageNet. The context mentions AlexNet in comparison to other models or discusses it as a historical reference point, but does not provide its specific accuracy percentage.","is_blank","percent","[""is_blank""]","is_blank","is_blank","While there are multiple references to AlexNet in the provided context, none of them specifically mention the top-1 accuracy value of AlexNet on ImageNet. The context mentions AlexNet in comparison to other models or discusses it as a historical reference point, but does not provide its specific accuracy percentage."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","According to ref_id=li2025a, FLM-101B was trained on a cluster of 24 DGX-A800 GPU servers, with each server containing 8 A800 GPUs (80GB each). Therefore, the total number of GPUs used was 24 servers × 8 GPUs per server = 192 GPUs.","192","GPUs","[""li2025a""]","is_blank","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","According to ref_id=li2025a, FLM-101B was trained on a cluster of 24 DGX-A800 GPU servers, with each server containing 8 A800 GPUs (80GB each). Therefore, the total number of GPUs used was 24 servers × 8 GPUs per server = 192 GPUs."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","According to Strubell et al. (2019), the estimated CO2 emissions from training a large Transformer model with Neural Architecture Search (NAS) is 626,155 pounds or 284 metric tons of CO2. Given that the average American life produces 36,156 pounds of CO2 per year (also from Strubell et al.), this is equivalent to approximately 17.3 average American lifetimes (626,155 ÷ 36,156).","17.3","lifetimes","[""strubell2019""]","is_blank","Consumption CO2e (lbs)
Air travel, 1 passenger, NY ↔SF 1984
Human life, avg, 1 year 11,023
American life, avg, 1 year 36,156
Car, avg incl. fuel, 1 lifetime 126,000
Training one model (GPU)
NLP pipeline (parsing, SRL) 39
w/ tuning & experimentation 78,468
Transformer (big) 192
w/ neural architecture search 626,155
Table 1: Estimated CO2 emissions from training common NLP models, compared to familiar consumption.","According to Strubell et al. (2019), the estimated CO2 emissions from training a large Transformer model with Neural Architecture Search (NAS) is 626,155 pounds or 284 metric tons of CO2. Given that the average American life produces 36,156 pounds of CO2 per year (also from Strubell et al.), this is equivalent to approximately 17.3 average American lifetimes (626,155 ÷ 36,156)."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","According to the context, specifically in document [ref_id=schwartz2019], FAIR's RoBERTa was trained on 160GB of text and required around 25,000 GPU hours to train.","25000","hours","[""schwartz2019""]","is_blank","FAIR's RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","According to the context, specifically in document [ref_id=schwartz2019], FAIR's RoBERTa was trained on 160GB of text and required around 25,000 GPU hours to train."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","According to the context, Amazon's on-site solar energy systems are estimated to avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources. This information is explicitly stated in the document with reference ID 'amazon2023'.","47400","metric tons","[""amazon2023""]","is_blank","These on-site solar energy systems are estimated to generate 123,000 MWh annually—enough energy to power over 33,600 European homes—and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","According to the context, Amazon's on-site solar energy systems are estimated to avoid roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources. This information is explicitly stated in the document with reference ID 'amazon2023'."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","Based on the context, vllm library is mentioned as being used to deploy LLMs across multiple GPUs and nodes. Specifically, zschache2025 states 'LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.'","vllm","is_blank","[""zschache2025""]","[""https://github.com/vllm-project/vllm""]","LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.","Based on the context, vllm library is mentioned as being used to deploy LLMs across multiple GPUs and nodes. Specifically, zschache2025 states 'LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.'"
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","According to Table B2 in the zschache2025 document, when the Llama 3.1 70B model was deployed on two nodes instead of one, the energy consumption increased by a factor of 1.95, from 48.60 Wh for single node to 94.88 Wh for double node deployment.","1.95","multiplier","[""zschache2025""]","is_blank","Model Duration (s) Energy consumed (Wh)
single double ratio single double ratio
Llama 3.1 70B 161.59 304.77 1.89 48.60 94.88 1.95","According to Table B2 in the zschache2025 document, when the Llama 3.1 70B model was deployed on two nodes instead of one, the energy consumption increased by a factor of 1.95, from 48.60 Wh for single node to 94.88 Wh for double node deployment."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","According to the context, a study suggested that 10-50 queries on GPT-3 consumes around half a liter of water. Li et al. (2025b) further breaks this down by location, showing that one GPT-3 request consumes between 7.59 mL (Texas) to 47.51 mL (Washington) of water. This means a typical daily user session of 8 queries (as mentioned for GPT-4o users) would use approximately 60.7-380.1 mL of water, which converts to 0.016-0.10 gallons.","[0.016,0.10]","gallons of water","[""luccioni2025a"", ""li2025b"", ""jegham2025""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","According to the context, a study suggested that 10-50 queries on GPT-3 consumes around half a liter of water. Li et al. (2025b) further breaks this down by location, showing that one GPT-3 request consumes between 7.59 mL (Texas) to 47.51 mL (Washington) of water. This means a typical daily user session of 8 queries (as mentioned for GPT-4o users) would use approximately 60.7-380.1 mL of water, which converts to 0.016-0.10 gallons."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","According to the context, GPT-4o inference with 700 million daily queries would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum. These values are stated to exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh).","35000","homes","[""jegham2025""]","is_blank","These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even 325 universities (390,650 MWh) annually.","According to the context, GPT-4o inference with 700 million daily queries would require approximately 391,509 MWh annually at minimum and 463,269 MWh at maximum. These values are stated to exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh)."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","According to the context, Microsoft reported a 34% increase in global water consumption between 2021 and 2022. This is explicitly stated in the document by Luccioni et al. (2025), which mentions 'Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons'.","34","percent","[""luccioni2025a""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [ 42, 78].","According to the context, Microsoft reported a 34% increase in global water consumption between 2021 and 2022. This is explicitly stated in the document by Luccioni et al. (2025), which mentions 'Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons'."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","According to the context from zschache2025, 'In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.' This clearly states that traditional models underperform compared to LLMs on the Yelp dataset, making the statement false.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","According to the context from zschache2025, 'In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.' This clearly states that traditional models underperform compared to LLMs on the Yelp dataset, making the statement false."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","According to the context, the PUE for Google's Iowa datacenter where the Evolved Transformer was run is explicitly stated as 1.11. This is mentioned directly in the document and also appears in table data comparing it to the US average PUE of 1.59.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","According to the context, the PUE for Google's Iowa datacenter where the Evolved Transformer was run is explicitly stated as 1.11. This is mentioned directly in the document and also appears in table data comparing it to the US average PUE of 1.59."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","According to the context, a decaying growth pattern from June to December 2025 will yield a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024.","772000000000","queries","[""jegham2025""]","is_blank","This is followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].","According to the context, a decaying growth pattern from June to December 2025 will yield a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","According to Table 3 in the context, JetMoE-8B achieved a score of 53.0 as the OpenLLM Leaderboard Average. This is explicitly stated in the evaluation section where the performance of JetMoE-8B is compared with other models like LLaMA2, DeepseekMoE, and Gemma.","53.0","score","[""shen2024""]","is_blank","JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM leaderboard, achieving the best scores in all tasks except ARC-challenge and WinoGrande... OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0","According to Table 3 in the context, JetMoE-8B achieved a score of 53.0 as the OpenLLM Leaderboard Average. This is explicitly stated in the evaluation section where the performance of JetMoE-8B is compared with other models like LLaMA2, DeepseekMoE, and Gemma."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide specific information about the number of AI training runs conducted globally on renewable-only power in 2022. While there is discussion about energy sources for AI training in general and mentions of renewable energy adoption by companies like Amazon, there is no quantitative data specifically about the number of global AI training runs using only renewable power in 2022.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the number of AI training runs conducted globally on renewable-only power in 2022. While there is discussion about energy sources for AI training in general and mentions of renewable energy adoption by companies like Amazon, there is no quantitative data specifically about the number of global AI training runs using only renewable power in 2022."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","According to the context in document ebert2024, there is an 'open-source exemption' from reporting obligations that should be removed, as 'making parts of a model public does not justify exclusion from environmental accountability.' The document states that open-source models can have significant environmental impacts and should adhere to the same reporting standards as proprietary models.","1","is_blank","[""ebert2024""]","is_blank","• Elimination of the Open-Source Exemption : The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability [4]. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.","According to the context in document ebert2024, there is an 'open-source exemption' from reporting obligations that should be removed, as 'making parts of a model public does not justify exclusion from environmental accountability.' The document states that open-source models can have significant environmental impacts and should adhere to the same reporting standards as proprietary models."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","According to the context from Patterson 2021, 'The US national datacenter average in 2018 was 1.58, which is the value [Str19] used; In 2020, it was 1.59.' This clearly states the US national datacenter average PUE for 2020.","1.59","PUE","[""patterson2021""]","is_blank","The   US   national   datacenter   average   in   2018   was   1.58,   which   is   the   value   [Str19]     used ;    In   2020,   it   was   1.59 .","According to the context from Patterson 2021, 'The US national datacenter average in 2018 was 1.58, which is the value [Str19] used; In 2020, it was 1.59.' This clearly states the US national datacenter average PUE for 2020."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","According to the context from kim2025, when analyzing the OPT_2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens, the KV Cache expands to 5.312GB when the batch size increases to 32.","5.312","GB","[""kim2025""]","is_blank","When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","According to the context from kim2025, when analyzing the OPT_2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens, the KV Cache expands to 5.312GB when the batch size increases to 32."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","According to the paper by Cottier et al., they conducted an in-depth study of four notable models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra) and found that 'when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model.'","[29, 49]","percent","[""cottier2024""]","is_blank","We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model.","According to the paper by Cottier et al., they conducted an in-depth study of four notable models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra) and found that 'when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model.'"
"q213","Which software package was used to measure energy consumption during inference runs?","According to multiple references in the context, the energy consumption during inference runs was measured using the CodeCarbon package. This is explicitly stated in references [ref_id=zschache2025] and [ref_id=morrison2025].","CodeCarbon","is_blank","[""zschache2025"", ""morrison2025""]","is_blank","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon).","According to multiple references in the context, the energy consumption during inference runs was measured using the CodeCarbon package. This is explicitly stated in references [ref_id=zschache2025] and [ref_id=morrison2025]."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","According to the analysis of 100 news articles published as of April 11, 2025, 53% cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. This information is explicitly stated in the context from document luccioni2025c.","53","percent","[""luccioni2025c""]","is_blank","Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search 42, 22% mention other precise energy numbers for ChatGPT queries, comparing them to the number of American households or LED light bulbs43 (likely using the same 3 Wh figure)","According to the analysis of 100 news articles published as of April 11, 2025, 53% cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. This information is explicitly stated in the context from document luccioni2025c."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","According to the context, InferSave proposes the 'Compute Time Calibration Function (CTCF)' to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. This function is described as a linear transformation that corrects for differences between theoretical FLOPS values provided by GPU manufacturers and actual computational performance in LLM inference workloads.","Compute Time Calibration Function","is_blank","[""kim2025""]","is_blank","Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","According to the context, InferSave proposes the 'Compute Time Calibration Function (CTCF)' to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. This function is described as a linear transformation that corrects for differences between theoretical FLOPS values provided by GPU manufacturers and actual computational performance in LLM inference workloads."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","According to the context, increasing the number of shards for LLaMA 65B increases the energy costs of inference per response. This is explicitly stated in the context: 'Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall'.","1","is_blank","[""samsi2024""]","is_blank","Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall while increasing the maximum generation length from 512 (Figure 8) to 1024 (Figure 9) does not induce a clear or significant effect in inference energy costs.","According to the context, increasing the number of shards for LLaMA 65B increases the energy costs of inference per response. This is explicitly stated in the context: 'Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall'."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","According to the Morrison 2025 paper, an H100 GPU is 0.1% rare earth metal by mass, and mining 1 kg of rare earth materials consumes about 11 kL of water. One 12-inch silicon wafer weighs 125 grams and produces about 63 H100s. To calculate the water consumption for a single H100, I need to determine how much rare earth metal is in one H100 and then multiply by the water consumption rate.","0.0022","kL","[""morrison2025""]","is_blank","We additionally estimate the environmental impact from mining rare earth metals used during manufacturing, assuming an H100 is 0.1% rare earth metal by mass. Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams 12 and produces about 63 H100s. 13 14 Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU.","According to the Morrison 2025 paper, an H100 GPU is 0.1% rare earth metal by mass, and mining 1 kg of rare earth materials consumes about 11 kL of water. One 12-inch silicon wafer weighs 125 grams and produces about 63 H100s. To calculate the water consumption for a single H100, I need to determine how much rare earth metal is in one H100 and then multiply by the water consumption rate."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","According to the context, open-source GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). This is highlighted in the document as one of the issues with the current AI Act, with the authors recommending the 'Elimination of the Open-Source Exemption' as one of their policy proposals.","0","is_blank","[""ebert2024""]","is_blank","Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). The idea is that OS models, by definition, already disclose certain types of information.","According to the context, open-source GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). This is highlighted in the document as one of the issues with the current AI Act, with the authors recommending the 'Elimination of the Open-Source Exemption' as one of their policy proposals."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","According to the document with ref_id 'luccioni2025a', in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide, as cited from reference [131] in that document.","30","percent","[""luccioni2025a""]","is_blank","In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131], changing the scope and extent of the mechanism as a whole.","According to the document with ref_id 'luccioni2025a', in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide, as cited from reference [131] in that document."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","According to Table 1 in the paper, U.S. data centers resulted in a total public health cost of about $6.7 billion in 2023. This is explicitly stated in the text as 'the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023'.","6700000000","USD","[""han2024""]","is_blank","Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023.","According to Table 1 in the paper, U.S. data centers resulted in a total public health cost of about $6.7 billion in 2023. This is explicitly stated in the text as 'the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023'."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","According to Table 4, which shows energy consumption per model across three prompt sizes, the o3 model consumes 12.222 Wh for a long prompt (10k input-1.5k output), while GPT-4.1 nano consumes 0.827 Wh for the same prompt size. Dividing 12.222 by 0.827 gives approximately 14.78.","14.78","multiplier","[""jegham2025""]","is_blank","GPT-4.1 nano remains among the most efficient proprietary models at 0.827 Wh, but still consumes nearly twice the energy of LLaMA-3.1-8B. In contrast, DeepSeek-R1 (DS) consumes 29.075 Wh, around sixty five times more than the most efficient model, underscoring the large overhead of reasoning models. [...] Model Energy Consumption(100 input-300 output)(Wh) Energy Consumption(1k input-1k output)(Wh) Energy Consumption(10k input-1.5k output)(Wh) [...] GPT-4.1 nano 0.207 ± 0.047 0.575 ± 0.108 0.827 ± 0.094 [...] o3 1.177 ± 0.224 5.153 ± 2.107 12.222 ± 1.082","According to Table 4, which shows energy consumption per model across three prompt sizes, the o3 model consumes 12.222 Wh for a long prompt (10k input-1.5k output), while GPT-4.1 nano consumes 0.827 Wh for the same prompt size. Dividing 12.222 by 0.827 gives approximately 14.78."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","According to Table 3 in ref_id=griggs2024, for the short-context Arena dataset with SLO=120ms, Mélange achieved cost reductions ranging from 15.35% (compared to A10G-only at 1 req/s) to 77.25% (compared to H100-only at 1 req/s) across different request rates. This range is explicitly shown in the cost savings column.","[15.35, 77.25]","percent","[""griggs2024""]","is_blank","Table 3: Instance allocations for the short-context Arena dataset, SLO=120ms. Rate (req/s) Solver L4 A10G A100 H100 Norm. Cost ($/hr) Savings 1 Mélange 1 1 1.71 N/A H100-only 1 7.516 77.25% A100-only 1 3.67 53.41% A10G-only 2 2.02 15.35% L4-only 3 2.1 18.57%","According to Table 3 in ref_id=griggs2024, for the short-context Arena dataset with SLO=120ms, Mélange achieved cost reductions ranging from 15.35% (compared to A10G-only at 1 req/s) to 77.25% (compared to H100-only at 1 req/s) across different request rates. This range is explicitly shown in the cost savings column."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","According to Table 3 in the Li et al. (2025) reference, the net tCO2e (net carbon emissions in metric tons of CO2 equivalent) for the pre-training of FLM-101B is explicitly stated as 26 metric tons.","26","tCO2e","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. For details, please see the corresponding references. The definitions of TDP, nettCO2e, and their formulas are the same as (Patterson et al. 2021). [...] net tCO2e [...] FLM-101B [...] 26","According to Table 3 in the Li et al. (2025) reference, the net tCO2e (net carbon emissions in metric tons of CO2 equivalent) for the pre-training of FLM-101B is explicitly stated as 26 metric tons."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","From Figure 8 in the document, which shows query throughput of Mixtral and BlackMamba, I can see that the sparse Mixtral model with batch size of 1 has a throughput of 0.3 queries per second. Since throughput is queries/second, the execution time per query is 1/0.3 = 3.33 seconds.","3.33","seconds","[""xia2024""]","is_blank","Mixtral-CS0.0
0.5
1.0
1.5
2.0
0.3 0.5 0.3 0.7
1.7
Dense(bsz=1)
Dense(bsz=2)
Sparse(bsz=1)
Sparse(bsz=2)
Sparse(bsz=8)
      Throughput (quries/second)
Fig. 8. Query throughput of Mixtral and BlackMamba.","From Figure 8 in the document, which shows query throughput of Mixtral and BlackMamba, I can see that the sparse Mixtral model with batch size of 1 has a throughput of 0.3 queries per second. Since throughput is queries/second, the execution time per query is 1/0.3 = 3.33 seconds."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","According to the context, particularly in han2024, the public health costs of AI are highly unevenly distributed across communities in the U.S. The text explicitly states that 'the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities.' The document also mentions that 'disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas.'","0","is_blank","[""han2024""]","is_blank","The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities [31, 103]. For example, as shown in Table 6c, all the top-10 most impacted counties in the U.S. have lower median household incomes than the national median value. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.","According to the context, particularly in han2024, the public health costs of AI are highly unevenly distributed across communities in the U.S. The text explicitly states that 'the public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities.' The document also mentions that 'disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas.'"
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","According to the context in document wu2021b, Figure 2 explicitly states that 'As a result of Moore's law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years'.","1","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore's law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","According to the context in document wu2021b, Figure 2 explicitly states that 'As a result of Moore's law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years'."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","According to the case study in khan2025, 4-bit quantization was applied through Ollama, which is described as 'an open-source platform known for its support of edge computing principles and privacy-centric deployments'.","Ollama","is_blank","[""khan2025""]","is_blank","We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","According to the case study in khan2025, 4-bit quantization was applied through Ollama, which is described as 'an open-source platform known for its support of edge computing principles and privacy-centric deployments'."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","According to the context, the researchers used Backblaze (B2) as the independent S3 storage provider. They accessed the datasets on-demand via shards in the tar format using the WebDataset library, which was chosen for features like automatic local caching, streaming decompression, and streaming preprocessing.","Backblaze","is_blank","[""erben2023""]","is_blank","To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4]. Backblaze has replicated data centers that can better serve requests from anywhere worldwide, guaranteeing a reasonable ingress rate from every continent. Additionally, the cost is very manageable at $0.01/GB rate for egress and $0.005/GB/month for storage.","According to the context, the researchers used Backblaze (B2) as the independent S3 storage provider. They accessed the datasets on-demand via shards in the tar format using the WebDataset library, which was chosen for features like automatic local caching, streaming decompression, and streaming preprocessing."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","According to Zschache et al. (2025), there is 'a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible'. This suggests a nearly linear relationship between runtime and energy consumption during LLM inference.","zschache2025","is_blank","[""zschache2025""]","is_blank","Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.","According to Zschache et al. (2025), there is 'a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible'. This suggests a nearly linear relationship between runtime and energy consumption during LLM inference."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","According to the context from document ebert2024, 'The bill for an AI Environmental Impacts Act [...] was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024.'","Edward J. Markey","is_blank","[""ebert2024""]","is_blank","Relating to AI more specifically, although not limited to data centers, is a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024 [ 78].","According to the context from document ebert2024, 'The bill for an AI Environmental Impacts Act [...] was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024.'"
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","According to Chen et al. (2024) in Table 1, the price per chip for an NVIDIA H100 is listed as $11.06/hr. This is clearly stated in the specifications comparison table of H100, H20, and TPU v6e.","11.06","USD per hour","[""chen2024""]","is_blank","Table 1: H100, H20, and TPU v6e specifications.
H100 H20 TPU v6e [7]
BF16 TFLOPs 989 148 918
Memory capacity 80 GB 96 GB 32 GB
Memory bandwidth 3.35 TB/s 4.0 TB/s 1.64 TB/s
Power rating 700 W 400 W unlisted
Inter-chip bandwidth 450 GB/s 450 GB/s 448 GB/s
Network bandwidth 400 Gbps 400 Gbps 200 Gbps
Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","According to Chen et al. (2024) in Table 1, the price per chip for an NVIDIA H100 is listed as $11.06/hr. This is clearly stated in the specifications comparison table of H100, H20, and TPU v6e."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","According to the context in ref_id=luccioni2025a, GPUs in AI data centers can theoretically last about five years, but the push for higher performance is prompting more frequent upgrades, suggesting actual lifespans are shorter than the theoretical maximum.","5","years","[""luccioni2025a""]","is_blank","High turnover in AI hardware is accelerating e-waste output: although GPUs can theoretically last about five years, the push for higher performance is prompting more frequent upgrades – one recent study estimates that AI will generate an additional 1.2–5 million metric tons of e-waste by 2030 [134].","According to the context in ref_id=luccioni2025a, GPUs in AI data centers can theoretically last about five years, but the push for higher performance is prompting more frequent upgrades, suggesting actual lifespans are shorter than the theoretical maximum."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","According to Table II in the context, which lists the bare minimum hardware required for different LLaMA models, LLaMA 13B requires 2 V100 32GB GPUs as the minimum hardware configuration to run inference with a maximum response length of 256.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. These limits are imposed by a combination of GPU memory, model size, response length and the number of GPUs. While the 65B model can sharded across 6 V100 GPUs, we use 8 since the model architecture makes it better suited for balanced sharding across 8 GPUs. Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128","According to Table II in the context, which lists the bare minimum hardware required for different LLaMA models, LLaMA 13B requires 2 V100 32GB GPUs as the minimum hardware configuration to run inference with a maximum response length of 256."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","According to the context, Google reported that training their open source Gemma family of language models emitted 1247.61 tons CO2e, which is over 4 times the 'five cars' estimate from Strubell et al. (which was 284 metric tons CO2e).","1247.61","tCO2e","[""luccioni2025c"", ""strubell2019""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the 'five cars' number... they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.","According to the context, Google reported that training their open source Gemma family of language models emitted 1247.61 tons CO2e, which is over 4 times the 'five cars' estimate from Strubell et al. (which was 284 metric tons CO2e)."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","According to ref_id=strubell2019, Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). Although the context mentions training ELMo on 3 NVIDIA GTX 1080 Ti GPUs for measurement purposes, the original training time is reported as 336 hours.","336","hours","[""strubell2019""]","is_blank","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","According to ref_id=strubell2019, Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). Although the context mentions training ELMo on 3 NVIDIA GTX 1080 Ti GPUs for measurement purposes, the original training time is reported as 336 hours."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","According to multiple snippets from document li2025b, the U.S. national average water consumption for electricity generation is estimated at about 3.1-3.14 L/kWh. The document consistently refers to this figure across multiple sections, including on page 4 where it states 'the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.'","3.1","L/kWh","[""li2025b""]","is_blank","For electricity gener-ation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","According to multiple snippets from document li2025b, the U.S. national average water consumption for electricity generation is estimated at about 3.1-3.14 L/kWh. The document consistently refers to this figure across multiple sections, including on page 4 where it states 'the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.'"
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","According to the document [ref_id=wu2021b], Google's hyperscale data centers had improved from a PUE of 1.21 in 2008 to 1.10 in 2021. This information is explicitly stated in Figure 1 of the document.","1.10","PUE","[""wu2021b""]","is_blank","Figure 1: PUE of hyperscalar datacenters, such as Google's, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook] and the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].","According to the document [ref_id=wu2021b], Google's hyperscale data centers had improved from a PUE of 1.21 in 2008 to 1.10 in 2021. This information is explicitly stated in Figure 1 of the document."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","According to the context, research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","96","percent","[""amazon2023""]","is_blank","Research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","According to the context, research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","According to the text, the study's analytical model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","3460","USD","[""xia2024""]","is_blank","For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","According to the text, the study's analytical model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","According to the context, GPUs account for approximately 74% of the total energy consumption in a server during AI workloads. This is based on measurements when training BERT base on a single NVIDIA TITAN X GPU, where the GPU consumed 187.1W out of a total 251.6W across all measured components.","74","percent","[""dodge2022""]","is_blank","The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). Power consumption is averaged across instantaneous measurements over 12 hours of training on using the masked language modeling objective. The GPU alone accounts for 74% of the total energy consumption due to these components.","According to the context, GPUs account for approximately 74% of the total energy consumption in a server during AI workloads. This is based on measurements when training BERT base on a single NVIDIA TITAN X GPU, where the GPU consumed 187.1W out of a total 251.6W across all measured components."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","According to the context, JetMoE-8B training was conducted on a cluster containing 12 nodes and 96 H100s, as explicitly stated in the document with ref_id=shen2024.","96","H100 GPUs","[""shen2024""]","is_blank","We con-
duct training on a cluster containing 12 nodes and 96 H100s.","According to the context, JetMoE-8B training was conducted on a cluster containing 12 nodes and 96 H100s, as explicitly stated in the document with ref_id=shen2024."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","According to the context from morrison2025, during OLMo 2 7B training, the average GPU power for a single node while actively training is over 600W, which is over 85% of an H100's maximum power draw of 700W.","600","Watts","[""morrison2025""]","is_blank","When actively training, the average GPU power is over 600W, over 85% of an H100's maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.","According to the context from morrison2025, during OLMo 2 7B training, the average GPU power for a single node while actively training is over 600W, which is over 85% of an H100's maximum power draw of 700W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","According to the context from document strubell2019, the estimated CO2 emissions for an average human life globally for one year is 11,023 pounds of CO2e, as listed in a table comparing emissions from various sources including human life averages, American life averages, car lifetimes, and NLP model training.","11023","lbs","[""strubell2019""]","is_blank","Consumption CO 2e (lbs)
Air travel, 1 passenger, NY ↔SF 1984
Human life, avg, 1 year 11,023
American life, avg, 1 year 36,156
Car, avg incl.","According to the context from document strubell2019, the estimated CO2 emissions for an average human life globally for one year is 11,023 pounds of CO2e, as listed in a table comparing emissions from various sources including human life averages, American life averages, car lifetimes, and NLP model training."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","According to the context, for LLaMA 13B, there was a 1.25 times increase in inference throughput on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","1.25","multiplier","[""samsi2024""]","is_blank","particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","According to the context, for LLaMA 13B, there was a 1.25 times increase in inference throughput on the A100 when compared to the V100 across words per second, tokens per second, and responses per second."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","According to the context, a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). This is stated multiple times in the context, specifically in section 6.1 about the energy cost of a single GPT-4o user session.","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","According to the context, a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). This is stated multiple times in the context, specifically in section 6.1 about the energy cost of a single GPT-4o user session."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","According to Table V and the analysis in the context, with an SLO requirement of 400 TPS, Max-Performance selected g6e.xlarge at a cost of $2.699, while InferSave selected g4dn.xlarge at a cost of $0.71. The text explicitly states that g6e.xlarge is 'about 280% more expensive than InferSave's top choice'.","280","percent","[""kim2025""]","is_blank","With an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of $0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave's top choice.","According to Table V and the analysis in the context, with an SLO requirement of 400 TPS, Max-Performance selected g6e.xlarge at a cost of $2.699, while InferSave selected g4dn.xlarge at a cost of $0.71. The text explicitly states that g6e.xlarge is 'about 280% more expensive than InferSave's top choice'."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","According to the context from document zschache2025, for models generating a single token per inference, V100 or A30 GPUs are more efficient in inference compared to more powerful GPUs like H100.","V100 or even a A30","is_blank","[""zschache2025""]","is_blank","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.","According to the context from document zschache2025, for models generating a single token per inference, V100 or A30 GPUs are more efficient in inference compared to more powerful GPUs like H100."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","According to the text in ref_id=schwartz2019, ""Reporting the computational price tag of finding, training, and running models is a key Green AI practice."" This explicitly connects Green AI with reporting the financial cost of model development and operation.","1","is_blank","[""schwartz2019""]","is_blank","Reporting the computational price tag of ﬁnding, training, and running models is a key Green AI practice (see Equation 1).","According to the text in ref_id=schwartz2019, ""Reporting the computational price tag of finding, training, and running models is a key Green AI practice."" This explicitly connects Green AI with reporting the financial cost of model development and operation."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","According to the text from luccioni2025a, electronic waste (e-waste) reached 62 million tonnes in 2022. The document specifically states: 'AI's expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.'","62000000","metric tons","[""luccioni2025a""]","is_blank","AI's expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","According to the text from luccioni2025a, electronic waste (e-waste) reached 62 million tonnes in 2022. The document specifically states: 'AI's expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.'"
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","According to Table 3 in Patterson 2021, the average system power for TPU v2 is 221 Watts, while the average system power for V100 GPU is 325 Watts. The difference between these two values is 325 - 221 = 104 Watts.","104","Watts","[""patterson2021""]","is_blank","Table 3. Average system power per processor and standard deviation for DNNs in this paper... Processor Average (Watts) StDev % DNNs used to calculate average power TPU v2 221 5% Transformer (Big), Evolved Transformer (Medium), Neural Architecture Search [So19] ... V100 GPU 325 2% Transformer (Big), GPT-3 [Sut21]","According to Table 3 in Patterson 2021, the average system power for TPU v2 is 221 Watts, while the average system power for V100 GPU is 325 Watts. The difference between these two values is 325 - 221 = 104 Watts."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","According to Li et al.'s research, training the GPT-3 language model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This is explicitly stated in the abstract of their paper as scope-1 on-site water consumption.","700000","liters","[""li2025b""]","is_blank","For example, training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","According to Li et al.'s research, training the GPT-3 language model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This is explicitly stated in the abstract of their paper as scope-1 on-site water consumption."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","According to multiple statements in the provided context, Facebook's recommendation and ranking model sizes increased by 20 times (20×) between 2019 and 2021. This is explicitly stated in multiple sections of the paper by Wu et al. (2021).","20","multiplier","[""wu2021a""]","is_blank","Facebook's recommendation model sizes have increased by 20× between 2019 and 2021.","According to multiple statements in the provided context, Facebook's recommendation and ranking model sizes increased by 20 times (20×) between 2019 and 2021. This is explicitly stated in multiple sections of the paper by Wu et al. (2021)."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","According to Figure 8 and the accompanying text in the document, o3-mini achieved the highest cross-efficiency DEA score of 0.884, followed by o1-mini with 0.836 and Claude 3.7 Sonnet with 0.825. The document explicitly states 'o3-mini achieved the highest cross-efficiency score (0.884)'.","o3-mini","is_blank","[""jegham2025""]","is_blank","As shown in Figure 8, OpenAI's reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic's Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint.","According to Figure 8 and the accompanying text in the document, o3-mini achieved the highest cross-efficiency DEA score of 0.884, followed by o1-mini with 0.836 and Claude 3.7 Sonnet with 0.825. The document explicitly states 'o3-mini achieved the highest cross-efficiency score (0.884)'."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","According to the Cordella et al. study cited in the wu2021b document, smartphones currently have average lifetimes of less than 3 years. This short lifespan contributes to electronic waste concerns, as mentioned in multiple sources that discuss e-waste as a growing environmental problem.","1","is_blank","[""wu2021b""]","is_blank","develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020] and 4 to 5 years for servers [Ascierto and Lawrence, 2020].","According to the Cordella et al. study cited in the wu2021b document, smartphones currently have average lifetimes of less than 3 years. This short lifespan contributes to electronic waste concerns, as mentioned in multiple sources that discuss e-waste as a growing environmental problem."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context explicitly states that in intra-zone scaling with T4 GPUs, 'CV's per-GPU speedup (speedup/#GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41)' for different GPU counts. This shows the CV models maintained consistent per-GPU contribution as more GPUs were added.","1","is_blank","[""erben2023""]","is_blank","CV's per-GPU speedup (speedup/#GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41), while NLP starts dropping off faster (0.51, 0.47, 0.45, 0.40, 0.34) for 2, 3, 4, 6 and 8 GPUs, respectively.","The context explicitly states that in intra-zone scaling with T4 GPUs, 'CV's per-GPU speedup (speedup/#GPUs) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41)' for different GPU counts. This shows the CV models maintained consistent per-GPU contribution as more GPUs were added."
"q264","What is the context window size, in tokens, for the FLM-101B model?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","tokens","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","is_blank","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","percent","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","percent","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","is_blank","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","lbs/kWh","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","TWh","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","packages","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","tokens","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","is_blank","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","percent","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","times","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","is_blank","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","projects","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","percent","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","is_blank","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","percent","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","NVIDIA A100-80GB GPUs","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","percent","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","kilometers of fiberoptic cable","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","USD","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","is_blank","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","samples","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","is_blank","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","percent","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","percent","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","percent","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","percent","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","lbs CO2e","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","MWh","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","is_blank","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","samples","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","is_blank","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","hectares","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","g CO2eq","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","grams","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","year","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","days","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","liters of freshwater","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","is_blank","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","kWh","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","USD","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","USD","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","samples","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","seconds","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","is_blank","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","percent","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","V100_32GB_GPU","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","requests","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","metric tons","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","Error: Error when retrieving token from sso: Token has expired and refresh failed","is_blank","score","is_blank","is_blank","is_blank","Error: Error when retrieving token from sso: Token has expired and refresh failed"
