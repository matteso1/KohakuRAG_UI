"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context from the document wu2021b directly states that 'The total energy consumption of the US data centers increased by about 4% from 2010-2014'. This matches the question's request for the average increase in U.S. data center electricity consumption between 2010 and 2014.","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The context from the document wu2021b directly states that 'The total energy consumption of the US data centers increased by about 4% from 2010-2014'. This matches the question's request for the average increase in U.S. data center electricity consumption between 2010 and 2014."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context directly states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road.","13900","cars","[""amazon2023""]","is_blank","Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context directly states that the Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","In the context, AWS specifically states that in 2023, they increased the number of data centers using recycled water for cooling from 20 to 24. This means they added 4 new data centers using recycled water in 2023, including two in Virginia, one in California, and one in Singapore.","4","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","In the context, AWS specifically states that in 2023, they increased the number of data centers using recycled water for cooling from 20 to 24. This means they added 4 new data centers using recycled water in 2023, including two in Virginia, one in California, and one in Singapore."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","According to the document, NVIDIA does not release embodied carbon emissions for its hardware. However, the authors assume the same embodied carbon emissions reported by Luccioni et al. (2023), which is 3700 kg of CO2eq per 8x server node, equal to 463 kg per GPU.","463","kg/GPU","[""morrison2025""]","is_blank","Hardware manufacturing NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU.","According to the document, NVIDIA does not release embodied carbon emissions for its hardware. However, the authors assume the same embodied carbon emissions reported by Luccioni et al. (2023), which is 3700 kg of CO2eq per 8x server node, equal to 463 kg per GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The context shows that the FLM-101B model was trained for $100,000, while the amortized hardware cost for GPT-4's training was $40 million. By dividing $40 million by $100,000, the ratio would be 400.","400","ratio","[""li2025a"", ""cottier2024""]","is_blank","Li et al. state: 'To the best of our knowledge, this is the first attempt to use a growth strategy to train an LLM with 100B+ parameters from scratch. The training costs only 100,000 US dollars.' Cottier et al. report that 'the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at $40M'","The context shows that the FLM-101B model was trained for $100,000, while the amortized hardware cost for GPT-4's training was $40 million. By dividing $40 million by $100,000, the ratio would be 400."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","Multiple references in the Patterson 2021 paper consistently cite the CO2e for a single passenger round trip from San Francisco to New York as approximately 1.2 tCO2e. This figure is calculated using Google Flights data, considering a Boeing 767 with 175 seats and an average 84.5% seat occupancy.","1.2","tCO2e","[""patterson2021""]","is_blank","Google Flights uses the historical average of 84.5% seat occupancy, yielding 1.2t of CO2e per passenger round trip.","Multiple references in the Patterson 2021 paper consistently cite the CO2e for a single passenger round trip from San Francisco to New York as approximately 1.2 tCO2e. This figure is calculated using Google Flights data, considering a Boeing 767 with 175 seats and an average 84.5% seat occupancy."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","From Table 4 in the context, the average score for FLM-101B on the Open LLM Leaderboard is 43.94. This is the final performance score achieved by the model across the tasks of ARC, HellaSwag, MMLU, and TruthfulQA.","43.94","score","[""li2025a""]","is_blank","Table 4: Performance of FLM-101B and baselines including Llama series and GLM-130B. [Shows Average score of 43.94 for FLM-101B]","From Table 4 in the context, the average score for FLM-101B on the Open LLM Leaderboard is 43.94. This is the final performance score achieved by the model across the tasks of ARC, HellaSwag, MMLU, and TruthfulQA."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context directly states that between the Intel 4004 in 1971 and typical microprocessors in 2021, there was 'a more than 6,750 fold improvement in processor clock speed'. The original Intel 4004 ran at 740 kHz, while typical 2021 microprocessors run at 5,000,000 kHz.","6750","fold","[""wu2021b""]","[""https://www.intel.co.uk/content/www/uk/en/history/museum-story-of-intel-4004.html""]","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context directly states that between the Intel 4004 in 1971 and typical microprocessors in 2021, there was 'a more than 6,750 fold improvement in processor clock speed'. The original Intel 4004 ran at 740 kHz, while typical 2021 microprocessors run at 5,000,000 kHz."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","According to the context from Patterson et al. (2021), GPT-3 was trained using 10,000 V100 GPUs, taking approximately 14.8 days to compute 3.14E+23 FLOPS at a rate of 24.6 TeraFLOPS/sec. The source directly states the training time as ~14.8 days for the specified GPU configuration.","14.8","days","[""patterson2021""]","[""https://arxiv.org/abs/2104.10350""]","It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","According to the context from Patterson et al. (2021), GPT-3 was trained using 10,000 V100 GPUs, taking approximately 14.8 days to compute 3.14E+23 FLOPS at a rate of 24.6 TeraFLOPS/sec. The source directly states the training time as ~14.8 days for the specified GPU configuration."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","From the SGLang benchmarking table in Morrison et al. 2025, the GPU Power Usage for the Llama 3.2 1B model at 8 requests/second is 0.036 kWh. This value is directly reported in the table under the 'GPU Power Usage (kWh)' column for the 8 req/s frequency.","0.036","kWh","[""morrison2025""]","is_blank","Table 3: Measurements and estimates of resource costs from SGLang benchmarking on 2400 prompts from ShareGPT at varying request rates.","From the SGLang benchmarking table in Morrison et al. 2025, the GPU Power Usage for the Llama 3.2 1B model at 8 requests/second is 0.036 kWh. This value is directly reported in the table under the 'GPU Power Usage (kWh)' column for the 8 req/s frequency."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context from han2024 directly states the total permitted annual emission limits for diesel generators in Northern Virginia. The specific limit for nitrogen oxides (NOx) is 13,000 tons of NOx, which matches the question's timeframe (between January 1, 2023 and December 1, 2024).","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context from han2024 directly states the total permitted annual emission limits for diesel generators in Northern Virginia. The specific limit for nitrogen oxides (NOx) is 13,000 tons of NOx, which matches the question's timeframe (between January 1, 2023 and December 1, 2024)."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The paper states that using a growth strategy resulted in a total training time of 21.54 days, compared to an estimated 76.74 days for training the 101B model from scratch. This represents a time savings of 72%.","72","percent","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The paper states that using a growth strategy resulted in a total training time of 21.54 days, compared to an estimated 76.74 days for training the 101B model from scratch. This represents a time savings of 72%."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context provides a specific projection for U.S. data centers' public health impacts in 2028. While the question asks about 2030, I'll use the closest available data point, which is 1,300 premature deaths in 2028 due to U.S. data centers, focusing on scope-1 and scope-2 health impacts.","1300","deaths","[""han2024""]","is_blank","Using the reduced-complexity modeling tool COBRA (CO-Benefits Risk Assessment) provided by the EPA, our analysis demonstrates that driven by the growing demand for AI, the U.S. data centers could contribute to, among others, approximately 600,000 asthma symptom cases and 1,300 premature deaths in 2028, exceeding 1/3 of asthma deaths in the U.S. each year","The context provides a specific projection for U.S. data centers' public health impacts in 2028. While the question asks about 2030, I'll use the closest available data point, which is 1,300 premature deaths in 2028 due to U.S. data centers, focusing on scope-1 and scope-2 health impacts."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","In the context, Dodge et al. (2022) report training a 6.1 billion parameter model for 8 days, which was only 13% of the total training time. They estimated that a full training run would take approximately 60 days by extrapolating the partial training time.","60","days","[""dodge2022""]","is_blank","We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. This model was not trained to completion, but only until 13%; a full training run would take 60 days.","In the context, Dodge et al. (2022) report training a 6.1 billion parameter model for 8 days, which was only 13% of the total training time. They estimated that a full training run would take approximately 60 days by extrapolating the partial training time."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","From the Zschache 2025 appendix table (Table B1), I inspected the 'Energy (Wh)' column and found that 'DS Llama 70B' has the highest energy consumption at 702.06 Wh for a single query. With 1,000 queries, this would scale to approximately 702,060 Wh.","DS Llama 70B","is_blank","[""zschache2025""]","is_blank","Table B1: Model GPUs Energy (Wh) column shows DS Llama 70B with 702.06 Wh","From the Zschache 2025 appendix table (Table B1), I inspected the 'Energy (Wh)' column and found that 'DS Llama 70B' has the highest energy consumption at 702.06 Wh for a single query. With 1,000 queries, this would scale to approximately 702,060 Wh."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context directly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014, as detailed in the preface section of the document.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Year Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.","The context directly states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014, as detailed in the preface section of the document."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The UN's Global E-Waste Monitor 2024 directly states the percentage of e-waste that has been formally collected and recycled. The context specifies this value as 22%, matching the question's requirement for a percentage answer.","22","percent","[""luccioni2025a""]","is_blank","The UN's Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The UN's Global E-Waste Monitor 2024 directly states the percentage of e-waste that has been formally collected and recycled. The context specifies this value as 22%, matching the question's requirement for a percentage answer."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context provides a table in the Luccioni 2024 paper showing the training energy for BLOOM-176B (referred to as BLOOMz-7B) is 51,686 kWh. To convert this to MWh, I divided the value by 1000, resulting in 51.686 MWh.","51.686","MWh","[""luccioni2024""]","is_blank","Table 5 showing: BLOOMz-7B Training energy (kWh) 51,686","The context provides a table in the Luccioni 2024 paper showing the training energy for BLOOM-176B (referred to as BLOOMz-7B) is 51,686 kWh. To convert this to MWh, I divided the value by 1000, resulting in 51.686 MWh."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","From the context in Patterson et al. (2021), specifically the table in the document, the Switch Transformer's 1500 billion parameters have 0.10% of parameters activated on every token. This means a very small fraction of the total parameters are used for each input.","0.1","percent","[""patterson2021""]","is_blank","""Percent of model activated on every token: 0.10%""","From the context in Patterson et al. (2021), specifically the table in the document, the Switch Transformer's 1500 billion parameters have 0.10% of parameters activated on every token. This means a very small fraction of the total parameters are used for each input."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","In the context, the authors specify that they set the number of experts to 8 for every layer when discussing the JetMoE-8B model's hyperparameters. This is confirmed in multiple sections of the document, indicating a consistent use of 8 experts across the MoE layers.","8","experts","[""shen2024""]","is_blank","From section 4.2 Hyper-parameters: 'Then, we set the same number of experts to 8 and top-k to 2 for every layer.'","In the context, the authors specify that they set the number of experts to 8 for every layer when discussing the JetMoE-8B model's hyperparameters. This is confirmed in multiple sections of the document, indicating a consistent use of 8 experts across the MoE layers."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The total execution time for a dense BlackMamba model with batch size 30 can be found in Figure 4 of the Xia et al. 2024 paper. The graph shows the execution time breakdown for different batch sizes, and for the dense model with batch size 30, the total execution time is approximately 1.5 seconds.","1.5","second","[""xia2024""]","is_blank","From Fig. 4: Dense(bsz=30) bar on the BlackMamba section of the execution time breakdown graph","The total execution time for a dense BlackMamba model with batch size 30 can be found in Figure 4 of the Xia et al. 2024 paper. The graph shows the execution time breakdown for different batch sizes, and for the dense model with batch size 30, the total execution time is approximately 1.5 seconds."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","Based on Table 7 in the context, the computational cost for training the FLM-101B model is reported as 28.22 zettaFLOPs, which directly answers the question about the number of zettaFLOPs required to train the model's English portion.","28.22","zettaFLOPs","[""li2025a""]","is_blank","Table 7: Performance on SuperGLUE-IQ of GPT-3, GLM-130B, and FLM-101B. Cost is computed in zettaFLOPs.","Based on Table 7 in the context, the computational cost for training the FLM-101B model is reported as 28.22 zettaFLOPs, which directly answers the question about the number of zettaFLOPs required to train the model's English portion."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The experimental setup for energy-efficient local inference in financial sentiment classification used an 11th Gen Intel Core i7-1165G7 processor operating at 2.80 GHz, specifically described in the Khan et al. 2025 paper's experimental setup section.","11th Gen Intel Core i7-1165G7 processor","is_blank","[""khan2025""]","is_blank","The hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable).","The experimental setup for energy-efficient local inference in financial sentiment classification used an 11th Gen Intel Core i7-1165G7 processor operating at 2.80 GHz, specifically described in the Khan et al. 2025 paper's experimental setup section."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context from the Luccioni et al. 2024 paper provides a detailed table (Table 6) that lists task-specific models across various domains. By counting the models in this table, there are exactly 80 different machine learning models analyzed in the study.","80","models","[""luccioni2024""]","is_blank","Table 6. The full list of the 80 finetuned models that were tested for the ten tasks we analyzed.","The context from the Luccioni et al. 2024 paper provides a detailed table (Table 6) that lists task-specific models across various domains. By counting the models in this table, there are exactly 80 different machine learning models analyzed in the study."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context from the wu2021a paper shows that increasing GPU utilization up to 80% reduces the overall carbon footprint by a factor of 3. The paper specifically notes this in the context of language model (LM) training on GPUs.","3","multiplier","[""wu2021a""]","is_blank","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","The context from the wu2021a paper shows that increasing GPU utilization up to 80% reduces the overall carbon footprint by a factor of 3. The paper specifically notes this in the context of language model (LM) training on GPUs."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","In section A.6 of the paper, Cottier et al. estimate that the total compute used throughout model development ranges from 1.2x to 4x the compute of the final training run, with a median of 2.2x. This estimate is based on evidence from the development of models like GPT-3, BLOOM, and OPT-175B.","[1.2, 4]","multiplier","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015v2""]","We estimate that the ratio of total compute to final training run compute ranges from 1.2x to 4x, with a median of 2.2x.","In section A.6 of the paper, Cottier et al. estimate that the total compute used throughout model development ranges from 1.2x to 4x the compute of the final training run, with a median of 2.2x. This estimate is based on evidence from the development of models like GPT-3, BLOOM, and OPT-175B."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","According to the Dodge et al. (2022) paper, the 6.1 billion parameter transformer model was only trained to 13% completion. They estimated that a full training run would consume approximately 103.5 MWh, which matches the requested MWh unit for the answer.","103.5","MWh","[""dodge2022""]","is_blank","6 Billion Parameter Transformer. We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","According to the Dodge et al. (2022) paper, the 6.1 billion parameter transformer model was only trained to 13% completion. They estimated that a full training run would consume approximately 103.5 MWh, which matches the requested MWh unit for the answer."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The 2025 paper by Luccioni et al. discusses Jevons' Paradox as a key economic principle explaining why efficiency gains may not lead to net environmental benefits. The authors argue that just because AI models become more efficient, it does not necessarily mean overall resource consumption will decrease - and may in fact increase.","Jevons' Paradox","is_blank","[""luccioni2025a""]","is_blank","Similarly to Jevons' Paradox, just because an AI model becomes more efficient, that does not imply that overall AI resource consumption will decrease, and in fact the inverse effect is highly plausible.","The 2025 paper by Luccioni et al. discusses Jevons' Paradox as a key economic principle explaining why efficiency gains may not lead to net environmental benefits. The authors argue that just because AI models become more efficient, it does not necessarily mean overall resource consumption will decrease - and may in fact increase."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context provides a precise projection of global AI water withdrawal in 2027. Specifically, the paper states that the combined scope-1 and scope-2 water withdrawal of global AI is projected to reach 4.2 – 6.6 billion cubic meters in 2027.","[4.2, 6.6]","billion cubic meters","[""li2025b""]","is_blank","Total water usage. By adding up scope-1 and scope-2 water usage together, the total water withdrawal and water consumption of global AI may reach 4.2 – 6.6 billion cubic meters and 0.38 – 0.60 billion cubic meters, respectively.","The context provides a precise projection of global AI water withdrawal in 2027. Specifically, the paper states that the combined scope-1 and scope-2 water withdrawal of global AI is projected to reach 4.2 – 6.6 billion cubic meters in 2027."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","According to the context, specifically the Schwartz et al. paper, Red AI is actually on the rise despite known diminishing returns. The paper states that the computational cost of AI research is increasing exponentially, far exceeding Moore's Law, indicating that Red AI is not declining but growing.","0","is_blank","[""schwartz2019""]","is_blank","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3). This paper identifies key factors that contribute to Red AI...","According to the context, specifically the Schwartz et al. paper, Red AI is actually on the rise despite known diminishing returns. The paper states that the computational cost of AI research is increasing exponentially, far exceeding Moore's Law, indicating that Red AI is not declining but growing."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","From the context in the li2025a paper, the total wall-clock time for training FLM-101B using a growth strategy is provided in the training schedule. By summing the time for each stage (16B, 51B, 101B stages), the total training time is 21.54 days.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","From the context in the li2025a paper, the total wall-clock time for training FLM-101B using a growth strategy is provided in the training schedule. By summing the time for each stage (16B, 51B, 101B stages), the total training time is 21.54 days."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context directly contradicts this statement. According to the wu2021a paper, a significant portion of machine learning model experimentation at Facebook utilizes GPUs at only 30-50%, far below 80% capacity.","0","is_blank","[""wu2021a""]","is_blank","A vast majority of model experimentation (over tens of thousands of training workﬂows) utilizes GPUs at only 30-50%, leaving room for utilization and efﬁciency improvements.","The context directly contradicts this statement. According to the wu2021a paper, a significant portion of machine learning model experimentation at Facebook utilizes GPUs at only 30-50%, far below 80% capacity."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The training energy for GPT-3 is explicitly stated in the context from the Li et al. document. Specifically, the document notes that GPT-3 was 'trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh'.","1287","MWh","[""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh [29].","The training energy for GPT-3 is explicitly stated in the context from the Li et al. document. Specifically, the document notes that GPT-3 was 'trained and deployed by OpenAI in Microsoft's data centers, with an estimated training energy of 1287 MWh'."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context directly mentions the AI Energy Score project as a standardized methodology for comparing the inference efficiency of AI models across different tasks. This is referenced in the Luccioni et al. 2025c document.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","For instance, the AI Energy Score project 21 provides a standardized methodology for comparing models across different tasks, which can also be adapted for specific contexts and datasets.","The context directly mentions the AI Energy Score project as a standardized methodology for comparing the inference efficiency of AI models across different tasks. This is referenced in the Luccioni et al. 2025c document."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","From Figure 6 of the paper, which shows the execution time breakdown of different kernels for the MoE layer, the longest kernel for BlackMamba's dense configuration with batch size 30 appears to be the 'matmul(w1)' kernel at approximately 1600-2000 microseconds.","1800","microseconds","[""xia2024""]","is_blank","Fig. 6. Execution breakdown of the MoE layer for different kernels.","From Figure 6 of the paper, which shows the execution time breakdown of different kernels for the MoE layer, the longest kernel for BlackMamba's dense configuration with batch size 30 appears to be the 'matmul(w1)' kernel at approximately 1600-2000 microseconds."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","In the JetMoE-8B model, the authors explicitly state that they set the top-k to 2 for every layer. This means that for each token, 2 experts are selected for activation across both self-attention and feed-forward layers.","2","experts","[""shen2024""]","is_blank","Following the design in ModuleFormer (Shen et al., 2023), JetMoE replaces both self-attention and Feed-forward layers (FFD) with SMoE layer. We set the same number of experts to 8 and top-k to 2 for every layer.","In the JetMoE-8B model, the authors explicitly state that they set the top-k to 2 for every layer. This means that for each token, 2 experts are selected for activation across both self-attention and feed-forward layers."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context directly supports a 300,000x increase in compute used for deep learning model training between 2012-2018, not the 200,000x stated in the question. This is specifically referenced in the Schwartz et al. 2019 paper, which states 'the computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018'.","0","is_blank","[""schwartz2019""]","is_blank","The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018","The context directly supports a 300,000x increase in compute used for deep learning model training between 2012-2018, not the 200,000x stated in the question. This is specifically referenced in the Schwartz et al. 2019 paper, which states 'the computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018'."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","According to the context from the wu2021b reference, global carbon emissions dropped by 6.4% in 2020 during the COVID-19 pandemic, with vehicle transportation in the US contributing to a portion of this global reduction.","6.4","percent","[""wu2021b""]","[""https://www.nature.com/articles/d41586-021-00090-3""]","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","According to the context from the wu2021b reference, global carbon emissions dropped by 6.4% in 2020 during the COVID-19 pandemic, with vehicle transportation in the US contributing to a portion of this global reduction."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","In the context, Amazon clearly states that in 2023, 22 AWS data center regions had 100% of their electricity matched with renewable energy sources, which is an increase from 19 regions in 2022.","22","data centers","[""amazon2023""]","is_blank","Amazon's energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.","In the context, Amazon clearly states that in 2023, 22 AWS data center regions had 100% of their electricity matched with renewable energy sources, which is an increase from 19 regions in 2022."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The field of Artificial Intelligence was officially born in 1956 at the Dartmouth Summer Research Project on Artificial Intelligence. In 2025, this would make the field approximately 69 years old.","69","years","[""stone2022""]","[""http://www-formal.stanford.edu/jmc/history/dartmouth/dartmouth.html""]","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The field of Artificial Intelligence was officially born in 1956 at the Dartmouth Summer Research Project on Artificial Intelligence. In 2025, this would make the field approximately 69 years old."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The context indicates that the '5 cars' carbon footprint estimate originated from the process of neural architecture search (NAS), a large-scale procedure performed infrequently to identify an optimized variant of the Transformer architecture.","Neural Architecture Search (NAS)","is_blank","[""luccioni2025c""]","is_blank","In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.","The context indicates that the '5 cars' carbon footprint estimate originated from the process of neural architecture search (NAS), a large-scale procedure performed infrequently to identify an optimized variant of the Transformer architecture."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","According to the context in the paper, when targeting an average Time Per Output Token (TPOT) of 100 ms for a chatbot service, the energy consumption can be reduced by 44% compared to configurations that simply minimize latency. This finding is specifically noted for the Llama 3.1 8B model.","44","percent","[""chung2025""]","is_blank","This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","According to the context in the paper, when targeting an average Time Per Output Token (TPOT) of 100 ms for a chatbot service, the energy consumption can be reduced by 44% compared to configurations that simply minimize latency. This finding is specifically noted for the Llama 3.1 8B model."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","From Figure 13 and the related text discussing maximum batch sizes for different GPUs, the authors show that for a NVIDIA A40 with 48 GB memory, the maximum batch size for fine-tuning BlackMamba in a sparse setup can be determined from their analytical model.","20","samples","[""xia2024""]","is_blank","Fig. 8 shows BlackMamba-CS with sparse batch size of 20 for fine-tuning","From Figure 13 and the related text discussing maximum batch sizes for different GPUs, the authors show that for a NVIDIA A40 with 48 GB memory, the maximum batch size for fine-tuning BlackMamba in a sparse setup can be determined from their analytical model."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","According to the context, Amazon's energy storage capacity increased from 445 MW in 2022 to 1.3 GW in 2023. To match the requested unit of gigawatts (GW), the answer is 1.3 GW.","1.3","GW","[""amazon2023""]","is_blank","We now hold 1.3 GW of storage capacity, up from 445 MW in 2022.","According to the context, Amazon's energy storage capacity increased from 445 MW in 2022 to 1.3 GW in 2023. To match the requested unit of gigawatts (GW), the answer is 1.3 GW."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context from the Jegham et al. 2025 paper states that a single model like GPT-4o, serving hundreds of millions of daily requests, 'can emit as much carbon as thousands of transatlantic flights'. I've interpreted this to mean the annual carbon emissions are comparable to multiple thousands of transatlantic flights.","thousands","flights","[""jegham2025""]","is_blank","A single model, such as GPT-4o, serving hundreds of millions of daily requests, can emit as much carbon as thousands of transatlantic flights and consume water equivalent to the annual drinking needs of millions of people.","The context from the Jegham et al. 2025 paper states that a single model like GPT-4o, serving hundreds of millions of daily requests, 'can emit as much carbon as thousands of transatlantic flights'. I've interpreted this to mean the annual carbon emissions are comparable to multiple thousands of transatlantic flights."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","None of the provided context documents contain specific information about the percentage of AI inference workloads powered by coal in Asia in 2023. The context discusses AI energy consumption and environmental impacts broadly, but does not provide a detailed breakdown of energy sources for AI inference in Asia.","is_blank","percent","[""is_blank""]","is_blank","is_blank","None of the provided context documents contain specific information about the percentage of AI inference workloads powered by coal in Asia in 2023. The context discusses AI energy consumption and environmental impacts broadly, but does not provide a detailed breakdown of energy sources for AI inference in Asia."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context does not specify a global average power usage effectiveness (PUE) for AI-dedicated data centers in 2023. One document mentions a conservative PUE assumption of 1.1 for state-of-the-art data center facilities, but this is not a global average measurement.","is_blank","PUE","[""li2025b""]","is_blank","To account for the data center non-IT energy overheads, we conservatively assume a power usage effectiveness (PUE) of 1.1, which is a fairly low value even for state-of-the-art data center facilities [4].","The context does not specify a global average power usage effectiveness (PUE) for AI-dedicated data centers in 2023. One document mentions a conservative PUE assumption of 1.1 for state-of-the-art data center facilities, but this is not a global average measurement."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","According to the context, JetMoE-8B has 8B total parameters but only activates 2B parameters for each input token during inference. This is part of the model's sparse activation strategy to reduce computational costs.","2000000000","parameters","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1""]","JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","According to the context, JetMoE-8B has 8B total parameters but only activates 2B parameters for each input token during inference. This is part of the model's sparse activation strategy to reduce computational costs."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","In the context from Luccioni's 2025 survey of AI model environmental impacts, the pre-training GHG emissions for Llama 7B are reported as 14 tCO2e.","14","tCO2e","[""luccioni2025c""]","is_blank","Table 1: Range of Pre-Training Environmental Impacts - Llama 7B, Meta: 14 tCO2e","In the context from Luccioni's 2025 survey of AI model environmental impacts, the pre-training GHG emissions for Llama 7B are reported as 14 tCO2e."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","I calculated the total number of electric delivery vans added by Amazon in 2022 and 2023 by summing the regional increases. For the U.S., Amazon went from 2,600 to 11,800 electric delivery vans (a net increase of 9,200). For Europe, the increase was from 1,220 to 3,000+ (a net increase of approximately 1,780). For India, the increase was from 3,800 to 7,200 (a net increase of 3,400).","14380","electric delivery vans","[""amazon2023""]","is_blank","Last Mile Electric Delivery Vehicles by Region 7
Region 2022 2023
U.S. 2,600 11,800
Europe 1,220 3,000+
India 3,800 7,200+","I calculated the total number of electric delivery vans added by Amazon in 2022 and 2023 by summing the regional increases. For the U.S., Amazon went from 2,600 to 11,800 electric delivery vans (a net increase of 9,200). For Europe, the increase was from 1,220 to 3,000+ (a net increase of approximately 1,780). For India, the increase was from 3,800 to 7,200 (a net increase of 3,400)."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context from Dodge et al. (ref_id=dodge2022) specifically mentions HVAC (Heating, Ventilation, and Air Conditioning) as a significant source of electricity consumption in data centers, accounting for 25-40% of electricity use. This implies that cooling-related emissions are indeed part of the operational environmental impacts.","0","is_blank","[""dodge2022""]","is_blank","According to the U.S. Department of Energy: ""The electricity consumed in these data centers is mainly by the equipment (50%) and HVAC (25%–40%)""","The context from Dodge et al. (ref_id=dodge2022) specifically mentions HVAC (Heating, Ventilation, and Air Conditioning) as a significant source of electricity consumption in data centers, accounting for 25-40% of electricity use. This implies that cooling-related emissions are indeed part of the operational environmental impacts."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","In the energy consumption table (Table 4) in the Jegham et al. document, the o3 model's energy consumption for a long prompt (10k input-1.5k output) is 12.222 Wh. This matches the question's requirement to report the energy consumption in Wh for a long prompt.","12.222","Wh","[""jegham2025""]","is_blank","Table 4: Energy consumption (mean ± std dev) per model across three prompt sizes (Wh)","In the energy consumption table (Table 4) in the Jegham et al. document, the o3 model's energy consumption for a long prompt (10k input-1.5k output) is 12.222 Wh. This matches the question's requirement to report the energy consumption in Wh for a long prompt."
"q056","When was the field of Artificial Intelligence officially christened?","Multiple context snippets consistently state that the field of Artificial Intelligence was officially born and christened at a workshop in 1956, organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","1956","year","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","Multiple context snippets consistently state that the field of Artificial Intelligence was officially born and christened at a workshop in 1956, organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context provides a reference to Google's annualized global on-site water efficiency, which is stated as 1 L/kWh. While this is a water consumption metric rather than a strict Water Usage Effectiveness (WUE) measurement, it is the closest specific metric for Google's water efficiency mentioned in the text.","1","WUE","[""li2025b""]","[""https://arxiv.org/abs/2304.03271v5""]","On average, depending on the weather conditions and operational settings, data centers can evaporate approximately 1 – 9 liters per kWh of server energy: 1 L/kWh for Google's annualized global on-site water efficiency [4]","The context provides a reference to Google's annualized global on-site water efficiency, which is stated as 1 L/kWh. While this is a water consumption metric rather than a strict Water Usage Effectiveness (WUE) measurement, it is the closest specific metric for Google's water efficiency mentioned in the text."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","According to the document from Wu et al. (2021), it is explicitly stated that approximately 770 million people, which is about 1 out of 10 people in the world, do not have access to a stable supply of electricity. This supports the statement as true.","1","is_blank","[""wu2021b""]","is_blank","""Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].""","According to the document from Wu et al. (2021), it is explicitly stated that approximately 770 million people, which is about 1 out of 10 people in the world, do not have access to a stable supply of electricity. This supports the statement as true."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","According to the context, when analyzing energy per output token for LLaMA 65B at a maximum generation length of 512 tokens, the study found that it takes about 3-4 Joules per output token. This is specifically noted in the section discussing Energy per Decoded Token for LLaMA 65B.","[3, 4]","joules per token","[""samsi2024""]","is_blank","For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","According to the context, when analyzing energy per output token for LLaMA 65B at a maximum generation length of 512 tokens, the study found that it takes about 3-4 Joules per output token. This is specifically noted in the section discussing Energy per Decoded Token for LLaMA 65B."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","In the context document by Wu et al., there is a direct statement about quantization of the RM2 model from 32-bit to 16-bit floating-point representation. The text specifies that by converting to 16-bit, Facebook reduced the overall RM2 model size by 15%.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","In the context document by Wu et al., there is a direct statement about quantization of the RM2 model from 32-bit to 16-bit floating-point representation. The text specifies that by converting to 16-bit, Facebook reduced the overall RM2 model size by 15%."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context explicitly critiques the 5-10% GHG emissions reduction claim, stating that the underlying calculations lack scientific rigor and the reasoning is unclear. The source of the claim is a Boston Consulting Group report commissioned by Google, which does not provide detailed, transparent calculations for its global emissions reduction estimate.","0","is_blank","[""luccioni2025c""]","is_blank","Applying observations made from individual projects to the entire planet's GHG emissions lacks any scientific grounding—in fact, many of the emissions reductions on a global scale require individual, societal and political shifts. Moreover, rigorous calculation of avoided emissions requires defining counterfactual reference scenarios, conducting systematic consequence analysis, and accounting for rebound effects...","The context explicitly critiques the 5-10% GHG emissions reduction claim, stating that the underlying calculations lack scientific rigor and the reasoning is unclear. The source of the claim is a Boston Consulting Group report commissioned by Google, which does not provide detailed, transparent calculations for its global emissions reduction estimate."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context directly supports this statement in Patterson et al.'s paper, which explicitly highlights that large but sparsely activated Deep Neural Networks (DNNs) can consume less than 1/10th the energy of large, dense DNNs while maintaining the same accuracy level.","1","is_blank","[""patterson2021""]","is_blank","Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context directly supports this statement in Patterson et al.'s paper, which explicitly highlights that large but sparsely activated Deep Neural Networks (DNNs) can consume less than 1/10th the energy of large, dense DNNs while maintaining the same accuracy level."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","In the context from Schwartz et al. (2019), Grover was trained on 256 TPU chips for two weeks with an estimated cost of $25,000. I directly extracted this value as the answer, matching the USD unit specified in the additional info.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","In the context from Schwartz et al. (2019), Grover was trained on 256 TPU chips for two weeks with an estimated cost of $25,000. I directly extracted this value as the answer, matching the USD unit specified in the additional info."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context directly states that in BlackMamba sparse fine-tuning with a batch size of 1, the optimizer stage takes up to 53% of the running time. This percentage is specific to the experiment conducted on an NVIDIA A40 GPU.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1), while the execution time share of the optimizer stage in Mixtral fine-tuning is negligible.","The context directly states that in BlackMamba sparse fine-tuning with a batch size of 1, the optimizer stage takes up to 53% of the running time. This percentage is specific to the experiment conducted on an NVIDIA A40 GPU."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context from Luccioni et al. 2024 mentions that Google reported translating more than 100 billion words per day in 2016, assuming an average query length of 100 words, which yields an estimate of 1 billion queries per day. Using the energy consumption reference from the Flan-T5-xxl model at 0.083 kWh per 1,000 queries, I calculated the daily energy consumption by multiplying 1 billion queries by (0.083 kWh / 1,000 queries) and converting to MWh.","83","MWh","[""luccioni2024""]","[""https://arxiv.org/abs/""]","By painting a more detailed picture of widely varying energy requirements for ML model 1Google reported translating more than 100 billion words per day in 2016, assuming an average query length of 100 words yields an estimate of 1 billion queries to the model per day.","The context from Luccioni et al. 2024 mentions that Google reported translating more than 100 billion words per day in 2016, assuming an average query length of 100 words, which yields an estimate of 1 billion queries per day. Using the energy consumption reference from the Flan-T5-xxl model at 0.083 kWh per 1,000 queries, I calculated the daily energy consumption by multiplying 1 billion queries by (0.083 kWh / 1,000 queries) and converting to MWh."
"q067","What was the average global data center PUE in 2023?","The context from the Ebert et al. 2024 paper directly states the average global data center Power Usage Effectiveness (PUE) for 2023 as 1.58, sourced from the Statista reference [74].","1.58","PUE","[""ebert2024""]","is_blank","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context from the Ebert et al. 2024 paper directly states the average global data center Power Usage Effectiveness (PUE) for 2023 as 1.58, sourced from the Statista reference [74]."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","There is no specific information in the provided context documents about the number of wind turbines Microsoft directly contracted to power Azure AI clusters in 2023. None of the references provide details about Microsoft's direct wind turbine contracts for Azure AI infrastructure.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","There is no specific information in the provided context documents about the number of wind turbines Microsoft directly contracted to power Azure AI clusters in 2023. None of the references provide details about Microsoft's direct wind turbine contracts for Azure AI infrastructure."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","According to the context, Cottier et al. found that when equity is included, Gemini Ultra had the highest fraction of R&D staff costs at 49% of total amortized model development costs. This percentage is notable as being unusually high among frontier models due to specific circumstances like being trained on cheaper Google TPUs.","49","percent","[""cottier2024""]","is_blank","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","According to the context, Cottier et al. found that when equity is included, Gemini Ultra had the highest fraction of R&D staff costs at 49% of total amortized model development costs. This percentage is notable as being unusually high among frontier models due to specific circumstances like being trained on cheaper Google TPUs."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context directly states that the inaugural 2015 Study Panel was comprised of seventeen members. This is mentioned in multiple sections of the document, noting the panel's diverse composition of experts from academia, corporate laboratories, industry, and various scholarly disciplines.","17","people","[""stone2022""]","is_blank","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The context directly states that the inaugural 2015 Study Panel was comprised of seventeen members. This is mentioned in multiple sections of the document, noting the panel's diverse composition of experts from academia, corporate laboratories, industry, and various scholarly disciplines."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","In the context from wu2021a, the document directly states that manufacturing carbon cost accounts for 74% of the total carbon footprint of client devices.","74","percent","[""wu2021a""]","[""https://tech.fb.com/sustainable-computing/""]","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices.","In the context from wu2021a, the document directly states that manufacturing carbon cost accounts for 74% of the total carbon footprint of client devices."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context suggests that energy consumption during inference is variable and depends on multiple factors, not just parameter count. The Zschache paper specifically states that energy consumption is influenced by 'model type, model size, and hardware specifications', indicating that more parameters do not guarantee higher energy use.","0","is_blank","[""zschache2025""]","is_blank","Generally, we see significant variability in inference energy consumption, influenced by model type, model size, and hardware specifications.","The context suggests that energy consumption during inference is variable and depends on multiple factors, not just parameter count. The Zschache paper specifically states that energy consumption is influenced by 'model type, model size, and hardware specifications', indicating that more parameters do not guarantee higher energy use."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context shows the Study Panel is not portraying AI as an imminent threat, but rather emphasizing the importance of thoughtful development and societal integration of AI technologies. They suggest that an overly fearful approach could be counterproductive, and instead advocate for an open-minded perspective that recognizes AI's potential to positively transform society.","0","is_blank","[""stone2022""]","is_blank","If society approaches these technologies primarily with fear and suspicion, missteps that slow AI's development or drive it underground will result, impeding important work on ensuring the safety and reliability of AI technologies. On the other hand, if society approaches AI with a more open mind, the technologies emerging from the field could profoundly transform society for the better in the coming decades.","The context shows the Study Panel is not portraying AI as an imminent threat, but rather emphasizing the importance of thoughtful development and societal integration of AI technologies. They suggest that an overly fearful approach could be counterproductive, and instead advocate for an open-minded perspective that recognizes AI's potential to positively transform society."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","There are no details in the provided context snippets that specify the metric tons of CO2 emitted by OpenAI's API requests in January 2024. The documents discuss AI's environmental impact broadly, but do not provide a specific measurement for OpenAI's API emissions in that month.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","There are no details in the provided context snippets that specify the metric tons of CO2 emitted by OpenAI's API requests in January 2024. The documents discuss AI's environmental impact broadly, but do not provide a specific measurement for OpenAI's API emissions in that month."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","According to the context from Luccioni et al. (2025c), Meta reported that their Llama 3 family of models emitted 11,390 tons CO2e during pre-training, which is over 40x the original 'five cars' estimate from the 2019 Strubell et al. study.","11390","tCO2e","[""luccioni2025c""]","is_blank","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the 'five cars' estimate.","According to the context from Luccioni et al. (2025c), Meta reported that their Llama 3 family of models emitted 11,390 tons CO2e during pre-training, which is over 40x the original 'five cars' estimate from the 2019 Strubell et al. study."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","According to Figure 2(d) and the text, the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over the 1.5-year period between 2019 and 2021.","2.9","multiplier","[""wu2021a""]","is_blank","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","According to Figure 2(d) and the text, the explosive growth in AI use cases at Facebook drove a 2.9× increase in AI training infrastructure capacity over the 1.5-year period between 2019 and 2021."
"q079","How many miles is the Earth from the Sun?","None of the provided context documents contain information about the distance between the Earth and the Sun. The context covers topics like AI model training, carbon emissions, renewable energy, and water consumption, but not astronomical distances.","is_blank","miles","[""is_blank""]","is_blank","is_blank","None of the provided context documents contain information about the distance between the Earth and the Sun. The context covers topics like AI model training, carbon emissions, renewable energy, and water consumption, but not astronomical distances."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context explicitly states that AlphaGo, a computer program developed by Google DeepMind, beat the human Go champion in a five-game match. The specific passage confirms the program's success in defeating a human champion.","1","is_blank","[""stone2022""]","is_blank","The recent success of AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match, was due in large part to reinforcement learning.","The context explicitly states that AlphaGo, a computer program developed by Google DeepMind, beat the human Go champion in a five-game match. The specific passage confirms the program's success in defeating a human champion."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context explicitly describes continuous batching as a strategy that dynamically replaces completed requests with new ones, which helps improve GPU utilization and reduce idle time. This matches the description in the question exactly.","Continuous Batching","is_blank","[""fernandez2025""]","is_blank","Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context explicitly describes continuous batching as a strategy that dynamically replaces completed requests with new ones, which helps improve GPU utilization and reduce idle time. This matches the description in the question exactly."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context directly states that the entire alignment process for JetMoE, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours.","60","H100 GPU hours","[""shen2024""]","[""None""]","The entire alignment process takes 60 H100 GPU hours.","The context directly states that the entire alignment process for JetMoE, which includes both dSFT and dDPO fine-tuning, takes 60 H100 GPU hours."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","According to the context, in the offline workload experiment with a 100 TPS SLO, InferSave selected g4dn.xlarge with a total cost of $2.13, while the Max-Performance policy selected g6e.xlarge with a total cost of $2.699. I calculated the percentage difference by comparing these two costs.","26.7","percent","[""kim2025""]","is_blank","On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.","According to the context, in the offline workload experiment with a 100 TPS SLO, InferSave selected g4dn.xlarge with a total cost of $2.13, while the Max-Performance policy selected g6e.xlarge with a total cost of $2.699. I calculated the percentage difference by comparing these two costs."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","No information about stable-diffusion-xl-base-1.0's carbon emissions per 1,000 inferences is present in the provided context snippets.","is_blank","g CO2eq","[""is_blank""]","is_blank","is_blank","No information about stable-diffusion-xl-base-1.0's carbon emissions per 1,000 inferences is present in the provided context snippets."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context directly quotes a range of GPU energy usage for 1,000 queries from a 2025 study, spanning from 0.06 Wh for bert-tiny to over 3,426 Wh for Command-R Plus model. This range depends on model size, architecture, and task complexity.","[0.06, 3426]","Wh","[""luccioni2025c""]","is_blank","task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity","The context directly quotes a range of GPU energy usage for 1,000 queries from a 2025 study, spanning from 0.06 Wh for bert-tiny to over 3,426 Wh for Command-R Plus model. This range depends on model size, architecture, and task complexity."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context explicitly states that there is no universal solution for AI ethics and sustainability. The paper argues for a context-specific, nuanced approach that considers varied ethical dimensions and environmental implications.","0","is_blank","[""luccioni2025b""]","is_blank","""There is no one-size-fits-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.""","The context explicitly states that there is no universal solution for AI ethics and sustainability. The paper argues for a context-specific, nuanced approach that considers varied ethical dimensions and environmental implications."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The carbon intensity is directly stated in the Patterson 2021 document under section 2.4 Energy mix improvement. The value represents the gross carbon intensity of energy according to the U.S. average energy mix, and it matches the requested unit of kg of CO2e/KWh.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","The carbon intensity is directly stated in the Patterson 2021 document under section 2.4 Energy mix improvement. The value represents the gross carbon intensity of energy according to the U.S. average energy mix, and it matches the requested unit of kg of CO2e/KWh."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The paper explicitly identifies Hivemind as the PyTorch-based decentralized framework used to enable distributed spot instance training across clouds and continents. The framework was developed to handle collaborative deep learning training on heterogeneous and unreliable hardware.","Hivemind","is_blank","[""erben2023""]","is_blank","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion.","The paper explicitly identifies Hivemind as the PyTorch-based decentralized framework used to enable distributed spot instance training across clouds and continents. The framework was developed to handle collaborative deep learning training on heterogeneous and unreliable hardware."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context directly mentions 'social transparency' as a proposed term for expanding transparency in AI to include socio-technical aspects and the system's societal and environmental impacts. This is defined in the text as involving 'integrating socio-technical aspects in the description and understanding of AI systems' and including 'societal impacts, ethical considerations, and eventually its environmental footprint'.","social transparency","is_blank","[""luccioni2025b""]","is_blank","As proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass 'social transparency', which involves integrating socio-technical aspects in the description and understanding of AI systems. Social transparency involves a portrayal of an AI system's societal impacts, ethical considerations, and eventually its environmental footprint.","The context directly mentions 'social transparency' as a proposed term for expanding transparency in AI to include socio-technical aspects and the system's societal and environmental impacts. This is defined in the text as involving 'integrating socio-technical aspects in the description and understanding of AI systems' and including 'societal impacts, ethical considerations, and eventually its environmental footprint'."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context indicates that the linear model with a pretrained multilingual sentence embedding was among the top-performing models in the text classification experiments. The paper mentions that 'the linear model with sentence embeddings is among the top-performing models' and specifically notes it achieved the highest accuracy in some cases.","Linear model with sentence embeddings","is_blank","[""zschache2025""]","[""https://huggingface.co/sentence-transformers/paraphrase-multilingual-mpnet-base-v2""]","Finally, for emotion classification, the linear model with sentence embeddings is among the top-performing models.","The context indicates that the linear model with a pretrained multilingual sentence embedding was among the top-performing models in the text classification experiments. The paper mentions that 'the linear model with sentence embeddings is among the top-performing models' and specifically notes it achieved the highest accuracy in some cases."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The paper describes developing an LLM inference system called 'Lamina' that implements model-attention disaggregation. This is directly mentioned in multiple sections of the paper as the system they developed to validate their approach.","Lamina","is_blank","[""chen2024""]","is_blank","To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The paper describes developing an LLM inference system called 'Lamina' that implements model-attention disaggregation. This is directly mentioned in multiple sections of the paper as the system they developed to validate their approach."
"q093","How many parameters does the largest T5 model have?","The context does not specify the exact number of parameters for the largest T5 model. While T5 is mentioned in Patterson et al. (2021), the specific parameter count is not provided. The paper only notes that the largest T5 model has 11B parameters.","11000000000","parameters","[""patterson2021""]","is_blank","The largest size has 11B parameters, and training used 86 MWh and produced 47 tCO2e.","The context does not specify the exact number of parameters for the largest T5 model. While T5 is mentioned in Patterson et al. (2021), the specific parameter count is not provided. The paper only notes that the largest T5 model has 11B parameters."
"q094","What is the total number of parameters in the JetMoE-8B model?","In the context, the paper describes JetMoE-8B as having 8B total parameters, with only 2B being actively used for each input token. This is explicitly stated in multiple sections of the paper.","8000000000","parameters","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","From the text: 'JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.'","In the context, the paper describes JetMoE-8B as having 8B total parameters, with only 2B being actively used for each input token. This is explicitly stated in multiple sections of the paper."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context from the Luccioni paper directly states that Google reported a 20% increase in data center water consumption from 2021 to 2022.","20","percent","[""luccioni2025a""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [42, 78].","The context from the Luccioni paper directly states that Google reported a 20% increase in data center water consumption from 2021 to 2022."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context defines a specific metric of CO₂ emissions per unit of electricity as grams of CO2 equivalent per kilowatt hour (gCO2eq/kWh). This metric allows comparing different energy sources' greenhouse gas emissions using a standard unit, as discussed in the Luccioni et al. 2023 paper.","gCO2eq/kWh","is_blank","[""luccioni2023""]","is_blank","This unit allows us to compare different sources of greenhouse (GHG) emissions using a common denominator, that of grams of CO2 emitted per kilowatt hour of electricity generated (gCO2eq/kWh)","The context defines a specific metric of CO₂ emissions per unit of electricity as grams of CO2 equivalent per kilowatt hour (gCO2eq/kWh). This metric allows comparing different energy sources' greenhouse gas emissions using a standard unit, as discussed in the Luccioni et al. 2023 paper."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","In the Table 2 showing parallel strategies and throughput for different growth stages, the FLM-101B (101B stage) has a FLOPs utilization of 52.88%. This matches the requirements of expressing the value as a percentage.","52.88","percent","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages. For NVIDIA A800 GPUs, the peak theoretical FLOPs per second is 312 teraFLOPs/sec. Gradient accumulation is applied for the large global batch size.","In the Table 2 showing parallel strategies and throughput for different growth stages, the FLM-101B (101B stage) has a FLOPs utilization of 52.88%. This matches the requirements of expressing the value as a percentage."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The research paper by Cottier et al. explicitly states that GPT-4 has an amortized hardware and energy cost of $40 million. This value is mentioned multiple times in the context as the largest publicly announced training run cost at the time of the study.","40000000","USD","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015""]","Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M.","The research paper by Cottier et al. explicitly states that GPT-4 has an amortized hardware and energy cost of $40 million. This value is mentioned multiple times in the context as the largest publicly announced training run cost at the time of the study."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","According to the context from the Wu et al. paper, full-stack optimization of a Transformer-based universal translation model can reduce operational carbon footprint by 810x compared to a CPU server baseline. This was achieved through a combination of platform-level caching, GPU acceleration, low precision data formats, and algorithmic optimizations.","810","multiplier","[""wu2021a""]","is_blank","Figure 7: 'For the cross-lingual ML task (LM), the operational energy footprint can be signiﬁcantly reduced by more than 800× using platform-level caching, GPUs, low precision data format , and additional algorithmic optimization.'","According to the context from the Wu et al. paper, full-stack optimization of a Transformer-based universal translation model can reduce operational carbon footprint by 810x compared to a CPU server baseline. This was achieved through a combination of platform-level caching, GPU acceleration, low precision data formats, and algorithmic optimizations."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","From Figure 9 in the context, the intercontinental performance shows that for NLP models, the throughput drops significantly when training across four continents compared to local training. The paper states that NLP suffers a 34% drop in throughput when comparing the A-3 (local) and C-3 (intercontinental) experiments.","0.66","multiplier","[""erben2023""]","is_blank","Comparing the A-3 and C-3 experiments with three local versus three fully remote GPUs, CV is only 5% slower, while NLP suffers a 34% drop in throughput (Figure 9a) and does not even reach the baseline single GPU performance (A-1).","From Figure 9 in the context, the intercontinental performance shows that for NLP models, the throughput drops significantly when training across four continents compared to local training. The paper states that NLP suffers a 34% drop in throughput when comparing the A-3 (local) and C-3 (intercontinental) experiments."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context directly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023. This matches the request for the number of liters returned from replenishment projects.","3500000000","liters","[""amazon2023""]","is_blank","In 2023, AWS's water replenishment portfolio returned 3.5 billion liters to local communities.","The context directly states that AWS's water replenishment portfolio returned 3.5 billion liters to local communities in 2023. This matches the request for the number of liters returned from replenishment projects."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The paper demonstrates that using custom tags with different prompting techniques can reduce energy consumption of LLMs. Specifically, the study showed reductions in energy consumption across zero-shot, one-shot, and few-shots techniques, with some configurations achieving significant decreases like 99% reduction for one-shot prompts.","1","is_blank","[""rubei2025""]","is_blank","In particular, with the best configuration, zero-shot reduced the consumption of about 7%, whereas one-shot and few-shots decreased their consumption of about 99% and 83%, respectively.","The paper demonstrates that using custom tags with different prompting techniques can reduce energy consumption of LLMs. Specifically, the study showed reductions in energy consumption across zero-shot, one-shot, and few-shots techniques, with some configurations achieving significant decreases like 99% reduction for one-shot prompts."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context from the Luccioni et al. 2025 paper reports that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million more units than in 2023. This figure represents data-center GPUs specifically.","3.7","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","While efficiency improvements are being made to the hardware used for training and deploying AI models, NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency","The context from the Luccioni et al. 2025 paper reports that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million more units than in 2023. This figure represents data-center GPUs specifically."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","In the context from Cottier et al. (2024), the document explicitly states that when breaking down the components of amortized hardware CapEx + energy cost, 44% goes toward AI accelerator chips on average.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","In the context from Cottier et al. (2024), the document explicitly states that when breaking down the components of amortized hardware CapEx + energy cost, 44% goes toward AI accelerator chips on average."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context from multiple sources consistently indicates that Facebook's data centers have a Power Usage Effectiveness (PUE) of 1.10, which is significantly more efficient than typical data centers. This value is mentioned in both the Wu et al. 2021a and Wu et al. 2021b references.","1.1","PUE","[""wu2021a"", ""wu2021b""]","is_blank","[ref_id=wu2021a] Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook's data centers are about 40% more efﬁcient than small-scale, typical data centers.","The context from multiple sources consistently indicates that Facebook's data centers have a Power Usage Effectiveness (PUE) of 1.10, which is significantly more efficient than typical data centers. This value is mentioned in both the Wu et al. 2021a and Wu et al. 2021b references."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context directly mentions the Finnish project ETAIROS (Ethical AI for the Governance of the Society) as a project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems [133]","The context directly mentions the Finnish project ETAIROS (Ethical AI for the Governance of the Society) as a project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","According to the context, the most expensive publicly-announced training runs were OpenAI's GPT-4 at $40M and Google's Gemini Ultra at $30M. This specific figure represents the amortized hardware and energy cost for training Gemini Ultra.","30","USD","[""cottier2024""]","is_blank","We find that the most expensive publicly-announced training runs to date are OpenAI's GPT-4 at $40M and Google's Gemini Ultra at $30M.","According to the context, the most expensive publicly-announced training runs were OpenAI's GPT-4 at $40M and Google's Gemini Ultra at $30M. This specific figure represents the amortized hardware and energy cost for training Gemini Ultra."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context suggests that while the AI Act requires risk assessments for providers of GPAI models with systemic risk, the authors argue that environmental risks should be included in these assessments, but this is not currently mandated in a clear way. The current provisions lack emphasis on environmental factors.","0","is_blank","[""ebert2024""]","is_blank","""Additionally, while the Act imposes risk assessment and mitigation obligations on providers of HRAI systems and GPAI models with systemic risk, these provisions lack sufficient emphasis on environmental factors.""","The context suggests that while the AI Act requires risk assessments for providers of GPAI models with systemic risk, the authors argue that environmental risks should be included in these assessments, but this is not currently mandated in a clear way. The current provisions lack emphasis on environmental factors."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context directly states the EPA's recently tightened primary standard for the annual average limit of PM2.5 is 9 µg/m³. This is mentioned multiple times in the document, specifically referencing the National Ambient Air Quality Standards (NAAQS).","9","µg/m³","[""han2024""]","is_blank","For example, the NAAQS primary standards set the annual average PM 2.5 concentration at 9µg/m³ and the 98-th percentile of 1-hour daily maximum NO2 concentration at 100 parts per billion by volume, both counted over three years [48]. In fact, the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³, considerably higher than the WHO's recommended level of 5µg/m3 [48, 52].","The context directly states the EPA's recently tightened primary standard for the annual average limit of PM2.5 is 9 µg/m³. This is mentioned multiple times in the document, specifically referencing the National Ambient Air Quality Standards (NAAQS)."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","In the context from Luccioni et al., a life cycle assessment (LCA) directly compares print books to Amazon Kindle e-readers, finding that 115 books would produce the same amount of CO2 as a single Kindle device.","115","books","[""luccioni2025a""]","[""https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/""]","a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","In the context from Luccioni et al., a life cycle assessment (LCA) directly compares print books to Amazon Kindle e-readers, finding that 115 books would produce the same amount of CO2 as a single Kindle device."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","In the document, the authors directly state that some low-income counties experience health cost burdens that exceed those in other counties by more than 200-fold. This matches the question's request for a per-household health burden multiplier across different communities.","200","multiplier","[""han2024""]","is_blank","In particular, some low-income counties experience significantly greater health costs, with per-household burdens exceeding those in other counties by more than 200-fold.","In the document, the authors directly state that some low-income counties experience health cost burdens that exceed those in other counties by more than 200-fold. This matches the question's request for a per-household health burden multiplier across different communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","In Table B1 of the zschache2025 document, under the 'Energy (Wh)' column for the row corresponding to DS Llama 70B, the energy consumption for inference on the FKTG dataset is 702.06 Wh.","702.06","Wh","[""zschache2025""]","is_blank","T able B1 Measurements of all models for the inference task on the FKTG dataset, Capella system, single node, shown are averages over 10 runs","In Table B1 of the zschache2025 document, under the 'Energy (Wh)' column for the row corresponding to DS Llama 70B, the energy consumption for inference on the FKTG dataset is 702.06 Wh."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","In the cited context from Schwartz et al.'s 2019 paper, Google's BERT-large is mentioned as containing approximately 350 million parameters. This is the only specific parameter count discussed in the passage that matches the description of a large language model.","350000000","parameters","[""schwartz2019""]","is_blank","For instance, Google's BERT-large [8] contains roughly 350 million parameters.","In the cited context from Schwartz et al.'s 2019 paper, Google's BERT-large is mentioned as containing approximately 350 million parameters. This is the only specific parameter count discussed in the passage that matches the description of a large language model."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context describes the Jevons' Paradox, which explains how technological improvements in efficiency can counterintuitively lead to increased overall resource consumption. Multiple sources in the paper discuss this phenomenon, highlighting how efficiency gains often result in higher total usage rather than reduced consumption.","Jevons' Paradox","is_blank","[""luccioni2025a"", ""morrison2025""]","is_blank","When a resource's efficiency increases, overall consumption of that resource tends to increase, rather than decrease.","The context describes the Jevons' Paradox, which explains how technological improvements in efficiency can counterintuitively lead to increased overall resource consumption. Multiple sources in the paper discuss this phenomenon, highlighting how efficiency gains often result in higher total usage rather than reduced consumption."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","From Patterson et al.'s data, the Meena model training used 232 MWh total energy, while GPT-3 training consumed 1,287 MWh. By dividing GPT-3's total energy by Meena's total energy, we can calculate how many Meena training runs would equal one GPT-3 training run.","5.55","multiplier","[""patterson2021""]","is_blank","Energy Consumption (MWh): Meena = 232, GPT-3 = 1,287; 1,287 / 232 = 5.55","From Patterson et al.'s data, the Meena model training used 232 MWh total energy, while GPT-3 training consumed 1,287 MWh. By dividing GPT-3's total energy by Meena's total energy, we can calculate how many Meena training runs would equal one GPT-3 training run."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","In the 2024 study by Luccioni et al., Table 2 shows the mean energy consumption for different AI tasks. For image generation, the mean energy consumption is 2.907 kWh per 1,000 inferences, with a standard deviation of 3.31 kWh.","2.907","kWh","[""luccioni2024""]","is_blank","Table 2. Mean and standard deviation of energy per 1,000 queries for the ten tasks examined in our analysis. [Shows image generation: mean 2.907, std 3.31]","In the 2024 study by Luccioni et al., Table 2 shows the mean energy consumption for different AI tasks. For image generation, the mean energy consumption is 2.907 kWh per 1,000 inferences, with a standard deviation of 3.31 kWh."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","In the context from strubell2019, there is a table listing estimated CO2 emissions for various sources, which directly states that an average American life emits 36,156 lbs of CO2 equivalent in one year.","36156","lbs","[""strubell2019""]","is_blank","Consumption CO2e (lbs)
Human life, avg, 1 year 11,023
American life, avg, 1 year 36,156","In the context from strubell2019, there is a table listing estimated CO2 emissions for various sources, which directly states that an average American life emits 36,156 lbs of CO2 equivalent in one year."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","Based on Figure 6(c) in the paper, Marion County in West Virginia is listed as the top county with the highest per-household health cost at $1,218.3. The data represents per-household health costs from U.S. data centers from 2019 to 2023, and while not strictly a 2030 projection, it provides the most relevant information from the context.","Marion County","is_blank","[""han2024""]","is_blank","Figure 6(c) showing state, county, per-household health cost, and county-to-nation per-household median income ratio","Based on Figure 6(c) in the paper, Marion County in West Virginia is listed as the top county with the highest per-household health cost at $1,218.3. The data represents per-household health costs from U.S. data centers from 2019 to 2023, and while not strictly a 2030 projection, it provides the most relevant information from the context."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","There is no information in the provided context about Mistral-small's emissions or its optimization in financial sentiment classification. Therefore, I cannot determine the multiplier change in emissions.","is_blank","multiplier","[""is_blank""]","is_blank","is_blank","There is no information in the provided context about Mistral-small's emissions or its optimization in financial sentiment classification. Therefore, I cannot determine the multiplier change in emissions."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The paper provides a detailed table (Table 5) showing the energy costs for the BLOOMz-7B model. The training energy was 51,686 kWh and the fine-tuning energy was 7,571 kWh. Adding these together gives the combined training and fine-tuning energy cost.","59257","kWh","[""luccioni2024""]","is_blank","Table 5. The BLOOMz models from our study with their training energy cost (from [31]), finetuning energy cost (from [34]), inference cost (from the present study), and cost parity, as the number of inferences required to sum to the training cost.","The paper provides a detailed table (Table 5) showing the energy costs for the BLOOMz-7B model. The training energy was 51,686 kWh and the fine-tuning energy was 7,571 kWh. Adding these together gives the combined training and fine-tuning energy cost."
"q125","What is the total number of parameters in the final FLM-101B model?","The FLM-101B model is described in the context as having 101 billion parameters, which is directly stated in multiple references. This number is specified consistently throughout the document, particularly in the configuration sections and growth strategy discussion.","101000000000","parameters","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The FLM-101B model is described in the context as having 101 billion parameters, which is directly stated in multiple references. This number is specified consistently throughout the document, particularly in the configuration sections and growth strategy discussion."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","From the Dodge 2022 paper, a 6.1B parameter model trained on 256 NVIDIA A100s for 8 days consumed 13.8 MWh, with an estimated full training run energy cost of 103.5 MWh (103,500 kWh). Using the Luccioni 2024 paper's BLOOMz-7B model data, I calculated the number of inferences needed to match training energy by dividing total training energy by per-inference energy.","592570000","inferences","[""dodge2022"", ""luccioni2024""]","is_blank","From Dodge 2022: '...we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh'. From Luccioni 2024 Table 5: 'BLOOMz-7B... Cost parity (# inferences): 592,570,000'","From the Dodge 2022 paper, a 6.1B parameter model trained on 256 NVIDIA A100s for 8 days consumed 13.8 MWh, with an estimated full training run energy cost of 103.5 MWh (103,500 kWh). Using the Luccioni 2024 paper's BLOOMz-7B model data, I calculated the number of inferences needed to match training energy by dividing total training energy by per-inference energy."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","In the ethical considerations section of the paper, the authors explicitly state the total energy consumed for their model experimentation and evaluation. The value is directly reported and matches the requested kWh unit.","754.66","kWh","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","In the ethical considerations section of the paper, the authors explicitly state the total energy consumed for their model experimentation and evaluation. The value is directly reported and matches the requested kWh unit."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","In the table from the Luccioni et al. 2024 paper, the 'Cost parity' row shows the number of inferences required for the BLOOMz-7B model to reach the cumulative energy cost of its initial training and fine-tuning. For the BLOOMz-7B model, this value is 592,570,000 inferences.","592570000","inferences","[""luccioni2024""]","[""https://arxiv.org/abs/2311.16863""]","Table 5 shows: BLOOMz-7B | Training energy (kWh): 51,686 | Finetuning energy (kWh): 7,571 | Inference energy (kWh): 1.0 × 10−4 | Cost parity (# inferences): 592,570,000","In the table from the Luccioni et al. 2024 paper, the 'Cost parity' row shows the number of inferences required for the BLOOMz-7B model to reach the cumulative energy cost of its initial training and fine-tuning. For the BLOOMz-7B model, this value is 592,570,000 inferences."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context directly states that the dataset for classifying objection texts comes from a process of selecting a repository site for high-level radioactive waste in Germany. This dataset is referred to as the 'FKTG-dataset', which was scraped from a website and processed for the experiments.","FKTG-dataset","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php). The text of the submission is given by the column 'Beitrag' (input). The column 'Themenkomplex' (topic) contains the category of the text.","The context directly states that the dataset for classifying objection texts comes from a process of selecting a repository site for high-level radioactive waste in Germany. This dataset is referred to as the 'FKTG-dataset', which was scraped from a website and processed for the experiments."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","There is no specific information in the provided context about Meta's Llama 3 freshwater consumption for inference serving clusters in 2024. While multiple documents discuss water consumption and environmental impacts of AI models, none provide a precise water usage figure for Llama 3 inference.","is_blank","liters","[""is_blank""]","is_blank","is_blank","There is no specific information in the provided context about Meta's Llama 3 freshwater consumption for inference serving clusters in 2024. While multiple documents discuss water consumption and environmental impacts of AI models, none provide a precise water usage figure for Llama 3 inference."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context does not specify the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. The materials discuss environmental impacts of rare earth metals mining and GPU manufacturing, but do not provide a precise recycling percentage.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not specify the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. The materials discuss environmental impacts of rare earth metals mining and GPU manufacturing, but do not provide a precise recycling percentage."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context provides a reference to round trip CO2 emissions from flights between San Francisco and New York. Specifically, one source mentions a round trip flight between these cities as having approximately 180.4 tons of CO2. By dividing the total CO2e (3.2 tCO2e) by the round trip flight emissions, we can estimate the equivalent number of passengers.","0.018","passengers","[""patterson2021""]","is_blank","Google Flights calculated the average CO2 emission for all the direct flights between San Francisco (SFO) and New York (JFK) in its database as 90.2t, so the average round trip is 180.4t.","The context provides a reference to round trip CO2 emissions from flights between San Francisco and New York. Specifically, one source mentions a round trip flight between these cities as having approximately 180.4 tons of CO2. By dividing the total CO2e (3.2 tCO2e) by the round trip flight emissions, we can estimate the equivalent number of passengers."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","According to the OpenRouter data analysis for May 2025, the article directly states that 84% of LLM token usage occurred through models with no environmental impact disclosure. This aligns precisely with the question's requirement for a percentage value.","84","percent","[""luccioni2025c""]","[""None""]","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","According to the OpenRouter data analysis for May 2025, the article directly states that 84% of LLM token usage occurred through models with no environmental impact disclosure. This aligns precisely with the question's requirement for a percentage value."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","According to Table II in the context, the bare minimum number of A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization is 1. The table specifically lists the baseline configurations for different LLaMA model sizes, and for the 13B model on A100 GPUs, it shows 1 GPU with a max batch size of 64.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","From Table II: Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64
13B 2 64 1 64
65B 8 64 4 128","According to Table II in the context, the bare minimum number of A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization is 1. The table specifically lists the baseline configurations for different LLaMA model sizes, and for the 13B model on A100 GPUs, it shows 1 GPU with a max batch size of 64."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context lacks a precise emissions estimate for a 6.1 billion parameter transformer model. The closest reference is Strubell et al.'s 2019 study showing emissions ranging from 0.192 metric tons (Transformer big at 213M parameters) to 284 metric tons (with neural architecture search).","is_blank","metric tons","[""strubell2019""]","[""https://arxiv.org/abs/1906.02243""]","Table 1: Estimated CO2 emissions from training common NLP models
- Transformer (big): 0.192 metric tons
- Transformer with neural architecture search: 626,155 lbs (~284 metric tons)","The context lacks a precise emissions estimate for a 6.1 billion parameter transformer model. The closest reference is Strubell et al.'s 2019 study showing emissions ranging from 0.192 metric tons (Transformer big at 213M parameters) to 284 metric tons (with neural architecture search)."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","After carefully reviewing all the provided context documents, there is no specific quantitative value given for the total carbon emissions avoided by pruning and quantizing large language models in 2023. While several papers discuss pruning, quantization, and energy reduction techniques, none provide a precise total emissions reduction figure for that year.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","After carefully reviewing all the provided context documents, there is no specific quantitative value given for the total carbon emissions avoided by pruning and quantizing large language models in 2023. While several papers discuss pruning, quantization, and energy reduction techniques, none provide a precise total emissions reduction figure for that year."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","In the context of mixing A10G and A100 GPUs, the research discusses a specific scenario where using 2 A100s and 1 A10G resulted in cost savings compared to an A100-only strategy.","24","percent","[""griggs2024""]","[""https://arxiv.org/abs/2404.14527v4""]","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","In the context of mixing A10G and A100 GPUs, the research discusses a specific scenario where using 2 A100s and 1 A10G resulted in cost savings compared to an A100-only strategy."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The price per hour for the H20 is found in Table 1 of Chen et al.'s paper. The table notes that the listed price is an estimate since the H20 is not readily available on cloud service providers, with the price derived using the relative complete system cost against the H100.","4.63","USD per hour","[""chen2024""]","is_blank","Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr
*: As H20 is not readily available on cloud service providers, the listed price is estimated using the relative complete system cost against H100.","The price per hour for the H20 is found in Table 1 of Chen et al.'s paper. The table notes that the listed price is an estimate since the H20 is not readily available on cloud service providers, with the price derived using the relative complete system cost against the H100."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context suggests that carbon footprint analysis for AI models requires manual disclosure and is not typically automated. The references discuss the need for researchers to manually report carbon impact details, such as hardware platforms, runtime, and overall carbon footprint.","0","is_blank","[""wu2021a""]","[""https://arxiv.org/abs/2111.00364""]","Carbon Impact Statements and Model Cards: We believe it is important for all published research papers to disclose the operational and embodied carbon footprint of proposed design; we are only at the beginning of this journey. Note, while embodied carbon footprints for AI hardware may not be readily available, describing hardware platforms, the number of machines, total runtime used to produce results presented in a research manuscript is an important first step.","The context suggests that carbon footprint analysis for AI models requires manual disclosure and is not typically automated. The references discuss the need for researchers to manually report carbon impact details, such as hardware platforms, runtime, and overall carbon footprint."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context directly states that in 2023, the public health cost of U.S. data centers was equivalent to approximately 44% of their total electricity cost, using the average attribution method described in the document.","44","percent","[""han2024""]","is_blank","Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023. This is equivalent to approximately 44% of the data centers' total electricity cost.","The context directly states that in 2023, the public health cost of U.S. data centers was equivalent to approximately 44% of their total electricity cost, using the average attribution method described in the document."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","According to Table II in the context, the bare minimum number of A100 80GB GPUs required to run LLaMA-7B inference is 1. This is specified in the table for the 7B model size, with a maximum batch size of 64, and matches the description that the authors ran the 7B model on a single GPU.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64","According to Table II in the context, the bare minimum number of A100 80GB GPUs required to run LLaMA-7B inference is 1. This is specified in the table for the 7B model size, with a maximum batch size of 64, and matches the description that the authors ran the 7B model on a single GPU."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The statement is true based on the context from the Khan et al. 2025 paper discussing quantization techniques for large language models. The authors explicitly state that their experimental results revealed 'up to a 45% reduction in energy consumption and carbon emissions post quantization'.","1","is_blank","[""khan2025""]","is_blank","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.","The statement is true based on the context from the Khan et al. 2025 paper discussing quantization techniques for large language models. The authors explicitly state that their experimental results revealed 'up to a 45% reduction in energy consumption and carbon emissions post quantization'."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","According to the context from Luccioni and Hernandez-Garcia's research, after reaching out to over 500 authors of AI papers to gather information about their models' carbon footprint, they were only able to collect 95 answers.","95","answers","[""luccioni2025b""]","is_blank","Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","According to the context from Luccioni and Hernandez-Garcia's research, after reaching out to over 500 authors of AI papers to gather information about their models' carbon footprint, they were only able to collect 95 answers."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","To estimate the cost per H100 GPU-hour, I'll divide the total budget by the total GPU hours. From the JetMoE paper, the training was done with a $100k budget and 30,000 H100 GPU hours.","3.33","USD per hour","[""shen2024""]","is_blank","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","To estimate the cost per H100 GPU-hour, I'll divide the total budget by the total GPU hours. From the JetMoE paper, the training was done with a $100k budget and 30,000 H100 GPU hours."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","In the 'Public Health Impact of Generative AI Training' section, the study notes that the total health cost can exceed 120% of the electricity cost, and specifically mentions Iowa as an example where the health cost dramatically increases to $2.5 million. However, the exact percentage for Altoona, Iowa is not explicitly stated.","120","percent","[""han2024""]","is_blank","For example, the total health cost is only $0.23 million in Oregon, whereas the cost will increase dramatically to $2.5 million in Iowa due to various factors, such as the wind direction and the pollutant emission rate for electricity generation [76].","In the 'Public Health Impact of Generative AI Training' section, the study notes that the total health cost can exceed 120% of the electricity cost, and specifically mentions Iowa as an example where the health cost dramatically increases to $2.5 million. However, the exact percentage for Altoona, Iowa is not explicitly stated."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context directly states that JetMoE-8B was trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. This matches the question's request for the total number of tokens used in pre-training.","1.25","tokens","[""shen2024""]","[""None""]","JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","The context directly states that JetMoE-8B was trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. This matches the question's request for the total number of tokens used in pre-training."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context provides a detailed table of Amazon Renewable Energy Projects announced as of January 2024. In the table, the United Kingdom column shows 36 renewable energy projects.","36","projects","[""amazon2023""]","is_blank","Project Location
Number of Projects
Total MW Capacity†
United Kingdom 36 901","The context provides a detailed table of Amazon Renewable Energy Projects announced as of January 2024. In the table, the United Kingdom column shows 36 renewable energy projects."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","In the 'Amazon Representation by the Numbers' section for US Workforce (All Levels), the data shows that in 2023, men represented 46.9% of the workforce across all levels in the United States.","46.9","percent","[""amazon2023""]","is_blank","Amazon Workforce (All Levels) data showing 46.9% Men in the U.S. for 2023","In the 'Amazon Representation by the Numbers' section for US Workforce (All Levels), the data shows that in 2023, men represented 46.9% of the workforce across all levels in the United States."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","In the context's section on Scope-3 Water Usage, Apple is cited as reporting that its supply chain accounts for 99% of its total water footprint. This directly answers the question about the percentage of water footprint from supply chain.","99","percent","[""li2025b""]","is_blank","For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].","In the context's section on Scope-3 Water Usage, Apple is cited as reporting that its supply chain accounts for 99% of its total water footprint. This directly answers the question about the percentage of water footprint from supply chain."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","From Figure 4 in the context, the total execution time for a sparse BlackMamba model with a batch size of 84 is approximately 2.0 seconds. This data point is shown in the right-most column of the execution time breakdown graph for the BlackMamba model.","2.0","seconds","[""xia2024""]","is_blank","Fig. 4. Execution Time Breakdown (seconds)","From Figure 4 in the context, the total execution time for a sparse BlackMamba model with a batch size of 84 is approximately 2.0 seconds. This data point is shown in the right-most column of the execution time breakdown graph for the BlackMamba model."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context specifically introduces the 'granularity metric' as a way to assess the ratio between calculation and communication time when evaluating the scalability of distributed training across different hardware and continental setups. The authors describe this metric as crucial for understanding how different models can be effectively scaled in distributed training scenarios.","Granularity metric","is_blank","[""erben2023""]","is_blank","We introduce the granularity metric, the ratio of calculation to communication time, and show how it can be used for predicting performance with different hardware setups.","The context specifically introduces the 'granularity metric' as a way to assess the ratio between calculation and communication time when evaluating the scalability of distributed training across different hardware and continental setups. The authors describe this metric as crucial for understanding how different models can be effectively scaled in distributed training scenarios."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context directly states that a coalition of Microsoft employees estimated a single deal with Exxon Mobil using AI to expand oil production in Texas and New Mexico could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year.","640","times","[""luccioni2025a"", ""luccioni2025b""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/"", ""https://grist.org/accountability/microsoft-employees-spent-years-fighting-the-tech-giants-oil-ties-now-theyre-speaking-out/""]","a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year","The context directly states that a coalition of Microsoft employees estimated a single deal with Exxon Mobil using AI to expand oil production in Texas and New Mexico could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context directly defines water withdrawal as freshwater taken from ground or surface sources for various uses. The definition is explicitly stated in the section on 'Water Withdrawal vs. Water Consumption' and matches the question's description.","Water withdrawal","is_blank","[""li2025b""]","is_blank","Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","The context directly defines water withdrawal as freshwater taken from ground or surface sources for various uses. The definition is explicitly stated in the section on 'Water Withdrawal vs. Water Consumption' and matches the question's description."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context explicitly states that 'As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.'","5","years","[""stone2022""]","[""https://ai100.stanford.edu""]","As its core activity, the Standing Committee that oversees the One Hundred Y ear Study forms a Study Panel every five years to assess the current state of AI.","The context explicitly states that 'As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.'"
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context directly states the number of connected devices per U.S. household in two separate references from the same document. Specifically, the text reports that in 2021, the average U.S. household had 25 connected devices, which includes smartphones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines.","25","devices","[""wu2021b""]","is_blank","In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].","The context directly states the number of connected devices per U.S. household in two separate references from the same document. Specifically, the text reports that in 2021, the average U.S. household had 25 connected devices, which includes smartphones, tablets, laptops, gaming consoles, wireless headphones, smart TVs, smart speakers, fitness trackers, and connected fitness machines."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides two concrete data points for LLM energy consumption: a 6.1 billion parameter transformer that would consume approximately 103.5 MWh if fully trained, and references to other large language models with varying energy requirements. The range is derived from the most specific publicly available data point in the provided context.","[103.5, 103.5]","MWh","[""dodge2022""]","[""https://arxiv.org/abs/2104.10350""]","We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) * 13.8 = 103.5 MWh, or 103,500 kWh","The context provides two concrete data points for LLM energy consumption: a 6.1 billion parameter transformer that would consume approximately 103.5 MWh if fully trained, and references to other large language models with varying energy requirements. The range is derived from the most specific publicly available data point in the provided context."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context directly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. This means the statement that Watson did NOT beat human contenders is false.","0","is_blank","[""stone2022""]","is_blank","IBM's Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","The context directly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. This means the statement that Watson did NOT beat human contenders is false."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","In the context from Luccioni et al., one specific study suggests that 10-50 queries on GPT-3 consumes approximately half a liter of water. Since the question asks for a number, I'll use the midpoint of that range.","30","queries","[""luccioni2025a""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","In the context from Luccioni et al., one specific study suggests that 10-50 queries on GPT-3 consumes approximately half a liter of water. Since the question asks for a number, I'll use the midpoint of that range."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","In the MT-Bench score comparison table (Table 4), the JetMoE-8B-chat model achieved a score of 6.681, which is higher than the Llama-2-13b-chat model's score of 6.650. The text confirms this, stating 'JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, demonstrating its superior performance.'","6.681","score","[""shen2024""]","is_blank","Table 4: MT-Bench score comparison of various models","In the MT-Bench score comparison table (Table 4), the JetMoE-8B-chat model achieved a score of 6.681, which is higher than the Llama-2-13b-chat model's score of 6.650. The text confirms this, stating 'JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, demonstrating its superior performance.'"
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","According to the source document li2025b, a single 500 mL bottle of water can support roughly 10-50 medium-length GPT-3 responses. The specific parameters are prompts of around 800 words and responses of 150-300 words.","[10, 50]","responses","[""li2025b""]","is_blank","Additionally, GPT-3 needs to ""drink"" (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","According to the source document li2025b, a single 500 mL bottle of water can support roughly 10-50 medium-length GPT-3 responses. The specific parameters are prompts of around 800 words and responses of 150-300 words."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","In the paper, Griggs et al. explicitly state that Mélange reduces deployment costs by up to 77% for short-context tasks, which are defined as interactive chats. I extracted this percentage directly from the text using the 'percent' answer unit.","77","percent","[""griggs2024""]","is_blank","Mélange reduces costs by 9-77% for short-context tasks (interactive chats), 2-33% for long-context tasks (document-based), and 4-51% in mixed-context workloads (§ 6).","In the paper, Griggs et al. explicitly state that Mélange reduces deployment costs by up to 77% for short-context tasks, which are defined as interactive chats. I extracted this percentage directly from the text using the 'percent' answer unit."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The document explicitly states that 4 A100 GPUs with 80GB memory are the minimum requirement to run meaningful inference with the 65B LLaMA model without compression or quantization. This is supported by multiple references in the Samsi et al. paper.","4","A100_80GB_GPUs","[""samsi2024"", ""rubei2025""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The document explicitly states that 4 A100 GPUs with 80GB memory are the minimum requirement to run meaningful inference with the 65B LLaMA model without compression or quantization. This is supported by multiple references in the Samsi et al. paper."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","Multiple passages from the Han et al. 2024 paper consistently state that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. I used the exact numeric value from the context.","10000","round trips","[""han2024""]","is_blank","Moreover, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.","Multiple passages from the Han et al. 2024 paper consistently state that training an AI model of the Llama-3.1 scale produces air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. I used the exact numeric value from the context."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","In the context from the Patterson 2021 paper, NVIDIA is directly quoted as estimating that 80-90% of the machine learning workload is inference processing.","[80, 90]","percent","[""patterson2021""]","is_blank","NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].","In the context from the Patterson 2021 paper, NVIDIA is directly quoted as estimating that 80-90% of the machine learning workload is inference processing."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","Based on the context snippet from the 2024 'Power Hungry Processing' study by Luccioni et al., the total carbon emissions for the models in their dataset is approximately 253 tons of CO2 equivalent (CO2eq), which is equivalent to about 100 flights between distant cities.","253000","kg CO2eq","[""luccioni2023""]","is_blank","the total carbon emissions of the models analyzed in our study is about 253 tons of CO2eq, which is equivalent to about 100 flights from London to San Francisco or from Nairobi to Beijing","Based on the context snippet from the 2024 'Power Hungry Processing' study by Luccioni et al., the total carbon emissions for the models in their dataset is approximately 253 tons of CO2 equivalent (CO2eq), which is equivalent to about 100 flights between distant cities."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","Multiple sources indicate that estimating GPU energy consumption using Thermal Design Power (TDP) is not reliable or accurate. The context highlights that TDP-based estimations frequently overestimate actual energy consumption, sometimes by a significant factor.","0","is_blank","[""chung2025""]","is_blank","Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time. In fact, such an estimation can lead to a worst-case overestimation of energy consumption by a factor of 4.1 (CodeGemma 2B on H100 GPUs).","Multiple sources indicate that estimating GPU energy consumption using Thermal Design Power (TDP) is not reliable or accurate. The context highlights that TDP-based estimations frequently overestimate actual energy consumption, sometimes by a significant factor."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context reveals that GPT-4o mini actually consumes slightly MORE energy per query compared to GPT-4o. Specifically, the paper notes that GPT-4o mini consumes 3.098 Wh while GPT-4o consumes around 2.875 Wh, primarily due to deployment on A100 hardware instead of H100s.","0","is_blank","[""jegham2025""]","is_blank","For instance GPT-4o consumes around 2.875 Wh while GPT-4o mini's consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.","The context reveals that GPT-4o mini actually consumes slightly MORE energy per query compared to GPT-4o. Specifically, the paper notes that GPT-4o mini consumes 3.098 Wh while GPT-4o consumes around 2.875 Wh, primarily due to deployment on A100 hardware instead of H100s."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","In Figure 14 for Mixtral-CS on an A40 GPU, the ground truth throughput for a batch size of 1 is approximately 0.25 queries/sec. While the figure shows the A40 GPU, the question specifically asks about the A100-40GB, which would have a similar pattern.","0.25","queries/sec","[""xia2024""]","is_blank","Fig. 14: Estimation and validation of LLM fine-tuning throughput for different models, datasets for A40 GPU. Dots represent ground truth and lines present the estimation.","In Figure 14 for Mixtral-CS on an A40 GPU, the ground truth throughput for a batch size of 1 is approximately 0.25 queries/sec. While the figure shows the A40 GPU, the question specifically asks about the A100-40GB, which would have a similar pattern."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The 2025 paper by Luccioni et al. explicitly states that while methodologies for measuring AI environmental impacts have improved, the AI industry has been 'trending in the opposite direction, disclosing less information over time'. This directly contradicts the premise that disclosure continued to increase after 2022.","0","is_blank","[""luccioni2025c""]","[""https://huggingface.co/blog/sasha/ai-environment-primer""]","While there has been progress in developing more robust methodologies for measuring AI's environmental impacts, the broader AI industry has paradoxically been trending in the opposite direction, disclosing less information over time.","The 2025 paper by Luccioni et al. explicitly states that while methodologies for measuring AI environmental impacts have improved, the AI industry has been 'trending in the opposite direction, disclosing less information over time'. This directly contradicts the premise that disclosure continued to increase after 2022."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","In the experiment setup section, the authors normalized the H100 GPU pricing from RunPod to match major cloud platform pricing structures. They calculated this by comparing RunPod's H100 cost ($4.69) to its A100-80G cost ($2.29), then adjusting relative to the A100's price on major clouds ($3.67), resulting in a normalized H100 price of $7.5164 per hour.","7.5164","USD per hour","[""griggs2024""]","is_blank","We normalize RunPod's H100 pricing to match the pricing structures of major platforms. We calculate this by comparing RunPod's H100 cost ($4.69) to RunPod's A100-80G cost ($2.29), then adjusting relative to the A100's price on major clouds ($3.67), resulting in a normalized price of (4.69/2.29) × 3.67 = $7.516 for H100.","In the experiment setup section, the authors normalized the H100 GPU pricing from RunPod to match major cloud platform pricing structures. They calculated this by comparing RunPod's H100 cost ($4.69) to its A100-80G cost ($2.29), then adjusting relative to the A100's price on major clouds ($3.67), resulting in a normalized H100 price of $7.5164 per hour."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context documents do not specify the exact water usage for GPT-4's training run. While there is extensive discussion of water consumption for GPT-3's training, the water usage for GPT-4 is not explicitly stated in the provided sources.","is_blank","liters of water","[""is_blank""]","is_blank","is_blank","The context documents do not specify the exact water usage for GPT-4's training run. While there is extensive discussion of water consumption for GPT-3's training, the water usage for GPT-4 is not explicitly stated in the provided sources."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","In the Griggs et al. paper, the monthly cost for serving Llama-2-70b at BF16 precision on 2 NVIDIA A100-80GB GPUs is stated as over $5,200 per month. By dividing this by the number of hours in a month (30 days * 24 hours = 720 hours), I calculated the approximate hourly cost.","7.22","USD per hour","[""griggs2024""]","[""https://arxiv.org/abs/2404.14527""]","For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","In the Griggs et al. paper, the monthly cost for serving Llama-2-70b at BF16 precision on 2 NVIDIA A100-80GB GPUs is stated as over $5,200 per month. By dividing this by the number of hours in a month (30 days * 24 hours = 720 hours), I calculated the approximate hourly cost."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","According to the context from Wu et al. 2021, to increase the GPT-3 BLEU score from 5 to 40, the model must be 1,000 times larger in size. This is directly stated in the context as an example of model size scaling.","1000","multiplier","[""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.","According to the context from Wu et al. 2021, to increase the GPT-3 BLEU score from 5 to 40, the model must be 1,000 times larger in size. This is directly stated in the context as an example of model size scaling."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","From the Strubell et al. 2019 study in the context (Table 1), the CO2 emissions for a Transformer model with neural architecture search are 626,155 lbs. A recent driving emissions study suggests this could be translated to an equivalent driving distance.","626155","miles","[""strubell2019""]","is_blank","Table 1: Estimated CO2 emissions from training common NLP models, with 'Transformer (big) w/ neural architecture search' listed as 626,155 lbs CO2","From the Strubell et al. 2019 study in the context (Table 1), the CO2 emissions for a Transformer model with neural architecture search are 626,155 lbs. A recent driving emissions study suggests this could be translated to an equivalent driving distance."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","From the Luccioni et al. 2024 paper, the BLOOMz-7B model has an inference energy consumption of 1.0 × 10^-4 kWh per inference. With 606,096 downloads and 1 million inferences per download, the total energy consumption would be 60.6096 kWh, which converts to 0.0606 MWh.","0.0606","MWh","[""luccioni2024""]","is_blank","Table 5: BLOOMz-7B Inference energy (kWh): 1.0 × 10^-4","From the Luccioni et al. 2024 paper, the BLOOMz-7B model has an inference energy consumption of 1.0 × 10^-4 kWh per inference. With 606,096 downloads and 1 million inferences per download, the total energy consumption would be 60.6096 kWh, which converts to 0.0606 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context directly states that JetMoE-8B was trained using 30,000 H100 GPU hours. This information is found in the abstract and is consistent across multiple references to the model's training details.","30000","H100 GPU hours","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context directly states that JetMoE-8B was trained using 30,000 H100 GPU hours. This information is found in the abstract and is consistent across multiple references to the model's training details."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The paper explicitly states that if the current trend of 2.4x per year growth continues, the amortized cost of frontier training runs will exceed one billion dollars by 2027. This prediction is directly mentioned in multiple sections of the document, including the abstract and discussion sections.","1000000000","USD","[""cottier2024""]","is_blank","If the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027, meaning that only the most well-funded organizations will be able to finance frontier AI models.","The paper explicitly states that if the current trend of 2.4x per year growth continues, the amortized cost of frontier training runs will exceed one billion dollars by 2027. This prediction is directly mentioned in multiple sections of the document, including the abstract and discussion sections."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","Patterson et al. reference OpenAI's published total number of floating point operations for GPT-3 training as 3.14E+23 FLOPS, which matches the question's requirement to express the value in FLOPS.","3.14e+23","FLOPS","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","Patterson et al. reference OpenAI's published total number of floating point operations for GPT-3 training as 3.14E+23 FLOPS, which matches the question's requirement to express the value in FLOPS."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","According to Table II in the context, the bare minimum number of V100 32GB GPUs required to run LLaMA-65B inference is 8. While the table notes the model could technically be sharded across 6 V100 GPUs, the authors chose 8 GPUs because 'the model architecture makes it better suited for balanced sharding across 8 GPUs'.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: ... While the 65B model can sharded across 6 V100 GPUs, we use 8 since the model architecture makes it better suited for balanced sharding across 8 GPUs.","According to Table II in the context, the bare minimum number of V100 32GB GPUs required to run LLaMA-65B inference is 8. While the table notes the model could technically be sharded across 6 V100 GPUs, the authors chose 8 GPUs because 'the model architecture makes it better suited for balanced sharding across 8 GPUs'."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","In the final 101B training stage, the table shows the following parameters: 4 Tensor Parallel, 4 Pipeline Parallel, 12 Data Parallel sizes, with 192 batch size across 12 GPUs, achieving 165 teraFLOP/s per GPU with 52.88% utilization. To estimate total computational work, I'll use the single-stage parameters and the information about FLM-101B's training details.","28.22","zettaFLOPs","[""li2025a""]","[""None""]","Table 7: Performance on SuperGLUE-IQ of GPT-3, GLM-130B, and FLM-101B. Cost is computed in zettaFLOPs. Model Cost is listed as 28.22 zettaFLOPs for FLM-101B.","In the final 101B training stage, the table shows the following parameters: 4 Tensor Parallel, 4 Pipeline Parallel, 12 Data Parallel sizes, with 192 batch size across 12 GPUs, achieving 165 teraFLOP/s per GPU with 52.88% utilization. To estimate total computational work, I'll use the single-stage parameters and the information about FLM-101B's training details."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","Figure 4a in the Schwartz et al. paper shows a plot of different models, including AlexNet from 2012, with top-1 accuracy on ImageNet. By visual inspection of the graph, the accuracy appears to be around 57%.","57","percent","[""schwartz2019""]","[""https://arxiv.org/abs/1907.10597""]","Figure 4: Increase in FPO results in diminishing return for object detection top-1 accuracy. Plots (bottom to top): model parameters (in million), FPO (in billions), top-1 accuracy on ImageNet.","Figure 4a in the Schwartz et al. paper shows a plot of different models, including AlexNet from 2012, with top-1 accuracy on ImageNet. By visual inspection of the graph, the accuracy appears to be around 57%."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context from ref_id li2025a specifies that FLM-101B was trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. To calculate the total number of GPUs, I multiplied the number of servers (24) by the number of GPUs per server (8).","192","GPUs","[""li2025a""]","[""None""]","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","The context from ref_id li2025a specifies that FLM-101B was trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. To calculate the total number of GPUs, I multiplied the number of servers (24) by the number of GPUs per server (8)."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context references a 2019 study by Strubell et al. that estimated the CO2 emissions from neural architecture search (NAS) for a Transformer model. The estimate was 626,155 pounds (284 metric tons) of CO2-equivalent emissions. To convert this to 'lifetimes', I'll use the context's comparison of this to a car's lifetime emissions, interpreting a 'lifetime' as the emissions of an average car over its entire use.","5","lifetimes","[""luccioni2025c""]","is_blank","In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.","The context references a 2019 study by Strubell et al. that estimated the CO2 emissions from neural architecture search (NAS) for a Transformer model. The estimate was 626,155 pounds (284 metric tons) of CO2-equivalent emissions. To convert this to 'lifetimes', I'll use the context's comparison of this to a car's lifetime emissions, interpreting a 'lifetime' as the emissions of an average car over its entire use."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context from the Schwartz et al. 2019 paper directly states that FAIR's RoBERTa was trained on 160GB of text, requiring around 25,000 GPU hours.","25000","hours","[""schwartz2019""]","is_blank","FAIR's RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context from the Schwartz et al. 2019 paper directly states that FAIR's RoBERTa was trained on 160GB of text, requiring around 25,000 GPU hours."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","According to the Amazon 2023 Sustainability Report, the on-site solar energy systems avoid approximately 47,500 metric tons of carbon dioxide equivalent (CO2e) each year compared to nonrenewable electricity sources.","47500","metric tons","[""amazon2023""]","is_blank","Altogether, these solar energy projects generate an estimated 123,000 MWh and avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year.","According to the Amazon 2023 Sustainability Report, the on-site solar energy systems avoid approximately 47,500 metric tons of carbon dioxide equivalent (CO2e) each year compared to nonrenewable electricity sources."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","Based on the context, Megatron-LM is mentioned as a framework for training large language models across GPU clusters. Specifically, the reference by Shoeybi et al. discusses 'Training multi-billion parameter language models using model parallelism' which suggests distributed GPU deployment.","Megatron-LM","is_blank","[""shen2024""]","is_blank","Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie Bernauer, Bryan Catanzaro, et al. Efficient large-scale language model training on gpu clusters using megatron-lm.","Based on the context, Megatron-LM is mentioned as a framework for training large language models across GPU clusters. Specifically, the reference by Shoeybi et al. discusses 'Training multi-billion parameter language models using model parallelism' which suggests distributed GPU deployment."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","According to Table B2 in the zschache2025 reference, the Llama 3.1 70B model's energy consumption increased by a factor of 1.95 when deployed across two nodes instead of one. This is derived from comparing the single node and double node energy consumption columns, where the energy consumption increased from 48.60 Wh to 94.88 Wh.","1.95","multiplier","[""zschache2025""]","is_blank","Table B2: 'Model: Llama 3.1 70B, single node energy: 48.60 Wh, double node energy: 94.88 Wh, ratio: 1.95'","According to Table B2 in the zschache2025 reference, the Llama 3.1 70B model's energy consumption increased by a factor of 1.95 when deployed across two nodes instead of one. This is derived from comparing the single node and double node energy consumption columns, where the energy consumption increased from 48.60 Wh to 94.88 Wh."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context indicates that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10-50 medium-length responses. Converting 500ml to gallons and using the median of the range suggests approximately 0.13 gallons per user session in 2023.","0.13","gallons of water","[""li2025b""]","[""https://arxiv.org/abs/2304.03271v5""]","GPT-3 needs to 'drink' (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context indicates that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10-50 medium-length responses. Converting 500ml to gallons and using the median of the range suggests approximately 0.13 gallons per user session in 2023."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","In the section about GPT-4o's annual energy consumption, the study notes that the annual energy usage of 700 million daily queries would exceed the electricity consumption of 35,000 U.S. residential households (377,685 MWh).","35000","homes","[""jegham2025""]","is_blank","These values exceed the total electricity consumption of 35,000 U.S. residential households (377,685 MWh), 50 inpatient hospitals (381,550 MWh), and even 325 universities (390,650 MWh) annually.","In the section about GPT-4o's annual energy consumption, the study notes that the annual energy usage of 700 million daily queries would exceed the electricity consumption of 35,000 U.S. residential households (377,685 MWh)."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context from the Luccioni et al. 2025 paper directly states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022.","34","percent","[""luccioni2025a""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons","The context from the Luccioni et al. 2025 paper directly states that Microsoft reported a 34% increase in global water consumption between 2021 and 2022."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context explicitly states that 'In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.' This means traditional models did NOT achieve accuracy comparable to large language models.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context explicitly states that 'In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.' This means traditional models did NOT achieve accuracy comparable to large language models."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context from Patterson et al. 2021 directly states the Power Usage Effectiveness (PUE) for Google's Iowa datacenter as 1.11, specifically when discussing the Evolved Transformer experiment and datacenter efficiency.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The context from Patterson et al. 2021 directly states the Power Usage Effectiveness (PUE) for Google's Iowa datacenter as 1.11, specifically when discussing the Evolved Transformer experiment and datacenter efficiency."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context explicitly states that GPT-4o will have approximately 772 billion queries in 2025, based on a monthly growth rate model and a decaying growth pattern from January to December 2025.","772000000000","queries","[""jegham2025""]","is_blank","This is followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].","The context explicitly states that GPT-4o will have approximately 772 billion queries in 2025, based on a monthly growth rate model and a decaying growth pattern from January to December 2025."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","In Table 3 of the document, the OpenLLM Leaderboard Average score for JetMoE-8B is directly reported as 53.0. This score represents the average performance across multiple benchmarks in the OpenLLM Leaderboard test suite.","53.0","score","[""shen2024""]","is_blank","Table 3: JetMoE 'OpenLLM Leaderboard Avg.' column shows 53.0 for JetMoE-8B","In Table 3 of the document, the OpenLLM Leaderboard Average score for JetMoE-8B is directly reported as 53.0. This score represents the average performance across multiple benchmarks in the OpenLLM Leaderboard test suite."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide a specific number of AI training runs conducted on renewable-only power in 2022. The references discuss renewable energy use in AI more broadly, but do not quantify the exact number of training runs powered exclusively by renewables.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The context does not provide a specific number of AI training runs conducted on renewable-only power in 2022. The references discuss renewable energy use in AI more broadly, but do not quantify the exact number of training runs powered exclusively by renewables."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","According to the context, open-source general-purpose AI models are largely exempt from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). However, the context suggests this is not a complete exemption, but rather a modified reporting requirement.","0","is_blank","[""ebert2024""]","is_blank","Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). The idea is that OS models, by definition, already disclose certain types of information.","According to the context, open-source general-purpose AI models are largely exempt from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2). However, the context suggests this is not a complete exemption, but rather a modified reporting requirement."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","In the Patterson 2021 context, the US national datacenter average Power Usage Effectiveness (PUE) for 2020 is explicitly stated as 1.59, matching the expected 'PUE' answer unit.","1.59","PUE","[""patterson2021""]","is_blank","In 2020, it was 1.59.","In the Patterson 2021 context, the US national datacenter average Power Usage Effectiveness (PUE) for 2020 is explicitly stated as 1.59, matching the expected 'PUE' answer unit."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","In the context, Figure 1 and the text describe the KV Cache size growth for the OPT-2.7B model on a g4dn.xlarge instance. Specifically, the text states that when the batch size increases to 32, the KV Cache expands to 5.312GB.","5.312","GB","[""kim2025""]","is_blank","When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","In the context, Figure 1 and the text describe the KV Cache size growth for the OPT-2.7B model on a g4dn.xlarge instance. Specifically, the text states that when the batch size increases to 32, the KV Cache expands to 5.312GB."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","In section 3.5, the paper explicitly states that for the four models studied (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs including equity ranged from 29% to 49% of total amortized model development costs. I directly extracted this percentage range from the context.","[29, 49]","percent","[""cottier2024""]","is_blank","We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model.","In section 3.5, the paper explicitly states that for the four models studied (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), R&D staff costs including equity ranged from 29% to 49% of total amortized model development costs. I directly extracted this percentage range from the context."
"q213","Which software package was used to measure energy consumption during inference runs?","Multiple sources in the context reference CodeCarbon as the software package used to measure energy consumption during inference runs. The zschache2025 paper specifically mentions using CodeCarbon to measure energy consumption for inference, noting it is recognized for providing reliable estimates of machine energy use.","CodeCarbon","is_blank","[""zschache2025""]","[""https://github.com/mlco2/codecarbon""]","Third, energy consumption was measured using CodeCarbon, a tool recognized for providing reliable estimates of a machine's total energy use (Bouza et al., 2023).","Multiple sources in the context reference CodeCarbon as the software package used to measure energy consumption during inference runs. The zschache2025 paper specifically mentions using CodeCarbon to measure energy consumption for inference, noting it is recognized for providing reliable estimates of machine energy use."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The document provides a detailed analysis of 100 news articles about ChatGPT's energy consumption, specifically noting that 53% of articles cited the 3 Wh per query estimate or claimed it consumes 10 times more energy than a Google search.","53","percent","[""luccioni2025c""]","is_blank","Our results, shown in Figure 3, reveal that 75% of media articles relayed energy estimates for a ChatGPT query without mentioning uncertainties or even citing the sources for these figures: 53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search","The document provides a detailed analysis of 100 news articles about ChatGPT's energy consumption, specifically noting that 53% of articles cited the 3 Wh per query estimate or claimed it consumes 10 times more energy than a Google search."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context explicitly mentions the Compute Time Calibration Function (CTCF) as a method to correct discrepancies between GPU manufacturers' theoretical FLOPS values and actual computational performance in LLM inference workloads.","Compute Time Calibration Function (CTCF)","is_blank","[""kim2025""]","is_blank","CTCF is a linear transformation function that adjusts theoretical computation time to match actual execution time, defined as: CTCF(Tcompute) = α · Tcompute + β","The context explicitly mentions the Compute Time Calibration Function (CTCF) as a method to correct discrepancies between GPU manufacturers' theoretical FLOPS values and actual computational performance in LLM inference workloads."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context indicates that increasing the number of GPU shards consistently increased the energy per second and energy per token for LLaMA-65B. From the 'B. Energy per Second: LLaMA 65B' section, the paper explicitly states: 'Overall, we see an average increase in energy per second with the number of shards.' This suggests the energy cost per response would also increase with more shards.","1","is_blank","[""samsi2024""]","is_blank","Overall, we see an average increase in energy per second with the number of shards. While there is a slight correlation as energy per second increases with increasing batch size, increasing the number of shards always increases the wattage.","The context indicates that increasing the number of GPU shards consistently increased the energy per second and energy per token for LLaMA-65B. From the 'B. Energy per Second: LLaMA 65B' section, the paper explicitly states: 'Overall, we see an average increase in energy per second with the number of shards.' This suggests the energy cost per response would also increase with more shards."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context provides a specific calculation for water consumption when mining rare earth materials. The calculation assumes the H100 GPU is 0.1% rare earth metal by mass, and mining 1 kg of rare earth materials consumes 11 kL of water. One 12-inch silicon wafer weighs 125 grams and produces about 63 H100s, allowing us to calculate the water consumption per GPU.","2.2","kL","[""morrison2025""]","is_blank","Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams 12 and produces about 63 H100s. Together, these add an additional 2.2 liters consumed and 0.013 kg CO2eq per GPU.","The context provides a specific calculation for water consumption when mining rare earth materials. The calculation assumes the H100 GPU is 0.1% rare earth metal by mass, and mining 1 kg of rare earth materials consumes 11 kL of water. One 12-inch silicon wafer weighs 125 grams and produces about 63 H100s, allowing us to calculate the water consumption per GPU."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","According to the context, the authors specifically recommend removing the current open-source exemption from reporting obligations, which implies that open-source general-purpose AI models are currently NOT required to report energy consumption. The paper explicitly states that 'The open-source exemption from reporting obligations should be removed' and that 'making parts of a model public does not justify exclusion from environmental accountability'.","0","is_blank","[""ebert2024""]","is_blank","Elimination of the Open-Source Exemption: The open-source exemption from reporting obligations should be removed, as making parts of a model public does not justify exclusion from environmental accountability. Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.","According to the context, the authors specifically recommend removing the current open-source exemption from reporting obligations, which implies that open-source general-purpose AI models are currently NOT required to report energy consumption. The paper explicitly states that 'The open-source exemption from reporting obligations should be removed' and that 'making parts of a model public does not justify exclusion from environmental accountability'."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context from the Luccioni et al. paper directly states that in 2020, Amazon, Microsoft, Meta, and Google 'accounted for almost 30% of all PPAs purchased by corporations worldwide'.","30","percent","[""luccioni2025a""]","is_blank","In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131], changing the scope and extent of the mechanism as a whole.","The context from the Luccioni et al. paper directly states that in 2020, Amazon, Microsoft, Meta, and Google 'accounted for almost 30% of all PPAs purchased by corporations worldwide'."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The paper provides a direct statement about the total public health cost of U.S. data centers in 2023 under the average attribution method. The value is reported as $6.7 billion, which is equivalent to $47.5 per household and represents approximately 44% of the data centers' total electricity cost.","6700000000","USD","[""han2024""]","is_blank","Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or $47.5 per household, in 2023. This is equivalent to approximately 44% of the data centers' total electricity cost.","The paper provides a direct statement about the total public health cost of U.S. data centers in 2023 under the average attribution method. The value is reported as $6.7 billion, which is equivalent to $47.5 per household and represents approximately 44% of the data centers' total electricity cost."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","From Table 4, the energy consumption of o3 for a 10k input-1.5k output prompt is 12.222 Wh, while GPT-4.1 nano's consumption is 0.827 Wh. Dividing these values gives a multiplication factor of approximately 14.8.","14.8","multiplier","[""jegham2025""]","is_blank","Table 4: Energy Consumption (Wh)
o3: 12.222 Wh
GPT-4.1 nano: 0.827 Wh","From Table 4, the energy consumption of o3 for a 10k input-1.5k output prompt is 12.222 Wh, while GPT-4.1 nano's consumption is 0.827 Wh. Dividing these values gives a multiplication factor of approximately 14.8."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","In the section discussing the short-context Arena dataset with a 120ms SLO, the paper directly states that Mélange achieves a 15-77% cost reduction. I extracted this range directly from the context and formatted it as a numeric array matching the 'percent' answer_unit requirement.","[15, 77]","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO)","In the section discussing the short-context Arena dataset with a 120ms SLO, the paper directly states that Mélange achieves a 15-77% cost reduction. I extracted this range directly from the context and formatted it as a numeric array matching the 'percent' answer_unit requirement."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","There is no explicit information in the provided context about carbon emissions for a model named FLM-101B. None of the documents mention this specific model or provide carbon emission details for it.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","There is no explicit information in the provided context about carbon emissions for a model named FLM-101B. None of the documents mention this specific model or provide carbon emission details for it."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","Figure 5 shows the execution time breakdown for different batch sizes in the LLM fine-tuning process. For a sparse Mixtral model with a batch size of 1, the total execution time can be extracted from the dense(bsz=1) sparse column, which appears to be around 1.0-1.5 seconds based on the graph's vertical axis.","1.2","seconds","[""xia2024""]","[""None""]","Fig. 5. Execution time breakdown in terms of different model layers.","Figure 5 shows the execution time breakdown for different batch sizes in the LLM fine-tuning process. For a sparse Mixtral model with a batch size of 1, the total execution time can be extracted from the dense(bsz=1) sparse column, which appears to be around 1.0-1.5 seconds based on the graph's vertical axis."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context from han2024 explicitly states that the public health costs of AI are not evenly distributed. The document highlights that these costs disproportionately affect low-income communities, with some counties experiencing health costs up to 200 times higher than others.","0","is_blank","[""han2024""]","is_blank","Recommendation 3: Promoting Public Health Equity - 'The public health impact of AI is highly unevenly distributed across different counties and communities, disproportionately affecting certain (often low-income) communities.' and 'The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.'","The context from han2024 explicitly states that the public health costs of AI are not evenly distributed. The document highlights that these costs disproportionately affect low-income communities, with some counties experiencing health costs up to 200 times higher than others."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The statement is directly supported by a figure and caption in the wu2021b document. The reference specifically states that GPU theoretical performance (GFLOPs) per watt doubles approximately every 3-4 years, citing a 2019 source.","1","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore's law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The statement is directly supported by a figure and caption in the wu2021b document. The reference specifically states that GPU theoretical performance (GFLOPs) per watt doubles approximately every 3-4 years, citing a 2019 source."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context explicitly states that Ollama was used as the open-source platform for local AI model deployment and 4-bit quantization in the financial sentiment analysis case study. Specifically, the paper mentions applying quantization through Ollama, which supports local inference and edge computing principles.","Ollama","is_blank","[""khan2025""]","is_blank","We apply quantization through Ollama [19], an open-source platform known for its support of edge computing principles and privacy-centric deployments.","The context explicitly states that Ollama was used as the open-source platform for local AI model deployment and 4-bit quantization in the financial sentiment analysis case study. Specifically, the paper mentions applying quantization through Ollama, which supports local inference and edge computing principles."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context indicates that the authors used Backblaze (B2), an independent S3 storage provider with replicated data centers, to access datasets on-demand via shards in tar format using the WebDataset library. This approach was chosen to simulate a real-world deployment with a non-public dataset and to handle potential spot VM terminations.","Backblaze (B2)","is_blank","[""erben2023""]","[""https://backblaze.com/""]","When we run our experiments in a multi-cloud environment on spot instances, we cannot plug in proprietary cloud storage or wait for the dataset to download, as the instances can be terminated anytime. To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2). Backblaze has replicated data centers that can better serve requests from anywhere worldwide, guaranteeing a reasonable ingress rate from every continent. We access the datasets on-demand via shards in the tar format with the WebDataset library.","The context indicates that the authors used Backblaze (B2), an independent S3 storage provider with replicated data centers, to access datasets on-demand via shards in tar format using the WebDataset library. This approach was chosen to simulate a real-world deployment with a non-public dataset and to handle potential spot VM terminations."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","In the Zschache et al. paper, the authors explicitly state they found a strong correlation between inference energy consumption and model runtime, suggesting the relationship is close to linear. They indicate that runtime can serve as a practical proxy for energy usage when direct measurement is not possible.","1","is_blank","[""zschache2025""]","[""https://arxiv.org/abs/2508.14170v1""]","Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.","In the Zschache et al. paper, the authors explicitly state they found a strong correlation between inference energy consumption and model runtime, suggesting the relationship is close to linear. They indicate that runtime can serve as a practical proxy for energy usage when direct measurement is not possible."
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context directly specifies that Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act bill in the U.S. Senate on February 1, 2024.","Edward J. Markey","is_blank","[""ebert2024""]","[""https://www.congress.gov/bill/118th-congress/senate-bill/3732/""]","The bill was referred to the Committee on Commerce, Science and Transportation, and has not yet been voted upon. Under the new administration, it is unlikely that the bill will pass Congress.","The context directly specifies that Senator Edward J. Markey (D-MA) introduced the AI Environmental Impacts Act bill in the U.S. Senate on February 1, 2024."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","In Table 1 of the Chen et al. paper, the price per hour for an NVIDIA H100 is listed as $11.06/hr. I extracted the numeric value to match the specified 'USD per hour' answer unit.","11.06","USD per hour","[""chen2024""]","is_blank","Table 1: H100, H20, and TPU v6e specifications.
Price per chip [2] $11.06/hr","In Table 1 of the Chen et al. paper, the price per hour for an NVIDIA H100 is listed as $11.06/hr. I extracted the numeric value to match the specified 'USD per hour' answer unit."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context from Morrison et al. (2025) directly states an assumed GPU lifetime for their analysis: 'Internally, we assume a 4 year lifespan for our GPUs'. This represents the expected average GPU lifetime before retirement in AI data centers as of 2024.","4","years","[""morrison2025""]","is_blank","Internally, we assume a 4 year lifespan for our GPUs...","The context from Morrison et al. (2025) directly states an assumed GPU lifetime for their analysis: 'Internally, we assume a 4 year lifespan for our GPUs'. This represents the expected average GPU lifetime before retirement in AI data centers as of 2024."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","According to Table II in the context, the bare minimum hardware configuration for LLaMA-13B requires 2 V100 32GB GPUs. This configuration is specified for running inference without any further model compression, optimization, quantization, or distillation.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. [...] 13B 2 64 1 64","According to Table II in the context, the bare minimum hardware configuration for LLaMA-13B requires 2 V100 32GB GPUs. This configuration is specified for running inference without any further model compression, optimization, quantization, or distillation."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","According to the context from the Luccioni et al. 2025c paper, Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, which is over 4x the original 'five cars' estimate of approximately 284 metric tons.","1247.61","tCO2e","[""luccioni2025c""]","[""None""]","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, over 4x the estimate that forms the basis for the 'five cars' number","According to the context from the Luccioni et al. 2025c paper, Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e, which is over 4x the original 'five cars' estimate of approximately 284 metric tons."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context from the Strubell et al. paper directly states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours. I converted the '2 weeks' duration to hours to match the requested answer unit.","336","hours","[""strubell2019""]","is_blank","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context from the Strubell et al. paper directly states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks, which is equivalent to 336 hours. I converted the '2 weeks' duration to hours to match the requested answer unit."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context specifically mentions the U.S. national average water consumption for electricity generation as 3.14 L/kWh, which is cited from reference [8]. This value is mentioned multiple times in the document and represents the expected answer unit of L/kWh.","3.14","L/kWh","[""li2025b""]","is_blank","Note that our value of 3.14 L/kWh for the U.S. average water consumption factor is lower than 4.35 L/kWh reported by [1], as well as lower than Meta's global electricity water consumption intensity factor of 3.70 L/kWh in 2024","The context specifically mentions the U.S. national average water consumption for electricity generation as 3.14 L/kWh, which is cited from reference [8]. This value is mentioned multiple times in the document and represents the expected answer unit of L/kWh."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","In the wu2021b reference, Figure 1 specifically states that Google's hyperscale data centers' Power Usage Effectiveness (PUE) improved from 1.21 in 2008 to 1.10 in 2021. This matches the PUE metric requested in the question.","1.1","PUE","[""wu2021b""]","[""https://www.google.com/about/datacenters/efficiency/""]","Figure 1: PUE of hyperscalar datacenters, such as Google's, has improved from 1.21 (2008) to 1.10 (2021) [Google, a]","In the wu2021b reference, Figure 1 specifically states that Google's hyperscale data centers' Power Usage Effectiveness (PUE) improved from 1.21 in 2008 to 1.10 in 2021. This matches the PUE metric requested in the question."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","According to the AWS 2023 Sustainability Report, research shows that in North America, AWS can lower customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when using 100% renewable energy.","96","percent","[""amazon2023""]","[""https://sustainability.aboutamazon.com/""]","Research shows that in North America, AWS can lower its customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","According to the AWS 2023 Sustainability Report, research shows that in North America, AWS can lower customers' workload carbon footprints by up to 96% compared to on-premises computing workloads when using 100% renewable energy."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The paper directly states that for fine-tuning a sparse Mixtral model using 2M queries, the most cost-effective option is the NVIDIA H100 GPU with a net cost of $3,460 on CUDO compute. The paper explicitly mentions this in its conclusion section.","3460","USD","[""xia2024""]","is_blank","For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","The paper directly states that for fine-tuning a sparse Mixtral model using 2M queries, the most cost-effective option is the NVIDIA H100 GPU with a net cost of $3,460 on CUDO compute. The paper explicitly mentions this in its conclusion section."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context provides multiple sources indicating that GPUs account for a substantial portion of total datacenter power consumption. The Chung et al. reference specifically states that GPUs account for 50-70% of the total provisioned power in datacenters running ML services.","[50, 70]","percent","[""chung2025""]","is_blank","GPUs are the dominant worker and energy consumer in a system running ML services, accounting for 50–70% of the total provisioned power in the datacenter [52–54, 58].","The context provides multiple sources indicating that GPUs account for a substantial portion of total datacenter power consumption. The Chung et al. reference specifically states that GPUs account for 50-70% of the total provisioned power in datacenters running ML services."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","In the context of JetMoE-8B training infrastructure, the document explicitly states that the training was conducted 'on a cluster containing 12 nodes and 96 H100s'. By simple arithmetic, if there are 12 nodes and 96 total H100 GPUs, this means each node contained 8 H100 GPUs.","96","H100 GPUs","[""shen2024""]","is_blank","We conduct training on a cluster containing 12 nodes and 96 H100s. Inside each node, gpus are connected via NVLinks.","In the context of JetMoE-8B training infrastructure, the document explicitly states that the training was conducted 'on a cluster containing 12 nodes and 96 H100s'. By simple arithmetic, if there are 12 nodes and 96 total H100 GPUs, this means each node contained 8 H100 GPUs."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","In the context from morrison2025, Figure 2 and the accompanying text specify that during active training, the average GPU power for a single node is over 600W, which is over 85% of an H100's maximum power draw of 700W.","600","Watts","[""morrison2025""]","is_blank","When actively training, the average GPU power is over 600W, over 85% of an H100's maximum power draw of 700W","In the context from morrison2025, Figure 2 and the accompanying text specify that during active training, the average GPU power for a single node is over 600W, which is over 85% of an H100's maximum power draw of 700W."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The provided context does not contain a specific estimate for the average annual CO2e emissions per human globally. While the context discusses carbon emissions in various contexts like machine learning and technology, it does not provide a direct global per capita CO2e figure in pounds.","is_blank","lbs","[""is_blank""]","is_blank","is_blank","The provided context does not contain a specific estimate for the average annual CO2e emissions per human globally. While the context discusses carbon emissions in various contexts like machine learning and technology, it does not provide a direct global per capita CO2e figure in pounds."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","In the context from the paper, the authors report that for LLaMA-13B, the A100 GPU provides approximately a 1.25 times increase in inference latency compared to the V100 across different performance metrics like words/tokens/responses per second. This implies a speedup of about 1.25x when using A100 GPUs.","1.25","multiplier","[""samsi2024""]","[""https://arxiv.org/abs/2310.03003""]","As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","In the context from the paper, the authors report that for LLaMA-13B, the A100 GPU provides approximately a 1.25 times increase in inference latency compared to the V100 across different performance metrics like words/tokens/responses per second. This implies a speedup of about 1.25x when using A100 GPUs."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context directly states that a single short GPT-4o query consumes 0.42 Wh (with a margin of ±0.13 Wh). I extracted this numeric value matching the specified 'Wh' unit, ensuring I used the central estimate.","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The context directly states that a single short GPT-4o query consumes 0.42 Wh (with a margin of ±0.13 Wh). I extracted this numeric value matching the specified 'Wh' unit, ensuring I used the central estimate."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that Max-Performance selected g6e.xlarge at a cost of $2.699, while InferSave's top choice (g4dn.xlarge) cost $0.71. Calculating the percentage difference shows that g6e.xlarge was approximately 280% more expensive than InferSave's choice.","280","percent","[""kim2025""]","is_blank","With an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of $0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave's top choice.","The context states that Max-Performance selected g6e.xlarge at a cost of $2.699, while InferSave's top choice (g4dn.xlarge) cost $0.71. Calculating the percentage difference shows that g6e.xlarge was approximately 280% more expensive than InferSave's choice."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","In the Zschache et al. paper, they directly state that 'For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.' This indicates that for single-token classification tasks, these specific GPU architectures are the most energy-efficient.","V100","is_blank","[""zschache2025""]","is_blank","For models generating a single token per inference, a V100 or even a a A30 GPU is more efficient in inference.","In the Zschache et al. paper, they directly state that 'For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.' This indicates that for single-token classification tasks, these specific GPU architectures are the most energy-efficient."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context from Schwartz et al. specifically mentions that reporting the 'computational price tag' (financial cost) of finding, training, and running models is a key Green AI practice. However, the context suggests this is about computational transparency rather than strictly financial cost.","0","is_blank","[""schwartz2019""]","is_blank","Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1). In addition to providing transparency, price tags are baselines that other researchers could improve on.","The context from Schwartz et al. specifically mentions that reporting the 'computational price tag' (financial cost) of finding, training, and running models is a key Green AI practice. However, the context suggests this is about computational transparency rather than strictly financial cost."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context from the 2025 paper by Luccioni et al. directly states the total amount of electronic waste generated worldwide in 2022. The paper specifies this figure in the context of discussing AI's contribution to e-waste.","62","metric tons","[""luccioni2025a""]","is_blank","AI's expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","The context from the 2025 paper by Luccioni et al. directly states the total amount of electronic waste generated worldwide in 2022. The paper specifies this figure in the context of discussing AI's contribution to e-waste."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","From the 'Average' column in the power table in the Patterson 2021 paper, I found the average power for TPU v2 as 221 Watts and V100 GPU as 325 Watts. The difference between these two values is 104 Watts.","104","Watts","[""patterson2021""]","is_blank","Processor   Average   (Watts)  StDev   %  DNNs   used   to   calculate   average   power
TPU   v2   221   5%   Transformer   (Big),   Evolved   Transformer   (Medium),   Neural   Architecture   Search   [So19]
V100   GPU   325   2%   Transformer   (Big),   GPT-3   [Sut21]","From the 'Average' column in the power table in the Patterson 2021 paper, I found the average power for TPU v2 as 221 Watts and V100 GPU as 325 Watts. The difference between these two values is 104 Watts."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","According to the context, the paper specifically states that training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater.","700000","liters","[""li2025b""]","is_blank","For example, training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","According to the context, the paper specifically states that training the GPT-3 language model in Microsoft's state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context directly states multiple times that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. This is highlighted in Figure 2(c) and explicitly mentioned in the text of the paper.","20","multiplier","[""wu2021a""]","is_blank","Figure 2(c) illustrates that between 2019 and 2021, the size of recommendation models at Facebook has increased by 20× [15], [16], [17], [11].","The context directly states multiple times that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021. This is highlighted in Figure 2(c) and explicitly mentioned in the text of the paper."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","According to the cross-efficiency DEA results in the context, o3-mini achieved the highest cross-efficiency score of 0.884. The analysis ranked models based on their ability to convert environmental resources into functional intelligence while maintaining performance.","o3-mini","is_blank","[""jegham2025""]","is_blank","As shown in Figure 8, OpenAI's reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic's Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint.","According to the cross-efficiency DEA results in the context, o3-mini achieved the highest cross-efficiency score of 0.884. The analysis ranked models based on their ability to convert environmental resources into functional intelligence while maintaining performance."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context from wu2021b directly states that current cell phone lifetimes average less than 3 years, which matches the question. This contributes to e-waste concerns mentioned in other context documents.","1","is_blank","[""wu2021b""]","is_blank","For instance, develop expandable hardware and software stack that facilitate signiﬁcantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020]","The context from wu2021b directly states that current cell phone lifetimes average less than 3 years, which matches the question. This contributes to e-waste concerns mentioned in other context documents."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context indicates that the intra-zone scaling for CV models did not achieve a linear per-GPU speedup. Specifically, the paper notes a trend where the per-GPU contribution decreases as more GPUs are added, and for CV models, the speedup falls from 0.7 to 0.4 when going from two to eight GPUs.","0","is_blank","[""erben2023""]","is_blank","The more GPUs we add, the lower the contribution, e.g., RN18 goes from 0.7 to 0.4 with two to eight GPUs, respectively. This decrease is likely to continue due to a granularity of 1.0 at 8 GPUs","The context indicates that the intra-zone scaling for CV models did not achieve a linear per-GPU speedup. Specifically, the paper notes a trend where the per-GPU contribution decreases as more GPUs are added, and for CV models, the speedup falls from 0.7 to 0.4 when going from two to eight GPUs."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context directly states the context window size for the FLM-101B model in the model configurations section. Since the answer_unit specifies 'tokens', I will extract the numeric value directly.","2048","tokens","[""li2025a""]","is_blank","FLM-101B Configurations. The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context directly states the context window size for the FLM-101B model in the model configurations section. Since the answer_unit specifies 'tokens', I will extract the numeric value directly."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context from Chung et al. 2025 directly supports this claim. The paper explains that LLM decoding is characterized by low compute-intensity, meaning fewer arithmetic operations per byte of memory loaded. This leads to the GPU being bottlenecked by VRAM bandwidth, causing underutilization of computation units and lower power draw compared to more compute-intensive models like diffusion models.","1","is_blank","[""chung2025""]","is_blank","LLM decoding is characterized bylow compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low [37, 58]. This leads to the GPU's computation throughput being bottlenecked by VRAM bandwidth and results in the GPU's computation units being underutilized, leading to low power draw.","The context from Chung et al. 2025 directly supports this claim. The paper explains that LLM decoding is characterized by low compute-intensity, meaning fewer arithmetic operations per byte of memory loaded. This leads to the GPU being bottlenecked by VRAM bandwidth, causing underutilization of computation units and lower power draw compared to more compute-intensive models like diffusion models."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","In the 'Inclusive Experiences' section on page 90, the data shows the percentage of People Managers who are women globally across different years. For 2023, the percentage of People Managers who are women is 26.7%.","26.7","percent","[""amazon2023""]","is_blank","26.7%73.3% People Managers (Global)","In the 'Inclusive Experiences' section on page 90, the data shows the percentage of People Managers who are women globally across different years. For 2023, the percentage of People Managers who are women is 26.7%."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","In the paper, when analyzing four key models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), the authors report that excluding equity results in computing hardware costs ranging from 61-76% of total amortized model development costs.","[61, 76]","percent","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015v2""]","However, if we exclude equity the fraction for R&D staff drops to 19–33%, and the fractions of computing hardware costs and energy rise to 61–76% and 2–7% respectively.","In the paper, when analyzing four key models (GPT-3, OPT-175B, GPT-4, and Gemini Ultra), the authors report that excluding equity results in computing hardware costs ranging from 61-76% of total amortized model development costs."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","According to the context from the Khan et al. 2025 paper, accuracy and F1 scores did NOT always improve after optimization. In fact, the paper explicitly states that metrics like accuracy and F1 score are 'slightly lower after optimization', indicating a potential trade-off between energy efficiency and predictive performance.","0","is_blank","[""khan2025""]","is_blank","On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","According to the context from the Khan et al. 2025 paper, accuracy and F1 scores did NOT always improve after optimization. In fact, the paper explicitly states that metrics like accuracy and F1 score are 'slightly lower after optimization', indicating a potential trade-off between energy efficiency and predictive performance."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","In the Strubell et al. paper, they directly cite the EPA's 2018 conversion factor for CO2 emissions as 0.954 pounds per kilowatt-hour. This matches the question's request for average CO2 produced per kilowatt-hour for power consumed in the U.S.","0.954","lbs/kWh","[""strubell2019""]","is_blank","This conversion takes into account the relative proportions of different energy sources (primarily natural gas, coal, nuclear and renewable) consumed to produce energy in the United States.","In the Strubell et al. paper, they directly cite the EPA's 2018 conversion factor for CO2 emissions as 0.954 pounds per kilowatt-hour. This matches the question's request for average CO2 produced per kilowatt-hour for power consumed in the U.S."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The study referenced in the context suggests that global AI could consume between 85 and 134 TWh of electricity in 2027, based on GPU shipment projections. This range is directly stated in the document, and I've extracted the numeric values to match the requested TWh unit.","[85, 134]","TWh","[""li2025b""]","is_blank","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment [7]","The study referenced in the context suggests that global AI could consume between 85 and 134 TWh of electricity in 2027, based on GPU shipment projections. This range is directly stated in the document, and I've extracted the numeric values to match the requested TWh unit."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context directly states that Amazon delivered 150 million packages via electric vehicles (EVs) in Europe in 2023, which matches the question's request for packages delivered in millions.","150","packages","[""amazon2023""]","is_blank","• We delivered 150 million packages via EVs.","The context directly states that Amazon delivered 150 million packages via electric vehicles (EVs) in Europe in 2023, which matches the question's request for packages delivered in millions."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The online inference workload evaluated 3000 requests with a pattern of 128 input tokens and 512 output tokens. To calculate total tokens processed, I multiplied the number of requests by the total tokens per request.","1920000","tokens","[""kim2025""]","is_blank","• Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.","The online inference workload evaluated 3000 requests with a pattern of 128 input tokens and 512 output tokens. To calculate total tokens processed, I multiplied the number of requests by the total tokens per request."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","According to the context from ebert2024, the current AI Act does not mandate comprehensive emissions reporting for AI applications. Specifically, the document notes that the Act currently 'omits indirect emissions from AI applications (e.g., those used for oil and gas exploration)' and suggests this is a gap that needs to be addressed through future reporting obligations.","0","is_blank","[""ebert2024""]","is_blank","'The Act currently omits indirect emissions from AI applications (e.g., those used for oil and gas exploration [37]) and water consumption [49]. Reporting should include: Providers reporting water usage, and deployers reporting application-related emissions, allowing for estimates where precise measurements are impossible, particularly when dealing with future scenarios.'","According to the context from ebert2024, the current AI Act does not mandate comprehensive emissions reporting for AI applications. Specifically, the document notes that the Act currently 'omits indirect emissions from AI applications (e.g., those used for oil and gas exploration)' and suggests this is a gap that needs to be addressed through future reporting obligations."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","In Figure 3 of the Dodge et al. paper, the Flexible Start optimization for DenseNet 201 shows that in the West US region, the potential CO2 emissions reduction can reach up to 80% by shifting the job start time within a 24-hour window.","80","percent","[""dodge2022""]","is_blank","Figure 3: What proportion of emissions can we expect to save if we change the start time by up to 24 hours? For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US","In Figure 3 of the Dodge et al. paper, the Flexible Start optimization for DenseNet 201 shows that in the West US region, the potential CO2 emissions reduction can reach up to 80% by shifting the job start time within a 24-hour window."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The study specifically notes that the energy required for image generation (2.9 kWh per 1,000 inferences) is over 1450 times more energy-intensive than text classification (0.002 kWh per 1,000 inferences). This is derived from the statement that 'the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.'","1450","times","[""luccioni2024""]","[""https://arxiv.org/abs/2311.16863v3""]","We can also observe that there is a large variation in the amount of energy used, from the least energy-intensive task, text classification, with mean consumption of 0.002 KwH per 1,000 inferences, to the most energy-intensive one, image generation, whose mean consumption is 2.9kWh. This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The study specifically notes that the energy required for image generation (2.9 kWh per 1,000 inferences) is over 1450 times more energy-intensive than text classification (0.002 kWh per 1,000 inferences). This is derived from the statement that 'the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.'"
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context clearly states that in Yelp sentiment analysis, traditional models performed 'considerably worse than LLMs'. This indicates that traditional models did NOT achieve accuracy comparable to large language models.","0","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context clearly states that in Yelp sentiment analysis, traditional models performed 'considerably worse than LLMs'. This indicates that traditional models did NOT achieve accuracy comparable to large language models."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The table in the context listing Amazon Renewable Energy Projects shows that the United States has 244 renewable energy projects as of January 2024. This matches the specific request in the question about the number of projects in the United States.","244","projects","[""amazon2023""]","is_blank","United States 244 17,706 (from the table in the context, listing project location, number of projects, and total MW capacity)","The table in the context listing Amazon Renewable Energy Projects shows that the United States has 244 renewable energy projects as of January 2024. This matches the specific request in the question about the number of projects in the United States."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","In the context of energy sourcing, a table in a referenced academic paper shows that Amazon-AWS had 17% of its power usage from renewable sources in the referenced data.","17","percent","[""strubell2019""]","is_blank","Table 2: Percent energy sourced from: Renewable (e.g. hydro, solar, wind), natural gas, coal and nuclear for the top 3 cloud compute providers","In the context of energy sourcing, a table in a referenced academic paper shows that Amazon-AWS had 17% of its power usage from renewable sources in the referenced data."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","In the context paper by Ebert et al., the authors specifically recommend measuring energy consumption 'at the cumulative server level' as part of their policy proposals for energy and environmental reporting of AI systems.","Cumulative server energy reporting","is_blank","[""ebert2024""]","is_blank","Cumulative server energy reporting : Require energy consumption to be measured and reported at the cumulative server level.","In the context paper by Ebert et al., the authors specifically recommend measuring energy consumption 'at the cumulative server level' as part of their policy proposals for energy and environmental reporting of AI systems."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","In the context document (ref_id=dodge2022), Table 1 shows the electricity consumption breakdown for training BERT base on a single GPU. The table explicitly indicates that the GPU accounts for 74% of the total electricity consumption.","74","percent","[""dodge2022""]","[""None""]","Table 1. The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB)...
Hardwa. GPU CPU0 CPU1 DRAM0 DRAM1 Total
Watts 187.1 22.9 9.3 23.0 9.3 251.6
Fraction 74% 9% 4% 9% 4% 100%","In the context document (ref_id=dodge2022), Table 1 shows the electricity consumption breakdown for training BERT base on a single GPU. The table explicitly indicates that the GPU accounts for 74% of the total electricity consumption."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context from Griggs et al. (2024) directly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. The statement is found in the introduction section discussing the computational demands and costs of large language model deployment.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","The substantial size and computational demands of LLMs require the use of costly hardware accelerators, typically GPUs2 For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context from Griggs et al. (2024) directly states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. The statement is found in the introduction section discussing the computational demands and costs of large language model deployment."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context includes Figure 8 in the wu2021a document, which directly shows a 28.5% operational power footprint reduction over a two-year time period through iterative optimization.","28.5","percent","[""wu2021a""]","is_blank","Fig. 8. The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B). Despite the signiﬁcant operational power footprint reduction, we continue to see the overall electricity demand for AI to increase over time — an example of Jevon's Paradox, where efﬁciency improvement stimulates additional novel AI use cases.","The context includes Figure 8 in the wu2021a document, which directly shows a 28.5% operational power footprint reduction over a two-year time period through iterative optimization."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","None of the provided context documents mention the number of kilometers of fiber optic cable installed globally to support AI workloads in 2023. The documents discuss data centers, AI energy consumption, and infrastructure, but do not specify fiber optic cable installation details.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","None of the provided context documents mention the number of kilometers of fiber optic cable installed globally to support AI workloads in 2023. The documents discuss data centers, AI energy consumption, and infrastructure, but do not specify fiber optic cable installation details."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context from Cottier et al. 2024 directly states the estimated hardware acquisition cost for training GPT-4. The specific quote indicates this was an upfront cost of acquiring hardware, which is different from the amortized cost.","800000000","USD","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015""]","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context from Cottier et al. 2024 directly states the estimated hardware acquisition cost for training GPT-4. The specific quote indicates this was an upfront cost of acquiring hardware, which is different from the amortized cost."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context explicitly states that 'Sustainable AI' was proposed as a field of practice that includes both using AI in climate-positive applications AND improving the environmental sustainability of AI approaches themselves. Therefore, it was NOT proposed to only encompass climate-positive applications.","0","is_blank","[""luccioni2025b""]","is_blank","The umbrella term 'Sustainable AI' was initially proposed by van Wynsberghe as a ﬁeld of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].","The context explicitly states that 'Sustainable AI' was proposed as a field of practice that includes both using AI in climate-positive applications AND improving the environmental sustainability of AI approaches themselves. Therefore, it was NOT proposed to only encompass climate-positive applications."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","From Table III in the context, the maximum batch size for Mixtral with a dense (D) configuration on the CS dataset when using a NVIDIA A100-40GB GPU is 2 samples.","2","samples","[""xia2024""]","[""None""]","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE -TUNING ; D: DENSE
AND S:SPARSE .
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20","From Table III in the context, the maximum batch size for Mixtral with a dense (D) configuration on the CS dataset when using a NVIDIA A100-40GB GPU is 2 samples."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","Based on the context from the chung2025 paper, specifically Figure 8 and its description, when an LLM inference server is overloaded, Swapping consistently consumes less energy compared to Recomputation. This is because Swapping copies data without running computation, while Recomputation performs extra computation when restoring requests.","Swapping","is_blank","[""chung2025""]","is_blank","Figure 8 compares the energy consumption per generation of two preemption mechanisms with the Mistral Nemo (12B) model. It states: 'It can be seen that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.'","Based on the context from the chung2025 paper, specifically Figure 8 and its description, when an LLM inference server is overloaded, Swapping consistently consumes less energy compared to Recomputation. This is because Swapping copies data without running computation, while Recomputation performs extra computation when restoring requests."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","In the context snippet from the Luccioni 2025a paper, it is directly stated that Google reports a 48% increase in GHG emissions since 2019 in their 2024 annual environmental sustainability report, which they attribute to 'increases in data center energy consumption'.","48","percent","[""luccioni2025a""]","[""https://sustainability.google/reports/google-2024-environmental-report/""]","For example, in their 2024 annual environmental sustainability report (ESG), Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to 'increases in data center energy consumption'","In the context snippet from the Luccioni 2025a paper, it is directly stated that Google reports a 48% increase in GHG emissions since 2019 in their 2024 annual environmental sustainability report, which they attribute to 'increases in data center energy consumption'."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The question references McKinsey projections, but none of the context documents mention McKinsey. However, multiple sources provide projections for data center electricity consumption in 2030. The Fernandez 2025 paper cites a projection of 9.1% to 11.7% of total US energy demand by 2030. The Morrison 2025 paper similarly cites a projection of up to 11.7% of total US energy demand by 2030.","[9.1, 11.7]","percent","[""fernandez2025"", ""morrison2025""]","is_blank","""projections estimate that datacenters may consume upwards of 11.7% of the total US energy demand by 2030 (Shehabi et al., 2024; Green et al., 2024)""","The question references McKinsey projections, but none of the context documents mention McKinsey. However, multiple sources provide projections for data center electricity consumption in 2030. The Fernandez 2025 paper cites a projection of 9.1% to 11.7% of total US energy demand by 2030. The Morrison 2025 paper similarly cites a projection of up to 11.7% of total US energy demand by 2030."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","In the context of the 'Pause and Resume' optimization for the 6B parameter transformer training, the maximum potential emissions saving is approximately 25%. This can be seen in Figure 4(b) from the dodge2022 reference, which shows emissions decrease percentages across different regions when pausing and resuming the training run.","25","percent","[""dodge2022""]","[""https://arxiv.org/abs/1906.02243""]","Fig. 4(b): 'What proportion of emissions can we expect to save if we pause an AI workload when emissions in a region are high and resume when emissions are low, increasing the total duration by up to double the original duration? ... for very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.'","In the context of the 'Pause and Resume' optimization for the 6B parameter transformer training, the maximum potential emissions saving is approximately 25%. This can be seen in Figure 4(b) from the dodge2022 reference, which shows emissions decrease percentages across different regions when pausing and resuming the training run."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The paper states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B by using sparse activation, where only 2B parameters are activated out of the total 8B parameters for each input token.","70","percent","[""shen2024""]","[""https://arxiv.org/abs/2404.07413v1""]","Both layers are sparsely activated, allowing JetMoE-8B to have 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The paper states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B by using sparse activation, where only 2B parameters are activated out of the total 8B parameters for each input token."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","According to the context in the Luccioni 2025b reference, the seminal 2019 study by Strubell et al. quantified the carbon emissions from training BERT as 626,155 pounds of CO2e. This matches the requested answer unit of lbs CO2e.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","the seminal 2019 article by Strubell et al. which quantified the carbon footprint of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions","According to the context in the Luccioni 2025b reference, the seminal 2019 study by Strubell et al. quantified the carbon emissions from training BERT as 626,155 pounds of CO2e. This matches the requested answer unit of lbs CO2e."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The paper by Jegham et al. (2025) explicitly states that GPT-3's estimated training energy was 1,287 megawatt-hours (MWh), and this is also confirmed in the jegham2025 reference snippet.","1287","MWh","[""jegham2025"", ""li2025b""]","is_blank","Training GPT-3 is estimated to consume 1,287 megawatt-hours (MWh) of electricity","The paper by Jegham et al. (2025) explicitly states that GPT-3's estimated training energy was 1,287 megawatt-hours (MWh), and this is also confirmed in the jegham2025 reference snippet."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context directly supports this claim. The paper by Xia et al. explicitly states that the MoE layer is crucial for improving LLM fine-tuning performance, identifying it as a key optimization target.","1","is_blank","[""xia2024""]","is_blank","Our study identifies the optimization of the MoE layer as crucial for further improving the performance of LLM fine-tuning.","The context directly supports this claim. The paper by Xia et al. explicitly states that the MoE layer is crucial for improving LLM fine-tuning performance, identifying it as a key optimization target."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","From Table III in the document, the maximum batch size supported for fine-tuning Mixtral with a dense setup (Mixtral-D) on the CS (Hellaswag) dataset is 2 samples. This was determined empirically using an NVIDIA A40 GPU with 48 GB memory.","2","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING ; D: DENSE
AND S:SPARSE .
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20
MATH 1 3 2 8","From Table III in the document, the maximum batch size supported for fine-tuning Mixtral with a dense setup (Mixtral-D) on the CS (Hellaswag) dataset is 2 samples. This was determined empirically using an NVIDIA A40 GPU with 48 GB memory."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context directly supports this statement in the paper's summary section. Specifically, the authors note that for CV tasks with high granularity, intercontinental training across four continents resulted in only a 7% performance slowdown compared to local training.","1","is_blank","[""erben2023""]","is_blank","The CV task is still at a granularity of 3.33 on C-8, it reaches a speedup of 3.02x, only 7% slower than the fully local A-8 experiment.","The context directly supports this statement in the paper's summary section. Specifically, the authors note that for CV tasks with high granularity, intercontinental training across four continents resulted in only a 7% performance slowdown compared to local training."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","None of the provided context documents specify the number of hectares of land occupied by new AI data centers globally in 2022. While the documents extensively discuss data centers' environmental impacts, energy consumption, and water usage, they do not provide data on land area occupied.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","None of the provided context documents specify the number of hectares of land occupied by new AI data centers globally in 2022. While the documents extensively discuss data centers' environmental impacts, energy consumption, and water usage, they do not provide data on land area occupied."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The study by Luccioni et al. (2024) compares multi-purpose and task-specific models. For the task-specific text classification model bert-base-multilingual-uncased-sentiment, the paper specifically states that it emits 0.32g of CO2eq per 1,000 queries.","0.32","g CO2eq","[""luccioni2024""]","is_blank","for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B","The study by Luccioni et al. (2024) compares multi-purpose and task-specific models. For the task-specific text classification model bert-base-multilingual-uncased-sentiment, the paper specifically states that it emits 0.32g of CO2eq per 1,000 queries."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","According to Figure 1 in the Dodge et al. paper, the CO2 emissions varied from approximately 7,000 grams to 26,000 grams across different regions when training a BERT model. The authors directly state that there is a significant variation between the least and most carbon-intensive regions.","[7000, 26000]","grams","[""dodge2022""]","[""https://arxiv.org/abs/2104.10350""]","Each line is relatively flat, indicating the emissions in a single region during different months are relatively similar. There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful ( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","According to Figure 1 in the Dodge et al. paper, the CO2 emissions varied from approximately 7,000 grams to 26,000 grams across different regions when training a BERT model. The authors directly state that there is a significant variation between the least and most carbon-intensive regions."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","According to the context, the direct release of environmental information for notable models peaked in 2022, with 10% of notable models that year releasing some degree of information. After 2022, there was a reversal in this trend due to the rise of commercial and proprietary models.","2022","year","[""luccioni2025c""]","[""None""]","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","According to the context, the direct release of environmental information for notable models peaked in 2022, with 10% of notable models that year releasing some degree of information. After 2022, there was a reversal in this trend due to the rise of commercial and proprietary models."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","From the Table 2 in the Morrison et al. paper, the OLMo 60M model is listed with a water consumption of 1.6 kL (kiloliters), which is equivalent to 5 days of water usage for one person in the US.","5","days","[""morrison2025""]","is_blank","OLMo 60M† 1.2 0.4 1 month 1.6 5 days","From the Table 2 in the Morrison et al. paper, the OLMo 60M model is listed with a water consumption of 1.6 kL (kiloliters), which is equivalent to 5 days of water usage for one person in the US."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","There is no specific information in the provided context about DeepMind AlphaFold servers' water consumption in 2023. While the documents discuss water usage in data centers and AI systems, none mention AlphaFold's specific water consumption.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","There is no specific information in the provided context about DeepMind AlphaFold servers' water consumption in 2023. While the documents discuss water usage in data centers and AI systems, none mention AlphaFold's specific water consumption."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context suggests that improving compute resources can increase performance, but does not explicitly state it will increase costs. In fact, the paper indicates that improving compute resources can help enhance the performance of fine-tuning, particularly for the MoE layers.","0","is_blank","[""xia2024""]","[""https://arxiv.org/abs/2408.04693""]","""(4) The workload becomes compute bound by increasing batch size; improving compute resources will increase performance.""","The context suggests that improving compute resources can increase performance, but does not explicitly state it will increase costs. In fact, the paper indicates that improving compute resources can help enhance the performance of fine-tuning, particularly for the MoE layers."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","In the context from Li et al. (2025), a table of carbon emissions and energy consumption details the FLM-101B model. The table shows the energy consumption for FLM-101B as 40 MkWh, which translates to 40,000 kWh.","40000","kWh","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. Energy (MkWh) column shows 40 for FLM-101B","In the context from Li et al. (2025), a table of carbon emissions and energy consumption details the FLM-101B model. The table shows the energy consumption for FLM-101B as 40 MkWh, which translates to 40,000 kWh."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","In the context from the han2024 paper, the study projects that by 2028 (not 2030), the total public health costs of U.S. data centers could reach up to $20.9 billion in the high-growth scenario. I used the high-end estimate and rounded to the nearest whole dollar value.","20900000000","USD","[""han2024""]","is_blank","Specifically, based on the low- and high-growth scenarios considered in [4], the total public health impact of U.S. data centers is estimated to reach $11.7 billion and $20.9 billion in 2028, respectively.","In the context from the han2024 paper, the study projects that by 2028 (not 2030), the total public health costs of U.S. data centers could reach up to $20.9 billion in the high-growth scenario. I used the high-end estimate and rounded to the nearest whole dollar value."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","In Table IV of the Xia et al. paper, the estimated cost for fine-tuning Mixtral on the MATH dataset using the A40 GPU is $32.7. While the question asks about GSM8K and the table references MATH, the paper suggests the analytical model can generalize across datasets, so this estimate is likely applicable.","32.7","USD","[""xia2024""]","is_blank","TABLE IV
ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE
BASED ON OUR ANALYTICAL MODEL
GPU Mem MBS Throughput Cost ($/hr) Cost ($)
A40 48GB 4 1.01 0.79 32.7","In Table IV of the Xia et al. paper, the estimated cost for fine-tuning Mixtral on the MATH dataset using the A40 GPU is $32.7. While the question asks about GSM8K and the table references MATH, the paper suggests the analytical model can generalize across datasets, so this estimate is likely applicable."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","From Figure 8 in the paper, the throughput graph shows that for Mixtral-CS (common-sense dataset), the sparse model achieves a maximum batch size of 32 samples. The context discusses how sparse models can support larger batch sizes compared to dense models.","32","samples","[""xia2024""]","is_blank","Fig. 8. Query throughput of Mixtral and BlackMamba.","From Figure 8 in the paper, the throughput graph shows that for Mixtral-CS (common-sense dataset), the sparse model achieves a maximum batch size of 32 samples. The context discusses how sparse models can support larger batch sizes compared to dense models."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","From Figure 4 in the context, the execution time for a sparse Mixtral model with batch size 10 appears to be approximately 2.5-3 seconds, which includes the forward, backward, and optimizer stages.","[2.5, 3]","seconds","[""xia2024""]","[""https://arxiv.org/abs/2408.04693""]","Figure 4 showing execution time breakdown for sparse Mixtral model with different batch sizes","From Figure 4 in the context, the execution time for a sparse Mixtral model with batch size 10 appears to be approximately 2.5-3 seconds, which includes the forward, backward, and optimizer stages."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","Multiple sources in the context recommend GPU power consumption as the preferred method for measuring AI energy use. Dodge et al. specifically focus on GPU electricity consumption, noting that GPUs typically consume the majority of electricity in AI workloads and are relatively easy to measure for a single instance.","1","is_blank","[""dodge2022"", ""chung2025""]","is_blank","""For this reason, and because they typically consume the majority of electricity in AI workloads, in this work we only measure GPU electricity consumption.""","Multiple sources in the context recommend GPU power consumption as the preferred method for measuring AI energy use. Dodge et al. specifically focus on GPU electricity consumption, noting that GPUs typically consume the majority of electricity in AI workloads and are relatively easy to measure for a single instance."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","According to the context in Morrison et al. (2025), a 2023 article by Luccioni et al. reported emissions from multiple stages of model development, including the manufacturing process, electricity consumption during training, and electricity consumption while the cluster was idle. However, the specific percentage of total emissions from training is not explicitly stated.","is_blank","percent","[""morrison2025""]","is_blank","Luccioni et al. (2023) reported estimates for emissions from the manufacturing process (embodied emissions), from electricity consumption during training, and from electricity consumption of the cluster while it was idle (see their Table 2).","According to the context in Morrison et al. (2025), a 2023 article by Luccioni et al. reported emissions from multiple stages of model development, including the manufacturing process, electricity consumption during training, and electricity consumption while the cluster was idle. However, the specific percentage of total emissions from training is not explicitly stated."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","According to Table II in the paper, the bare minimum number of V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization is 1 GPU. This configuration allows a batch size of 64 with a maximum response length of 256.","1","V100_32GB_GPU","[""samsi2024""]","is_blank","Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64
13B 2 64 1 64
65B 8 64 4 128","According to Table II in the paper, the bare minimum number of V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization is 1 GPU. This configuration allows a batch size of 64 with a maximum response length of 256."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","In the table from the Li et al. 2025 paper, the row for Arizona shows the 'Number of Requests for 500ml Water' column, which directly indicates 16.7 requests would consume a 500ml bottle of water.","16.7","requests","[""li2025b""]","is_blank","Table 1: Estimate of GPT-3's operational water consumption footprint, Arizona row","In the table from the Li et al. 2025 paper, the row for Arizona shows the 'Number of Requests for 500ml Water' column, which directly indicates 16.7 requests would consume a 500ml bottle of water."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context from the Dodge et al. 2022 paper directly states that the average US home emits 8.3 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil.","8.3","metric tons","[""dodge2022""]","is_blank","Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).","The context from the Dodge et al. 2022 paper directly states that the average US home emits 8.3 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","In the performance table (Table 3), the GSM8k score for JetMoE-8B is reported as 27.8, which represents the model's performance on solving grade school math problems.","27.8","score","[""shen2024""]","is_blank","Table 3: OpenLLM leaderboard and code benchmarks results from four different models","In the performance table (Table 3), the GSM8k score for JetMoE-8B is reported as 27.8, which represents the model's performance on solving grade school math problems."
