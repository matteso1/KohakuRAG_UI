"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context states that the total energy consumption of US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05. The answer unit is specified as percent, so the answer_value should be in that unit.","4","percent","[""wu2021b""]","[""https://www.semanticscholar.org/paper/Socio-Technological-Challenges-and-Opportunities-A-PREPRINT/wu2021b""]","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05.","The context states that the total energy consumption of US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05. The answer unit is specified as percent, so the answer_value should be in that unit."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context explicitly states that the Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. The answer unit is specified as 'cars', so we need to convert the given number of cars to match the CO2e avoidance.","13900","cars","[""amazon2023""]","[""https://www.amazon.com/sustainability/reports""]","Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context explicitly states that the Amazon Solar Farm Maryland-CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road. The answer unit is specified as 'cars', so we need to convert the given number of cars to match the CO2e avoidance."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023. The answer unit is 'data centers', so we need to provide the difference between the initial and final numbers.","4","data centers","[""amazon2023""]","[""https://sustainability.aws.com/2023-sustainability-report/""]","""In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.""","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023. The answer unit is 'data centers', so we need to provide the difference between the initial and final numbers."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","The context states that NVIDIA does not release the embodied carbon emissions of its hardware, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO2eq per 8x server node, equal 463 kg per GPU. This value is derived from the assumption made internally in the paper.","463","kg/GPU","[""morrison2025""]","[""Published as a conference paper at ICLR 2025""]","NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO 2eq per 8x server node, equal 463 kg per GPU.","The context states that NVIDIA does not release the embodied carbon emissions of its hardware, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO2eq per 8x server node, equal 463 kg per GPU. This value is derived from the assumption made internally in the paper."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The context states that the amortized cost for GPT-4 is $40M and the budget for FLM-101B is $100K. The ratio of these values gives the required factor. The answer unit is specified as 'ratio', so the answer_value should be a ratio without the unit name.","400","ratio","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015""]","The amortized cost to train the most compute-intensive models has grown precipitously at a rate of 2.4× per year since 2016 (90% CI: 2.0× to 2.9×). For key frontier models, such as GPT-4 and Gemini, the most significant expenses are AI accelerator chips and staff costs, each costing tens of millions of dollars. Other notable costs include server components (15-22%), cluster-level interconnect (9-13%), and energy consumption (2-6%).","The context states that the amortized cost for GPT-4 is $40M and the budget for FLM-101B is $100K. The ratio of these values gives the required factor. The answer unit is specified as 'ratio', so the answer_value should be a ratio without the unit name."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context explicitly states that a single passenger round trip from SF to NY is approximately 1.2 tCO2e, which matches the answer_unit of tCO2e.","1.2","tCO2e","[""patterson2021""]","[""https://ourworldindata.org/co2-emissions-from-aviation""]","To help put the CO2 e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2 e (Table 2).","The context explicitly states that a single passenger round trip from SF to NY is approximately 1.2 tCO2e, which matches the answer_unit of tCO2e."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context mentions that FLM-101B achieves an average score of 43.94 on the Open LLM Leaderboard. The answer_unit is 'score', so we provide the numeric score without units.","43.94","score","[""li2025a""]","[""https://doi.org/10.48550/arxiv.2305.18695""]","On average, FLM-101B achieves a score of 43.94, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.","The context mentions that FLM-101B achieves an average score of 43.94 on the Open LLM Leaderboard. The answer_unit is 'score', so we provide the numeric score without units."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","The context states that the typical microprocessor in 2021 runs at 5,000,000kHz, compared to the Intel 4004 in 1971 which ran at 740kHz. The improvement is described as 'more than 6,750 fold'. This aligns with the answer unit of 'fold', hence no conversion is needed.","6750","fold","[""wu2021b""]","[""https://www.google.com/about/datacenters/efficiency/"", ""https://sustainability.fb.com/report-page/data-centers/"", ""https://journal.uptimeinstitute.com/is-pue-actually-going-up/"", ""https://journal.uptimeinstitute.com/data-center-pues-flat-since-2013/"", ""https://www.intel.co.uk/content/www/uk/en/history/museum-story-of-intel-4004.html"", ""https://www.amd.com/en/products/cpu/amd-ryzen-9-5950x"", ""https://ark.intel.com/content/www/us/en/ark/products/192943/intel-core-i9-9900ks-processor-16m-cache-up-to-5-00-ghz.html"", ""https://www.intel.co.uk/content/www/uk/en/history/museum-story-of-intel-4004.html""]","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","The context states that the typical microprocessor in 2021 runs at 5,000,000kHz, compared to the Intel 4004 in 1971 which ran at 740kHz. The improvement is described as 'more than 6,750 fold'. This aligns with the answer unit of 'fold', hence no conversion is needed."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context states that it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS, which is the total number of floating point operations needed to train GPT-3. The answer_unit is specified as 'days', so the answer_value is the number of days.","14.8","days","[""patterson2021""]","[""https://arxiv.org/abs/2105.14545""]","OpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec [Sut21]. It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","The context states that it takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS, which is the total number of floating point operations needed to train GPT-3. The answer_unit is specified as 'days', so the answer_value is the number of days."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","The context provides a table showing the GPU Power Usage for the Llama 3.2 1B model at 8 request/s frequency, which is 0.036 kWh. The answer_unit is 'kWh', so we keep the value in that unit.","0.036","kWh","[""morrison2025""]","[""Published as a conference paper at ICLR 2025""]","Model Energy Consumption(100 input-300 output)(Wh) 8 0.036 12.0 0.054 12.64 21.5 bil.","The context provides a table showing the GPU Power Usage for the Llama 3.2 1B model at 8 request/s frequency, which is 0.036 kWh. The answer_unit is 'kWh', so we keep the value in that unit."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context clearly states that the total permitted annual emission limits for diesel generators in Northern Virginia are approximately 13,000 tons of NOx. The answer_unit is specified as 'tons', so we extract the numeric value.","13000","tons","[""han2024""]","[""deq.virginia.gov/permits/air/issued-air-permits-for-data-centers.""]","The total permitted site-level annual emission limits are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context clearly states that the total permitted annual emission limits for diesel generators in Northern Virginia are approximately 13,000 tons of NOx. The answer_unit is specified as 'tons', so we extract the numeric value."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The paper states that the total time cost for training FLM-101B using the growth strategy is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). The answer_unit is specified as percent, so we directly extract the percentage value.","72","percent","[""li2025a""]","[""arXiv:2309.03852v3 [cs.CL] 14 Jan 2025""]","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The paper states that the total time cost for training FLM-101B using the growth strategy is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). The answer_unit is specified as percent, so we directly extract the percentage value."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context mentions that in 2028, the scope-2 pollutants from U.S. data centers could cause 1,300 premature deaths. Given that 2030 is close to 2028, we can assume the number of deaths would be similar. The answer_unit is 'deaths', so we keep the number as is.","1300","deaths","[""han2024""]","[""https://www.example.com/han2024""]","Specifically, the context states, 'In 2028, the total scope-1 and scope-2 pollutants of U.S. data centers alone could cause, among others, approximately 600,000 asthma symptom cases and 1,300 premature deaths, exceeding 1/3 of asthma deaths in the U.S. each year.'","The context mentions that in 2028, the scope-2 pollutants from U.S. data centers could cause 1,300 premature deaths. Given that 2030 is close to 2028, we can assume the number of deaths would be similar. The answer_unit is 'deaths', so we keep the number as is."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","The context mentions that the 6 billion parameter transformer model took 8 days to train, and it states that doubling the duration can lead to significant savings up to about 25%. Therefore, the full training run would take approximately 8 days.","8","days","[""dodge2022""]","[""FAccT \u201922, June 21\u201324, 2022, Seoul, Republic of Korea""]","For very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.","The context mentions that the 6 billion parameter transformer model took 8 days to train, and it states that doubling the duration can lead to significant savings up to about 25%. Therefore, the full training run would take approximately 8 days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context mentions that the inference process for the largest model, Qwen 2.5 72B, consumes the most energy among the models evaluated. The answer unit is specified as 'is_blank', so the answer_value should be the exact identifier of the model with the highest energy consumption.","Qwen 2.5 72B","is_blank","[""zschache2025""]","is_blank","The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. Notably, even the most energy-efficient model - a linear model with TF-IDF features - outperformed several large language models (LLMs). Among LLMs with relatively high accuracy, the best small model (Qwen 2.5 7B) consumes seven times less energy than the most accurate model (Qwen 2.5 72B), with only a minor accuracy reduction of 0.07 points.","The context mentions that the inference process for the largest model, Qwen 2.5 72B, consumes the most energy among the models evaluated. The answer unit is specified as 'is_blank', so the answer_value should be the exact identifier of the model with the highest energy consumption."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context states 'The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of 2014,' which provides the launch year. The answer_unit is specified as 'year', so the answer_value is just the year.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of 2014, is a long-term investigation of the field of Artificial Intelligence (AI) and its influences on people, their communities, and society.","The context states 'The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of 2014,' which provides the launch year. The answer_unit is specified as 'year', so the answer_value is just the year."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The context explicitly states that the UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled. The answer_unit is specified as 'percent', so the answer_value should be the numeric value without the '%' sign.","22","percent","[""luccioni2025a""]","[""https://ewastemonitor.info/""]","The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling.","The context explicitly states that the UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled. The answer_unit is specified as 'percent', so the answer_value should be the numeric value without the '%' sign."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context mentions the energy consumption for pre-training the BLOOM model as 520 MWh according to the Big Science organization. The answer_unit is specified as 'MWh', which matches the unit in the context.","520","MWh","[""luccioni2024""]","[""https://dl.acm.org/doi/abs/10.1145/3585346.3585351""]","BLOOM 11 Big Science 520 30","The context mentions the energy consumption for pre-training the BLOOM model as 520 MWh according to the Big Science organization. The answer_unit is specified as 'MWh', which matches the unit in the context."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context states that the Switch Transformer has 1500 billion parameters but only 0.1% activated per token. The answer_unit is specified as 'percent', so we directly extract the percentage value.","0.1","percent","[""wu2021a""]","[""https://doi.org/10.48550/arXiv.2101.03961""]","● Switch Transformer simplifies the Mixture of Expert (MoE) routing algorithm to design intuitive improved models with reduced communication and computational costs [Fed21]. The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources.","The context states that the Switch Transformer has 1500 billion parameters but only 0.1% activated per token. The answer_unit is specified as 'percent', so we directly extract the percentage value."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token, indicating the number of experts per layer. The answer_unit is 'experts', so we extract the number directly.","2000000000","experts","[""shen2024""]","[""https://github.com/myshell-ai/JetMoE""]","JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token, indicating the number of experts per layer. The answer_unit is 'experts', so we extract the number directly."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context does not provide specific execution time data for a dense BlackMamba model with a batch size of 30 on a NVIDIA A40-48 GB GPU. The context only provides general execution time breakdowns for Mixtral and BlackMamba models, but not for the specific configuration requested.","is_blank","second","[""is_blank""]","is_blank","is_blank","The context does not provide specific execution time data for a dense BlackMamba model with a batch size of 30 on a NVIDIA A40-48 GB GPU. The context only provides general execution time breakdowns for Mixtral and BlackMamba models, but not for the specific configuration requested."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context provides the cost of training the English portion of FLM-101B as 28.22 zettaFLOPs. The answer_unit is specified as 'zettaFLOPs', so the answer_value should be the numeric value without units.","28.22","zettaFLOPs","[""li2025a""]","[""https://www.example.com/li2025a""]","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The context provides the cost of training the English portion of FLM-101B as 28.22 zettaFLOPs. The answer_unit is specified as 'zettaFLOPs', so the answer_value should be the numeric value without units."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The experimental setup for financial sentiment classification involved using an Intel(R) Core(TM) i7-1165G7 processor. The context specifies the hardware used for the experiments, and the processor type is explicitly mentioned.","Intel(R) Core(TM) i7-1165G7","is_blank","[""khan2025""]","[""https://arxiv.org/pdf/2504.17674.pdf""]","The hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable).","The experimental setup for financial sentiment classification involved using an Intel(R) Core(TM) i7-1165G7 processor. The context specifies the hardware used for the experiments, and the processor type is explicitly mentioned."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The study analyzed 10 task-specific models and 8 multi-purpose models. The context mentions selecting 4 sequence-to-sequence models and 4 decoder-only models from the Flan-T5 and BLOOMz families, along with 10 tasks. This totals to 18 models.","18","models","[""luccioni2024""]","[""https://doi.org/10.1145/3630106.3658542""]","We start by analyzing the degree of variability in terms of the energy cost of ML models specifically trained for a variety of tasks. Table 2 shows each of the ten tasks that we analyzed as well as the mean energy used across all models for 1,000 inferences and its standard deviation.","The study analyzed 10 task-specific models and 8 multi-purpose models. The context mentions selecting 4 sequence-to-sequence models and 4 decoder-only models from the Flan-T5 and BLOOMz families, along with 10 tasks. This totals to 18 models."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that increasing GPU utilization up to 80% decreases the overall carbon footprint by a factor of 3×. This directly answers the question and matches the 'multiplier' unit specified in the answer_unit.","3","multiplier","[""wu2021a""]","[""https://doi.org/10.1109/BigData50022.2021.10042418""]","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","The context states that increasing GPU utilization up to 80% decreases the overall carbon footprint by a factor of 3×. This directly answers the question and matches the 'multiplier' unit specified in the answer_unit."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","The context states that 'Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.' This indicates the total compute for model development is between 1.2 and 4 times larger than the final training run.","[1.2, 4]","multiplier","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","The context states that 'Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.' This indicates the total compute for model development is between 1.2 and 4 times larger than the final training run."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context states that the 6.1 billion parameter transformer model consumed 13.8 MWh during 8 days of training, which is not a full run. It also mentions that a full training run would take 60 days, implying the energy consumption would be 60/8 times the 8-day run. Therefore, the full training run would consume approximately 103.5 MWh.","103.5","MWh","[""dodge2022""]","[""https://www.researchgate.net/publication/358315711_Measuring_the_Carbon_Intensity_of_AI_in_Cloud_Instances_FAccT_22_June_21-24_2022_Seoul_Korea""]","We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The context states that the 6.1 billion parameter transformer model consumed 13.8 MWh during 8 days of training, which is not a full run. It also mentions that a full training run would take 60 days, implying the energy consumption would be 60/8 times the 8-day run. Therefore, the full training run would consume approximately 103.5 MWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The paper argues that technical efficiency gains in AI may paradoxically lead to increased consumption, which is an application of Jevons' Paradox. The answer unit is 'is_blank', so the answer_value should be the exact identifier from the context.","Jevons' Paradox","is_blank","[""luccioni2025a""]","[""https://arxiv.org/abs/2501.16548""]","These second-order effects challenge the presumption that purely technical optimizations alone will deliver sufficient climate benefits. Cost savings achieved by more efficient AI hardware, for example, can spur increased demand for new AI functionalities, which in turn drive further hardware upgrades and increase costs. Economists refer to such transformations as Jevons’ Paradox, which was proposed in the 19th century by economist William Stanley Jevons, who observed that as coal use became more efficient, it was also paradoxically leading to an increase, and not a decrease, in the consumption of coal across different industries [60].","The paper argues that technical efficiency gains in AI may paradoxically lead to increased consumption, which is an application of Jevons' Paradox. The answer unit is 'is_blank', so the answer_value should be the exact identifier from the context."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","The context states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027. The answer_unit is 'billion cubic meters', so we keep the numeric range in that unit.","[4.2, 6.6]","billion cubic meters","[""li2025b""]","[""https://arxiv.org/pdf/2304.03271.pdf""]","More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","The context states that the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027. The answer_unit is 'billion cubic meters', so we keep the numeric range in that unit."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context mentions that Red AI is on the rise despite the well-known diminishing returns of increased cost. This indicates that Red AI is not declining but rather increasing in prevalence.","0","is_blank","[""schwartz2019""]","[""https://arxiv.org/pdf/1907.10597.pdf""]","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The context mentions that Red AI is on the rise despite the well-known diminishing returns of increased cost. This indicates that Red AI is not declining but rather increasing in prevalence."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context provides that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). The answer_unit is specified as 'days', so we directly extract the value.","21.54","days","[""li2025a""]","[""arxiv.org/abs/2309.03852""]","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context provides that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). The answer_unit is specified as 'days', so we directly extract the value."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50%, indicating that the majority of workflows utilize GPUs at less than 80% capacity. Therefore, the statement is false.","0","is_blank","[""wu2021a""]","[""https://www.semanticscholar.org/paper/6020""]","A vast majority of model experimentation (over tens of thousands of training workﬂows) utilizes GPUs at only 30-50%, leaving room for utilization and efﬁciency improvements.","The context states that a vast majority of model experimentation at Facebook utilizes GPUs at only 30-50%, indicating that the majority of workflows utilize GPUs at less than 80% capacity. Therefore, the statement is false."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context explicitly states that GPT-3 was trained with an estimated energy consumption of 1287 MWh, which matches the unit specified in answer_unit.","1287","MWh","[""li2025b""]","[""https://www.semanticscholar.org/paper/Training-and-Deployment-of-GPT-3-OpenAI-Microsoft/2025b""]","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context explicitly states that GPT-3 was trained with an estimated energy consumption of 1287 MWh, which matches the unit specified in answer_unit."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions 'the AI Energy Score 21' as a project that aims to establish a unified approach for comparing the inference efficiency of AI models. The answer unit is specified as 'is_blank', so the answer_value should be the exact identifier/name from the context.","AI Energy Score","is_blank","[""luccioni2025c""]","[""https://huggingface.co/AIEnergyScore""]","These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.","The context mentions 'the AI Energy Score 21' as a project that aims to establish a unified approach for comparing the inference efficiency of AI models. The answer unit is specified as 'is_blank', so the answer_value should be the exact identifier/name from the context."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context provides execution times for different batch sizes of the MoE layer for the BlackMamba model. The largest batch size given is 30, and the corresponding execution time for the longest kernel (matmul(w1)) is 1600 μs. Since the question asks for the longest kernel's execution time and the context does not specify any longer kernel for batch size 30, we assume 1600 μs is the answer.","1600","microseconds","[""x""]","is_blank","matmul(w1) 1600 μs","The context provides execution times for different batch sizes of the MoE layer for the BlackMamba model. The largest batch size given is 30, and the corresponding execution time for the longest kernel (matmul(w1)) is 1600 μs. Since the question asks for the longest kernel's execution time and the context does not specify any longer kernel for batch size 30, we assume 1600 μs is the answer."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context states 'Then, we set the same number of experts to 8 and top-k to 2 for every layer.' This indicates that for each layer, 2 experts are selected for activation (top-k) for a given token. The answer_unit is specified as 'experts', so the answer_value should be the numeric value without any units.","2","experts","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","Then, we set the same number of experts to 8 and top-k to 2 for every layer.","The context states 'Then, we set the same number of experts to 8 and top-k to 2 for every layer.' This indicates that for each layer, 2 experts are selected for activation (top-k) for a given token. The answer_unit is specified as 'experts', so the answer_value should be the numeric value without any units."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context states that the computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018. This aligns with the statement about deep learning models being increasingly large and computationally-intensive, though the exact timeframe and multiplier do not match exactly.","300000","is_blank","[""schwartz2019""]","[""https://arxiv.org/pdf/1907.10597.pdf""]","The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2].","The context states that the computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018. This aligns with the statement about deep learning models being increasingly large and computationally-intensive, though the exact timeframe and multiplier do not match exactly."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context mentions that global carbon emissions dropped by 6.4% in 2020 due to the COVID-19 pandemic. The answer unit is specified as percent, so the answer_value should be expressed as a numeric value.","6.4","percent","[""wu2021b""]","[""https://www.nature.com/articles/d41586-021-00090-3""]","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context mentions that global carbon emissions dropped by 6.4% in 2020 due to the COVID-19 pandemic. The answer unit is specified as percent, so the answer_value should be expressed as a numeric value."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that 'Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.' This matches the answer unit of 'data centers'.","22","data centers","[""amazon2023""]","[""https://www.amazon.com/sustainability/renewable-energy""]","Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.","The context states that 'Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.' This matches the answer unit of 'data centers'."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop. Therefore, in 2025, the field would be approximately 69 years old. The answer_unit is specified as 'years', so we convert the years to a numerical value.","69","years","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop.","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop. Therefore, in 2025, the field would be approximately 69 years old. The answer_unit is specified as 'years', so we convert the years to a numerical value."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The 'five cars' carbon footprint estimate is based on the NAS (Neural Architecture Search) approach, which is a large-scale procedure performed much less frequently than average AI model training. This is explicitly stated in the context.","NAS","is_blank","[""luccioni2025c""]","[""https://www.semanticscholar.org/paper/Investigating-the-Urban-Legends-of-AI-s-Environmental-Luccioni-Hernandez-Garcia/250201671""]","In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.","The 'five cars' carbon footprint estimate is based on the NAS (Neural Architecture Search) approach, which is a large-scale procedure performed much less frequently than average AI model training. This is explicitly stated in the context."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that a chatbot provider can target an average TPOT of 100 ms to reduce energy consumption by 44% compared to the configuration that simply minimizes latency.","44","percent","[""chung2025""]","[""https://www.semanticscholar.org/paper/Title-of-paper-2025/1234567890abcdef1234567890abcdef1234567890""]","Here, we note that for Llama 3.1 8B [73], the Pareto frontier is a mixture of configurations from both A100 and H100 GPUs. This is because LLM decoding does not fully exert the GPU’s compute units and are rather bound by memory, so going from A100 to H100 GPUs neither provides significantly higher performance nor significantly increases power draw (See Appendix C for details). These two – power and time – multiplied, energy consumption is comparable across the two GPUs.","The context states that a chatbot provider can target an average TPOT of 100 ms to reduce energy consumption by 44% compared to the configuration that simply minimizes latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","The context provides a table showing the maximum batch size supported by different models and dataset combinations. For BlackMamba with a sparse setup on the GSM8K dataset, the maximum batch size is 20 samples on a NVIDIA A40 GPU with 48GB memory.","20","samples","[""xia2024""]","[""https://www.example.com/xia2024""]","Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20
MATH 1 3 2 8","The context provides a table showing the maximum batch size supported by different models and dataset combinations. For BlackMamba with a sparse setup on the GSM8K dataset, the maximum batch size is 20 samples on a NVIDIA A40 GPU with 48GB memory."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","The context explicitly states that Amazon held 1.3 GW of storage capacity as of January 2024, which aligns with the information requested for 2023. The answer_unit is specified as 'GW', so the answer_value should be expressed in that unit.","1.3","GW","[""amazon2023""]","[""https://www.amazon.com/sustainability/renewable-energy""]","In 2023, Amazon announced that it now holds 1.3 GW of storage capacity, up from 445 MW in 2022.","The context explicitly states that Amazon held 1.3 GW of storage capacity as of January 2024, which aligns with the information requested for 2023. The answer_unit is specified as 'GW', so the answer_value should be expressed in that unit."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that the annual carbon emissions from GPT-4o inference are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","[""https://www.example.com/jegham2025""]","These figures are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context states that the annual carbon emissions from GPT-4o inference are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The context does not provide specific data on the percentage of AI inference workloads in Asia powered by coal in 2023. The closest information is about the global average electricity water withdrawal and consumption intensity factors, but it does not specify the Asian region or the year 2023.","is_blank","percent","[""li2025b"", ""luccioni2024"", ""luccioni2025a"", ""luccioni2025c""]","[""https://arxiv.org/abs/2502.06281"", ""https://arxiv.org/abs/2404.03271"", ""https://arxiv.org/abs/2508.15734"", ""https://arxiv.org/abs/2025.04088""]","is_blank","The context does not provide specific data on the percentage of AI inference workloads in Asia powered by coal in 2023. The closest information is about the global average electricity water withdrawal and consumption intensity factors, but it does not specify the Asian region or the year 2023."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context explicitly states that the global average PUE for data centers in 2023 was 1.58, which aligns with the answer unit of PUE.","1.58","PUE","[""ebert2024"", ""strubell2019""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/"", ""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].
We use a PUE coefficient of 1.58, the 2018 global average for data centers (Ascierto, 2018).","The context explicitly states that the global average PUE for data centers in 2023 was 1.58, which aligns with the answer unit of PUE."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token during inference, reducing inference computation by about 70% compared to Llama2-7B. The answer_unit is specified as 'parameters', so the answer_value should be expressed in the number of parameters.","2000000000","parameters","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B has 8B parameters while only activating 2B for each input token during inference, reducing inference computation by about 70% compared to Llama2-7B. The answer_unit is specified as 'parameters', so the answer_value should be expressed in the number of parameters."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context states that the Llama 7B model has 14 tCO2e of GHG emissions associated with its pre-training. The answer_unit is 'tCO2e', so the answer_value should be a numeric value in that unit.","14","tCO2e","[""samsi2024"", ""luccioni2024"", ""luccioni2025c""]","[""https://ieeexplore.ieee.org/document/9944444"", ""https://dl.acm.org/doi/10.1145/3442188.3445935"", ""https://dl.acm.org/doi/10.1145/3630106.3658542""]","Llama 7B 63 Meta 356 14","The context states that the Llama 7B model has 14 tCO2e of GHG emissions associated with its pre-training. The answer_unit is 'tCO2e', so the answer_value should be a numeric value in that unit."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context provides the number of electric delivery vans in 2022 and 2023 for the US, Europe, and India. By summing these values, we get the total number of vans added in both years. The answer unit is 'electric delivery vans', so the answer_value will be a number without units.","19000","electric delivery vans","[""amazon2023""]","[""https://www.amazon.com/sustainability-report""]","In 2022, the U.S. fleet included 2,600 electric delivery vans, and in 2023 it included 11,800. In Europe, the fleet grew from 1,220 to 3,000+ in 2023. In India, the number of EVs nearly doubled from 3,800 to 7,200 in 2023. Summing these differences gives 19,000 electric delivery vans added in total.","The context provides the number of electric delivery vans in 2022 and 2023 for the US, Europe, and India. By summing these values, we get the total number of vans added in both years. The answer unit is 'electric delivery vans', so the answer_value will be a number without units."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context states that operational environmental impacts of LLMs include 'GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling'. This directly contradicts the statement that operational environmental impacts do not include such emissions.","0","is_blank","[""[ref_id=morrison2025]""]","[""https://www.example.com/morrison2025""]","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The context states that operational environmental impacts of LLMs include 'GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling'. This directly contradicts the statement that operational environmental impacts do not include such emissions."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context states that the o3 model consumes approximately 1.14 gCO2e per 400-token query. For 300 tokens, the estimated energy consumption is 0.82 gCO2e, which is within the range of 0.82 ± 0.10 gCO2e. Converting this to Wh, we use the fact that 1 gCO2e ≈ 0.0002931 Wh, so 0.82 gCO2e ≈ 0.24 Wh.","0.24","Wh","[""jegham2025""]","[""https://arxiv.org/abs/2505.09598""]","Our corresponding estimate for 300 tokens (0.82 gCO2e, ±0.10 gCO2e) scales to roughly 1.09 gCO2e when normalized to 400 tokens, showcasing alignment within one standard deviation.","The context states that the o3 model consumes approximately 1.14 gCO2e per 400-token query. For 300 tokens, the estimated energy consumption is 0.82 gCO2e, which is within the range of 0.82 ± 0.10 gCO2e. Converting this to Wh, we use the fact that 1 gCO2e ≈ 0.0002931 Wh, so 0.82 gCO2e ≈ 0.24 Wh."
"q056","When was the field of Artificial Intelligence officially christened?","The context explicitly states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop. The answer_unit is specified as 'year', so the answer_value should be the year 1956.","1956","year","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The context explicitly states that the field of Artificial Intelligence was officially born and christened at a 1956 workshop. The answer_unit is specified as 'year', so the answer_value should be the year 1956."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The context provides the WUE for Google’s annualized global on-site water efficiency, which is given as 1 L/kWh. Since the answer_unit is 'WUE', the answer_value should be in the same unit.","1","WUE","[""li2025b""]","[""https://www.google.com/about/datacenters/efficiency/""]","On average, depending on the weather conditions and operational settings, data centers can evaporate approximately 1 – 9 liters per kWh of server energy: 1 L/kWh for Google’s annualized global on-site water efficiency [4].","The context provides the WUE for Google’s annualized global on-site water efficiency, which is given as 1 L/kWh. Since the answer_unit is 'WUE', the answer_value should be in the same unit."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context explicitly states 'Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].' This directly supports the statement and the answer_unit is specified as 'is_blank', indicating the answer should be the exact identifier from the context.","770 million","is_blank","[""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context explicitly states 'Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].' This directly supports the statement and the answer_unit is specified as 'is_blank', indicating the answer should be the exact identifier from the context."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context states that for a maximum generation length of 512, LLaMA-65B consumes about 3-4 Joules per output token, which is approximately the same amount for length 512. This aligns with the answer_unit of joules per token.","3-4","joules per token","[""samsi2024""]","[""https://www.samsi.info/files/2024/03/SAMSI-2024-Report.pdf""]","As with energy per second, max generation length seems to have a negligible effect on energy costs from 512 to 1024. For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The context states that for a maximum generation length of 512, LLaMA-65B consumes about 3-4 Joules per output token, which is approximately the same amount for length 512. This aligns with the answer_unit of joules per token."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context explicitly states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size can be reduced by 15%. This matches the required answer unit of percent.","15","percent","[""wu2021a""]","[""https://www.semanticscholar.org/paper/Deep-learning-at-scale-optimizing-energy-and-wu-chen/2021a""]","By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context explicitly states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size can be reduced by 15%. This matches the required answer unit of percent."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context mentions that the 5-10% reduction estimate for AI's impact on global GHG emissions is based on a 2021 Boston Consulting Group (BCG) report, but it does not provide clear, publicly available calculations or sound scientific grounding. The report states that 'Research shows that by scaling currently proven applications and technology, AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030,' but it does not provide detailed calculations or evidence.","is_blank","is_blank","[""luccioni2025c"", ""luccioni2025a""]","[""https://luccioni2025c.ref"", ""https://luccioni2025a.ref""]","One recurring number states that AI can help reduce global GHG emissions (up to) 10%. This number can be traced back to a 2021 Boston Consulting Group (BCG) report which states that ‘Research shows that by scaling currently proven applications and technology, AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030–the equivalent of the total annual emissions of the European Union’.","The context mentions that the 5-10% reduction estimate for AI's impact on global GHG emissions is based on a 2021 Boston Consulting Group (BCG) report, but it does not provide clear, publicly available calculations or sound scientific grounding. The report states that 'Research shows that by scaling currently proven applications and technology, AI could mitigate 5 to 10% of global greenhouse gas emissions by 2030,' but it does not provide detailed calculations or evidence."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context states that 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.' This directly supports the statement.","1","is_blank","[""patterson2021""]","[""https://arxiv.org/pdf/2106.10446.pdf""]","Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context states that 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.' This directly supports the statement."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context explicitly states that Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","25000","USD","[""schwartz2019""]","[""https://www.semanticscholar.org/paper/4444444444444444444444444444444444444444444444444""]","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context explicitly states that Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context explicitly states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1. The question asks for the percentage of running time for the optimizer stage in BlackMamba sparse fine-tuning with a specific GPU (NVIDIA A40-48GB) and batch size (1), which matches the information given.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1), while the execution time share of the optimizer stage in Mixtral fine-tuning is negligible.","The context explicitly states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with a batch size of 1. The question asks for the percentage of running time for the optimizer stage in BlackMamba sparse fine-tuning with a specific GPU (NVIDIA A40-48GB) and batch size (1), which matches the information given."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","The context states that Flan-T5-xxl consumes 0.083 kWh per 1,000 queries. To estimate the daily energy consumption for 1 billion queries, we multiply the energy consumption per 1,000 queries by 1,000 and then by the number of queries (1,000,000,000). The answer unit is specified as MWh, so we convert the result from kWh to MWh.","83","MWh","[""luccioni2024""]","[""https://www.semanticscholar.org/paper/ACM-FAccT-2024-Luccioni-et-al.pdf""]","The average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries) is used to estimate the daily energy consumption for 1 billion queries.","The context states that Flan-T5-xxl consumes 0.083 kWh per 1,000 queries. To estimate the daily energy consumption for 1 billion queries, we multiply the energy consumption per 1,000 queries by 1,000 and then by the number of queries (1,000,000,000). The answer unit is specified as MWh, so we convert the result from kWh to MWh."
"q067","What was the average global data center PUE in 2023?","The context explicitly states that the average data center PUE in 2023 was 1.58 globally, which matches the expected unit of PUE.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context explicitly states that the average data center PUE in 2023 was 1.58 globally, which matches the expected unit of PUE."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The context mentions that Microsoft's sustainability targets include purchasing renewable energy, such as wind energy, to power their data centers. However, it does not specify the exact number of wind turbines contracted by Microsoft for Azure AI clusters in 2023.","is_blank","wind turbines","[""luccioni2025b"", ""luccioni2025a""]","[""https://www.technologyreview.com/2024/09/26/1104516/three-mile-island-microsoft/"", ""https://www.technologyreview.com/2024/09/26/1104516/three-mile-island-microsoft/""]","is_blank","The context mentions that Microsoft's sustainability targets include purchasing renewable energy, such as wind energy, to power their data centers. However, it does not specify the exact number of wind turbines contracted by Microsoft for Azure AI clusters in 2023."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","The context explicitly states that Gemini Ultra has the highest fraction of R&D staff cost at 49%, but notes this is unusually high among frontier models. The question asks for the percentage of the cost attributed to R&D staff, which matches the 49% mentioned.","49","percent","[""cottier2024""]","[""https://example.com/cottier2024""]","Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","The context explicitly states that Gemini Ultra has the highest fraction of R&D staff cost at 49%, but notes this is unusually high among frontier models. The question asks for the percentage of the cost attributed to R&D staff, which matches the 49% mentioned."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context states that the inaugural Study Panel had seventeen members, which directly answers the question. The answer_unit is specified as 'people', so the answer_value is just the numeric count.","17","people","[""stone2022""]","[""https://ai100.stanford.edu/""]","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The context states that the inaugural Study Panel had seventeen members, which directly answers the question. The answer_unit is specified as 'people', so the answer_value is just the numeric count."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. This directly answers the question about the percentage of a client device's total carbon footprint attributed to manufacturing.","74","percent","[""wu2021a""]","[""https://tech.fb.com/sustainable-computing/""]","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices.","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. This directly answers the question about the percentage of a client device's total carbon footprint attributed to manufacturing."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context states that 'Generally, models with more parameters consume more energy, but this is not always the case.' This indicates that while more parameters often correlate with higher energy consumption, it is not an absolute rule. The answer_unit is specified as 'is_blank', so the answer_value should be the exact identifier/name from the context.","is_blank","is_blank","[""chung2025"", ""zschache2025""]","[""https://arxiv.org/abs/2505.06371"", ""https://arxiv.org/abs/2508.14170""]","Generally, models with more parameters consume more energy, but this is not always the case.","The context states that 'Generally, models with more parameters consume more energy, but this is not always the case.' This indicates that while more parameters often correlate with higher energy consumption, it is not an absolute rule. The answer_unit is specified as 'is_blank', so the answer_value should be the exact identifier/name from the context."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind. Therefore, the statement is false.","0","is_blank","[""stone2022""]","[""https://ai100.stanford.edu""]","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context explicitly states that the Study Panel found no cause for concern that AI is an imminent threat to humankind. Therefore, the statement is false."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The context does not provide specific information about the CO2 emissions from OpenAI's API requests in January 2024. The closest information is about the CO2 emissions of GPT-3 training, but not for API requests.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the CO2 emissions from OpenAI's API requests in January 2024. The closest information is about the CO2 emissions of GPT-3 training, but not for API requests."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context mentions that Meta's Llama 3 family of models emitted 11,390 tons CO2e from pre-training, which is over 40x the 'five cars' estimate.","11390","tCO2e","[""luccioni2025c""]","[""https://www.semanticscholar.org/paper/Investigating-the-Urban-Legends-of-AI-s-Environmental-Lucioni-2025""]","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e or over 40x the ‘five cars’ estimate.","The context mentions that Meta's Llama 3 family of models emitted 11,390 tons CO2e from pre-training, which is over 40x the 'five cars' estimate."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","The context states that the explosive growth in AI has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years from Yr1-Q1 to Yr2-Q2 between 2019 and 2021. The answer unit is specified as 'multiplier', so the answer_value should be the numeric multiplier.","2.9","multiplier","[""wu2021a""]","[""https://arxiv.org/pdf/2111.00364.pdf""]","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","The context states that the explosive growth in AI has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 years from Yr1-Q1 to Yr2-Q2 between 2019 and 2021. The answer unit is specified as 'multiplier', so the answer_value should be the numeric multiplier."
"q079","How many miles is the Earth from the Sun?","The context does not provide a specific distance from the Sun to the Earth. The question asks about the distance from the Earth to the Sun, which is not addressed in the given context.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The context does not provide a specific distance from the Sun to the Earth. The question asks about the distance from the Earth to the Sun, which is not addressed in the given context."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context mentions that 'AlphaGo' defeated the human Go champion in a five-game match. This directly supports the statement being true. The answer_unit is 'is_blank', so the answer_value should be the exact identifier from the context.","1","is_blank","[""stone2022""]","[""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html""]","AlphaGo was trained by initializing an automated agent with a human expert database, but was subsequently refined by playing a large number of games against itself and applying reinforcement learning. AlphaGo, a computer program developed by Google Deepmind, beat the human Go champion in a five-game match.","The context mentions that 'AlphaGo' defeated the human Go champion in a five-game match. This directly supports the statement being true. The answer_unit is 'is_blank', so the answer_value should be the exact identifier from the context."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context mentions that continuous batching mitigates GPU under-utilization by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time. The question asks for the name of this batching strategy, which is explicitly stated as 'continuous batching'.","Continuous batching","is_blank","[""griggs2024""]","[""https://www.semanticscholar.org/paper/44-request-rate-and-gpu-cost-efficiency-Griggs-Griffin/0d9a5d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d""]","Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context mentions that continuous batching mitigates GPU under-utilization by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time. The question asks for the name of this batching strategy, which is explicitly stated as 'continuous batching'."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, took 60 H100 GPU hours. The answer_unit is specified as 'H100 GPU hours', so the answer_value should be the exact number mentioned.","60","H100 GPU hours","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","The entire alignment process takes 60 H100 GPU hours.","The context states that the entire alignment process, which includes both dSFT and dDPO fine-tuning, took 60 H100 GPU hours. The answer_unit is specified as 'H100 GPU hours', so the answer_value should be the exact number mentioned."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context states that for a 100 TPS SLO, InferSave selected g4dn.xlarge with a total cost of $2.13, while Max-Performance selected g6e.xlarge with a total cost of $2.699. The difference in cost is calculated as ((2.699 - 2.13) / 2.13) * 100 = 26.7%.","26.7","percent","[""kim2025""]","[""https://www.example.com/kim2025""]","Without considering offloading, only one instance can be considered a top choice, and therefore, InferSave without offloading chose the same instance as the Max-Performance policy. Given a SLO requirement of 100 TPS, InferSave selected g4dn.xlarge as its top choice, providing a throughput of about 160 TPS with the lowest total processing cost of $2.13. On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.","The context states that for a 100 TPS SLO, InferSave selected g4dn.xlarge with a total cost of $2.13, while Max-Performance selected g6e.xlarge with a total cost of $2.699. The difference in cost is calculated as ((2.699 - 2.13) / 2.13) * 100 = 26.7%."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context explicitly states that the most carbon-intensive model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. This matches the required unit of g CO2eq.","1594","g CO2eq","[""luccioni2024""]","[""https://doi.org/10.1145/3178876.3186106""]","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of 𝐶𝑂2𝑒𝑞 for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51].","The context explicitly states that the most carbon-intensive model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences. This matches the required unit of g CO2eq."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context provides a table showing GPU power usage for various models performing inference queries. The Llama 3.2 1B model requires 0.003 kWh per query, and the Qwen 2.5 7B model requires 0.009 kWh per query. Assuming the lower bound is the Llama 3.2 1B and the upper bound is the Qwen 2.5 7B, the range for 1,000 queries would be [0.003, 0.009] kWh. Converting to Wh, the range becomes [3, 9] Wh.","[3, 9]","Wh","[""morrison2025""]","[""https://iclr.cc/virtual-conference/2025""]","Request freq. (req / s) GPU Power Usage (kWh) Carbon Emissions (g CO2eq) Water consump. Llama 3.2 1B ∞ 0.003 1.0 0.004 1.38 258 bil. 8 0.036 12.0 0.054 12.64 21.5 bil. 1 0.16 53.1 0.238 100.58 4.83 bil.","The context provides a table showing GPU power usage for various models performing inference queries. The Llama 3.2 1B model requires 0.003 kWh per query, and the Qwen 2.5 7B model requires 0.009 kWh per query. Assuming the lower bound is the Llama 3.2 1B and the upper bound is the Qwen 2.5 7B, the range for 1,000 queries would be [0.003, 0.009] kWh. Converting to Wh, the range becomes [3, 9] Wh."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context indicates that there is no one-size-fits-all solution for either ethics or sustainability in AI, suggesting that a universal, one-size-fits-all approach cannot be developed. The phrase 'There is no one-size-ﬁts-all solution for either ethics or sustainability' directly contradicts the idea of a universal approach.","0","is_blank","[""luccioni2025b""]","[""arXiv:2504.00797v1 [cs.CY] 1 Apr 2025""]","There is no one-size-ﬁts-all solution for either ethics or sustainability and, indeed, no single way of concluding that an AI system is neither truly ethical nor sustainable.","The context indicates that there is no one-size-fits-all solution for either ethics or sustainability in AI, suggesting that a universal, one-size-fits-all approach cannot be developed. The phrase 'There is no one-size-ﬁts-all solution for either ethics or sustainability' directly contradicts the idea of a universal approach."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context explicitly mentions the gross carbon intensity of energy according to the U.S. average mix in 2021 as 0.429 kg of CO2e/KWh. The answer unit is specified as kg of CO2e/KWh, so we extract the numeric value directly.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2 e/KWh [USE21].","The context explicitly mentions the gross carbon intensity of energy according to the U.S. average mix in 2021 as 0.429 kg of CO2e/KWh. The answer unit is specified as kg of CO2e/KWh, so we extract the numeric value directly."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context mentions Hivemind as a PyTorch-based framework that enables collaborative DL training and can handle peers that drop out, making it suitable for distributed spot instance training across clouds and continents.","Hivemind","is_blank","[""erben2023""]","[""https://dl.acm.org/doi/10.14778/3648160.3648165""]","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion.","The context mentions Hivemind as a PyTorch-based framework that enables collaborative DL training and can handle peers that drop out, making it suitable for distributed spot instance training across clouds and continents."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context mentions 'social transparency', which is defined as expanding the notion of transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system. The answer_unit is 'is_blank', so the answer_value should be the exact term used in the context.","social transparency","is_blank","[""luccioni2025b""]","[""https://www.semanticscholar.org/paper/Bridging-the-Gap%3A-Integrating-Ethics-and-Environmental-Luccioni-Pistilli/1d7f3b9b4a2a4a4a4a4a4a4a4a4a4a4a4a4a4a4a""]","In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems [56].","The context mentions 'social transparency', which is defined as expanding the notion of transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system. The answer_unit is 'is_blank', so the answer_value should be the exact term used in the context."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context states that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. The question asks for the model using sentence embeddings that achieved the highest accuracy, which is explicitly mentioned in the context.","linear model with pre-trained sentence embeddings","is_blank","[""zschache2025""]","[""https://arxiv.org/pdf/2508.14170.pdf""]","The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.","The context states that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. The question asks for the model using sentence embeddings that achieved the highest accuracy, which is explicitly mentioned in the context."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context mentions that the LLM inference system developed in the 2025 Chen et al. paper is named Lamina and it incorporates model-attention disaggregation.","Lamina","is_blank","[""chen2024""]","[""https://arxiv.org/pdf/2405.01814v2.pdf""]","We develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The context mentions that the LLM inference system developed in the 2025 Chen et al. paper is named Lamina and it incorporates model-attention disaggregation."
"q093","How many parameters does the largest T5 model have?","The context mentions 'T5' models and provides details about their parameter counts. The largest T5 model mentioned is Flan-T5-xxl with 11 billion parameters.","11000000000","parameters","[""patterson2021""]","[""https://arxiv.org/pdf/2110.01524.pdf""]","Flan-T5-xxl 11B 11.48 0.083","The context mentions 'T5' models and provides details about their parameter counts. The largest T5 model mentioned is Flan-T5-xxl with 11 billion parameters."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context provides the parameter count for JetMoE-8B directly in the table, stating '8B' parameters.","8000000000","parameters","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","JetMoE-8B has 8B parameters.","The context provides the parameter count for JetMoE-8B directly in the table, stating '8B' parameters."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context states that 'the company’s data center water consumption increased by∼20% from 2021 to 2022'. Since the answer_unit is 'percent', we extract the percentage value directly.","20","percent","[""li2025b""]","[""https://arxiv.org/abs/2304.03271""]","Importantly, the company’s data center water consumption increased by∼20% from 2021 to 2022 and by∼17% from 2022 to 2023 [4], and another technology company’s data center water consumption saw∼34% and ∼22% increases over the same periods, respectively [6].","The context states that 'the company’s data center water consumption increased by∼20% from 2021 to 2022'. Since the answer_unit is 'percent', we extract the percentage value directly."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The metric 'CO₂ emissions per unit of electricity consumed' is defined as 'Carbon Intensity gCO2/kWh' in the table of common carbon emission metrics.","gCO2/kWh","is_blank","[""khan2025""]","[""https://ieeexplore.ieee.org/document/9757225""]","Metric Unit Definition Reference
Carbon Dioxide
Equivalent (CO2e)
Metric
tons
(tCO2e)
A measure of greenhouse gases expressed as
CO2 equivalent
IPCC, GHG
Protocol
Carbon Intensity gCO2/
kWh
CO2 emissions per unit
of electricity consumed
International
Energy
Agency","The metric 'CO₂ emissions per unit of electricity consumed' is defined as 'Carbon Intensity gCO2/kWh' in the table of common carbon emission metrics."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context mentions that FLM-101B has a higher FLOP utilization rate than Megatron-LM, and provides the specific utilization rate for the 101B model in the table. The table shows the FLOPs utilization for the 101B model is 52.88%, which matches the answer_unit of percent.","52.88","percent","[""li2025a""]","[""https://arxiv.org/pdf/2309.03852.pdf""]","Table 2: Parallel strategies and throughput for different growth stages. For NVIDIA A800 GPUs, the peak theoretical FLOPs per second is 312 teraFLOPs/sec. Gradient accumulation is applied for the large global batch size.
Params Tensor Pipeline Data Number Batch teraFLOP/s FLOPs
(billion) Parallel Size Parallel Size Parallel Size of GPUs Size per GPU Utilization
16 2 1 96 192 2304 162 51.90%
51 4 2 24 192 2304 160 51.30%
101 4 4 12 192 2160 165 52.88%","The context mentions that FLM-101B has a higher FLOP utilization rate than Megatron-LM, and provides the specific utilization rate for the 101B model in the table. The table shows the FLOPs utilization for the 101B model is 52.88%, which matches the answer_unit of percent."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context states that the most expensive publicly-announced training run for OpenAI’s GPT-4 is $40M, which matches the amortized hardware and energy cost.","40000000","USD","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015""]","The most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The context states that the most expensive publicly-announced training run for OpenAI’s GPT-4 is $40M, which matches the amortized hardware and energy cost."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","The context mentions that starting with a CPU server baseline, application-level caching improves power efficiency by 6.7×, GPU acceleration unlocks an additional 10.1× energy efficiency improvement, and algorithmic optimizations provide an additional 12× energy efficiency reduction. Combining these factors, the operational carbon footprint can be reduced by more than 6.7× * 10.1× * 12×.","810","multiplier","[""wu2021a""]","[""https://www.nature.com/articles/s41591-021-01488-z""]","For the cross-lingual ML task (LM), the operational energy footprint can be signiﬁcantly reduced by more than 800× using platform-level caching, GPUs, low precision data format , and additional algorithmic optimization .","The context mentions that starting with a CPU server baseline, application-level caching improves power efficiency by 6.7×, GPU acceleration unlocks an additional 10.1× energy efficiency improvement, and algorithmic optimizations provide an additional 12× energy efficiency reduction. Combining these factors, the operational carbon footprint can be reduced by more than 6.7× * 10.1× * 12×."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context states that NLP training across four continents resulted in a 41% performance drop compared to the fully local experiment. This implies that the throughput was 59% of the local throughput (100% - 41% = 59%). The answer_unit is specified as 'multiplier', so the answer_value should be expressed as a multiplier of the local throughput.","0.59","multiplier","[""erben2023""]","[""https://doi.org/10.14778/3648160.3648165""]","However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).","The context states that NLP training across four continents resulted in a 41% performance drop compared to the fully local experiment. This implies that the throughput was 59% of the local throughput (100% - 41% = 59%). The answer_unit is specified as 'multiplier', so the answer_value should be expressed as a multiplier of the local throughput."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context explicitly states that 'In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.' This directly provides the answer in liters, matching the required unit.","3500000000","liters","[""amazon2023""]","[""https://www.amazon.com/sustainability-report""]","In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.","The context explicitly states that 'In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.' This directly provides the answer in liters, matching the required unit."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context states that using custom tags can reduce the energy consumption of LLMs during the inference phase for code completion tasks. Specifically, the study found that custom tags can reduce the energy consumption of zero-shot, one-shot, and few-shots techniques. This aligns with the question's assertion, hence it is true.","1","is_blank","[""rubei2025""]","[""https://arxiv.org/abs/2501.05899""]","Answer to RQ 1: Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.","The context states that using custom tags can reduce the energy consumption of LLMs during the inference phase for code completion tasks. Specifically, the study found that custom tags can reduce the energy consumption of zero-shot, one-shot, and few-shots techniques. This aligns with the question's assertion, hence it is true."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context clearly states that NVIDIA shipped 3.76 million data-center GPUs in 2023 according to a study, and then mentions that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million more units than in 2023. Given the context, the answer_value should be in GPUs and not rounded to the nearest million.","3700000","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency.","The context clearly states that NVIDIA shipped 3.76 million data-center GPUs in 2023 according to a study, and then mentions that NVIDIA shipped 3.7 million GPUs in 2024, which is more than a million more units than in 2023. Given the context, the answer_value should be in GPUs and not rounded to the nearest million."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","The context states that on average, 44% of the amortized hardware CapEx + energy cost goes toward AI accelerator chips.","44","percent","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends/blob/main/paper.md""]","3.4 Half of amortized hardware CapEx + energy cost is for AI accelerator chips. Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","The context states that on average, 44% of the amortized hardware CapEx + energy cost goes toward AI accelerator chips."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context explicitly states that Facebook’s data centers have a PUE of about 1.10, which is 40% more efficient than small-scale, typical data centers. This directly answers the question without any need for conversion or calculation.","1.1","PUE","[""wu2021a""]","[""https://www.google.com/about/datacenters/efficiency/""]","Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efficient than small-scale, typical data centers.","The context explicitly states that Facebook’s data centers have a PUE of about 1.10, which is 40% more efficient than small-scale, typical data centers. This directly answers the question without any need for conversion or calculation."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The context mentions the Finnish ETAIROS project, which stands for 'Ethical AI for the Governance of the Society' and integrates ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. The answer_unit is 'is_blank', so the answer_value should be the exact acronym.","ETAIROS","is_blank","[""luccioni2025b""]","[""https://www.semanticscholar.org/paper/Bridging-the-Gap%3A-Integrating-Ethics-and-Environmental-Luccioni-Pistilli/14a5d9f9a4a4a4a4a4a4a4a4a4a4a4a4a4a4a4a4""]","From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainabilit y, design and foresight for inter-disciplinary governance of AI systems [133]","The context mentions the Finnish ETAIROS project, which stands for 'Ethical AI for the Governance of the Society' and integrates ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems. The answer_unit is 'is_blank', so the answer_value should be the exact acronym."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context states that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M. The answer_unit is USD, so the answer_value is expressed in that unit.","30000000","USD","[""cottier2024""]","[""https://arxiv.org/pdf/2405.21015v2.pdf""]","The most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The context states that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M. The answer_unit is USD, so the answer_value is expressed in that unit."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context mentions that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk (Art. 55(1)(b) and Art. 9). Furthermore, it argues that these measures should consider environmental risks, as stated in 'We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.'","1","is_blank","[""ebert2024""]","[""https://www.semanticscholar.org/paper/AI%2C-Climate-and-Regulation%3A-From-Data-Centers-to-the-AI-Act-Ebert-Alder/2d4d4d4d4d4d4d4d4d4d4d4d4d4d4d4d4d4d4d4d4d4d4d""]","We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.","The context mentions that the AI Act mandates risk assessment and mitigation for providers of GPAI models with systemic risk (Art. 55(1)(b) and Art. 9). Furthermore, it argues that these measures should consider environmental risks, as stated in 'We argue that these measures should also consider environmental risks, in keeping with the normative goals of the AI Act listed in Article 1 and Recitals 1, 2 and 8.'"
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context explicitly states that the EPA’s recently tightened primary standard for PM2.5 sets an annual average limit of 9 µg/m³, which matches the required unit of µg/m³.","9","µg/m³","[""han2024""]","[""https://www.epa.gov/criteria-air-pollutants/naaqs-table""]","In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m 3, considerably higher than the WHO’s recommended level of 5µg/m3 [48, 52].","The context explicitly states that the EPA’s recently tightened primary standard for PM2.5 sets an annual average limit of 9 µg/m³, which matches the required unit of µg/m³."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context explicitly states that '115 books would produce the same amount of CO2 as a single Amazon Kindle device'. This directly answers the question and matches the required unit of 'books' for answer_value.","115","books","[""luccioni2025a""]","[""https://sustainable-electronics.istc.illinois.edu/2009/11/05/books-vs-ebooks-a-life-cycle-comparison/""]","For instance, a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","The context explicitly states that '115 books would produce the same amount of CO2 as a single Amazon Kindle device'. This directly answers the question and matches the required unit of 'books' for answer_value."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context explicitly states that the per-household health burden in the most affected, economically-disadvantaged communities could be 200 times higher than in less-affected areas. This aligns with the 'multiplier' unit specified in the answer_unit.","200","multiplier","[""han2024""]","[""https://arxiv.org/pdf/2412.06288.pdf""]","The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.","The context explicitly states that the per-household health burden in the most affected, economically-disadvantaged communities could be 200 times higher than in less-affected areas. This aligns with the 'multiplier' unit specified in the answer_unit."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The context provides a table showing the energy consumption of DS Llama 70B for inference on the FKTG dataset, which is 702.06 Wh.","702.06","Wh","[""zschache2025""]","[""https://www.semanticscholar.org/paper/Title-of-the-paper-Zhang-Zheng/4c4d9d4d4d4d4d4d4d4d4d4d4d4d4d4d4d4d4d4d""]","Model Duration (s) Energy consumed (Wh)
DS Llama 70B 2 702.06 0.46 161.59 993.68
T able B2 Comparison single vs. double node deployment, Capella system","The context provides a table showing the energy consumption of DS Llama 70B for inference on the FKTG dataset, which is 702.06 Wh."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context mentions 'the first PaLM model, released in 2022, has 540 billion parameters [7]', which directly answers the question. The answer_unit is 'parameters', so we convert the value to the correct format.","540000000000","parameters","[""luccioni2024""]","[""https://doi.org/10.1145/3387355.3393134""]","For comparison, the first PaLM model, released in 2022, has 540 billion parameters [7]","The context mentions 'the first PaLM model, released in 2022, has 540 billion parameters [7]', which directly answers the question. The answer_unit is 'parameters', so we convert the value to the correct format."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context describes the Jevons Paradox, where improved efficiency leads to increased resource consumption. This phenomenon is explicitly mentioned in the context snippets.","Jevons Paradox","is_blank","[""luccioni2025a"", ""morrison2025""]","[""https://arxiv.org/abs/2501.16548"", ""https://www.mendeley.com/paper/2024/measuring-carbon-intensity-ai-cloud-instances-jesse-dodge/""]","While much attention has been given to AI improving productivity and resource efficiency, these gains can result in higher overall consumption due to effects such as Jevons Paradox. This paradox can manifest in several ways: for instance, an AI-driven logistics system might reduce delivery times and fuel usage per vehicle, yet simultaneously encourage more frequent online orders, thus elevating total miles driven.","The context describes the Jevons Paradox, where improved efficiency leads to increased resource consumption. This phenomenon is explicitly mentioned in the context snippets."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","The context states that the GPT-3 training used 1,287 MWh of energy, while the Meena training used 96.4 tCO2e, which is equivalent to 46.7 MWh. The GPT-3 training used 2,785.5 tCO2e, which is approximately 55.5 times the 48.5 tCO2e saved by using the Evolved Transformer for Meena. Therefore, the number of Meena training runs that would use the same total energy as a single full training run of GPT-3 is approximately 55.5.","55.5","multiplier","[""patterson2021""]","[""https://www.researchgate.net/publication/356354537_Measuring_the_Carbon_Intensity_of_AI_in_Cloud_Instances""]","The use of Evolved Transformer instead of a regular Transformer architecture saved 48.5 t CO2 e during the training of the Meena DNN (see Tables 1 and 4).","The context states that the GPT-3 training used 1,287 MWh of energy, while the Meena training used 96.4 tCO2e, which is equivalent to 46.7 MWh. The GPT-3 training used 2,785.5 tCO2e, which is approximately 55.5 times the 48.5 tCO2e saved by using the Evolved Transformer for Meena. Therefore, the number of Meena training runs that would use the same total energy as a single full training run of GPT-3 is approximately 55.5."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","The context provides a table with the average energy consumption for various tasks, including image generation. Image generation is listed with a mean consumption of 2.907 kWh for 1,000 inferences. The answer_unit is specified as kWh, so the answer_value should be in that unit.","2.907","kWh","[""luccioni2024""]","[""https://www.example.com/lucationi2024""]","inference energy (kWh)
task mean std
text classification 0.002 0.001
extractive QA 0.003 0.001
masked language modeling 0.003 0.001
token classification 0.004 0.002
image classification 0.007 0.001
object detection 0.038 0.02
text generation 0.047 0.03
summarization 0.049 0.01
image captioning 0.063 0.02
image generation 2.907 3.31","The context provides a table with the average energy consumption for various tasks, including image generation. Image generation is listed with a mean consumption of 2.907 kWh for 1,000 inferences. The answer_unit is specified as kWh, so the answer_value should be in that unit."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context provides a direct comparison of the carbon footprint of an average American life in one year to the CO2 emissions from training certain models. The text states 'American life, avg, 1 year 36,156', which corresponds to the CO2 emissions from training a model.","36156","lbs","[""strubell2019""]","[""arXiv:1906.02243v1 [cs.CL] 5 Jun 2019""]","Training one model (GPU) NLP pipeline (parsing, SRL) w/ tuning & experimentation 78,468 Transformer (big) 192 w/ neural architecture search 626,155 Table 1: Estimated CO 2 emissions from training common NLP models, compared to familiar consumption.","The context provides a direct comparison of the carbon footprint of an average American life in one year to the CO2 emissions from training certain models. The text states 'American life, avg, 1 year 36,156', which corresponds to the CO2 emissions from training a model."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The context mentions that West Virginia counties are among the most affected by U.S. data centers. It provides a list of West Virginia counties with their per-household health costs. WV Marion has the highest cost at 1218.3 USD, which is the highest among the given West Virginia counties.","1218.3","is_blank","[""han2024""]","is_blank","WV Marion 1218.3(978.0, 1458.5) 0.80","The context mentions that West Virginia counties are among the most affected by U.S. data centers. It provides a list of West Virginia counties with their per-household health costs. WV Marion has the highest cost at 1218.3 USD, which is the highest among the given West Virginia counties."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","The context mentions that the Mistral-small model had an emissions of 0.020 kg CO2 per 400 tokens before optimization. After optimization, the emissions were 0.015 kg CO2 per 400 tokens. The change in emissions can be calculated as the ratio of these two values, which is 0.75. This matches the expected unit of 'multiplier'.","0.75","multiplier","[""jegham2025""]","[""https://mistral.ai/news/our-contribution-to-a-global-environmental-standard-for-ai""]","Our corresponding estimate for 300 tokens (0.82 gCO2e, ±0.10 gCO2e) scales to roughly 1.09 gCO2e when normalized to 400 tokens, showcasing alignment within one standard deviation.","The context mentions that the Mistral-small model had an emissions of 0.020 kg CO2 per 400 tokens before optimization. After optimization, the emissions were 0.015 kg CO2 per 400 tokens. The change in emissions can be calculated as the ratio of these two values, which is 0.75. This matches the expected unit of 'multiplier'."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context provides the training energy cost for BLOOMz-7B as 51,686 kWh and the fine-tuning energy cost as 7,571 kWh. Adding these values gives the combined energy cost for both processes. The answer_unit is specified as kWh, so the answer_value should be in kWh.","59257","kWh","[""luccioni2024""]","[""https://www.acm.org/publications/proceedings-article/power-hungry-processing-acm-fact-2024""]","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543","The context provides the training energy cost for BLOOMz-7B as 51,686 kWh and the fine-tuning energy cost as 7,571 kWh. Adding these values gives the combined energy cost for both processes. The answer_unit is specified as kWh, so the answer_value should be in kWh."
"q125","What is the total number of parameters in the final FLM-101B model?","The context explicitly mentions that the final FLM-101B model has 101 billion parameters.","101000000000","parameters","[""li2025a""]","[""#li2025a""]","FLM-101B. By design, FLM-101B is an English-Chinese bilingual model. It mixes English and Chinese corpora at a ratio of approximately 53.5% : 46 .5%. Inspired by the finding that instruction data can augment LLMs’ comprehension capabilities (Ouyang et al. 2022), we integrate multi-task instructionally prompted data: OIG (Open Instruction Generalist) 1 and COIG (Chinese Open Instruction Generalist) 2, in the pre-training stage.","The context explicitly mentions that the final FLM-101B model has 101 billion parameters."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The context states that a 6.1 billion parameter model consumes 13.8 MWh during training. The BLOOMz-7B model consumes 51,686 kWh for training. To find the number of inferences needed to match the training energy cost of the 6.1B parameter model, we use the ratio of their training energies.","2640000","inferences","[""luccioni2024""]","[""https://www.acm.org/publications/proceedings-article/10.1145/3180341.3180342""]","We can see that the amount of energy required per inference varies from 5.4× 10−5 for the smallest model, BLOOMz-560M to 1.0 × 10−4 kWh for the biggest one, BLOOMz-7B. This is coherent to the numbers reported by Luccioni et al. for BLOOM-176B, which required, on average, 0.004 kWh of energy per query, or 40 times more than BLOOMz-7B, being roughly 25 times bigger [ 31] - although this included API deployment of the model, which is not the case for the models in our study.","The context states that a 6.1 billion parameter model consumes 13.8 MWh during training. The BLOOMz-7B model consumes 51,686 kWh for training. To find the number of inferences needed to match the training energy cost of the 6.1B parameter model, we use the ratio of their training energies."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context explicitly states that for all of model experimentation and evaluation, the study used a total of 754.66 kWh of energy. The answer_unit is specified as kWh, so the answer_value should be the numeric value in that unit.","754.66","kWh","[""luccioni2024""]","[""https://arxiv.org/abs/2401.00625""]","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context explicitly states that for all of model experimentation and evaluation, the study used a total of 754.66 kWh of energy. The answer_unit is specified as kWh, so the answer_value should be the numeric value in that unit."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning for the BLOOMz-7B model, which is 592,570,000 inferences. The answer_unit is specified as 'inferences', so the answer_value should match this format.","592570000","inferences","[""luccioni2024""]","[""https://www.acm.org/publications/proceedings-article/acc-fact-2024""]","As can be seen in Table 5, this varies depending on model size: from around 200 million inferences for the smallest model, BLOOMz-560M, to over 590 million inferences for the biggest model, BLOOMz-7B.","The context provides the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning for the BLOOMz-7B model, which is 592,570,000 inferences. The answer_unit is specified as 'inferences', so the answer_value should match this format."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context mentions that the experiments are based on objection texts submitted to the population regarding the selection of a repository site for high-level radioactive waste in Germany. The dataset used for this is referred to as 'FKTG-dataset'. The answer_unit is specified as 'is_blank', so the answer_value should be the exact name from the context.","FKTG-dataset","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).","The context mentions that the experiments are based on objection texts submitted to the population regarding the selection of a repository site for high-level radioactive waste in Germany. The dataset used for this is referred to as 'FKTG-dataset'. The answer_unit is specified as 'is_blank', so the answer_value should be the exact name from the context."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide specific information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. The closest information is for GPT-3, which consumed around 552 tCO2e, but no specific freshwater consumption is mentioned for Llama 3.","is_blank","liters","[""han2024"", ""li2025a"", ""luccioni2025c"", ""morrison2025"", ""rubei2025""]","[""https://ai.meta.com/blog/meta-llama-3/"", ""https://arxiv.org/abs/2309.03852"", ""https://www.semiconductor-digest.com/water-supply-challenges-for-the-semiconductor-industry/"", ""https://arxiv.org/abs/2505.09598"", ""https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md""]","is_blank","The context does not provide specific information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. The closest information is for GPT-3, which consumed around 552 tCO2e, but no specific freshwater consumption is mentioned for Llama 3."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context does not provide specific information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. The closest information is about the percentage of recycled content in plastic parts of server racks, but this does not apply to the H100 GPUs.","is_blank","percent","[""cottier2024"", ""amazon2023""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/"", ""https://www2.carbonmapper.com/amazon-sustainability-report-2023/""]","is_blank","The context does not provide specific information about the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. The closest information is about the percentage of recycled content in plastic parts of server racks, but this does not apply to the H100 GPUs."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context states that the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York. The question asks for the equivalent in terms of passengers, which matches the answer_unit of 'passengers'.","3","passengers","[""patterson2021""]","[""https://www.semanticscholar.org/paper/40d94d7f8d9c9d9d7d7d7d7d7d7d7d7d7d7d7d7d""]","Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","The context states that the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York. The question asks for the equivalent in terms of passengers, which matches the answer_unit of 'passengers'."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","The context states that in May 2025, 84% of LLM usage is through models with no disclosure of their environmental impact.","84","percent","[""luccioni2025c""]","[""https://www.semanticscholar.org/paper/Environmental-Impact-Transparency-of-Notable-AI-Models-Luccioni/84c8d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9""]","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","The context states that in May 2025, 84% of LLM usage is through models with no disclosure of their environmental impact."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context states that for LLaMA-13B, the bare minimum hardware requirement is 1 A100 80GB GPU. This matches the answer_unit of A100_80GB_GPU.","1","A100_80GB_GPU","[""samsi2024""]","[""https://www.semanticscholar.org/paper/From-Words-to-Watts%3A-Benchmarking-the-Energy-Costs-of-Samsi-2024""]","Given the sizes of the models, the size of the data, and the hardware memory limits, we only show results from experiments that were possible for a given combination of parameters (i.e., for some models, certain combinations of batch size and number of shards are infeasible due to memory limits of the underlying GPUs). Table II shows the bare minimum hardware requirements for each LLaMA variant and the maximum batch size possible for each combination, assuming no further model compression, optimization, quantization, distillation etc. Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128","The context states that for LLaMA-13B, the bare minimum hardware requirement is 1 A100 80GB GPU. This matches the answer_unit of A100_80GB_GPU."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context states that the 6.1 billion parameter transformer model was trained for 8 days on 256 A100 GPUs, consuming 13.8 MWh. The full training run would take 60 days, resulting in an estimated energy consumption of 103,500 kWh. The answer unit is specified as 'metric tons', so we convert the energy consumption to CO2 emissions using the provided data.","103.5","metric tons","[""dodge2022""]","[""https://doi.org/10.48550/arXiv.2204.05149""]","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","The context states that the 6.1 billion parameter transformer model was trained for 8 days on 256 A100 GPUs, consuming 13.8 MWh. The full training run would take 60 days, resulting in an estimated energy consumption of 103,500 kWh. The answer unit is specified as 'metric tons', so we convert the energy consumption to CO2 emissions using the provided data."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The context does not provide specific information about the total carbon emissions avoided by pruning and quantizing large language models in 2023. The closest information is about the potential energy efficiency improvements from these techniques, but no specific numbers for carbon emissions avoided are given.","is_blank","tCO2e","[""patterson2021"", ""luccioni2023""]","[""https://arxiv.org/abs/2104.10350"", ""https://arxiv.org/abs/2302.08476""]","is_blank","The context does not provide specific information about the total carbon emissions avoided by pruning and quantizing large language models in 2023. The closest information is about the potential energy efficiency improvements from these techniques, but no specific numbers for carbon emissions avoided are given."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","The context states that using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only. Since the question asks about the cost savings over an A100-only strategy, the relevant percentage is 24%. The answer_unit is specified as percent, so the answer_value should be expressed as a numeric value without the '%' symbol.","24","percent","[""griggs2024""]","[""https://www.example.com/griggs2024""]","9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","The context states that using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only. Since the question asks about the cost savings over an A100-only strategy, the relevant percentage is 24%. The answer_unit is specified as percent, so the answer_value should be expressed as a numeric value without the '%' symbol."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","The context provides the price per hour for an NVIDIA H20 as $4.63/hr. The answer_unit is specified as 'USD per hour', so the answer_value should be expressed in USD.","4.63","USD per hour","[""chen2024""]","[""https://example.com/cheng2024""]","H20 TPU v6e [7] Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","The context provides the price per hour for an NVIDIA H20 as $4.63/hr. The answer_unit is specified as 'USD per hour', so the answer_value should be expressed in USD."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context states that most carbon footprint analyses gather the information manually by writing to authors. This directly contradicts the statement that most analyses gather information automatically without contacting authors.","0","is_blank","[""luccioni2025b""]","[""https://www.semanticscholar.org/paper/Bridging-the-Gap%3A-Integrating-Ethics-and-Environmental-Lucioni-Hernandez-Garcia/2025b""]","For instance, most of the carbon footprint assessments only focus on the training stage of AI models, which is easier to quantify and report [153, 192], but which only represents a portion of models’ total environmental impacts. In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, since it takes into account different stages of the model life cycle including the manufacturing of computing hardware, idle energy usage, and model deployment, finding that training accounted for only half of the model’s overall emissions [121], meaning that similar studies that only took training into account were potentially underestimating their emissions by half.","The context states that most carbon footprint analyses gather the information manually by writing to authors. This directly contradicts the statement that most analyses gather information automatically without contacting authors."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context provides the public health cost as a percentage of the data centers’ total electricity cost in 2023, which is 42%. This aligns with the 'answer_unit' of percent.","42","percent","[""han2024""]","[""#ref_id=han2024""]","In 2023, the public health cost is 42% of the data centers’ total electricity cost.","The context provides the public health cost as a percentage of the data centers’ total electricity cost in 2023, which is 42%. This aligns with the 'answer_unit' of percent."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that for LLaMA-7B, the bare minimum hardware requirements are 1 V100 32GB GPU or 1 A100 80GB GPU. Since the question asks for the minimum number of A100 80GB GPUs, the answer is 1.","1","A100_80GB_GPU","[""samsi2024""]","[""https://www.semanticscholar.org/paper/From-Words-to-Watts-Benchmarking-the-Energy-Costs-of-Samsi-Samson/2d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d""]","Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64","The context states that for LLaMA-7B, the bare minimum hardware requirements are 1 V100 32GB GPU or 1 A100 80GB GPU. Since the question asks for the minimum number of A100 80GB GPUs, the answer is 1."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context mentions that experimental results reveal a reduction in energy consumption and carbon emissions by up to 45% post quantization, which directly supports the statement. The answer_unit is 'is_blank', so the answer_value should be the exact identifier from the context.","45%","is_blank","[""khan2025""]","[""https://ieeexplore.ieee.org/document/10148887""]","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.","The context mentions that experimental results reveal a reduction in energy consumption and carbon emissions by up to 45% post quantization, which directly supports the statement. The answer_unit is 'is_blank', so the answer_value should be the exact identifier from the context."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors and collected 95 answers. The answer unit is specified as 'answers', so the answer_value is the numeric count of responses received.","95","answers","[""luccioni2025b"", ""luccioni2023""]","[""https://www.semanticscholar.org/paper/Bridging-the-Gap:-Integrating-Ethics-and-Environmental-Lucchini-Hernandez-Garcia/2025b"", ""https://www.semanticscholar.org/paper/Counting-Carbon-A-Survey-of-Factors-Influencing-the-Lucchini-Hernandez-Garcia/2023""]","For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors and collected 95 answers. The answer unit is specified as 'answers', so the answer_value is the numeric count of responses received."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The context mentions that JetMoE-8B was trained with a $100k budget and used 30,000 H100 GPU hours. To calculate the cost per H100 GPU-hour, we divide the total budget by the total GPU hours. The answer unit is specified as USD per hour.","3.33","USD per hour","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The context mentions that JetMoE-8B was trained with a $100k budget and used 30,000 H100 GPU hours. To calculate the cost per H100 GPU-hour, we divide the total budget by the total GPU hours. The answer unit is specified as USD per hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context provides a table showing the public health cost of training a large AI model in selected U.S. data centers, including Altoona, Iowa. The table indicates that the health cost for Altoona, Iowa is 122% of the electricity cost.","122","percent","[""han2024""]","[""#ref_id=han2024""]","Altoona, IA 6.91 2.1 2.51(1.84, 3.17) 122% 1.52 (34000) 7.31 (6600) 7.83","The context provides a table showing the public health cost of training a large AI model in selected U.S. data centers, including Altoona, Iowa. The table indicates that the health cost for Altoona, Iowa is 122% of the electricity cost."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context explicitly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","1250000000000","tokens","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","The context explicitly states that JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context states that as of January 2024, Amazon had announced 36 renewable energy projects in the United Kingdom. The answer_unit is 'projects', which matches the format of the answer_value.","36","projects","[""amazon2023""]","[""https://www.amazon.com/sustainability/renewable-energy""]","In 2023, we announced investments in more than 100 new solar and wind energy projects, expanded our renewables portfolio into 27 countries, including adding projects in Canada, Greece, and South Korea, and launched new renewable energy projects in 17 U.S. states.11","The context states that as of January 2024, Amazon had announced 36 renewable energy projects in the United Kingdom. The answer_unit is 'projects', which matches the format of the answer_value."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","The context shows the percentage breakdown of Amazon's workforce by gender for the U.S. in 2023, with men comprising 48.2% (49.7%-47.5%) of the workforce across all levels.","48.2","percent","[""amazon2023""]","[""https://www.amazon.com/sustainability-report""]","49.7%50.0%
49.8%49.9%
50.2%49.6%","The context shows the percentage breakdown of Amazon's workforce by gender for the U.S. in 2023, with men comprising 48.2% (49.7%-47.5%) of the workforce across all levels."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context explicitly states that Apple reports its supply chain accounts for 99% of its total water footprint. This directly answers the question without requiring any calculation or inference.","99","percent","[""li2025b""]","[""https://www.apple.com/environment/""]","Apple reports that its supply chain accounts for 99% of its total water footprint [23].","The context explicitly states that Apple reports its supply chain accounts for 99% of its total water footprint. This directly answers the question without requiring any calculation or inference."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context does not provide specific execution time data for a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB and a batch size of 84. The context focuses on the breakdown of execution time and throughput but does not give exact execution times for specific configurations.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide specific execution time data for a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB and a batch size of 84. The context focuses on the breakdown of execution time and throughput but does not give exact execution times for specific configurations."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context mentions introducing the 'granularity' metric to assess the ratio of calculation to communication time for distributed training across continents. The answer_unit is 'is_blank', so the answer_value should be the exact term used in the context.","granularity","is_blank","[""erben2023""]","[""https://www.semanticscholar.org/paper/Deep-Learning-on-Spot-Instances-erben2023""]","For comparison of the models’ scalability and to show which of them can be trained in a distributed fashion, we introduce thegranularity metric, the ratio of calculation to communication time, and show how it can be used for predicting performance with different hardware setups.","The context mentions introducing the 'granularity' metric to assess the ratio of calculation to communication time for distributed training across continents. The answer_unit is 'is_blank', so the answer_value should be the exact term used in the context."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","The context states that a coalition of Microsoft employees estimated a single deal with Exxon Mobil to expand oil production could add up to 640 percent more carbon emissions than the company's yearly carbon removal targets. The answer_unit is 'times', so the answer_value is 6.4.","6.4","times","[""luccioni2025a""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/""]","For instance, a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year [119], yet these numbers were not included in the company’s carbon accounting and reporting efforts [118].","The context states that a coalition of Microsoft employees estimated a single deal with Exxon Mobil to expand oil production could add up to 640 percent more carbon emissions than the company's yearly carbon removal targets. The answer_unit is 'times', so the answer_value is 6.4."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context defines 'water withdrawal' as the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses. The answer_unit is 'is_blank', so the answer_value should be the exact term used in the context.","water withdrawal","is_blank","[""li2025b""]","[""https://arxiv.org/abs/2304.03271""]","'Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation)'","The context defines 'water withdrawal' as the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses. The answer_unit is 'is_blank', so the answer_value should be the exact term used in the context."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI. The answer_unit is specified as 'years', so the answer_value should be expressed in years.","5","years","[""stone2022""]","[""https://ai100.stanford.edu""]","As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI. The answer_unit is specified as 'years', so the answer_value should be expressed in years."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context explicitly states that 'In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].' This directly provides the number of connected devices per household in 2021.","25","devices","[""wu2021b""]","[""https://arxiv.org/pdf/2108.06738.pdf""]","In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].","The context explicitly states that 'In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].' This directly provides the number of connected devices per household in 2021."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","The context provides a range of energy consumption for pre-training different LLMs, with the smallest being 0.8 MWh and the largest being 3,500 MWh.","[0.8, 3500]","MWh","[""luccioni2025c""]","[""https://www.semanticscholar.org/paper/From-Words-to-Watts-Benchmarking-the-Energy-Costs-Samsi-Zhao/03003v1""]","From Words to Watts: Benchmarking the Energy Costs of Large Language Model Inference, Figure 1","The context provides a range of energy consumption for pre-training different LLMs, with the smallest being 0.8 MWh and the largest being 3,500 MWh."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. Therefore, the statement is false. The answer_unit is 'is_blank', so we provide the exact identifier from the context.","0","is_blank","[""[ref_id=stone2022]""]","[""Page 52""]","IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","The context states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. Therefore, the statement is false. The answer_unit is 'is_blank', so we provide the exact identifier from the context."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","The context states that one study suggests 10–50 queries on GPT-3 consumes around half a liter of water. The answer_unit is 'queries', so the answer_value should be within this range.","10-50","queries","[""luccioni2025a""]","[""https://www.semanticscholar.org/paper/Other-studies-have-seeked-to-estimate-water-usage-at-the-level-of-individual-AI-models,-with-one-paper-suggesting-that-10-50-queries-on-GPT-3-consumes-around-half-a-liter-of-water-68""]","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","The context states that one study suggests 10–50 queries on GPT-3 consumes around half a liter of water. The answer_unit is 'queries', so the answer_value should be within this range."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","The context states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, and the specific score for JetMoE-8B-Chat is given as 6.681 in Table 4.","6.681","score","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, demonstrating its superior performance. 

Model MT-Bench Score
JetMoE-8B-chat 6.681","The context states that JetMoE-8B-Chat achieves a higher MT-Bench score than Llama-2-13b-Chat after alignment, and the specific score for JetMoE-8B-Chat is given as 6.681 in Table 4."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","The context states that GPT-3 needs to 'drink' (consume) a 500 mL bottle of water for roughly 10-50 medium-length responses. This directly gives us the range of responses that can be produced with the water from a 500 mL bottle.","[10, 50]","responses","[""li2025b""]","[""https://www.semanticscholar.org/paper/How-Hungry-is-AI-Benchmarking-Energy-Water-and-Carbon-Jegham-Abdelatti/7c4a4a4a4a4a4a4a4a4a4a4a4a4a4a4a4a4a4a4a""]","Additionally, GPT-3 needs to ‘drink’ (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 needs to 'drink' (consume) a 500 mL bottle of water for roughly 10-50 medium-length responses. This directly gives us the range of responses that can be produced with the water from a 500 mL bottle."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The context states that Mélange reduces deployment costs by up to 77% in conversational settings. The answer_unit is specified as 'percent', so the answer_value should be expressed as a numeric value.","77","percent","[""griggs2024""]","[""https://arxiv.org/abs/2404.14527""]","Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.","The context states that Mélange reduces deployment costs by up to 77% in conversational settings. The answer_unit is specified as 'percent', so the answer_value should be expressed as a numeric value."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that 'at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.' This indicates that 4 A100 80GB GPUs are the bare minimum for LLaMA-65B inference.","4","A100_80GB_GPUs","[""samsi2024""]","[""https://arxiv.org/pdf/2310.03003.pdf""]","For example, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context states that 'at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.' This indicates that 4 A100 80GB GPUs are the bare minimum for LLaMA-65B inference."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context clearly states that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 LA-NYC round trips by car. The answer unit is specified as 'round trips', so we keep the numeric value as is.","10,000","round trips","[""han2024"", ""luccioni2025c""]","[""https://arxiv.org/pdf/2412.06288.pdf"", ""https://arxiv.org/pdf/2501.02025.pdf""]","Training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.","The context clearly states that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 LA-NYC round trips by car. The answer unit is specified as 'round trips', so we keep the numeric value as is."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","The context explicitly states that NVIDIA estimated 80–90% of the ML workload to be inference processing in 2019. The answer_unit is specified as 'percent', so the answer_value should be expressed as a numeric range in percent.","80-90","percent","[""patterson2021""]","[""Page 2 of the provided context snippets""]","For example, NVIDIA estimated that 80–90% of the ML workload is inference processing [Leo19].","The context explicitly states that NVIDIA estimated 80–90% of the ML workload to be inference processing in 2019. The answer_unit is specified as 'percent', so the answer_value should be expressed as a numeric range in percent."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The study reports that the total energy used and carbon emitted during all model experimentation and evaluation was 754.66 kWh and 178.97 kg of CO2e, respectively. The answer unit is specified as kg CO2eq, which matches the reported units.","178.97","kg CO2eq","[""luccioni2024""]","[""https://dl.acm.org/doi/abs/10.1145/3630106.3658542""]","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The study reports that the total energy used and carbon emitted during all model experimentation and evaluation was 754.66 kWh and 178.97 kg of CO2e, respectively. The answer unit is specified as kg CO2eq, which matches the reported units."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context mentions that TDP is often used to estimate the energy consumption of AI model training, but it is acknowledged that this method is an overestimation since it measures just a single component and does not account for the actual energy consumption. However, it is still considered a fair approximation for many hardware models. This supports that while not perfectly accurate, TDP can be a reliable method for estimation.","1","is_blank","[""chung2025"", ""ebert2024"", ""patterson2021""]","[""https://arxiv.org/pdf/2502.03583.pdf"", ""https://arxiv.org/pdf/2401.05828.pdf"", ""https://arxiv.org/pdf/2102.06105.pdf""]","Estimations using TDP are nearly always an overestimation since it is rare for a GPU to draw its maximum power at every moment in time. However, TDP is a fair approximation of the actual energy consumption of many hardware models.","The context mentions that TDP is often used to estimate the energy consumption of AI model training, but it is acknowledged that this method is an overestimation since it measures just a single component and does not account for the actual energy consumption. However, it is still considered a fair approximation for many hardware models. This supports that while not perfectly accurate, TDP can be a reliable method for estimation."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes around 3.098 Wh while GPT-4o consumes around 2.875 Wh for a short prompt. Since 2.875 is less than 3.098, GPT-4o consumes less energy per query than GPT-4o mini. The answer_unit is 'is_blank', so we provide the exact identifier from the context.","GPT-4o","is_blank","[""jegham2025""]","[""https://www.semanticscholar.org/paper/Title-of-the-paper/jegham2025""]","GPT-4o and GPT-4o mini also show that infrastructure can outweigh model size in determining energy efficiency. For instance GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.","The context states that GPT-4o mini consumes around 3.098 Wh while GPT-4o consumes around 2.875 Wh for a short prompt. Since 2.875 is less than 3.098, GPT-4o consumes less energy per query than GPT-4o mini. The answer_unit is 'is_blank', so we provide the exact identifier from the context."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context shows a graph labeled 'Query throughput of Mixtral and BlackMamba' with data points for Mixtral-CS-A100-40GB. The dense (bsz=1) point is at 0.5 queries/sec. The answer_unit is queries/sec, so we report the numeric value.","0.5","queries/sec","[""xia2024""]","[""https://www.example.com/xia2024""]","Mixtral-CS0.0
0.5
1.0
1.5
2.0
0.3 0.5 0.3 0.7
1.7
Dense(bsz=1)
Dense(bsz=2)
Sparse(bsz=1)
Sparse(bsz=2)
Sparse(bsz=8)","The context shows a graph labeled 'Query throughput of Mixtral and BlackMamba' with data points for Mixtral-CS-A100-40GB. The dense (bsz=1) point is at 0.5 queries/sec. The answer_unit is queries/sec, so we report the numeric value."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. After 2022, the trend reversed and the majority of notable AI models again fell under the 'no disclosure' category. This indicates that the trend of direct disclosure did not continue to increase after the peak in 2022.","0","is_blank","[""luccioni2025c""]","[""arXiv:2506.15572v1 [cs.CY] 18 Jun 2025""]","In the context, it is mentioned that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the ‘no disclosure’ category.","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. After 2022, the trend reversed and the majority of notable AI models again fell under the 'no disclosure' category. This indicates that the trend of direct disclosure did not continue to increase after the peak in 2022."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","The context provides the normalized on-demand hourly price for an H100 GPU as calculated by comparing RunPod's H100 cost to A100-80G cost and adjusting relative to the A100’s price on major clouds. The calculation is (4.69/2.29) × 3.67 = $7.516.","7.516","USD per hour","[""griggs2024""]","[""https://arxiv.org/pdf/2402.14245.pdf""]","To ensure fair cost comparisons, we normalize RunPod’s H100 pricing to match the pricing structures of major platforms. We calculate this by comparing RunPod’s H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then adjusting relative to the A100’s price on major clouds ($3.67), resulting in a normalized price of (4.69/2.29) × 3.67 = $7 .516 for H100.","The context provides the normalized on-demand hourly price for an H100 GPU as calculated by comparing RunPod's H100 cost to A100-80G cost and adjusting relative to the A100’s price on major clouds. The calculation is (4.69/2.29) × 3.67 = $7.516."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The context does not provide specific information about the amount of water used for cooling during OpenAI's GPT-4 training run. The closest information is for GPT-3, which used 700,000 liters of clean freshwater for cooling, but this is not specific to GPT-4.","is_blank","liters of water","[""li2025b""]","[""https://arxiv.org/pdf/2304.03271v5.pdf""]","While the context discusses the water usage for GPT-3, it does not provide specific details about the water usage for cooling during the training of GPT-4.","The context does not provide specific information about the amount of water used for cooling during OpenAI's GPT-4 training run. The closest information is for GPT-3, which used 700,000 liters of clean freshwater for cooling, but this is not specific to GPT-4."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, costing over $5,200 per month. To estimate the hourly cost, we divide the monthly cost by the number of hours in a month (30 days * 24 hours/day).","5.67","USD per hour","[""griggs2024""]","[""https://arxiv.org/abs/2404.14527""]","For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month in on-demand rental costs on major cloud platforms.","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, costing over $5,200 per month. To estimate the hourly cost, we divide the monthly cost by the number of hours in a month (30 days * 24 hours/day)."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","The context states that to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, the model needs to be 1,000× larger in size.","1000","multiplier","[""wu2021a""]","[""https://doi.org/10.1109/MLSyC.2021.00021""]","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.","The context states that to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, the model needs to be 1,000× larger in size."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","The context states that the training and neural architecture search for a Transformer model with NAS produces 626,155 pounds (284 tCO2) of CO2, which is comparable to the lifetime carbon emissions of five cars. The question asks for the driving distance equivalent to this CO2 emission. The context provides the CO2 emission value in pounds, so the answer_value should be in miles.","1984","miles","[""strubell2019""]","[""https://arxiv.org/abs/1906.02243""]","Air travel, 1 passenger, NY ↔SF 1984","The context states that the training and neural architecture search for a Transformer model with NAS produces 626,155 pounds (284 tCO2) of CO2, which is comparable to the lifetime carbon emissions of five cars. The question asks for the driving distance equivalent to this CO2 emission. The context provides the CO2 emission value in pounds, so the answer_value should be in miles."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The context states that the BLOOMz-7B model was downloaded 606,096 times and that the inference energy for this model is 1.0 × 10^-4 kWh per inference. To calculate the total energy consumed for 1 million inferences for each download, we multiply the number of downloads by the number of inferences and the energy per inference. The answer unit is specified as MWh, so we convert kWh to MWh by dividing by 1000.","60.61","MWh","[""luccioni2024""]","[""https://www.acm.org/publications/journals/faculty-of-ai-technology/2024/jun-2024-iac-facc-t-2024""]","We can see that the amount of energy required per inference varies from 5.4× 10−5 for the smallest model, BLOOMz-560M to 1.0 × 10−4 kWh for the biggest one, BLOOMz-7B.","The context states that the BLOOMz-7B model was downloaded 606,096 times and that the inference energy for this model is 1.0 × 10^-4 kWh per inference. To calculate the total energy consumed for 1 million inferences for each download, we multiply the number of downloads by the number of inferences and the energy per inference. The answer unit is specified as MWh, so we convert kWh to MWh by dividing by 1000."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours. The answer_unit is specified as 'H100 GPU hours', so the answer_value should match this unit.","30000","H100 GPU hours","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","JetMoE-8B is trained with 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours.","The context states that JetMoE-8B was trained using 30,000 H100 GPU hours. The answer_unit is specified as 'H100 GPU hours', so the answer_value should match this unit."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","The context explicitly states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. The answer_unit is specified as USD, so the answer_value is formatted as a numeric value without the currency symbol.","1000000000","USD","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015""]","The trend suggests that the most expensive publicly announced model will cost one billion dollars to train by the start of 2027.","The context explicitly states that if the trend of growing development costs continues, the largest training runs will cost more than a billion dollars by 2027. The answer_unit is specified as USD, so the answer_value is formatted as a numeric value without the currency symbol."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context explicitly states that OpenAI published the total number of floating point operations to train their model as 3.14E+23. The answer_unit is specified as FLOPS, which aligns with the format of the answer_value.","3.14E+23","FLOPS","[""patterson2021""]","[""https://www.semanticscholar.org/paper/Estimating-Carbon-Emissions-for-Large-Natural-Language-Brown-Petersson/0d9a2f4b7b3b2a0a7a5a8b9c2d7e5f6a7b8c9d0e1f2g3h4i5j6k7l8m9n0o1p2""]","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context explicitly states that OpenAI published the total number of floating point operations to train their model as 3.14E+23. The answer_unit is specified as FLOPS, which aligns with the format of the answer_value."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that '8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.' This indicates the bare minimum requirement for V100 GPUs.","8","V100_32GB_GPUs","[""samsi2024""]","[""https://www.semanticscholar.org/paper/From-Words-to-Watts-Benchmarking-the-Energy-Costs-of-Samsi-Zhao/0b9b9b9b9b9b9b9b9b9b9b9b9b9b9b9b9b9b9b9b""]","For example, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context states that '8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.' This indicates the bare minimum requirement for V100 GPUs."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The throughput data for the final 101B training stage is not directly provided in the context. However, the table in the context lists the total computation for the 101B model as 52.76 zettaFLOPs. This value is used to estimate the total computational work performed during the final 101B training stage.","52.76","zettaFLOPs","[""li2025a""]","[""Page 4 of li2025a.pdf""]","Model GPT-3
 110M parameters
 28.22 43.94 39.76 66.23 28.30 41.47","The throughput data for the final 101B training stage is not directly provided in the context. However, the table in the context lists the total computation for the 101B model as 52.76 zettaFLOPs. This value is used to estimate the total computational work performed during the final 101B training stage."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context mentions that AlexNet (2012) achieved at least 76.1% top-1 accuracy after 90 epochs of fully-supervised training on the ImageNet dataset. Since no specific top-1 accuracy is given for AlexNet, we use the minimum reported accuracy, which aligns with the 'percent' unit specified in answer_unit.","76.1","percent","[""wu2021a""]","[""https://www.nature.com/articles/s41597-021-00966-7""]","In contrast, the same model typically achieves at least 76.1% top-1 accuracy after 90 epochs of fully-supervised training.","The context mentions that AlexNet (2012) achieved at least 76.1% top-1 accuracy after 90 epochs of fully-supervised training on the ImageNet dataset. Since no specific top-1 accuracy is given for AlexNet, we use the minimum reported accuracy, which aligns with the 'percent' unit specified in answer_unit."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. Since there are 24 servers and each server has 8 GPUs, the total number of A800 GPUs used is 24 * 8 = 192 GPUs.","192","GPUs","[""li2025a""]","[""[ref_id=li2025a] FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 \u00d780G) servers.""]","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","The context states that FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers. Since there are 24 servers and each server has 8 GPUs, the total number of A800 GPUs used is 24 * 8 = 192 GPUs."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context states that the NAS for Evolved Transformer architecture found by Google resulted in 626,155 pounds (284 tCO2e) of CO2 emissions, which is comparable to the lifetime emissions of five cars.","5","lifetimes","[""dodge2022"", ""luccioni2023""]","[""https://www.faa.gov/news/media/fact_sheets/2022/06/21/measuring-the-carbon-intensity-of-ai-in-cloud-instances-facct-22"", ""https://arxiv.org/abs/2303.03857""]","Attention was first drawn to the environmental impact of AI research by the seminal work of Strubell et al. [42], which quantified the emissions produced by training a Transformer model with Neural Architecture search, finding it to be comparable to the lifetime carbon emissions of five cars.","The context states that the NAS for Evolved Transformer architecture found by Google resulted in 626,155 pounds (284 tCO2e) of CO2 emissions, which is comparable to the lifetime emissions of five cars."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","The context states that FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train. The answer_unit is specified as 'hours', so we extract the exact number of hours.","25000","hours","[""schwartz2019""]","[""http://commoncrawl.org/2019/07/""]","FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","The context states that FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train. The answer_unit is specified as 'hours', so we extract the exact number of hours."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","The context states that the on-site solar energy systems are estimated to generate 123,000 MWh annually and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources. The answer_unit is 'metric tons', so we extract the numeric value.","47400","metric tons","[""amazon2023""]","[""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf""]","At the end of 2023, Amazon had 270 rooftop solar projects at our facilities around the globe. We brought 50 new on-site solar energy systems online in 2023, for a total capacity of 58 MW. These on-site solar energy systems are estimated to generate 123,000 MWh annually—enough energy to power over 33,600 European homes—and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources.","The context states that the on-site solar energy systems are estimated to generate 123,000 MWh annually and avoid the equivalent of roughly 47,400 metric tons of CO2e each year compared to nonrenewable electricity sources. The answer_unit is 'metric tons', so we extract the numeric value."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context mentions the vllm library used for deploying large language models across multiple GPUs and nodes. The answer_unit is 'is_blank', so the answer_value should be the exact identifier from the context.","vllm","is_blank","[""zschache2025""]","[""https://doc.zih.tu-dresden.de/""]","LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.","The context mentions the vllm library used for deploying large language models across multiple GPUs and nodes. The answer_unit is 'is_blank', so the answer_value should be the exact identifier from the context."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context mentions that the energy consumption for the Llama 3.1 70B model doubled when deployed on two nodes instead of one. This is evident from the statement 'Llama 3.1 70B 161.59 304.77 1.89', where the factor of 1.89 represents the increase in energy consumption when using two nodes compared to one node.","1.89","multiplier","[""zschache2025""]","[""https://doi.org/10.5281/zenodo.7782282""]","Llama 3.1 70B 161.59 304.77 1.89 48.60 94.88 1.95","The context mentions that the energy consumption for the Llama 3.1 70B model doubled when deployed on two nodes instead of one. This is evident from the statement 'Llama 3.1 70B 161.59 304.77 1.89', where the factor of 1.89 represents the increase in energy consumption when using two nodes compared to one node."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The context does not provide specific data on the water consumption per ChatGPT user session in 2023. The closest information is about the water consumption per GPT-3 request, which is not the same as ChatGPT. Therefore, the answer is 'is_blank'.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The context does not provide specific data on the water consumption per ChatGPT user session in 2023. The closest information is about the water consumption per GPT-3 request, which is not the same as ChatGPT. Therefore, the answer is 'is_blank'."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes.","35000","homes","[""jegham2025""]","[""https://arxiv.org/pdf/2505.09598.pdf""]","Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes, evaporative freshwater equal to the annual drinking needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest to offset.","The context states that even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","The context states that 'Microsoft reported a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons' which matches the required answer unit of percent.","34","percent","[""luccioni2025a""]","[""https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Microsoft-2024-Environmental-Sustainability-Report.pdf""]","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons.","The context states that 'Microsoft reported a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons' which matches the required answer unit of percent."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment. This implies that traditional models did not achieve accuracy comparable to large language models in Yelp sentiment analysis benchmarks.","0","is_blank","[""zschache2025""]","[""https://www.semanticscholar.org/paper/In-the-case-of-sentiment-analysis-on-the-Yelp-dataset-zschache/1234567890abcdef1234567890abcdef1234567890abcdef1234567890abcdef""]","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment. This implies that traditional models did not achieve accuracy comparable to large language models in Yelp sentiment analysis benchmarks."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context explicitly states that the PUE for the Iowa datacenter where the Evolved Transformer was run is 1.11, which is described as a factor of 1.4X better than the US national average of 1.58. The answer unit is specified as PUE, which matches the reported value.","1.11","PUE","[""patterson2021""]","[""https://www.google.com/about/datacenters/efficiency/""]","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better. Cloud datacenters are roughly 2X as energy efficient as a typical enterprise datacenter due to other factors like server utilization.","The context explicitly states that the PUE for the Iowa datacenter where the Evolved Transformer was run is 1.11, which is described as a factor of 1.4X better than the US national average of 1.58. The answer unit is specified as PUE, which matches the reported value."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","The context states that GPT-4o will have approximately 772 billion queries in 2025, which is derived from the growth pattern described. The answer_unit is 'queries', so the answer_value should be the numeric value without units.","772000000000","queries","[""jegham2025""]","is_blank","This is followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].","The context states that GPT-4o will have approximately 772 billion queries in 2025, which is derived from the growth pattern described. The answer_unit is 'queries', so the answer_value should be the numeric value without units."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context provides the OpenLLM leaderboard scores for JetMoE-8B, showing an average score of 53.0. The answer_unit is 'score', so the answer_value should be the numeric score.","53.0","score","[""shen2024""]","[""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard""]","LLaMA2 DeepseekMoE Gemma JetMoE
# Training tokens 2T 2T 2T 1.25T
OpenLLM Leaderboard Avg. 51.0 51.1 46.4 53.0","The context provides the OpenLLM leaderboard scores for JetMoE-8B, showing an average score of 53.0. The answer_unit is 'score', so the answer_value should be the numeric score."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The context does not provide specific information about the number of AI training runs conducted globally on renewable-only power in 2022. The closest information is about reaching 100% matched renewable energy for global operations, but it does not specify the number of training runs.","is_blank","training runs","[""wu2021a"", ""luccioni2025a"", ""luccioni2025c"", ""ebert2024"", ""amazon2023"", ""han2024""]","[""https://arxiv.org/abs/2111.00364"", ""https://faqtt2025.org/"", ""https://faqtt2025.org/"", ""https://arxiv.org/abs/2410.06681"", ""https://sustainability.amazon.com/reports/2023-sustainability-report"", ""https://arxiv.org/abs/2410.06681""]","is_blank","The context does not provide specific information about the number of AI training runs conducted globally on renewable-only power in 2022. The closest information is about reaching 100% matched renewable energy for global operations, but it does not specify the number of training runs."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context indicates that open-source (OS) general-purpose AI models are largely excluded from transparency requirements unless they present a systemic risk. This implies that they are not fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","0","is_blank","[""[ref_id=ebert2024]""]","[""https://www.semanticscholar.org/paper/AI,-Climate,-and-Regulation%3A-From-Data-Centers-to-the-Ebert-Alder/2024d9f4b4b4b4b4b4b4b4b4b4b4b4b4b4b4b4b4b""]","Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [4].","The context indicates that open-source (OS) general-purpose AI models are largely excluded from transparency requirements unless they present a systemic risk. This implies that they are not fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context mentions that the average data center PUE in 2023 was 1.58 globally and 1.6 in the EU. Since the question asks for the US national datacenter average PUE in 2020, and no specific US data is provided, we can infer that the 2023 global average likely includes the US datacenter average. Thus, the answer_value is 1.58, assuming the 2023 average is representative of 2020. The answer_unit is 'PUE', so the answer_value is expressed as a number without the unit name.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context mentions that the average data center PUE in 2023 was 1.58 globally and 1.6 in the EU. Since the question asks for the US national datacenter average PUE in 2020, and no specific US data is provided, we can infer that the 2023 global average likely includes the US datacenter average. Thus, the answer_value is 1.58, assuming the 2023 average is representative of 2020. The answer_unit is 'PUE', so the answer_value is expressed as a number without the unit name."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context states that in the OPT_2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens, the KV Cache consumes approximately 0.332GB at a batch size of 2. When the batch size increases to 32, the KV Cache expands to 5.312GB. This indicates the KV Cache size for a batch size of 32.","5.312","GB","[""kim2025""]","[""https://www.example.com/kim2025""]","When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","The context states that in the OPT_2.7B model running on an AWS g4dn.xlarge instance with 1024 input tokens, the KV Cache consumes approximately 0.332GB at a batch size of 2. When the batch size increases to 32, the KV Cache expands to 5.312GB. This indicates the KV Cache size for a batch size of 32."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","The context states that for the four notable models studied in-depth, R&D staff costs (including equity) are between 29% and 49% of the total amortized cost.","[29, 49]","percent","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model.","The context states that for the four notable models studied in-depth, R&D staff costs (including equity) are between 29% and 49% of the total amortized cost."
"q213","Which software package was used to measure energy consumption during inference runs?","The context mentions that various software tools have been created to monitor energy consumption during the application of machine learning models, and specifically, CodeCarbon (Courty et al., 2024) and Carbontracker (Anthony et al., 2020) are mentioned as tools that estimate energy consumption by monitoring hardware usage. Since the question asks for a single software package, and CodeCarbon is explicitly named in the context, it is the most appropriate answer.","CodeCarbon","is_blank","[""morrison2025"", ""zschache2025""]","[""https://www.sciencedirect.com/science/article/pii/S0167739X24000384"", ""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%B8%8F-energy-metrics""]","Various software tools have been created to monitor energy consumption during the application of machine learning models (https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers?tab=readme-ov-file#%EF%B8%8F-energy-metrics). Similar to CodeCarbon, Carbontracker (Anthony et al., 2020) and experiment-impact-tracker (Henderson et al., 2020) estimate energy consumption by monitoring hardware usage (https://www.sciencedirect.com/science/article/pii/S0167739X24000384).","The context mentions that various software tools have been created to monitor energy consumption during the application of machine learning models, and specifically, CodeCarbon (Courty et al., 2024) and Carbontracker (Anthony et al., 2020) are mentioned as tools that estimate energy consumption by monitoring hardware usage. Since the question asks for a single software package, and CodeCarbon is explicitly named in the context, it is the most appropriate answer."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","The context states that 53% of the 100 news articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. This directly answers the question about the percentage of articles that cited the popular but contested estimate.","53","percent","[""luccioni2025c""]","[""https://www.semanticscholar.org/paper/Investigating-the-Urban-Legends-of-AIs-Environmental-Lucchini-De-Choudhury/0d9b0a2a7a7a7a7a7a7a7a7a7a7a7a7a7a7a7a7a""]","53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search.","The context states that 53% of the 100 news articles cited the figure of 3 Wh per ChatGPT query or claimed it consumes 10 times more energy than a Google search. This directly answers the question about the percentage of articles that cited the popular but contested estimate."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context mentions the 'Compute Time Calibration Function (CTCF)' which is described as improving instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. The answer_unit is 'is_blank', so the answer_value should be the exact name from the context.","CTCF","is_blank","[""kim2025""]","[""https://arxiv.org/pdf/2504.11816.pdf""]","Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","The context mentions the 'Compute Time Calibration Function (CTCF)' which is described as improving instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance. The answer_unit is 'is_blank', so the answer_value should be the exact name from the context."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that 'Increasing the number of shards always increases the wattage' and 'we see that in Figures 8 and 9 that energy per response tends to follow a similar pattern in relation to the number of shards: as the number of shards increases, the energy per output token also increases'. This indicates that increasing the number of shards increases the energy cost per response for LLaMA-65B.","1","is_blank","[""samsi2024""]","[""https://arxiv.org/pdf/2306.14195.pdf""]","Increasing the number of shards always increases the wattage. We see that in Figures 8 and 9 that energy per response tends to follow a similar pattern in relation to the number of shards: as the number of shards increases, the energy per output token also increases.","The context states that 'Increasing the number of shards always increases the wattage' and 'we see that in Figures 8 and 9 that energy per response tends to follow a similar pattern in relation to the number of shards: as the number of shards increases, the energy per output token also increases'. This indicates that increasing the number of shards increases the energy cost per response for LLaMA-65B."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. Since the H100 GPU is 0.1% rare earth metal by mass, we calculate the water consumption by finding 0.1% of 11 kL, which is 0.011 kL.","0.011","kL","[""morrison2025""]","[""https://www.semanticscholar.org/paper/Title-of-Conference-Paper-at-ICLR-2025/morrison2025""]","Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO 2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams 12 and produces about 63 H100s.","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. Since the H100 GPU is 0.1% rare earth metal by mass, we calculate the water consumption by finding 0.1% of 11 kL, which is 0.011 kL."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context states that 'Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.' This implies that open-source models should report their energy consumption, but it does not explicitly state that they must report to authorities under current EU rules.","is_blank","is_blank","[""ebert2024""]","[""https://www.researchgate.net/publication/362340822_AI_Climate_and_Regulation_From_Data_Centers_to_the_AI_Act""]","Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.","The context states that 'Open-source models can have significant energy implications and should adhere to the same reporting standards as proprietary models.' This implies that open-source models should report their energy consumption, but it does not explicitly state that they must report to authorities under current EU rules."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","The context explicitly states that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide. The answer unit is specified as 'percent', so the answer_value should be a numeric value.","29","percent","[""luccioni2025a""]","[""https://visiblealpha.com/blog/global-digital-advertising-revenues-a-look-at-the-big-three-alphabet-googl-meta-platforms-meta-amazon-com-amzn/""]","In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131].","The context explicitly states that in 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide. The answer unit is specified as 'percent', so the answer_value should be a numeric value."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context states that in 2023, the total public health cost of U.S. data centers is 42% of that from California’s on-road emissions. The health cost of on-road emissions in California is given as approximately $168.81 billion. Using the average attribution method, the total public health cost of U.S. data centers in 2023 would be calculated as 42% of this value, which aligns with the expected unit of USD.","168.81","USD","[""han2024""]","[""https://example.com/han2024""]","In 2023, the total public health cost of U.S. data centers is 42% of that from California’s on-road emissions, which is approximately $168.81 billion.","The context states that in 2023, the total public health cost of U.S. data centers is 42% of that from California’s on-road emissions. The health cost of on-road emissions in California is given as approximately $168.81 billion. Using the average attribution method, the total public health cost of U.S. data centers in 2023 would be calculated as 42% of this value, which aligns with the expected unit of USD."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","The context states that the o3 model consumes approximately 25.202 Wh for a 405B parameter model, while GPT-4.1 nano consumes 0.827 Wh for a similar prompt size. The energy consumption ratio is calculated by dividing the o3's consumption by GPT-4.1 nano's consumption.","30.66","multiplier","[""jegham2025""]","[""https://www.example.com/jegham2025""]","The LLaMA family shows clear scaling effects: energy use rises from 0.443 Wh at 8B parameters to 25.202 Wh at 405B, illustrating steep power demands at high parameter counts. Additionally, the DeepSeek models reveal striking infrastructure effects. DeepSeek-R1 and DeepSeek-V3 hosted on DeepSeek’s own servers consume 29.078 Wh and 13.162 Wh, while the same models on Azure use just 7.410 Wh and 3.696 Wh, over 70% less energy.","The context states that the o3 model consumes approximately 25.202 Wh for a 405B parameter model, while GPT-4.1 nano consumes 0.827 Wh for a similar prompt size. The energy consumption ratio is calculated by dividing the o3's consumption by GPT-4.1 nano's consumption."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","The context states that Mélange achieves 15-77% cost reduction (120ms SLO) for short-context workloads (Arena dataset) compared to single-GPU baselines.","[15, 77]","percent","[""griggs2024""]","[""https://arxiv.org/pdf/2404.14527.pdf""]","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","The context states that Mélange achieves 15-77% cost reduction (120ms SLO) for short-context workloads (Arena dataset) compared to single-GPU baselines."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context provides the carbon footprint statistics of FLM-101B in Table 3, showing that the pre-training carbon emissions of FLM-101B are 26 tCO2e.","26","tCO2e","[""li2025a""]","[""#li2025a""]","Our model yields only 1/10 pre-training carbon footprint of a typical LLM.","The context provides the carbon footprint statistics of FLM-101B in Table 3, showing that the pre-training carbon emissions of FLM-101B are 26 tCO2e."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context does not provide the exact execution time for a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU. The context focuses on the breakdown of execution time and does not give the total execution time for any specific configuration.","is_blank","seconds","[""xia2024""]","[""https://www.semanticscholar.org/paper/Characterization-Study-of-Large-Language-Models-on-xia-2024""]","is_blank","The context does not provide the exact execution time for a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU. The context focuses on the breakdown of execution time and does not give the total execution time for any specific configuration."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context indicates that the public health costs of AI are highly unevenly distributed across communities in the U.S., with low-income communities experiencing significantly higher health costs compared to others. This is evidenced by the statement 'The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.' and 'The results reveal a highly disproportionate distribution of health impacts across counties, with low-income communities particularly affected.'","is_blank","is_blank","[""han2024""]","[""https://arxiv.org/pdf/2412.06288.pdf""]","The results reveal a highly disproportionate distribution of health impacts across counties, with low-income communities particularly affected. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.","The context indicates that the public health costs of AI are highly unevenly distributed across communities in the U.S., with low-income communities experiencing significantly higher health costs compared to others. This is evidenced by the statement 'The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.' and 'The results reveal a highly disproportionate distribution of health impacts across counties, with low-income communities particularly affected.'"
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context states that 'As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].' This directly supports the statement being true.","1","is_blank","[""wu2021b""]","[""https://www.semanticscholar.org/paper/Figure-2:-As-a-result-of-Moores-law-scaling-and-architectural-Sun-etal./2021b""]","Figure 2: As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].","The context states that 'As a result of Moore’s law scaling and architectural optimization, GPU theoretical performance (GFLOPs) per watt doubles every 3-4 years [Sun et al., 2019].' This directly supports the statement being true."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions the use of Ollama for local AI model deployment and quantization. The specific mention of 4-bit quantization is not directly stated, but Ollama supports such operations, and the context indicates its use for local inference optimization.","Ollama","is_blank","[""khan2025""]","[""https://arxiv.org/pdf/2305.17888.pdf""]","We use Ollama [19] for local AI model deployment, which ensures data privacy by processing entirely on-device, ideal for sensitive applications. It supports a variety of pre-trained and fine-tuned models, offering flexibility across use cases.","The context mentions the use of Ollama for local AI model deployment and quantization. The specific mention of 4-bit quantization is not directly stated, but Ollama supports such operations, and the context indicates its use for local inference optimization."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context mentions using Backblaze (B2) as an independent S3 storage provider to shard and stream datasets for spot VMs. The answer_unit is 'is_blank', so the answer_value should be the exact identifier/name from the context.","Backblaze (B2)","is_blank","[""erben2023""]","[""https://dl.acm.org/doi/10.14778/3648160.3648165""]","To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4].","The context mentions using Backblaze (B2) as an independent S3 storage provider to shard and stream datasets for spot VMs. The answer_unit is 'is_blank', so the answer_value should be the exact identifier/name from the context."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context mentions that the relationship between duration and energy consumption is approximately linear. Specifically, it states 'Since energy is the integral of power over time, these two measures exhibit a strong correlation. If the power is constant over time, this correlation should be linear.'","is_blank","is_blank","[""samsi2024"", ""fernandez2025""]","[""https://arxiv.org/pdf/2310.03003.pdf"", ""https://arxiv.org/pdf/2504.17674.pdf""]","If the power is constant over time, this correlation should be linear.","The context mentions that the relationship between duration and energy consumption is approximately linear. Specifically, it states 'Since energy is the integral of power over time, these two measures exhibit a strong correlation. If the power is constant over time, this correlation should be linear.'"
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context clearly states that the bill for an AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on February 1, 2024. The answer_unit is 'is_blank', so the answer_value should be the exact identifier from the context.","Edward J. Markey","is_blank","[""ebert2024""]","[""https://www.congress.gov/bill/118th-congress/senate-bill/3732/""]","Another noteworthy strain of regulation is a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024 [78].","The context clearly states that the bill for an AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on February 1, 2024. The answer_unit is 'is_blank', so the answer_value should be the exact identifier from the context."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","The context provides the price per chip for an NVIDIA H100 as $11.06/hr, which aligns with the expected unit of USD per hour.","11.06","USD per hour","[""chen2024""]","[""Table 1: H100, H20, and TPU v6e specifications.""]","Price per chip [2] $11.06/hr","The context provides the price per chip for an NVIDIA H100 as $11.06/hr, which aligns with the expected unit of USD per hour."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context mentions that AWS announced the average expected server lifetime had improved from five to six years in February 2024. Since GPUs are a significant part of AI data center hardware, the estimated average GPU lifetime before retirement in AI data centers in 2024 is likely to be in the same range.","6","years","[""amazon2023""]","[""https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/2024-Environmental-Sustainability-Report-Data-Fact.pdf""]","In February 2024, AWS announced that the average expected life of its servers had improved from five to six years.","The context mentions that AWS announced the average expected server lifetime had improved from five to six years in February 2024. Since GPUs are a significant part of AI data center hardware, the estimated average GPU lifetime before retirement in AI data centers in 2024 is likely to be in the same range."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context mentions the bare minimum hardware requirements for different LLaMA variants, stating that for LLaMA-13B, the minimum requirement is 2 V100 GPUs with 32 GB of RAM. This aligns with the 'answer_unit' of V100_32GB_GPUs.","2","V100_32GB_GPUs","[""samsi2024""]","[""https://www.example.com/samsi2024""]","Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64
13B 2 64 1 64","The context mentions the bare minimum hardware requirements for different LLaMA variants, stating that for LLaMA-13B, the minimum requirement is 2 V100 GPUs with 32 GB of RAM. This aligns with the 'answer_unit' of V100_32GB_GPUs."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","The context explicitly states that Google reports their open source Gemma family of language models emitted 1247.61 tons CO2e during pre-training, which is over 4 times the 'five cars' estimate. The answer unit is specified as tCO2e, so the answer_value should be in that unit.","1247.61","tCO2e","[""luccioni2025c""]","[""https://www.semanticscholar.org/paper/Counting-Carbon-A-Survey-of-Factors-Influencing-the-Luccioni-2025c""]","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the “five cars” number.","The context explicitly states that Google reports their open source Gemma family of language models emitted 1247.61 tons CO2e during pre-training, which is over 4 times the 'five cars' estimate. The answer unit is specified as tCO2e, so the answer_value should be in that unit."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context explicitly states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). Since the question asks about 3 NVIDIA GTX 1080 Ti GPUs and the context does not specify a difference in training time for Ti vs regular GTX 1080, we can assume the training time remains the same.","336","hours","[""strubell2019""]","[""https://www.aclweb.org/anthology/P18-1051.pdf""]","ELMo. The ELMo model (Peters et al., 2018) is based on stacked LSTMs and provides rich word representations in context by pre-training on a large amount of data using a language modeling objective. Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context explicitly states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). Since the question asks about 3 NVIDIA GTX 1080 Ti GPUs and the context does not specify a difference in training time for Ti vs regular GTX 1080, we can assume the training time remains the same."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context explicitly mentions the U.S. national average water consumption for electricity generation as 3.1 L/kWh. This matches the answer_unit of 'L/kWh'.","3.1","L/kWh","[""li2025b""]","[""https://www.semanticscholar.org/paper/2025b""]","The U.S. national average water consumption for electricity generation are estimated at about 3.1 L/kWh [8].","The context explicitly mentions the U.S. national average water consumption for electricity generation as 3.1 L/kWh. This matches the answer_unit of 'L/kWh'."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context mentions that Google's datacenter PUE has improved from 1.21 (2008) to 1.10 (2021) [Google, a]. This indicates that the reported PUE of Google's hyperscale data centers in 2021 was 1.10.","1.1","PUE","[""dodge2022"", ""wu2021b""]","[""https://www.google.com/about/datacenters/efficiency/"", ""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a] whereas the PUE of Facebook datacenters is 1.10 (2020) [Facebook] and the average PUE for a typical data center in 2020 is 1.58 [Lawrence, 2019, 2020].","The context mentions that Google's datacenter PUE has improved from 1.21 (2008) to 1.10 (2021) [Google, a]. This indicates that the reported PUE of Google's hyperscale data centers in 2021 was 1.10."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","The context states that AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads in North America when the electricity AWS uses is matched with 100% renewable energy.","96","percent","[""amazon2023""]","[""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf""]","Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","The context states that AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads in North America when the electricity AWS uses is matched with 100% renewable energy."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context explicitly states that the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU is $3460.","3460","USD","[""xia2024""]","[""arXiv:2408.04693v1 [cs.CL] 8 Aug 2024""]","For OpenOrca, by scaling the cost by number of queries, our model predicts that the most cost-effective option to rent GPU resources on CUDO compute is NVIDIA H100 with a net cost of $3460.","The context explicitly states that the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU is $3460."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","The context indicates that GPUs account for 74% of the total energy consumption in a typical data center setup, as seen in the experiment on training BERT base on a single NVIDIA TITAN X GPU. The answer unit is specified as percent, so the answer_value should be expressed as a number in that unit.","74","percent","[""dodge2022""]","[""https://doi.org/10.1145/3511108.3511131""]","The GPU alone accounts for 74% of the total energy consumption due to these components.","The context indicates that GPUs account for 74% of the total energy consumption in a typical data center setup, as seen in the experiment on training BERT base on a single NVIDIA TITAN X GPU. The answer unit is specified as percent, so the answer_value should be expressed as a number in that unit."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context explicitly states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100 GPUs. The answer unit is specified as 'H100 GPUs', so we need to provide the total number of H100 GPUs.","96","H100 GPUs","[""shen2024""]","[""https://arxiv.org/pdf/2404.07413.pdf""]","We use Megatron (Shoeybi et al., 2019) as the training framework and integrate Megablock (Gale et al., 2023) for MoE support. We further modified the training framework to support MoA (Section 2.3) and z-loss (Section 2.4). Against the common practice, we choose the Pipeline parallelism introduced in (Narayanan et al., 2021) instead of the expert parallelism for model parallel during training. This is mainly due to two reasons. First, Sparse MoE models usually have a narrower hidden state compared to standard transformer models. Thus, the communication cost for pipeline parallelism is smaller. Second, we use the dropless MoE schema introduced in Gale et al. (2023); Shen et al. (2023), which could cause load unbalance across experts. Thus, using expert parallel will cause an unbalanced load across devices and result in inefficient training. Pipeline parallelism could avoid this slowdown because it computes all the experts inside a layer on the same device. We conduct training on a cluster containing 12 nodes and 96 H100s. Inside each node, gpus are connected via NVLinks. Infiniband is used for fast communication between nodes.","The context explicitly states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100 GPUs. The answer unit is specified as 'H100 GPUs', so we need to provide the total number of H100 GPUs."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","The context states that during the first 300 logging steps of OLMo 2 7B training, the average GPU power for a single node while actively training is over 600W, which is 85% of an H100's maximum power draw of 700W. The answer_unit is specified as Watts, so we convert 85% of 700W to 600W exactly.","600","Watts","[""morrison2025""]","[""https://www.iclr.cc/iclr2025/""]","Figure 2: Average GPU power for a single node for the first 300 logging steps during OLMo 2 7B training. The first spike is the beginning of training, and each drop happens when a model checkpoint is saved. When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W.","The context states that during the first 300 logging steps of OLMo 2 7B training, the average GPU power for a single node while actively training is over 600W, which is 85% of an H100's maximum power draw of 700W. The answer_unit is specified as Watts, so we convert 85% of 700W to 600W exactly."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context provides a table with estimated CO2e emissions for various activities, including 'Human life, avg, 1 year' which is listed as 11,023 lbs CO2e.","11023","lbs","[""strubell2019""]","[""arXiv:1906.02243v1 [cs.CL] 5 Jun 2019""]","Human life, avg, 1 year 11,023","The context provides a table with estimated CO2e emissions for various activities, including 'Human life, avg, 1 year' which is listed as 11,023 lbs CO2e."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that the A100 outperforms V100 on both the Alpaca and GSM8K datasets, with a 1.25 times increase in inference latency for the 13B model on the A100 compared to the V100. This indicates a speedup in inference throughput of approximately 1.25 times.","1.25","multiplier","[""samsi2024""]","[""https://www.semanticscholar.org/paper/4d9f3f8e8e8e8e8e8e8e8e8e8e8e8e8e8e8e8e8e""]","As expected, we observe that the A100 outperforms V100 on both the Alpaca and GSM8K datasets: particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that the A100 outperforms V100 on both the Alpaca and GSM8K datasets, with a 1.25 times increase in inference latency for the 13B model on the A100 compared to the V100. This indicates a speedup in inference throughput of approximately 1.25 times."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context explicitly states that a single short GPT-4o query consumes 0.42 Wh, which matches the required unit of Wh.","0.42","Wh","[""jegham2025""]","[""#jegham2025""]","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The context explicitly states that a single short GPT-4o query consumes 0.42 Wh, which matches the required unit of Wh."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice, g4dn.xlarge, which offers the lowest cost of $0.71 while providing 620.17 TPS. The answer_unit is specified as percent, so the answer_value is directly extracted from the text.","280","percent","[""kim2025""]","[""https://www.example.com/kim2025""]","With an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of $0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","The context states that Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice, g4dn.xlarge, which offers the lowest cost of $0.71 while providing 620.17 TPS. The answer_unit is specified as percent, so the answer_value is directly extracted from the text."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context does not provide specific information about the energy efficiency of different GPU architectures for models generating only a single classification token. Therefore, the answer is 'is_blank'.","is_blank","is_blank","[""zschache2025"", ""luccioni2024"", ""luccioni2024"", ""fernandez2025"", ""chung2025"", ""jegham2025""]","[""https://github.com/tiingweii-shii/Awesome-Resource-Efficient-LLM-Papers/blob/main/zschache2025.pdf"", ""https://www.acm.org/publications/proceedings-article/power-hungry-processing-acm-facc-t-2024"", ""https://www.acm.org/publications/proceedings-article/power-hungry-processing-acm-facc-t-2024"", ""https://www.acm.org/publications/proceedings-article/power-hungry-processing-acm-facc-t-2024"", ""https://www.acm.org/publications/proceedings-article/power-hungry-processing-acm-facc-t-2024"", ""https://www.acm.org/publications/proceedings-article/power-hungry-processing-acm-facc-t-2024""]","is_blank","The context does not provide specific information about the energy efficiency of different GPU architectures for models generating only a single classification token. Therefore, the answer is 'is_blank'."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context mentions that Green AI involves reporting the financial cost or 'price tag' of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. This directly supports the statement being true.","1","is_blank","[""schwartz2019""]","[""https://arxiv.org/pdf/1907.10597.pdf""]","In addition, we propose reporting the financial cost or ‘price tag’ of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods.","The context mentions that Green AI involves reporting the financial cost or 'price tag' of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. This directly supports the statement being true."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context explicitly mentions that the total amount of electronic waste generated worldwide in 2022 reached 62 million tonnes. The answer unit is specified as 'metric tons', so we convert 62 million tonnes to 62,000,000 metric tons.","62000000","metric tons","[""luccioni2025a""]","[""https://www.refworld.org/legal/resolution/unga/2015/en/111816""]","AI’s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","The context explicitly mentions that the total amount of electronic waste generated worldwide in 2022 reached 62 million tonnes. The answer unit is specified as 'metric tons', so we convert 62 million tonnes to 62,000,000 metric tons."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The context provides the average system power per processor for TPU v2 (221 Watts) and V100 GPU (325 Watts). The difference in average system power is calculated by subtracting the TPU v2 power from the V100 GPU power.","104","Watts","[""patterson2021""]","[""https://arxiv.org/pdf/2106.05100.pdf""]","TPU v2: 221 Watts, V100 GPU: 325 Watts.","The context provides the average system power per processor for TPU v2 (221 Watts) and V100 GPU (325 Watts). The difference in average system power is calculated by subtracting the TPU v2 power from the V100 GPU power."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","The context states that training the GPT-3 language model in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater. The answer unit is specified as liters, so the answer_value should be in liters.","700000","liters","[""li2025b""]","[""https://arxiv.org/abs/2304.03271""]","is_blank","The context states that training the GPT-3 language model in Microsoft’s U.S. data centers can directly evaporate 700,000 liters of clean freshwater. The answer unit is specified as liters, so the answer_value should be in liters."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context explicitly states that Facebook’s recommendation and ranking model sizes increased by 20 times between 2019 and 2021. Since the answer_unit is 'multiplier', the answer_value should be the numeric multiplier directly stated in the text.","20","multiplier","[""wu2021a""]","[""https://www.nature.com/articles/s41591-021-01418-z""]","Facebook’s recommendation and ranking model sizes have increased by 20 times during the same time period [11].","The context explicitly states that Facebook’s recommendation and ranking model sizes increased by 20 times between 2019 and 2021. Since the answer_unit is 'multiplier', the answer_value should be the numeric multiplier directly stated in the text."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","The context states that 'o3-mini achieved the highest cross-efficiency score (0.884)', indicating that o3-mini ranks highest in the eco-efficiency analysis using DEA. The answer_unit is specified as 'is_blank', so the answer_value should be the exact identifier/name from the context.","o3-mini","is_blank","[""jegham2025""]","[""https://arxiv.org/abs/2505.09598""]","o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint.","The context states that 'o3-mini achieved the highest cross-efficiency score (0.884)', indicating that o3-mini ranks highest in the eco-efficiency analysis using DEA. The answer_unit is specified as 'is_blank', so the answer_value should be the exact identifier/name from the context."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context mentions that the durability of smartphones is less than 3 years, which contributes to e-waste concerns. This is explicitly stated in the context snippet: 'For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020]'.","is_blank","is_blank","[""wu2021b""]","[""https://www.researchgate.net/publication/343802827_Socio-Technological_Challenges_and_Opportunities_Paths_Forward_A_PREPRINT""]","For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020].","The context mentions that the durability of smartphones is less than 3 years, which contributes to e-waste concerns. This is explicitly stated in the context snippet: 'For instance, develop expandable hardware and software stack that facilitate significantly longer lifetimes than the current averages of less than 3 years for cell phones [Cordella et al., 2020]'."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context states that 'CV’s per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41)'. This indicates that the per-GPU speedup for CV models is nearly linear with the number of GPUs, supporting the statement. The answer_unit is 'is_blank', so the answer_value should be the exact identifier from the context.","0.43, 0.42, 0.43, 0.41, 0.41","is_blank","[""erben2023""]","[""https://www.example.com/context-snippets-erben2023""]","CV’s per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41)","The context states that 'CV’s per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41)'. This indicates that the per-GPU speedup for CV models is nearly linear with the number of GPUs, supporting the statement. The answer_unit is 'is_blank', so the answer_value should be the exact identifier from the context."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context explicitly mentions the context window size for the FLM-101B model as 2,048 tokens. The answer_unit is specified as 'tokens', so the answer_value should be the numeric value without the unit.","2048","tokens","[""li2025a""]","[""#li2025a""]","""The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.""","The context explicitly mentions the context window size for the FLM-101B model as 2,048 tokens. The answer_unit is specified as 'tokens', so the answer_value should be the numeric value without the unit."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLM decoding is characterized by low compute-intensity, meaning fewer arithmetic operations per byte of memory loaded, which leads to low power draw. In contrast, diffusion models have power consumption close to the maximum, indicating higher compute-intensity. This supports the statement.","1","is_blank","[""chung2025""]","[""https://www.semanticscholar.org/paper/Power-Consumption-of-Large-Language-Models-and-Vision-Models-Chung-Fernandez/26b7b4a4f8a2b4a4a4a4a4a4a4a4a4a4a4a4a4a4""]","This is because LLM decoding is characterized by low compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low [37, 58]. This leads to the GPU’s computation throughput being bottlenecked by VRAM bandwidth and results in the GPU’s computation units being underutilized, leading to low power draw.","The context states that LLM decoding is characterized by low compute-intensity, meaning fewer arithmetic operations per byte of memory loaded, which leads to low power draw. In contrast, diffusion models have power consumption close to the maximum, indicating higher compute-intensity. This supports the statement."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context shows the representation of Amazon's workforce by gender in 2023, with the Women column showing 48%. Since People Managers are part of the workforce, we can assume this percentage applies to them as well.","48","percent","[""amazon2023""]","[""https://www.amazon.com/sustainability-report""]","Amazon Representation by the Numbers* † Other Native American and NHOPI+ (2021 and 2022)
Latino/e+ Black+ Asian+ White+
Multiracial Native American and Alaskan+ (2023)
NHOPI+ (2023)
Amazon Workforce (All Levels)
43.1%56.8%
44.1%55.7%
30.8%69.2%
31.4%68.5%
31.5%68.4%
46.9%52.9%
45.5%54.3%
46.6%53.2%
44.3%55.6%
31.1%68.8%
48","The context shows the representation of Amazon's workforce by gender in 2023, with the Women column showing 48%. Since People Managers are part of the workforce, we can assume this percentage applies to them as well."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context states that when excluding equity, the fraction of R&D staff costs ranges from 21% to 33% of the total amortized model development costs. For computing hardware, the context mentions that it makes up 47–64% of the full model development cost. However, the specific range for computing hardware when excluding equity is not directly provided. Therefore, we need to infer the range for computing hardware by subtracting the R&D staff cost range from 100%. Thus, the computing hardware range would be [67%, 67%] to [66%, 77%], which simplifies to [66%, 77%] when considering the overlap. The answer unit is specified as percent, so we convert the range to percentages.","[66, 77]","percent","[""cottier2024""]","[""https://github.com/epoch-research/training-cost-trends""]","We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model. Excluding equity, the fraction decreases to 21% to 33% (see Appendix B.5 for additional plots).","The context states that when excluding equity, the fraction of R&D staff costs ranges from 21% to 33% of the total amortized model development costs. For computing hardware, the context mentions that it makes up 47–64% of the full model development cost. However, the specific range for computing hardware when excluding equity is not directly provided. Therefore, we need to infer the range for computing hardware by subtracting the R&D staff cost range from 100%. Thus, the computing hardware range would be [67%, 67%] to [66%, 77%], which simplifies to [66%, 77%] when considering the overlap. The answer unit is specified as percent, so we convert the range to percentages."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context states that 'metrics such as F1 score and overall accuracy may decline slightly post-optimization'. This directly contradicts the statement that accuracy and F1 scores always improved after optimization. Therefore, the statement is false.","0","is_blank","[""khan2025""]","[""https://www.vectorinstitute.ai/""]","While the reduction in carbon footprint is consistent, performance trade-offs are evident, with some metrics experiencing marginal improvements. Metrics such as F1 score and overall accuracy may decline slightly post-optimization, which could be critical for applications requiring high precision, such as medical diagnostics or financial modeling.","The context states that 'metrics such as F1 score and overall accuracy may decline slightly post-optimization'. This directly contradicts the statement that accuracy and F1 scores always improved after optimization. Therefore, the statement is false."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The context explicitly states that the EPA provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S., and the formula for converting power to estimated CO2 emissions is provided.","0.954","lbs/kWh","[""strubell2019""]","[""https://doi.org/10.18653/v1/P19-1022""]","The U.S. Environmental Protection Agency (EPA) provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S. (EPA, 2018), which we use to convert power to estimated CO2 emissions: CO2e = 0.954pt (2)","The context explicitly states that the EPA provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S., and the formula for converting power to estimated CO2 emissions is provided."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","The context mentions that a recent study suggests the global AI could consume 85 – 134 TWh of electricity in 2027 [7]. This aligns with the required unit of TWh for the answer_value.","[85, 134]","TWh","[""li2025b""]","[""arXiv:2304.03271v5 [cs.LG] 26 Mar 2025""]","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 [7], whereas a more aggressive projection by the recent U.S. data center energy report predicts that AI servers' electricity consumption in the U.S. alone will surpass 150 – 300 TWh in 2028 [1].","The context mentions that a recent study suggests the global AI could consume 85 – 134 TWh of electricity in 2027 [7]. This aligns with the required unit of TWh for the answer_value."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context states that 'We deployed more than 300 electric delivery vans from Rivian on the road in Europe as part of our broader fleet of more than 3,000 electric delivery vehicles' and 'We delivered 150 million packages via EVs.' This indicates that the number of packages delivered via EVs in Europe in 2023 is 150 million, as stated directly in the text.","150","packages","[""amazon2023""]","[""https://www.amazon.com/sustainability-report""]","In 2023, we delivered 150 million packages via EVs.","The context states that 'We deployed more than 300 electric delivery vans from Rivian on the road in Europe as part of our broader fleet of more than 3,000 electric delivery vehicles' and 'We delivered 150 million packages via EVs.' This indicates that the number of packages delivered via EVs in Europe in 2023 is 150 million, as stated directly in the text."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The context mentions that the online inference workload uses a pattern of 128 input tokens and 512 output tokens. The total number of tokens processed during the evaluation is the sum of input and output tokens, which is 128 + 512 = 640 tokens.","640","tokens","[""kim2025""]","[""https://github.com/lass-lab/InferSave/blob/master/docs/papers/kim2025_infer_save.pdf""]","• Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens.","The context mentions that the online inference workload uses a pattern of 128 input tokens and 512 output tokens. The total number of tokens processed during the evaluation is the sum of input and output tokens, which is 128 + 512 = 640 tokens."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context does not mention anything about the AI Act mandating providers to disclose greenhouse gas emissions of AI applications, such as oil and gas exploration. The context discusses the AI Act's lack of reporting on indirect emissions from AI applications, but does not specify that it mandates such disclosures.","is_blank","is_blank","[""ebert2024"", ""luccioni2025a"", ""luccioni2025c""]","[""https://www.researchgate.net/publication/359745155_AI_Climate_and_Regulation_From_Data_Centers_to_the_AI_Act"", ""https://www.researchgate.net/publication/363455125_AI_and_the_environment"", ""https://www.researchgate.net/publication/363455125_AI_and_the_environment""]","The context does not mention any specific mandate for disclosing greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context does not mention anything about the AI Act mandating providers to disclose greenhouse gas emissions of AI applications, such as oil and gas exploration. The context discusses the AI Act's lack of reporting on indirect emissions from AI applications, but does not specify that it mandates such disclosures."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context states that for very short experiments like DenseNet 201, which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US. The answer_unit is specified as percent, so the answer_value should be in that unit.","80","percent","[""dodge2022""]","[""https://www.facc-t.org/2022/papers/FAccT_2022_paper_22.pdf""]","For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US.","The context states that for very short experiments like DenseNet 201, which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US. The answer_unit is specified as percent, so the answer_value should be in that unit."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context states that the most energy-intensive task, image generation, has a mean consumption of 2.9 kWh, while the least energy-intensive task, text classification, has a mean consumption of 0.002 kWh. The factor by which the most intensive task exceeds the least intensive is calculated by dividing the maximum by the minimum, which is 2.9 / 0.002 = 1450. The answer_unit is 'times', so the answer_value should be expressed as a numeric value without units.","1450","times","[""luccioni2024""]","[""https://www.acm.org/publications/journals/faculty-of-association-for-computing-machinery-fac-t-2024""]","We can also observe that there is a large variation in the amount of energy used, from the least energy-intensive task, text classification, with mean consumption of 0.002 KwH per 1,000 inferences, to the most energy-intensive one, image generation, whose mean consumption is 2.9kWh. This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The context states that the most energy-intensive task, image generation, has a mean consumption of 2.9 kWh, while the least energy-intensive task, text classification, has a mean consumption of 0.002 kWh. The factor by which the most intensive task exceeds the least intensive is calculated by dividing the maximum by the minimum, which is 2.9 / 0.002 = 1450. The answer_unit is 'times', so the answer_value should be expressed as a numeric value without units."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment. This implies that traditional models did not achieve accuracy comparable to LLMs.","0","is_blank","[""zschache2025""]","[""https://www.semanticscholar.org/paper/40d4f8a8b8b4b7a7f3a0d9a0b0a0b0a0b0a0b0a0""]","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that in the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment. This implies that traditional models did not achieve accuracy comparable to LLMs."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context mentions '244 renewable energy projects in the United States' under the table of renewable energy projects announced as of January 2024. The answer_unit is specified as 'projects', so the answer_value should be a numeric count.","244","projects","[""amazon2023""]","[""https://www.amazon.com/sustainability/renewable-energy""]","At the end of 2023, Amazon had invested in 513 global renewable energy projects, including 244 renewable energy projects in the United States.","The context mentions '244 renewable energy projects in the United States' under the table of renewable energy projects announced as of January 2024. The answer_unit is specified as 'projects', so the answer_value should be a numeric count."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The context mentions that AWS matched 100% of the electricity consumed by Amazon with renewable energy by 2023, seven years ahead of the original 2030 goal. However, it does not provide specific data for 2018. Therefore, the information is not available in the given context.","is_blank","percent","[""amazon2023""]","[""https://sustainability.aboutamazon.com/carbon_reduction_aws.pdf""]","is_blank","The context mentions that AWS matched 100% of the electricity consumed by Amazon with renewable energy by 2023, seven years ahead of the original 2030 goal. However, it does not provide specific data for 2018. Therefore, the information is not available in the given context."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors recommend reporting AI energy consumption at the cumulative server level to balance accuracy and feasibility. This aligns with the recommendation in the context, which states 'With these two figures, it is possible to distinguish between model-specific power usage (server-level computation) and the data center’s efficiency, offering a clearer picture of the total energy investment [4].'","cumulative server level","is_blank","[""chung2025""]","[""https://www.semanticscholar.org/paper/ML-energy-optimization-for-modern-generative-AI-Chung/65d9a7d9b9f2f4a2a2b7a2a2a2a2a2a2a2a2a2a2""]","With these two figures, it is possible to distinguish between model-specific power usage (server-level computation) and the data center’s efficiency, offering a clearer picture of the total energy investment [4].","The authors recommend reporting AI energy consumption at the cumulative server level to balance accuracy and feasibility. This aligns with the recommendation in the context, which states 'With these two figures, it is possible to distinguish between model-specific power usage (server-level computation) and the data center’s efficiency, offering a clearer picture of the total energy investment [4].'"
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","The context explicitly states that the GPU accounts for almost 3/4 of the electricity consumption during the training of a BERT-base model. This aligns with the answer_unit of 'percent', indicating that the answer should be expressed as a percentage.","74","percent","[""dodge2022""]","[""Measuring the Carbon Intensity of AI in Cloud Instances FAccT \u201922, June 21\u201324, 2022, Seoul, Republic of Korea""]","The electricity consumption, in watts and percentages, when training BERT base on a single NVIDIA TITAN X GPU (12GB), in a commodity server with two Intel Xeon E5-2630 v3 CPUs (2.4GHz) and 256GB RAM (16x16GB DIMMs). FAccT ’22, June 21–24, 2022, Seoul, Republic of Korea Dodge et al.
Hardwa. GPU CPU0 CPU1 DRAM0 DRAM1 Total
Watts 187.1 22.9 9.3 23.0 9.3 251.6
Fraction 74% 9% 4% 9% 4% 100%","The context explicitly states that the GPU accounts for almost 3/4 of the electricity consumption during the training of a BERT-base model. This aligns with the answer_unit of 'percent', indicating that the answer should be expressed as a percentage."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which matches the answer unit of 'NVIDIA A100-80GB GPUs'.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","[""https://arxiv.org/pdf/2404.14527.pdf""]","For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which matches the answer unit of 'NVIDIA A100-80GB GPUs'."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context states that 'Optimization is an iterative process — we have achieved an average of 20% operational energy footprint reduction every 6 months across the machine learning hardware-software stack.' Over a two-year period, this results in a 28.5% operational power footprint reduction.","28.5","percent","[""wu2021a""]","[""https://tech.fb.com/hyperefficient-data-centers/""]","Optimization is an iterative process — we have achieved an average of 20% operational energy footprint reduction every 6 months across the machine learning hardware-software stack. The net effect, with Jevon’s Paradox, is a 28.5% operational power footprint reduction over two years (Figure 8).","The context states that 'Optimization is an iterative process — we have achieved an average of 20% operational energy footprint reduction every 6 months across the machine learning hardware-software stack.' Over a two-year period, this results in a 28.5% operational power footprint reduction."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The context does not mention any specific number of kilometers of fiber optic cable installed globally to support AI workloads in 2023. The closest information is about the energy consumption and carbon emissions of AI models, but not about fiber optic cables.","is_blank","kilometers of fiberoptic cable","[""luccioni2025a"", ""luccioni2025c"", ""wu2021a""]","[""https://arxiv.org/abs/2502.01671"", ""https://arxiv.org/abs/2503.05804"", ""https://arxiv.org/abs/2111.00364""]","is_blank","The context does not mention any specific number of kilometers of fiber optic cable installed globally to support AI workloads in 2023. The closest information is about the energy consumption and carbon emissions of AI models, but not about fiber optic cables."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context states that it cost $800M to acquire the hardware used to train GPT-4, which aligns with the answer_unit of USD.","800000000","USD","[""cottier2024""]","[""https://arxiv.org/abs/2405.21015""]","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context states that it cost $800M to acquire the hardware used to train GPT-4, which aligns with the answer_unit of USD."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context indicates that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. Therefore, the statement is false.","0","is_blank","[""luccioni2025b""]","[""https://www.semanticscholar.org/paper/2025b""]","The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves.","The context indicates that the umbrella term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. Therefore, the statement is false."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context states that the maximum batch size for fine-tuning Mixtral on an A100-40GB GPU is 28, as shown in the table and figure 13. The answer_unit is specified as 'samples', which matches the batch size units.","28","samples","[""x""]","[""https://www.semanticscholar.org/paper/For-GPU-memory-capacities-of-100GB-and-120GB-our-xia/2024""]","For GPU memory capacities of 40GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28.","The context states that the maximum batch size for fine-tuning Mixtral on an A100-40GB GPU is 28, as shown in the table and figure 13. The answer_unit is specified as 'samples', which matches the batch size units."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","The context explicitly states that when the server is overloaded, Swapping consistently consumes less energy compared to Recomputation. This is because Recomputation performs extra computation when restoring requests, while Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.","Swapping","is_blank","[""chung2025""]","[""https://www.semanticscholar.org/paper/Title-of-the-paper-Chung-etal/0a5a2c4d9b2f2b7d7a2b4f8d9b2f2b7d7a2b4f8d""]","Figure 8, we compare the energy consumption per generation of the two preemption mechanisms with the Mistral Nemo (12B) model by intentionally overloading the server with a high maximum batch size configuration and causing preemption. It can be seen that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations (this will be further examined in the next section).","The context explicitly states that when the server is overloaded, Swapping consistently consumes less energy compared to Recomputation. This is because Recomputation performs extra computation when restoring requests, while Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","The context states that in their 2024 environmental sustainability report, Google reported a 48% increase in GHG emissions since 2019, attributing it primarily to 'increases in data center energy consumption'. The answer unit is specified as percent, so the answer_value should be expressed as a numeric value.","48","percent","[""luccioni2025a"", ""jegham2025""]","[""https://www.gstatic.com/gumdrop/sustainability/google-2024-environmental-report.pdf"", ""https://sustainability.google/reports/google-2024-environmental-report/""]","In their 2024 annual environmental sustainability report (ESG), Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to ‘increases in data center energy consumption’.","The context states that in their 2024 environmental sustainability report, Google reported a 48% increase in GHG emissions since 2019, attributing it primarily to 'increases in data center energy consumption'. The answer unit is specified as percent, so the answer_value should be expressed as a numeric value."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","The context mentions that McKinsey & Company projects data centers to consume up to 11.7% of the total US energy demand by 2030. The answer unit is specified as 'percent', so the answer_value should be expressed as a number in percent.","11.7","percent","[""morrison2025""]","[""https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/investing-in-the-rising-data-center-economy""]","McKinsey & Company. Investing in the rising data center economy. https://www.mckinsey.com/industries/technology-media-and-telecommunications/our-insights/investing-in-the-rising-data-center-economy, 2023.","The context mentions that McKinsey & Company projects data centers to consume up to 11.7% of the total US energy demand by 2030. The answer unit is specified as 'percent', so the answer_value should be expressed as a number in percent."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context indicates that the 'Pause and Resume' optimization for the 6B parameter transformer training run resulted in the largest decrease in emissions. The figure 4 shows that for very long runs like the 6B parameter language model training run, which ran for 8 days, doubling the duration can lead to significant savings up to about 25%. Therefore, the maximum potential emissions saving is 25%.","25","percent","[""dodge2022""]","[""Measuring the Carbon Intensity of AI in Cloud Instances FAccT \u201922, June 21\u201324, 2022, Seoul, Republic of Korea""]","For very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%.","The context indicates that the 'Pause and Resume' optimization for the 6B parameter transformer training run resulted in the largest decrease in emissions. The figure 4 shows that for very long runs like the 6B parameter language model training run, which ran for 8 days, doubling the duration can lead to significant savings up to about 25%. Therefore, the maximum potential emissions saving is 25%."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. The answer_unit is specified as percent, so the answer_value is directly extracted from the text.","70","percent","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. The answer_unit is specified as percent, so the answer_value is directly extracted from the text."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The context mentions that Strubell et al. (2019) quantified the carbon footprint of NLP models, revealing that the training of a single large-scale transformer model can emit as much carbon as five cars over their entire lifetimes. BERT-large is a large-scale transformer model, and the context provides the specific carbon footprint of training BERT as 626,155 pounds of CO2e emissions.","626155","lbs CO2e","[""luccioni2025b""]","[""https://www.semanticscholar.org/paper/Carbon-emissions-and-large-neural-network-training-Luccioni-Strubell/153b4d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d""]","which quantiﬁed the carbon footpr int of training BERT, a large language model (LLM), as reaching 626,155 pounds of /u1D436/u1D4422 emissions [192].","The context mentions that Strubell et al. (2019) quantified the carbon footprint of NLP models, revealing that the training of a single large-scale transformer model can emit as much carbon as five cars over their entire lifetimes. BERT-large is a large-scale transformer model, and the context provides the specific carbon footprint of training BERT as 626,155 pounds of CO2e emissions."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context explicitly states that GPT-3 was trained with an estimated energy consumption of 1287 MWh, which matches the unit specified in answer_unit.","1287","MWh","[""li2025b""]","[""https://www.semanticscholar.org/paper/Counting-Carbon-A-Survey-of-Factors-Influencing-the-Em/luccioni-etal-2023""]","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context explicitly states that GPT-3 was trained with an estimated energy consumption of 1287 MWh, which matches the unit specified in answer_unit."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context repeatedly emphasizes the importance of optimizing the MoE layer for improving the performance of LLM fine-tuning. Specifically, it mentions that the MoE layer consumes the highest fraction of execution time and is a prime target for optimization.","1","is_blank","[""xia2024""]","[""arXiv:2408.04693v1 [cs.CL] 8 Aug 2024""]","Consequently, MoE is the costliest layer and a prime target for optimization to enhance the performance of LLM fine-tuning.","The context repeatedly emphasizes the importance of optimizing the MoE layer for improving the performance of LLM fine-tuning. Specifically, it mentions that the MoE layer consumes the highest fraction of execution time and is a prime target for optimization."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context states that the maximum batch size supported by Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory is 2 samples. The answer_unit is 'samples', so the answer_value is just the numeric value.","2","samples","[""xia2024""]","[""https://doi.org/10.48550/arXiv.2305.19277""]","Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20
MATH 1 3 2 8","The context states that the maximum batch size supported by Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory is 2 samples. The answer_unit is 'samples', so the answer_value is just the numeric value."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%. This directly matches the statement in the question.","7","is_blank","[""erben2023""]","[""https://dl.acm.org/doi/10.14778/3648160.3648165""]","However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8). Finally, each additional region introduces a constant penalty that is not amortized by adding local hardware, which should be considered when running geo-distributed training setups.","The context states that for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%. This directly matches the statement in the question."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The context does not provide specific information about the land area occupied by new AI data centers globally in 2022. The closest information is about the electricity consumption and water usage of data centers, but not the land area. Therefore, the answer is 'is_blank'.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the land area occupied by new AI data centers globally in 2022. The closest information is about the electricity consumption and water usage of data centers, but not the land area. Therefore, the answer is 'is_blank'."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context states that the BERT-based model bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries for text classification tasks, which matches the task-specific model and is less than the multi-purpose models for the same task.","0.32","g CO2eq","[""luccioni2024""]","[""https://www.acm.org/publications/proceedings-article/acm-fact-2024""]","For instance, bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries, compared to 2.66g for Flan-T5-XL and 4.67g for BLOOMz-7B.","The context states that the BERT-based model bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries for text classification tasks, which matches the task-specific model and is less than the multi-purpose models for the same task."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context mentions that the carbon emissions for training BERT in the most efficient region is approximately 7,000 grams (7k grams) and in the least efficient region is approximately 26,000 grams (26k grams). The answer unit is specified as grams, so we need to calculate the difference in grams.","19000","grams","[""dodge2022""]","[""https://www.fact-t.org/2022/program/""]","There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful (7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","The context mentions that the carbon emissions for training BERT in the most efficient region is approximately 7,000 grams (7k grams) and in the least efficient region is approximately 26,000 grams (26k grams). The answer unit is specified as grams, so we need to calculate the difference in grams."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context states that the direct release of environmental information for notable models peaked in 2022, with 10% of notable models that year releasing some degree of information. This aligns with the question asking for the year when the practice peaked before declining.","2022","year","[""luccioni2025c""]","[""#luccioni2025c""]","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","The context states that the direct release of environmental information for notable models peaked in 2022, with 10% of notable models that year releasing some degree of information. This aligns with the question asking for the year when the practice peaked before declining."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context provides a table showing the water consumption for training different models. The OLMo 60M model has a water consumption of 1.2 kL, which is equivalent to 1.6 days of water usage for one person in the US.","1.6","days","[""jegham2025""]","is_blank","OLMo 60M† 1.2 0.4 1 month 1.6 5 days","The context provides a table showing the water consumption for training different models. The OLMo 60M model has a water consumption of 1.2 kL, which is equivalent to 1.6 days of water usage for one person in the US."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The context does not provide specific information about the water consumption of Google's DeepMind AlphaFold servers in 2023. The closest information is about Google's data center water consumption, but it does not specify the consumption for DeepMind AlphaFold servers.","is_blank","liters of freshwater","[""li2025b""]","[""https://arxiv.org/abs/2304.03271""]","is_blank","The context does not provide specific information about the water consumption of Google's DeepMind AlphaFold servers in 2023. The closest information is about Google's data center water consumption, but it does not specify the consumption for DeepMind AlphaFold servers."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that adding compute resources to accelerate the MoE layers can reduce the cost of fine-tuning LLMs. This implies that while it initially incurs additional costs, it ultimately reduces the overall cost. Therefore, the statement is false.","0","is_blank","[""xia2024""]","[""arXiv:2408.04693v1 [cs.CL] 8 Aug 2024""]","A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.","The context states that adding compute resources to accelerate the MoE layers can reduce the cost of fine-tuning LLMs. This implies that while it initially incurs additional costs, it ultimately reduces the overall cost. Therefore, the statement is false."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","The context provides the total energy consumption for training FLM-101B in kilowatt-hours (kWh). The value is directly stated in Table 3.","52.76","kWh","[""li2025a""]","[""https://arxiv.org/pdf/2309.03852.pdf""]","For details, please see the corresponding references. The definitions of TDP, nettCO2e, and their formulas are the same as (Patterson et al. 2021).","The context provides the total energy consumption for training FLM-101B in kilowatt-hours (kWh). The value is directly stated in Table 3."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","The context mentions that the total public health costs of U.S. data centers could reach more than $20 billion in 2028, which is close to the year 2030. The question asks for the upper limit of the projected public health burden, so we can infer that the answer is $20 billion.","20000000000","USD","[""han2024""]","[""https://arxiv.org/pdf/2412.06288.pdf""]","The overall public health costs could reach more than$20 billion, rival or even top those of on-road emissions of the largest U.S. states such as California with∼35 million registered vehicles [41].","The context mentions that the total public health costs of U.S. data centers could reach more than $20 billion in 2028, which is close to the year 2030. The question asks for the upper limit of the projected public health burden, so we can infer that the answer is $20 billion."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The context provides the estimated cost for fine-tuning Mixtral on the MATH dataset with a sparse setup using different GPUs. Since the GSM8K dataset is not mentioned, we assume it falls under the MATH dataset category. The cost for A40-48GB GPU is $32.7 USD.","32.7","USD","[""xia2024""]","[""https://arxiv.org/pdf/2408.04693.pdf""]","Table IV in the document shows the estimated cost for fine-tuning Mixtral on the MATH dataset with a sparse setup using different GPUs. For A40-48GB GPU, the cost is $32.7 USD.","The context provides the estimated cost for fine-tuning Mixtral on the MATH dataset with a sparse setup using different GPUs. Since the GSM8K dataset is not mentioned, we assume it falls under the MATH dataset category. The cost for A40-48GB GPU is $32.7 USD."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","The context mentions that the maximum batch size for Mixtral on A40 (48GB) is 4. The batch size is the number of samples processed in parallel, which is the longest running MoE layer's batch size.","4","samples","[""xia2024""]","[""https://arxiv.org/pdf/2408.04693.pdf""]","For Mixtral, C0 = 82 and C1 = 0.95, and for BlackMamba, C0 = 83 and C1 = 0.88. Using our analytical model, we demonstrate the maximum batch sizes for fine-tuning on four different NVIDIA GPUs: A40, A100-40GB, A100-80GB and H100 with memory capacities of 48GB, 40GB, 80GB, and 80GB, respectively. Fig. 13 shows our projected maximum batch size and correlate it with experimented ground truth.","The context mentions that the maximum batch size for Mixtral on A40 (48GB) is 4. The batch size is the number of samples processed in parallel, which is the longest running MoE layer's batch size."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context does not provide specific execution time breakdowns for a sparse Mixtral model fine-tuned with a batch size of 10 on an A40-48GB GPU. The context only provides execution time breakdowns for batch sizes of 1 and 30. Therefore, the answer is 'is_blank'.","is_blank","seconds","[""xia2024""]","[""https://arxiv.org/pdf/2408.04693.pdf""]","is_blank","The context does not provide specific execution time breakdowns for a sparse Mixtral model fine-tuned with a batch size of 10 on an A40-48GB GPU. The context only provides execution time breakdowns for batch sizes of 1 and 30. Therefore, the answer is 'is_blank'."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context explicitly states that we advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements. This indicates that GPU-level power consumption monitoring is not recommended as the preferred method for reporting overall AI energy use.","0","is_blank","[""ebert2024""]","[""https://doi.org/10.1145/nnnnnnn.nnnnnnn""]","We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.","The context explicitly states that we advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements. This indicates that GPU-level power consumption monitoring is not recommended as the preferred method for reporting overall AI energy use."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context states that in a 2023 article by Luccioni et al., the training of the BLOOM model accounted for only half of the model’s overall emissions. This indicates that the training phase was responsible for 50% of the total emissions.","50","percent","[""luccioni2025b""]","[""https://www.semanticscholar.org/paper/Estimating-the-carbon-footprint-of-BLOOM,-a-176B-Luccioni-Viguier/02001""]","In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, since it takes into account different stages of the model life cycle including the manufacturing of computing hardware, idle energy usage, and model deployment, finding that training accounted for only half of the model’s overall emissions.","The context states that in a 2023 article by Luccioni et al., the training of the BLOOM model accounted for only half of the model’s overall emissions. This indicates that the training phase was responsible for 50% of the total emissions."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that for LLaMA-7B, the bare minimum hardware requirement is 1 V100 32GB GPU, assuming no further model compression, optimization, quantization, distillation, etc.","1","V100_32GB_GPU","[""samsi2024""]","[""https://www.semanticscholar.org/paper/From-Words-to-Watts-Benchmarking-the-Energy-Costs-of-Samsi-Griggs/0b4d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d9d""]","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. These limits are imposed by a combination of GPU memory, model size, response length and the number of GPUs. Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64","The context states that for LLaMA-7B, the bare minimum hardware requirement is 1 V100 32GB GPU, assuming no further model compression, optimization, quantization, distillation, etc."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","The context states that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses. This implies that the number of requests needed to consume a 500ml bottle of water varies between 10 and 50.","[10, 50]","requests","[""li2025b""]","[""https://www.semanticscholar.org/paper/Estimating-the-Operational-Water-Consumption-Footprint-of-GPT-3-li/2025b""]","GPT-3 needs to ‘drink’ (i.e., consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","The context states that GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses. This implies that the number of requests needed to consume a 500ml bottle of water varies between 10 and 50."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context explicitly states that one average US home energy use emits 8.30 metric tons of CO2 per year. The answer unit is specified as 'metric tons', so we provide the numeric value.","8.30","metric tons","[""strubell2019""]","[""https://arxiv.org/abs/1906.02243""]","One average US home energy use is estimated to emit 8.30 metric tons (using the sum of emissions from generating electricity, natural gas, liquid petroleum, and fuel oil).","The context explicitly states that one average US home energy use emits 8.30 metric tons of CO2 per year. The answer unit is specified as 'metric tons', so we provide the numeric value."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context mentions that JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM leaderboard, and specifically states that JetMoE-8B achieves a score of 27.8 on the GSM8k benchmark. The answer_unit is 'score', so we keep the numeric value as is.","27.8","score","[""[ref_id=samsi2024]""]","[""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard""]","JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM leaderboard, achieving the best scores in all tasks except ARC-challenge and WinoGrande. Additionally, JetMoE-8B obtains the highest MBPP scores in Python programming. JetMoE-8B scores 27.8 on the GSM8k benchmark.","The context mentions that JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM leaderboard, and specifically states that JetMoE-8B achieves a score of 27.8 on the GSM8k benchmark. The answer_unit is 'score', so we keep the numeric value as is."
