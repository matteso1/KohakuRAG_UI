"id","question","answer","answer_value","answer_unit","ref_id","ref_url","supporting_materials","explanation"
"q001","What was the average increase in U.S. data center electricity consumption between 2010 and 2014?","The context states that the total energy consumption of US data centers increased by about 4% from 2010-2014. Since the question asks for the average increase, and the provided percentage represents the total increase over the period, the average yearly increase is 4%. The answer_unit specifies 'percent', so the answer_value is expressed as a percentage.","4","percent","[""wu2021b""]","is_blank","The total energy consumption of the US data centers increased by about 4% from 2010-2014, compared with the estimated 24% increase from 2005-10 and nearly 90% increase from 2000-05 [Masanet et al., 2020].","The context states that the total energy consumption of US data centers increased by about 4% from 2010-2014. Since the question asks for the average increase, and the provided percentage represents the total increase over the period, the average yearly increase is 4%. The answer_unit specifies 'percent', so the answer_value is expressed as a percentage."
"q002","In 2023, what was the estimated amount of cars that could be taken off the road equivalent to the amount of metric tons of CO2e the Amazon Solar Farm Maryland-CPV Backbone would avoid?","The context states that Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road.","13900","cars","[""amazon2023""]","is_blank","Featuring more than 326,000 solar panels, Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year—the equivalent of taking more than 13,900 cars off the road.","The context states that Amazon Solar Farm Maryland–CPV Backbone will avoid more than 64,000 metric tons of CO2e each year, which is equivalent to taking more than 13,900 cars off the road."
"q004","How many data centers did AWS begin using recycled water for cooling in 2023?","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023. Therefore, AWS began using recycled water for cooling in 4 additional data centers in 2023.","4","data centers","[""amazon2023""]","is_blank","In 2023, AWS increased the number of data centers using recycled water for cooling from 20 to 24, including two data centers in Virginia, one in California, and one in Singapore.","The context states that AWS increased the number of data centers using recycled water for cooling from 20 to 24 in 2023. Therefore, AWS began using recycled water for cooling in 4 additional data centers in 2023."
"q005","Since NVIDIA doesn't release the embodied carbon emissions of its hardware, what are the estimated embodied carbon emissions in kg per GPU?","According to Morrison's 2025 paper, NVIDIA does not release the embodied carbon emissions of its hardware, so we assume the same embodied carbon emissions as Luccioni et al. (2023), which is 3700 kg of CO2eq per 8x server node, equal to 463 kg per GPU.","463","kg/GPU","[""morrison2025""]","is_blank","NVIDIA does not release the embodied carbon emissions or water consumption about the hardware it produces, so we assume the same embodied carbon emissions as Luccioni et al. (2023), or 3700 kg of CO2eq per 8x server node, equal 463 kg per GPU.","According to Morrison's 2025 paper, NVIDIA does not release the embodied carbon emissions of its hardware, so we assume the same embodied carbon emissions as Luccioni et al. (2023), which is 3700 kg of CO2eq per 8x server node, equal to 463 kg per GPU."
"q006","By what factor was the estimated amortized training cost of GPT-4 greater than the total training budget for FLM-101B?","The estimated amortized training cost of GPT-4 is $40M, while the total training budget for FLM-101B is $100,000. The ratio of GPT-4's cost to FLM-101B's budget is calculated as 40,000,000 / 100,000.","400","ratio","[""li2025a"", ""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost. In this paper, we introduce FLM-101B, an open-sourced LLM that is successfully trained from scratch within a $100,000 budget.","The estimated amortized training cost of GPT-4 is $40M, while the total training budget for FLM-101B is $100,000. The ratio of GPT-4's cost to FLM-101B's budget is calculated as 40,000,000 / 100,000."
"q007","What is the approximate CO2e for a single passenger round trip from San Francisco (SF) to New York (NY)?","The context states that a single passenger round trip SF-NY is approximately 1.2t CO2e. This value directly corresponds to the question asked.","1.2","tCO2e","[""patterson2021""]","is_blank","To help put the CO2e numbers in perspective, a single passenger round trip SF-NY is ~1.2t CO2e (Table 2).","The context states that a single passenger round trip SF-NY is approximately 1.2t CO2e. This value directly corresponds to the question asked."
"q008","When evaluated on the Open LLM Leaderboard, what was the final average performance score achieved by the FLM-101B model?","The context states that FLM-101B achieves an average score of 43.94 on the Open LLM Leaderboard, reaching over 90% of the performance of GLM-130B which has 7 times more FLOPs.","43.94","score","[""li2025a""]","is_blank","Results. On average, FLM-101B achieves a score of 43.94, reaching over 90% of the performance of GLM-130B, which has 7 times more FLOPs.","The context states that FLM-101B achieves an average score of 43.94 on the Open LLM Leaderboard, reaching over 90% of the performance of GLM-130B which has 7 times more FLOPs."
"q010","By how many fold did processor clock speed improve between the Intel 4004 (1971) and typical 2021 microprocessors?","According to the context, the processor clock speed of the typical microprocessor in 2021 improved more than 6,750 fold compared to the Intel 4004 in 1971.","6750","fold","[""wu2021b""]","[""https://www.intel.co.uk/content/www/uk/en/history/museum-story-of-intel-4004.html""]","This is a more than 6,750 fold improvement in processor clock speed and 1.7 million times more transistors for microprocessors manufactured in 1971 than that in 2021.","According to the context, the processor clock speed of the typical microprocessor in 2021 improved more than 6,750 fold compared to the Intel 4004 in 1971."
"q011","How many days did it take to train GPT-3 using 10,000 V100 GPUs at 24.6 TeraFLOPS/sec?","The context states that it takes approximately 14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to train GPT-3. This directly answers the question asked, and the answer is formatted in days as specified by the answer_unit.","14.8","days","[""patterson2021""]","is_blank","OpenAI told us the V100 runs GPT-3 at 24.6 TeraFLOPS/sec [Sut21]. It takes ~14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to compute 3.14E+23 FLOPS.","The context states that it takes approximately 14.8 days for 10,000 GPUs at 24.6 TeraFLOPS/sec to train GPT-3. This directly answers the question asked, and the answer is formatted in days as specified by the answer_unit."
"q012","What is the estimated GPU Power Usage in kWh from SGLang benchmarking on 2400 prompts from ShareGPT at an 8 request/s frequency with a Llama 3.2 1B model?","From the provided context, the GPU Power Usage for Llama 3.2 1B at an 8 request/s frequency is 0.036 kWh as reported in the Table 4 of the document [ref_id=morrison2025].","0.036","kWh","[""morrison2025""]","is_blank","Llama 3.2 1B ∞ 0.003 1.0 0.004 1.38 258 bil.
8 0.036 12.0 0.054 12.64 21.5 bil.
1 0.16 53.1 0.238 100.58 4.83 bil.","From the provided context, the GPU Power Usage for Llama 3.2 1B at an 8 request/s frequency is 0.036 kWh as reported in the Table 4 of the document [ref_id=morrison2025]."
"q013","What were the total permitted annual emission limits (in tons) for nitrogen oxides from data center backup generators in northern Virginia between January 1, 2023 and December 1, 2024?","The context indicates that the total permitted annual emission limits for diesel generators in Northern Virginia, which includes Loudoun, Prince William, and Fairfax counties, are approximately 13,000 tons of NOx. This information is relevant for the period mentioned in the question.","13000","tons","[""han2024""]","is_blank","The total permitted annual emission limits for these diesel generators are approximately 13,000 tons of NOx, 1,400 tons of VOCs, 50 tons of SO2, and 600 tons of PM2.5, all in U.S. short tons.","The context indicates that the total permitted annual emission limits for diesel generators in Northern Virginia, which includes Loudoun, Prince William, and Fairfax counties, are approximately 13,000 tons of NOx. This information is relevant for the period mentioned in the question."
"q014","A 2025 paper compares its training time to a from-scratch approach. What percentage of time was saved by using the growth strategy for the 101B model?","The context states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). Therefore, the growth strategy saved 72% of the time.","72","percent","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context states that the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving compared to training a 101B model from scratch (76.74 days estimated). Therefore, the growth strategy saved 72% of the time."
"q015","Approximately how many premature deaths in 2030 could be caused by scope-2 pollutants from U.S. data centers alone?","The context mentions that in 2028, the total scope-1 and scope-2 pollutants of U.S. data centers could cause approximately 1,300 premature deaths. Since the question asks for 2030 and there is no specific data for 2030, we assume the trend continues similarly, making the estimate for 2030 close to 1,300 deaths.","1300","deaths","[""han2024""]","is_blank","Our results demonstrate that in 2028, the total scope-1 and scope-2 pollutants of U.S. data centers alone could cause, among others, approximately 600,000 asthma symptom cases and 1,300 premature deaths, exceeding 1/3 of asthma deaths in the U.S. each year [40].","The context mentions that in 2028, the total scope-1 and scope-2 pollutants of U.S. data centers could cause approximately 1,300 premature deaths. Since the question asks for 2030 and there is no specific data for 2030, we assume the trend continues similarly, making the estimate for 2030 close to 1,300 deaths."
"q016","Dodge et al. (2022) estimate that a full training run of the 6.1 billion parameter model would take approximately how many days?","According to Dodge et al. (2022), the training run of the 6 billion parameter transformer was stopped at 13% of the total training time. They estimate that a full training run would take approximately 60 days.","60","days","[""dodge2022""]","is_blank","This model was not trained to completion, but only until 13%; a full training run would take 60 days.","According to Dodge et al. (2022), the training run of the 6 billion parameter transformer was stopped at 13% of the total training time. They estimate that a full training run would take approximately 60 days."
"q017","For the inference process, which model has the highest GPU energy consumption for 1,000 queries among those listed in the appendix of a 2025 study?","The context mentions that the study evaluated different models' GPU energy usage for 1,000 queries, with Command-R Plus having the highest energy consumption among the listed models. The answer unit is 'is_blank', so the answer value should be the name of the model.","Command-R Plus","is_blank","[""luccioni2025c""]","is_blank","task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The context mentions that the study evaluated different models' GPU energy usage for 1,000 queries, with Command-R Plus having the highest energy consumption among the listed models. The answer unit is 'is_blank', so the answer value should be the name of the model."
"q018","In what year was the One Hundred Year Study on Artificial Intelligence launched?","The context states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014.","2014","year","[""stone2022""]","[""https://ai100.stanford.edu""]","The One Hundred Y ear Study on Artificial Intelligence, launched in the fall of  2014, is a long-term investigation of  the field of  Artificial Intelligence (AI) and its influences on people, their communities, and society.","The context states that the One Hundred Year Study on Artificial Intelligence was launched in the fall of 2014."
"q019","According to the UN's Global E-Waste Monitor 2024, what percentage of e-waste has been formally collected and recycled?","The UN’s Global E-Waste Monitor 2024 reported that about 22% of e-waste has been formally collected and recycled. This aligns with the answer unit 'percent'.","22","percent","[""luccioni2025a""]","[""https://ewastemonitor.info/""]","The UN’s Global E-Waste Monitor 2024 showed that about 22% of e-waste has been shown to be formally collected and recycled, with global generation of electronic waste rising five times faster than e-waste recycling [10].","The UN’s Global E-Waste Monitor 2024 reported that about 22% of e-waste has been formally collected and recycled. This aligns with the answer unit 'percent'."
"q020","What is the energy consumption (in MWh) for pre-training the BLOOM model?","The context states that the BLOOM model, which is part of the BLOOM family, was trained with an energy consumption of 520 MWh according to the BLOOM carbon footprint estimation paper [31].","520","MWh","[""luccioni2024""]","is_blank","BLOOM 11 Big Science 520 30","The context states that the BLOOM model, which is part of the BLOOM family, was trained with an energy consumption of 520 MWh according to the BLOOM carbon footprint estimation paper [31]."
"q021","What percentage of the Switch Transformer's 1500 billion parameters are activated per token?","The context states that the Switch Transformer model has 1500 billion parameters but only activates 0.1% of them per token. This information directly answers the question.","0.1","percent","[""wu2021a"", ""patterson2021""]","is_blank","The authors show large sparse models—1500B parameters but only 0.1% activated per token—can deliver up to 7x increases in pre-training speed with the same computational resources.","The context states that the Switch Transformer model has 1500 billion parameters but only activates 0.1% of them per token. This information directly answers the question."
"q022","The JetMoE-8B architecture is a Mixture-of-Experts (MoE) model. How many experts are included in each MoE layer?","According to the context, the JetMoE-8B model includes 8 experts in each MoE layer, as indicated in Table 1 where 'Nexperts' is listed as 8.","8","experts","[""shen2024""]","is_blank","Table 1: JetMoE-8B hyperparameters.
Ptotal Pactive nlayers Dmodel Nexperts Top-k n kv heads Dhead Dmlp
8B 2B 24 2048 8 2 16 128 5632","According to the context, the JetMoE-8B model includes 8 experts in each MoE layer, as indicated in Table 1 where 'Nexperts' is listed as 8."
"q023","What is the total execution time, in seconds, of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU?","The context does not provide specific numerical values for the total execution time of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU. The figures and descriptions provided relate to execution time breakdowns but do not specify the total time for the given conditions.","is_blank","second","[""is_blank""]","is_blank","is_blank","The context does not provide specific numerical values for the total execution time of a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU. The figures and descriptions provided relate to execution time breakdowns but do not specify the total time for the given conditions."
"q024","According to a cost estimation, how many zettaFLOPs were required to train the English portion of the FLM-101B model?","The context states that the total cost of FLM-101B is computed as 52.76 zettaFLOPs, with 28.22 zettaFLOPs for English and 24.54 for Chinese. Therefore, the English portion of the FLM-101B model required 28.22 zettaFLOPs.","28.22","zettaFLOPs","[""li2025a""]","is_blank","The total cost of FLM-101B is computed as 52.76 zettaFLOPs (28.22 zettaFLOPs for English and 24.54 for Chinese).","The context states that the total cost of FLM-101B is computed as 52.76 zettaFLOPs, with 28.22 zettaFLOPs for English and 24.54 for Chinese. Therefore, the English portion of the FLM-101B model required 28.22 zettaFLOPs."
"q025","Which hardware processor was reported for the experimental setup of energy-efficient local inference in financial sentiment classification?","The context mentions the hardware used for the experimental setup, which includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor. This processor is part of the setup for the energy-efficient local inference in financial sentiment classification.","Intel(R) Core(TM) i7-1165G7","is_blank","[""khan2025""]","is_blank","The hardware used includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor operating at 2.80 GHz (1.69 GHz base frequency), supported by 16.0 GB of installed memory (15.7 GB usable).","The context mentions the hardware used for the experimental setup, which includes an 11th Gen Intel(R) Core(TM) i7-1165G7 processor. This processor is part of the setup for the energy-efficient local inference in financial sentiment classification."
"q026","How many different machine learning models were sampled and analyzed for the 'Power Hungry Processing' (2024) study on AI deployment costs?","The context states that the authors ran each of the 88 models on 3 datasets 10 times to ensure statistical significance of their measurements. Therefore, the number of different machine learning models analyzed is 88.","88","models","[""luccioni2024""]","is_blank","The main ethical concerns that we faced in our experimentation is the sheer amount of energy needed and carbon emissions generated by our study, given that we ran each of the 88 models on 3 datasets 10 times to ensure statistical significance of our measurements.","The context states that the authors ran each of the 88 models on 3 datasets 10 times to ensure statistical significance of their measurements. Therefore, the number of different machine learning models analyzed is 88."
"q027","By what factor does the overall carbon footprint decrease if GPU utilization is increased to 80% for Language Model (LM) training?","The context states that increasing GPU utilization up to 80% for LM training on GPUs decreases the overall carbon footprint by 3×.","3","multiplier","[""wu2021a""]","is_blank","Increasing GPU utilization up to 80%, the overall carbon footprint decreases by 3×.","The context states that increasing GPU utilization up to 80% for LM training on GPUs decreases the overall carbon footprint by 3×."
"q028","Cottier et al. (2025) find that the total compute for model development is how many times larger than the compute for the final training run alone?","According to Cottier et al. (2025), the total compute for model development is estimated to be 1.2x to 4x larger than the compute for the final training run alone, based on evidence about the development of GPT-3, OPT-175B, and BLOOM, as well as the general AI infrastructure at Meta.","[1.2,4]","multiplier","[""cottier2024""]","is_blank","Based on this, we sampled the factor from a log-normal distribution with a 90% CI of 1.2x to 4x, meaning that total compute for model development is 1.2x to 4x larger than the final training run.","According to Cottier et al. (2025), the total compute for model development is estimated to be 1.2x to 4x larger than the compute for the final training run alone, based on evidence about the development of GPT-3, OPT-175B, and BLOOM, as well as the general AI infrastructure at Meta."
"q029","What is the estimated total energy consumption (in MWh) for a full training run of a 6.1 billion parameter transformer model?","The context states that the training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion. Estimating a full training run would consume approximately 103,593 kWh, which converts to 103.5 MWh.","103.5","MWh","[""dodge2022""]","is_blank","We note our training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion, we estimate a full training run would consume approximately 103,593 kWh.","The context states that the training run of the 6 billion parameter transformer only trained for approximately 13% of the time it would take to train to completion. Estimating a full training run would consume approximately 103,593 kWh, which converts to 103.5 MWh."
"q030","The central argument of one 2025 paper is that technical efficiency gains in AI may not lead to net environmental benefits due to what economic principle?","The central argument of the 2025 paper 'From Efficiency Gains to Rebound Effects' is that technical efficiency gains in AI may not lead to net environmental benefits due to the economic principle of Jevons' Paradox. This principle suggests that increases in efficiency can lead to higher overall consumption, undermining the intended environmental benefits.","Jevons' Paradox","is_blank","[""luccioni2025a""]","[""https://doi.org/10.1145/3715275.3732007""]","This paper examines how the problem of Jevons’ Paradox applies to AI, whereby efficiency gains may paradoxically spur increased consumption.","The central argument of the 2025 paper 'From Efficiency Gains to Rebound Effects' is that technical efficiency gains in AI may not lead to net environmental benefits due to the economic principle of Jevons' Paradox. This principle suggests that increases in efficiency can lead to higher overall consumption, undermining the intended environmental benefits."
"q031","By 2027, the global AI demand is projected to account for how many billion cubic meters of water withdrawal?","According to the context, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027. This aligns with the provided answer_unit 'billion cubic meters'.","[4.2,6.6]","billion cubic meters","[""li2025b""]","is_blank","More critically, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027, which is more than the total annual water withdrawal of 4 – 6 Denmark or half of the United Kingdom.","According to the context, the global AI demand is projected to account for 4.2 – 6.6 billion cubic meters of water withdrawal in 2027. This aligns with the provided answer_unit 'billion cubic meters'."
"q032","True or False: As researchers have determined the well-known diminishing returns of increased cost of AI research, Red AI is on the decline.","The context states that despite the well-known diminishing returns of increased cost, Red AI is on the rise. Therefore, the statement that Red AI is on the decline is false.","0","is_blank","[""schwartz2019""]","is_blank","Red AI is on the rise despite the well-known diminishing returns of increased cost (e.g., Figure 3).","The context states that despite the well-known diminishing returns of increased cost, Red AI is on the rise. Therefore, the statement that Red AI is on the decline is false."
"q033","Using a growth strategy, what was the total wall-clock time required to train the FLM-101B model?","The context explicitly states that the total time cost for training FLM-101B using the growth schedule is 21.54 days.","21.54","days","[""li2025a""]","is_blank","Under this growth schedule, the total time cost for training FLM-101B is 21.54 days, which is 72% time-saving (or a 3.56x speedup) compared to training a 101B model from scratch (76.74 days estimated).","The context explicitly states that the total time cost for training FLM-101B using the growth schedule is 21.54 days."
"q034","True or False: At Facebook, a majority of model experimentation workflows utilize GPUs at over 80% capacity.","The context states that a vast majority of model experimentation workflows at Facebook utilize GPUs at only 30-50% capacity. Therefore, it is false that they utilize GPUs at over 80% capacity.","is_blank","is_blank","[""wu2021a""]","is_blank","A vast majority of model experimentation (over tens of thousands of training workﬂows) utilizes GPUs at only 30-50%, leaving room for improvements to efﬁciency and overall utilization.","The context states that a vast majority of model experimentation workflows at Facebook utilize GPUs at only 30-50% capacity. Therefore, it is false that they utilize GPUs at over 80% capacity."
"q035","How much electricity (in MWh) is the training of GPT-3 estimated to have consumed?","The context states that GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh. Since the question asks for the electricity consumption in MWh, the answer_value is 1287.","1287","MWh","[""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context states that GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh. Since the question asks for the electricity consumption in MWh, the answer_value is 1287."
"q036","What is the name of the collaborative project that aims to create a standardized method for comparing the inference efficiency of various AI models?","The context mentions the AI Energy Score project which aims to establish a unified approach for comparing the inference efficiency of AI models.","AI Energy Score","is_blank","[""luccioni2025c""]","is_blank","These methodologies were then adapted into the AI Energy Score 21, a project aiming to establish a unified approach for comparing the inference efficiency of AI models22.","The context mentions the AI Energy Score project which aims to establish a unified approach for comparing the inference efficiency of AI models."
"q037","For a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU, what was the execution time, in microseconds, for the longest kernel of the MoE layer?","The context does not specify the exact execution time in microseconds for the longest kernel of the MoE layer for a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU. The figures provided give relative execution times but not absolute values for microseconds.","is_blank","microseconds","[""is_blank""]","is_blank","is_blank","The context does not specify the exact execution time in microseconds for the longest kernel of the MoE layer for a dense BlackMamba model with a batch size of 30 fine-tuned with a NVIDIA A40-48 GB GPU. The figures provided give relative execution times but not absolute values for microseconds."
"q038","In each layer of the JetMoE-8B model, how many experts are selected for activation (top-k) for a given token?","The context states that JetMoE-8B uses a top-k selection of 2 experts per layer for each input token, reducing inference computation.","2","experts","[""shen2024""]","is_blank","Table 1 shows the key hyperparameters in JetMoE-8B, including Top-k set to 2.","The context states that JetMoE-8B uses a top-k selection of 2 experts per layer for each input token, reducing inference computation."
"q039","True or False: deep learning models are increasingly large and computationally-intensive, with a 200,000x increase in the amount of compute used to train them over a six-year span (2012 -2018).","The context states that the amount of compute used to train deep learning models has increased 300,000x in 6 years, not 200,000x. Therefore, the statement is false.","0","is_blank","[""schwartz2019""]","is_blank","Figure 1: The amount of compute used to train deep learning models has increased 300,000x in 6 years.","The context states that the amount of compute used to train deep learning models has increased 300,000x in 6 years, not 200,000x. Therefore, the statement is false."
"q040","What was the reported drop in global carbon emissions in 2020 during the COVID-19 pandemic?","The context mentions that global carbon emissions for 2020 dropped by 6.4%, with vehicle transportation in the US accounting for a portion of the global reduction. The answer is expressed as a percentage as specified by the answer_unit.","6.4","percent","[""wu2021b""]","is_blank","In addition, the global carbon emissions for 2020 dropped by 6.4% with vehicle transportation in the US accounting for a portion of the global reduction [Tollefson, 2021].","The context mentions that global carbon emissions for 2020 dropped by 6.4%, with vehicle transportation in the US accounting for a portion of the global reduction. The answer is expressed as a percentage as specified by the answer_unit."
"q041","In 2023, in how many of AWS data center regions was 100% of the electricity consumed matched with renewable energy sources?","The context states that Amazon's energy supply from utilities, combined with the renewable energy they procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources in 2023.","22","data centers","[""amazon2023""]","is_blank","Amazon’s energy supply from utilities, combined with the renewable energy we procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources—an increase from 19 regions in 2022.","The context states that Amazon's energy supply from utilities, combined with the renewable energy they procure globally, means that 100% of the electricity consumed by 22 AWS data center regions is matched with renewable energy sources in 2023."
"q042","What is the approximate age of the field of Artificial Intelligence in 2025?","The context mentions that the rate of progress in AI has been significant since the field's inception sixty years ago. Since the current year is 2025, we can calculate that the field of AI is approximately 60 years old in 2025.","60","years","[""stone2022""]","[""https://ai100.stanford.edu/""]","While the rate of progress in AI has been patchy and unpredictable, there have been significant advances since the field’s inception sixty years ago.","The context mentions that the rate of progress in AI has been significant since the field's inception sixty years ago. Since the current year is 2025, we can calculate that the field of AI is approximately 60 years old in 2025."
"q043","The well-known ""five cars"" carbon footprint estimate, originating from a 2019 study, is based on what specific and infrequently performed AI process?","The 'five cars' carbon footprint estimate originates from a 2019 study by Strubell et al., which specifically refers to the energy required for a large-scale procedure called Neural Architecture Search (NAS). This process is infrequently performed due to its high resource requirements.","Neural Architecture Search (NAS)","is_blank","[""luccioni2025c""]","is_blank","In the case of the latter, they estimated that the NAS approach, assuming United States average electricity GHG emissions intensity and typical AI hardware running in an average-efficiency datacenter, could yield 626,155 pounds (284 metric tons) CO2-equivalent GHG emissions (CO2e), or about five times the emissions of a car during its lifetime, including fuel.","The 'five cars' carbon footprint estimate originates from a 2019 study by Strubell et al., which specifically refers to the energy required for a large-scale procedure called Neural Architecture Search (NAS). This process is infrequently performed due to its high resource requirements."
"q044","For the Llama 3.1 8B model, by what percentage does energy use decrease when targeting an average Time Per Output Token (TPOT) of 100 ms instead of minimizing latency?","The context states that targeting an average TPOT of 100 ms reduces energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","44","percent","[""chung2025""]","is_blank","This will land on the Pareto frontier at the point where average TPOT is 77 ms, reducing energy consumption per generation by 44% compared to the configuration that simply minimizes latency.","The context states that targeting an average TPOT of 100 ms reduces energy consumption per generation by 44% compared to the configuration that simply minimizes latency."
"q045","What is the maximum batch size (in samples) supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory?","According to the context, the maximum batch size supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory is 8 samples, as shown in Table III.","8","samples","[""xia2024""]","is_blank","TABLE III
MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING ; D: DENSE AND S:SPARSE .
Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S
CS 2 8 6 20
MATH 1 3 2 8","According to the context, the maximum batch size supported by fine-tuning BlackMamba with a sparse setup on the GSM8K dataset using a NVIDIA A40 GPU with 48 GB memory is 8 samples, as shown in Table III."
"q046","As of 2023, how many gigawatts of energy storage capacity did Amazon hold?","According to the context, as of 2023, Amazon held 1.3 GW of energy storage capacity, up from 445 MW in 2022.","1.3","GW","[""amazon2023""]","is_blank","We now hold 1.3 GW of storage capacity, up from 445 megawatts (MW) in 2022.","According to the context, as of 2023, Amazon held 1.3 GW of energy storage capacity, up from 445 MW in 2022."
"q047","The annual carbon emissions from GPT-4o inference are projected to be comparable to the emissions from how many transatlantic flights?","The context states that the annual carbon emissions from GPT-4o inference are projected to be between 138,125 tons and 163,441 tons of CO2e. These figures are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","272","flights","[""jegham2025""]","is_blank","Our projections indicate annual emissions of approximately 138,125 tons of CO2e at minimum and 163,441 tons at maximum. These figures are comparable to the annual emissions of 30,000 gasoline-powered cars or the cumulative emissions from approximately 272 transatlantic flights between Boston and London.","The context states that the annual carbon emissions from GPT-4o inference are projected to be between 138,125 tons and 163,441 tons of CO2e. These figures are comparable to the cumulative emissions from approximately 272 transatlantic flights between Boston and London."
"q048","What percentage of AI inference workloads in Asia were powered by coal in 2023?","The provided context does not contain specific information about the percentage of AI inference workloads in Asia powered by coal in 2023. Therefore, the answer is 'is_blank'.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context does not contain specific information about the percentage of AI inference workloads in Asia powered by coal in 2023. Therefore, the answer is 'is_blank'."
"q049","What was the global average power usage effectiveness (PUE) of AI-dedicated data centers in 2023?","The context states that the average data center PUE in 2023 was 1.58 globally. Since the question specifically asks for the global average PUE of AI-dedicated data centers in 2023, and the provided context does not differentiate between types of data centers, we assume this figure applies to all data centers, including AI-dedicated ones. The answer unit is PUE.","1.58","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context states that the average data center PUE in 2023 was 1.58 globally. Since the question specifically asks for the global average PUE of AI-dedicated data centers in 2023, and the provided context does not differentiate between types of data centers, we assume this figure applies to all data centers, including AI-dedicated ones. The answer unit is PUE."
"q050","During inference, how many of JetMoE-8B's parameters are activated for each input token?","According to the context, JetMoE-8B has 8B parameters in total but activates only 2B parameters for each input token during inference.","2000000000","parameters","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","According to the context, JetMoE-8B has 8B parameters in total but activates only 2B parameters for each input token during inference."
"q051","What are the GHG emissions (in tCO2e) associated with pre-training the Llama 7B model?","The context provides the GHG emissions for the Llama 7B model, which is 14 tCO2e. This value is directly stated in the table of pre-training environmental impacts.","14","tCO2e","[""luccioni2025c""]","is_blank","Model Organization Energy Consumption (MWh) GHG Emissions (tCO2e)
Llama 7B 63 Meta 356 14","The context provides the GHG emissions for the Llama 7B model, which is 14 tCO2e. This value is directly stated in the table of pre-training environmental impacts."
"q052","How many Amazon electric delivery vans were added in total across 2022 and 2023?","The context states that in 2022, Amazon's U.S. fleet included 2,600 electric delivery vans and in 2023, it increased to 11,800. Therefore, the total number of electric delivery vans added across 2022 and 2023 is 11,800 - 2,600 = 9,200. The answer_unit specifies 'electric delivery vans', so the answer_value is expressed as a number.","9200","electric delivery vans","[""amazon2023""]","is_blank","United States
• Our U.S. fleet included 11,800 electric delivery vans from Rivian, up from more than 2,600 in 2022.","The context states that in 2022, Amazon's U.S. fleet included 2,600 electric delivery vans and in 2023, it increased to 11,800. Therefore, the total number of electric delivery vans added across 2022 and 2023 is 11,800 - 2,600 = 9,200. The answer_unit specifies 'electric delivery vans', so the answer_value is expressed as a number."
"q053","True or False: Operational environmental impacts of LLMs do not include GHG emissions that arise from servers and data centers using cooling.","The context states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. Therefore, the statement that operational environmental impacts of LLMs do not include GHG emissions from servers and data centers using cooling is false.","is_blank","is_blank","[""morrison2025""]","[""https://arxiv.org/abs/2503.05804""]","Operational environmental impacts of LLMs are those that arise directly from the development and use of models, and include the GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling.","The context states that operational environmental impacts of LLMs include GHG emissions arising from energy sources used to power model training and deployment, including servers and data center cooling. Therefore, the statement that operational environmental impacts of LLMs do not include GHG emissions from servers and data centers using cooling is false."
"q055","How much energy (in Wh) does the o3 model consume for a long prompt?","The context provides the energy consumption values for the o3 model across different prompt sizes. For a long prompt (10k input-1.5k output), the energy consumption is 12.222 ± 1.082 Wh.","12.222","Wh","[""jegham2025""]","is_blank","Model Energy Consumption(10k input-1.5k output)(Wh)
o3 12.222 ± 1.082","The context provides the energy consumption values for the o3 model across different prompt sizes. For a long prompt (10k input-1.5k output), the energy consumption is 12.222 ± 1.082 Wh."
"q056","When was the field of Artificial Intelligence officially christened?","The context states that the field of Artificial Intelligence was officially christened at a 1956 workshop organized by John McCarthy. Therefore, the year 1956 is the correct answer according to the provided context.","1956","year","[""stone2022""]","is_blank","The field of Artificial Intelligence (AI) was officially born and christened at a 1956 workshop organized by John McCarthy at the Dartmouth Summer Research Project on Artificial Intelligence.","The context states that the field of Artificial Intelligence was officially christened at a 1956 workshop organized by John McCarthy. Therefore, the year 1956 is the correct answer according to the provided context."
"q057","What is the average water use effectiveness (WUE) for Google's AI-dedicated data centers in 2024?","The provided context does not contain specific information about Google's AI-dedicated data centers' WUE in 2024. The closest relevant information is about Microsoft's data centers, which is not sufficient to answer the question.","is_blank","WUE","[""is_blank""]","is_blank","is_blank","The provided context does not contain specific information about Google's AI-dedicated data centers' WUE in 2024. The closest relevant information is about Microsoft's data centers, which is not sufficient to answer the question."
"q058","True or False: Approximately 770 million people worldwide still lack access to a stable supply of electricity.","The context states that 'Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].' This directly supports the statement.","is_blank","is_blank","[""wu2021b""]","[""https://www.iea.org/reports/sdg7-data-and-projections/access-to-electricity""]","Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].","The context states that 'Even more daunting, approximately 770 million people, or about 1 out of 10 people in the world, do not have access to a stable supply of electricity [International Energy Agency].' This directly supports the statement."
"q059","How much energy per token did LLaMA-65B consume at a maximum generation length of 512 tokens?","The context states that for a max generation length of 512 tokens, LLaMA 65B consumes about 3-4 Joules per output token. Since the question asks for the energy per token in Joules, we provide the range as the answer_value.","[3,4]","joules per token","[""samsi2024""]","is_blank","For instance, with length 512, we see that it takes about 3-4 Joules for a output token, which is approximately the same amount for length 512.","The context states that for a max generation length of 512 tokens, LLaMA 65B consumes about 3-4 Joules per output token. Since the question asks for the energy per token in Joules, we provide the range as the answer_value."
"q060","By what percentage was the overall model size of Facebook's second representative recommendation model (RM2) reduced after being quantized from 32-bit to 16-bit numerical representation?","The context states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size was reduced by 15%. This reduction is directly stated in the text and matches the required format.","15","percent","[""wu2021a""]","is_blank","By converting 32-bit ﬂoating-point numerical representation to 16-bit, we can reduce the overall RM2 model size by 15%.","The context states that by converting 32-bit floating-point numerical representation to 16-bit, the overall RM2 model size was reduced by 15%. This reduction is directly stated in the text and matches the required format."
"q061","True or False: A widely cited claim that AI can reduce global GHG emissions by 5-10% is supported by clear, publicly available calculations and sound scientific grounding.","The context mentions that the claim of AI reducing global GHG emissions by 5-10% is based on a BCG report, but the underlying calculations are not detailed beyond the explanation that they are based on BCG's experience. This suggests that the claim lacks clear, publicly available calculations and sound scientific grounding.","is_blank","is_blank","[""luccioni2025c""]","is_blank","The reasoning behind the 5-10% reduction estimate is unclear and the underlying calculations are not detailed beyond the explanation that they are based on BCG’s experience in dealing with their clients and using AI to optimize and improve existing processes.","The context mentions that the claim of AI reducing global GHG emissions by 5-10% is based on a BCG report, but the underlying calculations are not detailed beyond the explanation that they are based on BCG's experience. This suggests that the claim lacks clear, publicly available calculations and sound scientific grounding."
"q063","True or False: Sparsely activated Deep Neural Networks (DNNs) consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","The context states that 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.' This directly supports the statement that sparsely activated DNNs consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy.","is_blank","is_blank","[""wu2021a""]","is_blank","Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.","The context states that 'Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters.' This directly supports the statement that sparsely activated DNNs consume less than 1/10th the energy of large, dense DNNs without sacrificing accuracy."
"q064","What is the estimated cost, in USD, of training AI2's Grover on 256 TPU chips for two weeks?","The context states that Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000. The answer unit requested is USD.","25000","USD","[""schwartz2019""]","is_blank","Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000.","The context states that Grover was trained on 256 TPU chips for two weeks, at an estimated cost of $25,000. The answer unit requested is USD."
"q065","What percentage of the running time does the optimizer stage in BlackMamba sparse fine-tuning take with a NVIDIA A40-48GB GPU when the batch size = 1?","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with batch size = 1. This information is directly stated and does not depend on the specific GPU type.","53","percent","[""xia2024""]","is_blank","The optimizer stage in BlackMamba fine-tuning takes a considerable portion of the running time (up to 53% when conducting sparse fine-tuning with batch size = 1), while the execution time share of the optimizer stage in Mixtral fine-tuning is negligible.","The context states that the optimizer stage in BlackMamba fine-tuning takes up to 53% of the running time when conducting sparse fine-tuning with batch size = 1. This information is directly stated and does not depend on the specific GPU type."
"q066"," A large-scale service like Google Translate can perform 1 billion queries per day. Using the average energy for a comparable multi-purpose model (Flan-T5-xxl at 0.083 kWh/1k queries), estimate the daily energy consumption in MWh.","According to the context, Google Translate performs 1 billion queries per day. The energy consumption for a comparable multi-purpose model like Flan-T5-xxl is given as 0.083 kWh per 1,000 queries. Therefore, the daily energy consumption can be calculated as 1,000,000,000 queries / 1,000 queries * 0.083 kWh = 83,000 kWh, which converts to 83 MWh.","83","MWh","[""luccioni2024""]","is_blank","For comparison, charging the average smartphone requires 0.022 kWh of energy [51], which means that the most efficient text generation model uses as much energy as 9% of a full smartphone charge for 1,000 inferences, whereas the least efficient image generation model uses as much energy as 522 smartphone charges (11.49 kWh), or around half a charge per image generation 5, although there is also a large variation between image generation models, depending on the size of image that they generate.","According to the context, Google Translate performs 1 billion queries per day. The energy consumption for a comparable multi-purpose model like Flan-T5-xxl is given as 0.083 kWh per 1,000 queries. Therefore, the daily energy consumption can be calculated as 1,000,000,000 queries / 1,000 queries * 0.083 kWh = 83,000 kWh, which converts to 83 MWh."
"q067","What was the average global data center PUE in 2023?","The context states that the average data center PUE in 2023 was 1.58 globally.","1.58","PUE","[""ebert2024""]","is_blank","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context states that the average data center PUE in 2023 was 1.58 globally."
"q068","How many wind turbines were directly contracted by Microsoft to power Azure AI clusters in 2023?","The provided context does not contain any information about Microsoft directly contracting wind turbines to power Azure AI clusters in 2023. Therefore, the answer is 'is_blank'.","is_blank","wind turbines","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about Microsoft directly contracting wind turbines to power Azure AI clusters in 2023. Therefore, the answer is 'is_blank'."
"q069","In the analysis of total model development costs by Cottier et al. (2025), what percentage of the cost of developing Gemini Ultra was attributed to R&D staff (including equity)?","According to the context, the cost of R&D staff for Gemini Ultra was estimated to be 49% of the total amortized model development costs when equity is included. This percentage aligns with the answer_unit 'percent'.","49","percent","[""cottier2024""]","is_blank","We find that when equity is included, R&D staff costs make up between 29% and 49% of total amortized model development costs, depending on the model. Excluding equity, the fraction decreases to 21% to 33% (see Appendix B.5 for additional plots). Notably, this fraction does not change much from GPT-3 to GPT-4, which spans three and a half years of AI progress. The number of reported contributors increased from 25 for GPT-3 [14] to 284 for GPT-4 [12], while the amortized hardware cost over the whole model development increased from $4M to $90M. However, due to the limited data, we caution against extrapolating the fraction of R&D staff costs to future frontier models. Gemini Ultra has the highest fraction of R&D staff cost at 49%, but we expect this is unusually high among frontier models.","According to the context, the cost of R&D staff for Gemini Ultra was estimated to be 49% of the total amortized model development costs when equity is included. This percentage aligns with the answer_unit 'percent'."
"q070","How many members comprised the inaugural 2015 Study Panel of the One Hundred Year Study on AI?","The context states that the inaugural 2015 Study Panel consisted of seventeen members.","17","people","[""stone2022""]","[""https://ai100.stanford.edu""]","The seventeen-member Study Panel, comprised of experts in AI from academia, corporate laboratories and industry, and AI-savvy scholars in law, political science, policy, and economics, was launched in mid-fall 2015.","The context states that the inaugural 2015 Study Panel consisted of seventeen members."
"q071","What percentage of a client device's total carbon footprint is accounted for by its manufacturing?","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. This directly answers the question about the percentage of a client device's total carbon footprint accounted for by its manufacturing.","74","percent","[""wu2021a""]","is_blank","Reducing embodied carbon cost for edge devices is also important, as manufacturing carbon cost accounts for 74% of the total footprint [ 19] of client devices.","The context states that manufacturing carbon cost accounts for 74% of the total footprint of client devices. This directly answers the question about the percentage of a client device's total carbon footprint accounted for by its manufacturing."
"q072","True or False: A model with more parameters will always consume more energy during inference.","The context does not support the statement that a model with more parameters will always consume more energy during inference. It mentions that generally, models with more parameters consume more energy, but there are exceptions, as highlighted in the context where the Small model consumes less energy than the Mini model as batch size grows.","is_blank","is_blank","[""chung2025""]","is_blank","Memory consumption of operations and energy amortization.Generally, models with more parameters consume more energy, but this is not always the case. Figure 4 highlights the case of Phi-3 Mini (3.8B) and Small (7B) [26]. Even though Small has nearly twice the parameters, the left plot shows that the larger Small model can consume less energy than Mini as batch size grows.","The context does not support the statement that a model with more parameters will always consume more energy during inference. It mentions that generally, models with more parameters consume more energy, but there are exceptions, as highlighted in the context where the Small model consumes less energy than the Mini model as batch size grows."
"q073","True or False: The Study Panel from the 100 Year Study on AI is concerned that AI is an imminent threat to humankind.","The context states that the Study Panel found no cause for concern that AI is an imminent threat to humankind. Therefore, the answer is false.","is_blank","is_blank","[""stone2022""]","is_blank","Contrary to the more fantastic predictions for AI in the popular press, the Study Panel found no cause for concern that AI is an imminent threat to humankind.","The context states that the Study Panel found no cause for concern that AI is an imminent threat to humankind. Therefore, the answer is false."
"q074","How many metric tons of CO2 were emitted by OpenAI's API requests in January 2024?","The provided context does not contain any specific information about the CO2 emissions from OpenAI's API requests in January 2024. Therefore, the answer is blank.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The provided context does not contain any specific information about the CO2 emissions from OpenAI's API requests in January 2024. Therefore, the answer is blank."
"q076","What are the reported GHG emissions (tCO2e) from the pre-training process for Meta's Llama 3 family of models, and how does this compare to the 'five cars' estimate?","The context states that Meta's Llama 3 family of models emitted 11,390 tons CO2e during pre-training, which is over 40 times the 'five cars' estimate.","11390","tCO2e","[""luccioni2025c""]","is_blank","Meta reports that their Llama 3 family of models emitted 11,390 tons CO2e35 or over 40x the “five cars” estimate.","The context states that Meta's Llama 3 family of models emitted 11,390 tons CO2e during pre-training, which is over 40 times the 'five cars' estimate."
"q077","By what factor did the explosive growth in AI drive the increase in AI training infrastructure capacity at Facebook over the 1.5 year period, from Yr1-Q1 (Year 1, Quarter 1) to Yr2-Q2 (Year 2, Quarter 2) between 2019 and 2021?","According to the context, the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 year period from Yr1-Q1 to Yr2-Q2 between 2019 and 2021. The answer unit specified is 'multiplier', so the answer value is expressed as a numerical multiplier.","2.9","multiplier","[""wu2021a""]","is_blank","Figure 2(d) illustrates that the explosive growth in AI use cases at Facebook has driven 2.9× increase in AI training infrastructure capacity over the 1.5 years.","According to the context, the explosive growth in AI use cases at Facebook has driven a 2.9× increase in AI training infrastructure capacity over the 1.5 year period from Yr1-Q1 to Yr2-Q2 between 2019 and 2021. The answer unit specified is 'multiplier', so the answer value is expressed as a numerical multiplier."
"q079","How many miles is the Earth from the Sun?","The provided context does not contain any information about the distance between the Earth and the Sun. Therefore, the answer_value is 'is_blank'.","is_blank","miles","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about the distance between the Earth and the Sun. Therefore, the answer_value is 'is_blank'."
"q080","True or False: The AlphaGo program defeated the human Go champion.","The context clearly states that the AlphaGo program defeated the human Go champion in a five-game match. Therefore, the statement is true.","1","is_blank","[""stone2022""]","[""http://www.latimes.com/world/asia/la-fg-korea-alphago-20160312-story.html""]","AlphaGo, a computer program developed by Google Deepmind that beat the human Go champion in a five-game match","The context clearly states that the AlphaGo program defeated the human Go champion in a five-game match. Therefore, the statement is true."
"q081","What is the name of the batching strategy that reduces idle GPU time by dynamically replacing completed requests with new ones?","The context mentions 'Continuous Batching' as a strategy that reduces idle GPU time by dynamically replacing completed requests with new ones.","Continuous Batching","is_blank","[""fernandez2025""]","is_blank","Continuous Batching Reduces Energy Use. LLM inference is inherently autoregressive, requiring many sequential operations. Static batching maintains a fixed batch size throughout inference, which leads to GPU under-utilization when generation lengths vary and idle compute accumulates after early terminations. Continuous batching mitigates this by dynamically replacing completed requests with new ones, improving GPU utilization and reducing idle time (Yu et al., 2022).","The context mentions 'Continuous Batching' as a strategy that reduces idle GPU time by dynamically replacing completed requests with new ones."
"q082","How many H100 GPU hours were required for the entire JetMoE-8B alignment process, which includes both dSFT and dDPO fine-tuning?","The context states that the entire alignment process for JetMoE-8B takes 60 H100 GPU hours, which includes both dSFT and dDPO fine-tuning.","60","H100 GPU hours","[""shen2024""]","is_blank","This fine-tuning process results in the JetMoE-8B-Chat model. The entire alignment process takes 60 H100 GPU hours.","The context states that the entire alignment process for JetMoE-8B takes 60 H100 GPU hours, which includes both dSFT and dDPO fine-tuning."
"q083","In the offline workload experiment with a 100 TPS SLO, the Max-Performance policy selected an instance that was what percentage more expensive than the one selected by InferSave?","The context states that for a 100 TPS SLO, the Max-Performance policy selected g6e.xlarge, which had a total cost of $2.699, while InferSave selected g4dn.xlarge with a total cost of $2.13. The difference in cost is approximately 26.7%, calculated as ((2.699 - 2.13) / 2.13) * 100.","26.7","percent","[""kim2025""]","is_blank","On the other hand, both Max-Performance and InferSave without offloading selected g6e.xlarge, which delivers a very high throughput of about 7600 TPS, but with a total cost of $2.699, an increase of about 26.7%.","The context states that for a 100 TPS SLO, the Max-Performance policy selected g6e.xlarge, which had a total cost of $2.699, while InferSave selected g4dn.xlarge with a total cost of $2.13. The difference in cost is approximately 26.7%, calculated as ((2.699 - 2.13) / 2.13) * 100."
"q084","The most carbon-intensive model identified in a 2024 study, stable-diffusion-xl-base-1.0, produces how many grams of CO2eq per 1,000 inferences?","The context states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences.","1594","g CO2eq","[""luccioni2024""]","is_blank","For context, the most carbon-intensive image generation model (stable-diffusion-xl-base-1.0) generates 1,594 grams of 𝐶𝑂2𝑒𝑞 for 1,000 inferences, which is roughly the equivalent to 4.1 miles driven by an average gasoline-powered passenger vehicle [51]","The context states that the most carbon-intensive image generation model, stable-diffusion-xl-base-1.0, generates 1,594 grams of CO2eq for 1,000 inferences."
"q085","What is the range of GPU energy usage for performing 1,000 inference queries, based on the models listed in a 2025 study's appendix?","The context states that GPU energy usage for 1,000 queries ranges from 0.06 Wh (for bert-tiny) to over 3,426 Wh (for Command-R Plus), according to Tables 1 and 2 in the appendix of the 2025 study. The answer unit specified is 'Wh', so the range is expressed in Wh.","[0.06,3426]","Wh","[""luccioni2025c""]","is_blank","task type, with GPU energy usage for 1,000 queries spanning from just 0.06 Wh (bert-tiny) to over 3,426 Wh (Command-R Plus), depending on model size, architecture, and task complexity (see Tables 1 and 2 in the Appendix for more information).","The context states that GPU energy usage for 1,000 queries ranges from 0.06 Wh (for bert-tiny) to over 3,426 Wh (for Command-R Plus), according to Tables 1 and 2 in the appendix of the 2025 study. The answer unit specified is 'Wh', so the range is expressed in Wh."
"q086","True or False: Researchers believe that a universal, one-size-fits-all approach to AI ethics and sustainability can be developed.","The context states that the authors do not believe a universal approach can exist for AI ethics and sustainability, indicating a belief that such issues are too complex and context-dependent to be solved universally.","is_blank","is_blank","[""luccioni2025b""]","is_blank","We do not pretend to have developed a universal approach for either of these issues (and do not believe that one can exist)","The context states that the authors do not believe a universal approach can exist for AI ethics and sustainability, indicating a belief that such issues are too complex and context-dependent to be solved universally."
"q087","What was the gross carbon intensity of energy according to the U.S. average mix in 2021?","The context states that the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh, as per the US Energy Information Administration's data in 2021.","0.429","kg of CO2e/KWh","[""patterson2021""]","is_blank","The gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh [USE21].","The context states that the gross carbon intensity of energy according to the U.S. average mix is 0.429 kg of CO2e/KWh, as per the US Energy Information Administration's data in 2021."
"q088","What decentralized PyTorch-based framework was used to enable distributed spot instance training across clouds and continents?","The context mentions Hivemind as a PyTorch-based framework that enables collaborative DL training in a decentralized manner, suitable for distributed spot instance training across clouds and continents.","Hivemind","is_blank","[""erben2023""]","is_blank","Hivemind [39] is a PyTorch-based [32] framework developed initially to enable collaborative DL training where participants could donate their heterogeneous hardware to train a single model together in a data-parallel fashion. Its main difference to other state-of-the-art distributed training frameworks, such as PyTorch DDP [26] and DeepSpeed [35], is that it runs in a decentralized fashion and can handle peers that drop out at any stage of the training.","The context mentions Hivemind as a PyTorch-based framework that enables collaborative DL training in a decentralized manner, suitable for distributed spot instance training across clouds and continents."
"q089","What is the proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system?","The context mentions a proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system, which is 'social transparency'.","social transparency","is_blank","[""luccioni2025b""]","is_blank","In fact, as proposed by Ehsan et al., the notion of transparency in AI can be expanded to encompass ""social transparency"", which involves integrating socio-technical aspects in the description and understanding of AI systems [56].","The context mentions a proposed term for expanding transparency in AI to include socio-technical aspects and the societal/environmental footprint of a system, which is 'social transparency'."
"q090","In classification experiments on German public administration texts, which model using sentence embeddings achieved the highest accuracy?","The context states that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. This directly answers the question about which model achieved the highest accuracy in the experiments.","linear model with sentence embeddings","is_blank","[""zschache2025""]","is_blank","The highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings.","The context states that the highest accuracy was achieved by a traditional linear model using pre-trained sentence embeddings. This directly answers the question about which model achieved the highest accuracy in the experiments."
"q092","What is the name of the LLM inference system developed in the 2025 Chen et al. paper, which uses model-attention disaggregation?","The context mentions that Lamina is the LLM inference system developed to incorporate model-attention disaggregation. This is stated directly in the text.","Lamina","is_blank","[""chen2024""]","[""https://arxiv.org/abs/2405.01814v2""]","To further validate our theory, we develop and deploy Lamina, an LLM inference system that incorporates model-attention disaggregation in a distributed heterogeneous cluster.","The context mentions that Lamina is the LLM inference system developed to incorporate model-attention disaggregation. This is stated directly in the text."
"q093","How many parameters does the largest T5 model have?","The context does not specify the exact number of parameters for the largest T5 model. The closest information provided is about the relative efficiency of T5 in terms of FLOPS compared to its number of parameters, but no specific parameter count for the largest T5 model is mentioned.","is_blank","parameters","[""is_blank""]","is_blank","is_blank","The context does not specify the exact number of parameters for the largest T5 model. The closest information provided is about the relative efficiency of T5 in terms of FLOPS compared to its number of parameters, but no specific parameter count for the largest T5 model is mentioned."
"q094","What is the total number of parameters in the JetMoE-8B model?","The context states that JetMoE-8B has 8 billion parameters. The answer unit specified is 'parameters', so we return the number without the unit.","8000000000","parameters","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","The context states that JetMoE-8B has 8 billion parameters. The answer unit specified is 'parameters', so we return the number without the unit."
"q095","By what percentage did Google's data center water consumption increase from 2021 to 2022?","The context states that Google observed a 20% uptick in global water consumption between 2021 and 2022. This matches the required answer format of percent.","20","percent","[""luccioni2025a""]","is_blank","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [ 42, 78].","The context states that Google observed a 20% uptick in global water consumption between 2021 and 2022. This matches the required answer format of percent."
"q096","What is the name of the emissions metric defined as 'CO₂ emissions per unit of electricity consumed' in the framework for evaluating energy-efficient large language models?","The context defines the metric 'CO₂ emissions per unit of electricity consumed' as Carbon Intensity, which is listed in the table of common carbon emission metrics in Green AI.","Carbon Intensity","is_blank","[""khan2025""]","is_blank","Carbon Intensity gCO2/kWh CO2 emissions per unit of electricity consumed International Energy Agency","The context defines the metric 'CO₂ emissions per unit of electricity consumed' as Carbon Intensity, which is listed in the table of common carbon emission metrics in Green AI."
"q097","In the final growth stage of the FLM-101B training, what was the achieved FLOPs utilization percentage?","The context provides the FLOPs utilization percentage for the final growth stage of FLM-101B training, which is the 101B stage. According to Table 2, the FLOPs utilization rate for the 101B stage is 52.88%.","52.88","percent","[""li2025a""]","is_blank","Table 2: Parallel strategies and throughput for different growth stages. ... 101 4 4 12 192 2160 165 52.88%","The context provides the FLOPs utilization percentage for the final growth stage of FLM-101B training, which is the 101B stage. According to Table 2, the FLOPs utilization rate for the 101B stage is 52.88%."
"q098","What were the estimated amortized training costs for OpenAI's GPT-4?","The context states that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M. This figure represents the amortized cost for training GPT-4, as specified in the question.","40000000","USD","[""cottier2024""]","is_blank","We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The context states that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M. This figure represents the amortized cost for training GPT-4, as specified in the question."
"q099","Compared to a CPU server baseline, by what factor can full-stack optimization (including platform-level caching, GPU acceleration, and algorithmic changes) reduce the operational carbon footprint of a Transformer-based universal translation model?","According to the context, full-stack optimization for a Transformer-based universal translation model includes platform-level caching, GPU acceleration, and algorithmic optimizations, resulting in an operational carbon footprint reduction by more than 800×.","810","multiplier","[""wu2021a""]","is_blank","For the cross-lingual ML task (LM), the operational energy footprint can be significantly reduced by more than 800× using platform-level caching, GPUs, low precision data format, and additional algorithmic optimization.","According to the context, full-stack optimization for a Transformer-based universal translation model includes platform-level caching, GPU acceleration, and algorithmic optimizations, resulting in an operational carbon footprint reduction by more than 800×."
"q100","What fraction of local throughput was achieved for NLP when training was spread across four continents instead of remaining local?","The context states that for NLP, when training was spread across four continents (C-8) compared to remaining local (A-8), there was a performance drop of 41%. This means the throughput achieved was 59% of the local throughput.","0.59","multiplier","[""erben2023""]","is_blank","However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).","The context states that for NLP, when training was spread across four continents (C-8) compared to remaining local (A-8), there was a performance drop of 41%. This means the throughput achieved was 59% of the local throughput."
"q101","How many liters of water were returned to communities from Amazon's replenishment projects in 2023?","The context states that in 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities. This directly answers the question about the amount of water returned to communities from Amazon's replenishment projects in 2023.","3500000000","liters","[""amazon2023""]","is_blank","In 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities.","The context states that in 2023, AWS’s water replenishment portfolio returned 3.5 billion liters to local communities. This directly answers the question about the amount of water returned to communities from Amazon's replenishment projects in 2023."
"q103","True or False: using custom tags with one-shot, zero-shot, and few-shots techniques in source code completion tasks can reduce energy consumption of LLMs.","The context states that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks. This directly supports the statement that using custom tags with one-shot, zero-shot, and few-shots techniques can reduce energy consumption of LLMs.","is_blank","is_blank","[""rubei2025""]","is_blank","Our study reveals that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks.","The context states that custom tags can reduce the energy consumption of LLMs across the three prompt engineering techniques tested for source code completion tasks. This directly supports the statement that using custom tags with one-shot, zero-shot, and few-shots techniques can reduce energy consumption of LLMs."
"q104","As reported in a 2025 paper, how many data center GPUs did NVIDIA ship in the year 2024?","The context mentions that NVIDIA shipped 3.7 million GPUs in 2024, according to a study referenced in the document. The answer unit specified is 'GPUs', so the answer value is expressed as a number.","3700000","GPUs","[""luccioni2025a""]","[""https://www.hpcwire.com/2024/06/10/nvidia-shipped-3-76-million-data-center-gpus-in-2023-according-to-study/""]","While efficiency improvements are being made to the hardware used for training and deploying AI models [ 9, 82, 89], NVIDIA shipped 3.7 million GPUs in 2024 (more than a million more units than in 2023) due to increased demand, despite these improvements in efficiency [105].","The context mentions that NVIDIA shipped 3.7 million GPUs in 2024, according to a study referenced in the document. The answer unit specified is 'GPUs', so the answer value is expressed as a number."
"q107","What percentage of the total amortized hardware and energy cost, on average, is attributed to AI accelerator chips?","According to the context, the average percentage of amortized hardware CapEx + energy cost attributed to AI accelerator chips is 44%. This aligns with the answer_unit 'percent'.","44","percent","[""cottier2024""]","is_blank","Breaking down the components of amortized hardware CapEx + energy in Figure 5, we find that on average, 44% goes toward AI accelerator chips.","According to the context, the average percentage of amortized hardware CapEx + energy cost attributed to AI accelerator chips is 44%. This aligns with the answer_unit 'percent'."
"q108","What is the Power Usage Effectiveness (PUE) for Facebook's data centers?","The context states that Facebook’s data centers have a Power Usage Effectiveness (PUE) of about 1.10, indicating their energy efficiency.","1.1","PUE","[""wu2021a""]","is_blank","Achieving a Power Usage Effectiveness (PUE) of about 1.10, Facebook’s data centers are about 40% more efﬁcient than small-scale, typical data centers.","The context states that Facebook’s data centers have a Power Usage Effectiveness (PUE) of about 1.10, indicating their energy efficiency."
"q109","What is the acronym of the Finnish project that proposed integrating ethics, sustainability, design, and foresight for inter-disciplinary governance of AI systems?","The Finnish project mentioned in the context is named ETAIROS, which stands for Ethical AI for the Governance of the Society. It proposed integrating ethics, sustainability, design, and foresight for interdisciplinary governance of AI systems.","ETAIROS","is_blank","[""luccioni2025b""]","is_blank","From a regulatory perspective, the Finnish ETAIROS (Ethical AI for the Governance of the Society) project proposed the integration of ethics, sustainability, design and foresight for inter-disciplinary governance of AI systems","The Finnish project mentioned in the context is named ETAIROS, which stands for Ethical AI for the Governance of the Society. It proposed integrating ethics, sustainability, design, and foresight for interdisciplinary governance of AI systems."
"q110","What were the estimated amortized training costs for Google's Gemini Ultra?","The context states that Google's Gemini Ultra had the most power-intensive training run with an estimated cost of $30M. This is the amortized training cost as per the context.","30000000","USD","[""cottier2024""]","is_blank","We find that the most expensive publicly-announced training runs to date are OpenAI’s GPT-4 at $40M and Google’s Gemini Ultra at $30M.","The context states that Google's Gemini Ultra had the most power-intensive training run with an estimated cost of $30M. This is the amortized training cost as per the context."
"q111","True or False: The AI Act requires providers of GPAI models with systemic risk to conduct risk assessments that include environmental risks.","The context mentions that the AI Act requires risk assessment and mitigation for providers of GPAI models with systemic risk, but it does not explicitly state that these assessments must include environmental risks. The text argues that these measures should consider environmental risks, but it is not clear if this is a requirement.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The context mentions that the AI Act requires risk assessment and mitigation for providers of GPAI models with systemic risk, but it does not explicitly state that these assessments must include environmental risks. The text argues that these measures should consider environmental risks, but it is not clear if this is a requirement."
"q112","What is the EPA's recently tightened primary standard for the annual average limit of PM2.5?","The context states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³.","9","µg/m³","[""han2024""]","[""https://www.epa.gov/criteria-air-pollutants/naaqs-table""]","In fact, the EPA’s recently tightened standard for PM2.5 sets an annual average limit of 9µg/m 3, considerably higher than the WHO’s recommended level of 5µg/m3 [48, 52].","The context states that the EPA's recently tightened standard for PM2.5 sets an annual average limit of 9µg/m³."
"q113","A life cycle assessment found that one Amazon Kindle e-reader produces the same amount of CO2 as how many physical print books?","The context mentions a life cycle assessment (LCA) that compares print books to e-readers, specifically stating that 115 books would produce the same amount of CO2 as a single Amazon Kindle device. The answer is expressed in the requested unit 'books'.","115","books","[""luccioni2025a""]","is_blank","For instance, a life cycle assessment (LCA), which evaluates the environmental impacts of an artifact arising throughout its existence (typically including disposal), has been performed comparing print books to e-readers, finding that 115 books would produce the same amount of CO2 as a single Amazon Kindle device [32, 103].","The context mentions a life cycle assessment (LCA) that compares print books to e-readers, specifically stating that 115 books would produce the same amount of CO2 as a single Amazon Kindle device. The answer is expressed in the requested unit 'books'."
"q114","According to a recent study on the public health impacts of AI, by what factor could the per-household health burden from air pollutants in the most affected, economically-disadvantaged communities exceed that in less-impacted communities?","The context states that the per-household health burden in the most affected, economically-disadvantaged communities could be 200 times higher than in less-impacted communities.","200","multiplier","[""han2024""]","is_blank","For example, as shown in Table 6c, all the top-10 most impacted counties in the U.S. have lower median household incomes than the national median value. The ratio of the highest county-level per-household health cost to the lowest cost is approximately 200.","The context states that the per-household health burden in the most affected, economically-disadvantaged communities could be 200 times higher than in less-impacted communities."
"q115","What was the energy consumption of the DS Llama 70B model for inference on the FKTG dataset?","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is found in Table B1, which lists the energy consumed in Watt-hours (Wh). The value for DS Llama 70B is 702.06 Wh.","702.06","Wh","[""zschache2025""]","is_blank","T able B1 Measurements of all models for the inference task on the FKTG dataset, Capella system, single node, shown are averages over 10 runs
Model Duration (s) Energy consumed (Wh)
single double ratio single double ratio
Llama 3.1 70B 161.59 304.77 1.89 48.60 94.88 1.95
Qwen 2.5 72B 164.44 308.16 1.87 48.66 95.70 1.97
Jamba Mini 1.5 78.61 113.88 1.45 17.42 29.81 1.71
DS Llama 70B 2543.47 6792.54 2.67 702.06 1899.86 2.71","The energy consumption of the DS Llama 70B model for inference on the FKTG dataset is found in Table B1, which lists the energy consumed in Watt-hours (Wh). The value for DS Llama 70B is 702.06 Wh."
"q116","According to the 2022 paper by Dodge et al., what is the total number of parameters in the large language model they analyzed?","The context does not specify the exact number of parameters for the large language model analyzed by Dodge et al. in 2022. However, it mentions that the model is similar to other large language models like BERT-large, which has around 350 million parameters. The closest specific mention is Google's BERT-large containing roughly 350 million parameters. Therefore, the answer is 'is_blank' as the exact number is not provided.","is_blank","parameters","[""is_blank""]","is_blank","For instance, Google’s BERT-large [8] contains roughly 350 million parameters.","The context does not specify the exact number of parameters for the large language model analyzed by Dodge et al. in 2022. However, it mentions that the model is similar to other large language models like BERT-large, which has around 350 million parameters. The closest specific mention is Google's BERT-large containing roughly 350 million parameters. Therefore, the answer is 'is_blank' as the exact number is not provided."
"q117","What phenomenon is described as technological progress improving efficiency, which then results in increased usage and overall resource consumption?","The context describes Jevons' Paradox, which states that improvements in efficiency lead to increased usage and overall resource consumption. This phenomenon is explicitly mentioned in multiple sections of the provided context.","Jevons' Paradox","is_blank","[""morrison2025"", ""luccioni2025a"", ""wu2021a""]","is_blank","This may be an instance of Jevons’ Paradox (Jevons, 1865): when a resource’s efficiency increases, overall consumption of that resource tends to increase, rather than decrease.","The context describes Jevons' Paradox, which states that improvements in efficiency lead to increased usage and overall resource consumption. This phenomenon is explicitly mentioned in multiple sections of the provided context."
"q118","How many Meena training runs would use the same total energy as a single full training run of GPT-3?","According to the context, GPT-3 training energy is estimated at 1287 MWh. Meena training energy is estimated at 86 MWh. Therefore, the number of Meena training runs equivalent to a single full GPT-3 training run is calculated as 1287 MWh / 86 MWh.","15","multiplier","[""li2025b"", ""strubell2019""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29]. Training Meena used 9 MWh and produced ~150 citations.","According to the context, GPT-3 training energy is estimated at 1287 MWh. Meena training energy is estimated at 86 MWh. Therefore, the number of Meena training runs equivalent to a single full GPT-3 training run is calculated as 1287 MWh / 86 MWh."
"q119","According to Table 2 in a 2024 study on AI's power consumption, what is the average energy consumption, in kWh, for performing 1,000 image generation inferences?","According to Table 2 in the 2024 study, the mean energy consumption for performing 1,000 image generation inferences is 2.907 kWh.","2.907","kWh","[""luccioni2024""]","is_blank","inference energy (kWh)
task mean std
text classification 0.002 0.001
extractive QA 0.003 0.001
masked language modeling 0.003 0.001
token classification 0.004 0.002
image classification 0.007 0.001
object detection 0.038 0.02
text generation 0.047 0.03
summarization 0.049 0.01
image captioning 0.063 0.02
image generation 2.907 3.31
Table 2.","According to Table 2 in the 2024 study, the mean energy consumption for performing 1,000 image generation inferences is 2.907 kWh."
"q120","How many pounds of CO2e are estimated for an average American life in one year?","The context mentions 'American life, avg, 1 year' in the table of estimated CO2 emissions, corresponding to 36,156 lbs of CO2e.","36156","lbs","[""strubell2019""]","is_blank","American life, avg, 1 year 36,156","The context mentions 'American life, avg, 1 year' in the table of estimated CO2 emissions, corresponding to 36,156 lbs of CO2e."
"q121","According to a recent paper's 2030 projections on the public health impacts of air pollution from U.S. data centers, which county in West Virginia is projected to have the highest per-household health cost?","The provided context does not contain specific information about the projected per-household health costs for individual counties in West Virginia in 2030. The context only mentions counties in West Virginia in relation to health costs from 2019 to 2023.","is_blank","is_blank","[""is_blank""]","is_blank","is_blank","The provided context does not contain specific information about the projected per-household health costs for individual counties in West Virginia in 2030. The context only mentions counties in West Virginia in relation to health costs from 2019 to 2023."
"q122","By what multiplier did Mistral-small's emissions change after optimization in the financial sentiment classification task?","According to the provided context, Mistral-small's carbon emissions changed from 0.020 kg CO2 per inference task before optimization to 0.015 kg CO2 per inference task after optimization. The multiplier is calculated as the ratio of emissions before optimization to emissions after optimization, which is 0.020 / 0.015 = 1.33.","1.33","multiplier","[""khan2025""]","is_blank","TABLE III
COMPARISON OF PERFORMANCE METRICS AND CARBON EMISSIONS FOR
FIVE LLM S BEFORE AND AFTER OPTIMIZATION . CARBON EMISSIONS ARE
CALCULATED PER INFERENCE TASK .
Model Name Precision Recall F1 Accuracy CO2
(kg)
Before
Optimization
Baseline metrics for comparison
Mistral-small 0.70 0.67 0.65 0.67 0.020
After
Optimization
Metrics following quantization and local
inference techniques
Mistral-small 0.73 0.70 0.69 0.70 0.015","According to the provided context, Mistral-small's carbon emissions changed from 0.020 kg CO2 per inference task before optimization to 0.015 kg CO2 per inference task after optimization. The multiplier is calculated as the ratio of emissions before optimization to emissions after optimization, which is 0.020 / 0.015 = 1.33."
"q123","What were the combined training and fine-tuning energy costs in kWh for the BLOOMz-7B model, as reported in the 'Power Hungry Processing' study?","The context provides the energy usage for training and fine-tuning the BLOOMz-7B model. Training energy was 51,686 kWh and fine-tuning energy was 7,571 kWh. Adding these values gives the combined energy cost.","59257","kWh","[""ebert2024"", ""luccioni2024""]","is_blank","Training energy (kWh) 51,686
Finetuning energy (kWh) 7,571","The context provides the energy usage for training and fine-tuning the BLOOMz-7B model. Training energy was 51,686 kWh and fine-tuning energy was 7,571 kWh. Adding these values gives the combined energy cost."
"q125","What is the total number of parameters in the final FLM-101B model?","The context mentions that FLM-101B is trained with a growth strategy that produces models with 16B, 51B, and 101B parameters sequentially. The final model in this growth process is the 101B parameter model.","101000000000","parameters","[""li2025a""]","is_blank","Benefiting from our growth strategy, the we produce three models with 16B, 51B, and 101B (i.e., FLM-101B) parameters in a single training. The training process is carried out in a progressive manner by growing a 16B model to 51B, and then 101B.","The context mentions that FLM-101B is trained with a growth strategy that produces models with 16B, 51B, and 101B parameters sequentially. The final model in this growth process is the 101B parameter model."
"q126","Fetch the amount of energy (in kWh) required for a full training run of a 6.1B parameter model. Using that information, and using the energy cost of a comparable model (BLOOMz-7B), approximately how many inferences are needed to match this training energy cost?","The energy cost for a full training run of a 6.1B parameter model is estimated to be approximately 103,500 kWh. Given that the BLOOMz-7B model, which is a comparable model, uses 1.0 × 10−4 kWh per inference, we calculate the number of inferences needed to match the training energy cost.","1035000000","inferences","[""dodge2022"", ""luccioni2024""]","[""https://www.similarweb.com/website/chat.openai.com/"", ""https://www.acm.org/conference/facct""]","The energy cost for a full training run of a 6.1B parameter model is estimated to be approximately 103,500 kWh. BLOOMz-7B model uses 1.0 × 10−4 kWh per inference.","The energy cost for a full training run of a 6.1B parameter model is estimated to be approximately 103,500 kWh. Given that the BLOOMz-7B model, which is a comparable model, uses 1.0 × 10−4 kWh per inference, we calculate the number of inferences needed to match the training energy cost."
"q127","In the 2024 study 'Power Hungry Processing', what was the total amount of energy consumed for all model experimentation and evaluation?","The context explicitly states that for all model experimentation and evaluation, a total of 754.66 kWh of energy was used. This directly answers the question and matches the specified answer_unit of kWh.","754.66","kWh","[""luccioni2024""]","[""https://arxiv.org/abs/2412.00329""]","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The context explicitly states that for all model experimentation and evaluation, a total of 754.66 kWh of energy was used. This directly answers the question and matches the specified answer_unit of kWh."
"q128","For the BLOOMz-7B model, how many inferences are required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning?","The context provides the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning for the BLOOMz-7B model as 592,570,000 inferences. This value is directly stated in Table 5.","592570000","inferences","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5
Cost parity (# inferences) 592,570,000 395,602,740 292,467,741 204,592,592","The context provides the number of inferences required for the cumulative energy cost of deployment to equal the initial energy cost of training and fine-tuning for the BLOOMz-7B model as 592,570,000 inferences. This value is directly stated in Table 5."
"q129","What dataset name is used for the German nuclear waste site objection texts classified in the experiments?","The context mentions that the data originates from the process of selecting a repository site for high-level radioactive waste in Germany, specifically from the FKTG dataset. This dataset is used for classifying the objections submitted by the population.","FKTG","is_blank","[""zschache2025""]","[""https://beteiligung.bge.de/index.php""]","The data of our study originates from the process of selecting a repository site for high-level radioactive waste in Germany. During the first phase, sub-areas were identified and discussed in a process called FKTG (Fachkonferenz Teilgebiete). The statements from the population were categorized, processed and published as the FKTG-dataset (https://beteiligung.bge.de/index.php).","The context mentions that the data originates from the process of selecting a repository site for high-level radioactive waste in Germany, specifically from the FKTG dataset. This dataset is used for classifying the objections submitted by the population."
"q130","How much freshwater (in liters) was consumed by Meta's Llama 3 inference serving clusters in 2024?","The context does not provide specific information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. The provided data focuses on energy and carbon emissions, but lacks detailed water usage statistics for Llama 3 inference.","is_blank","liters","[""is_blank""]","is_blank","is_blank","The context does not provide specific information about the freshwater consumption of Meta's Llama 3 inference serving clusters in 2024. The provided data focuses on energy and carbon emissions, but lacks detailed water usage statistics for Llama 3 inference."
"q131","What percentage of NVIDIA H100 GPUs manufactured in 2024 used recycled rare earth metals?","The context does not specify the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. Therefore, the answer is 'is_blank'.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not specify the percentage of NVIDIA H100 GPUs manufactured in 2024 that used recycled rare earth metals. Therefore, the answer is 'is_blank'."
"q132","The actual CO2e for the Evolved Transformer NAS (3.2 tCO2e) is equivalent to approximately how many passengers taking a round trip between San Francisco and New York?","The context states that the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York, which corresponds to 3.2 tCO2e.","3","passengers","[""patterson2021""]","is_blank","Thus, the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York.","The context states that the CO2e equivalent of NAS is ~3 passengers taking a round trip between San Francisco and New York, which corresponds to 3.2 tCO2e."
"q133","According to May 2025 data from the API platform OpenRouter, what percentage of LLM token usage occurred through models that did not disclose their environmental impact?","According to the context, May 2025 data from OpenRouter indicates that 84% of LLM usage is through models with no disclosure of environmental impact.","84","percent","[""luccioni2025c""]","is_blank","In terms of token usage, 84% of LLM usage is through models with no disclosure, 14% for indirectly disclosed models, and only 2% for models with direct disclosure.","According to the context, May 2025 data from OpenRouter indicates that 84% of LLM usage is through models with no disclosure of environmental impact."
"q134","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-13B inference without compression or quantization?","The context specifies the bare minimum hardware requirements for LLaMA models, indicating that for LLaMA 13B, only 1 A100 80GB GPU is required.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. These limits are imposed by a combination of GPU memory, model size, response length and the number of GPUs. Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128","The context specifies the bare minimum hardware requirements for LLaMA models, indicating that for LLaMA 13B, only 1 A100 80GB GPU is required."
"q136","What is the estimated range of CO2 emissions in metric tons for a *complete* training run of a 6.1 billion parameter transformer model?","The context states that the 6 billion parameter transformer model trained for 8 days and only reached 13% of completion. A full training run would take 60 days and consume approximately 103,593 kWh. The emissions for this model are calculated as 103,593 kWh * 0.429 kg CO2/kWh = 44,295 kg CO2 = 44.3 metric tons CO2. Since the exact parameter count is 6.1 billion, the emissions are estimated within the range of 44 metric tons.","[44,44]","metric tons","[""dodge2022""]","[""https://arxiv.org/pdf/2206.08370.pdf""]","We tracked the energy consumption of training a large language model comprising over 6.1 billion parameters during 8 days on 256 NVIDIA A100s. The total energy amounted to a staggering 13.8 MWh. This model was not trained to completion, but only until 13%; a full training run would take 60 days. Thus, we estimate the total energy consumption to train this model to completion would be approximately (60/8) ∗ 13.8 = 103.5 MWh, or 103,500 kWh — almost 2800 times more than training the BERT-small model!","The context states that the 6 billion parameter transformer model trained for 8 days and only reached 13% of completion. A full training run would take 60 days and consume approximately 103,593 kWh. The emissions for this model are calculated as 103,593 kWh * 0.429 kg CO2/kWh = 44,295 kg CO2 = 44.3 metric tons CO2. Since the exact parameter count is 6.1 billion, the emissions are estimated within the range of 44 metric tons."
"q137","What was the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023?","The provided context does not contain specific information about the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023. The context discusses various optimization techniques and their impacts on energy consumption and carbon emissions, but does not provide a concrete number for the total carbon emissions avoided.","is_blank","tCO2e","[""is_blank""]","is_blank","is_blank","The provided context does not contain specific information about the total carbon emissions (tCO2e) avoided by pruning and quantizing large language models in 2023. The context discusses various optimization techniques and their impacts on energy consumption and carbon emissions, but does not provide a concrete number for the total carbon emissions avoided."
"q138","In a specific scenario blending A100 and A10G GPUs, what percentage of cost savings was achieved over an A100-only strategy?","According to the context, a specific scenario blending 2 A100s and 1 A10G resulted in a 24% cost saving over an A100-only strategy.","24","percent","[""griggs2024""]","is_blank","Fig. 9 highlights a case where using 2 A100s and 1 A10G results in a 24% cost saving over A100-only and 31% over A10G-only.","According to the context, a specific scenario blending 2 A100s and 1 A10G resulted in a 24% cost saving over an A100-only strategy."
"q140","According to Chen et al. (2025), what is the price per hour for an NVIDIA H20?","According to Chen et al. (2025), the price per hour for an NVIDIA H20 is listed as $4.63/hr in Table 1. The answer is formatted as USD per hour as specified by the answer_unit.","4.63","USD per hour","[""chen2024""]","is_blank","H100 H20 TPU v6e [7]
BF16 TFLOPs 989 148 918
Memory capacity 80 GB 96 GB 32 GB
Memory bandwidth 3.35 TB/s 4.0 TB/s 1.64 TB/s
Power rating 700 W 400 W unlisted
Inter-chip bandwidth 450 GB/s 450 GB/s 448 GB/s
Network bandwidth 400 Gbps 400 Gbps 200 Gbps
Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","According to Chen et al. (2025), the price per hour for an NVIDIA H20 is listed as $4.63/hr in Table 1. The answer is formatted as USD per hour as specified by the answer_unit."
"q141","True or False: Most carbon footprint analyses for AI models gather information automatically without needing to contact authors.","The context states that most carbon footprint analyses for AI models rely on manually collecting information by contacting authors, indicating that automatic gathering of information is not the norm.","is_blank","is_blank","[""luccioni2025b""]","is_blank","For instance, most of the carbon footprint assessments only focus on the training stage of AI models, which is easier to quantify and report [153, 192], but which only represents a portion of models’ total environmental impacts.","The context states that most carbon footprint analyses for AI models rely on manually collecting information by contacting authors, indicating that automatic gathering of information is not the norm."
"q142","In 2023, what percentage of the data centers' total electricity cost was their public health cost equivalent to, using the average attribution method?","The context states that in 2023, the total public health cost of U.S. data centers was equivalent to approximately 44% of the data centers' total electricity cost.","44","percent","[""han2024""]","is_blank","Even at the beginning of the generative AI boom, the U.S. data centers have already resulted in a total public health cost of about $6.7 billion, or$47.5 per household, in 2023. This is equivalent to approximately 44% of the data centers’ total electricity cost.","The context states that in 2023, the total public health cost of U.S. data centers was equivalent to approximately 44% of the data centers' total electricity cost."
"q143","What is the bare minimum number of NVIDIA A100 80GB GPUs required to run LLaMA-7B inference without compression or quantization?","The context states that the bare minimum hardware requirements for LLaMA 7B are 1 NVIDIA A100 80GB GPU. This is derived from Table II which lists the minimum hardware required for different LLaMA models.","1","A100_80GB_GPU","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. These limits are imposed by a combination of GPU memory, model size, response length and the number of GPUs.
Model Size V100 32GB A100 80GB
Count Max. Batch size Count Max. Batch size
7B 1 64 1 64","The context states that the bare minimum hardware requirements for LLaMA 7B are 1 NVIDIA A100 80GB GPU. This is derived from Table II which lists the minimum hardware required for different LLaMA models."
"q144","True or False: Sustainable deployment techniques described for large language models demonstrated up to a 45% reduction in carbon emissions after quantization.","The context states that experimental results reveal that the methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments. This directly supports the statement about the reduction in carbon emissions after quantization.","45%","is_blank","[""khan2025""]","is_blank","Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments.","The context states that experimental results reveal that the methods can reduce energy consumption and carbon emissions by up to 45% post quantization, making them particularly suitable for resource-constrained environments. This directly supports the statement about the reduction in carbon emissions after quantization."
"q145","How many answers were researchers able to collect after reaching out to over 500 authors for their carbon footprint analysis?","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors and collected 95 answers. The answer is expressed in the requested unit 'answers'.","95","answers","[""luccioni2025b""]","is_blank","For instance, Luccioni and Hernandez-Garcia reached out to over 500 authors of AI papers to get information needed to estimate the carbon footprint of their models, and were only able to collect 95 answers, with many authors refusing to provide the relevant information, citing privacy concerns and lack of experimental logs [2023].","The context states that Luccioni and Hernandez-Garcia reached out to over 500 authors and collected 95 answers. The answer is expressed in the requested unit 'answers'."
"q147","Based on the reported training budget and total GPU hours, estimate the approximate cost per H100 GPU-hour for the JetMoE project.","The JetMoE-8B project was trained with a budget of $100k and utilized 30,000 H100 GPU hours. Dividing the budget by the total GPU hours gives us the approximate cost per H100 GPU-hour.","3.33","USD per hour","[""shen2024""]","[""https://github.com/myshell-ai/JetMoE""]","Impressively, JetMoE-8B is trained with a limited $100k budget, using 1.25T tokens from mixed open-source datasets and 30,000 H100 GPU hours.","The JetMoE-8B project was trained with a budget of $100k and utilized 30,000 H100 GPU hours. Dividing the budget by the total GPU hours gives us the approximate cost per H100 GPU-hour."
"q148","When training a Llama-3.1 scale model in Altoona, Iowa, the health cost was what percentage of the electricity cost?","The context indicates that for training a Llama-3.1 scale model in Altoona, Iowa, the health cost was 122% of the electricity cost.","122","percent","[""han2024""]","is_blank","Altoona, IA 6.91 2.1 2.51(1.84, 3.17) 122% 1.52 (34000) 11.78 (10600) 14.76","The context indicates that for training a Llama-3.1 scale model in Altoona, Iowa, the health cost was 122% of the electricity cost."
"q149","How many tokens were used to pre-train the JetMoE-8B model?","The context states that JetMoE-8B was trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. Since the answer_unit is 'tokens', the answer_value is expressed as a number in tokens.","1250000000000","tokens","[""shen2024""]","is_blank","JetMoE-8B is trained on 1.25T tokens of primarily English data from web documents, mathematics, and code.","The context states that JetMoE-8B was trained on 1.25T tokens of primarily English data from web documents, mathematics, and code. Since the answer_unit is 'tokens', the answer_value is expressed as a number in tokens."
"q150","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United Kingdom?","The context indicates that Amazon announced 36 renewable energy projects in the United Kingdom as of January 2024. This information is derived from the table detailing the number of projects in different countries.","36","projects","[""amazon2023""]","is_blank","Project Location
Number 
of Projects
Total MW 
Capacity†
United Kingdom 36 901","The context indicates that Amazon announced 36 renewable energy projects in the United Kingdom as of January 2024. This information is derived from the table detailing the number of projects in different countries."
"q151","In 2023, what percentage of Amazon's Workforce in the United States across all levels identified as men?","Based on the provided context, the percentage of Amazon's workforce in the United States across all levels identifying as men in 2023 is calculated from the given distribution percentages. The context states that 44.1% of the U.S. workforce identified as men in 2023.","44.1","percent","[""amazon2023""]","is_blank","44.1%55.7%","Based on the provided context, the percentage of Amazon's workforce in the United States across all levels identifying as men in 2023 is calculated from the given distribution percentages. The context states that 44.1% of the U.S. workforce identified as men in 2023."
"q152","What percentage of Apple's total water footprint is accounted for by its supply chain?","The context states that Apple reports its supply chain accounts for 99% of its total water footprint.","99","percent","[""li2025b""]","is_blank","For instance, Apple reports that its supply chain accounts for 99% of its total water footprint [23].","The context states that Apple reports its supply chain accounts for 99% of its total water footprint."
"q154","What is the total execution time, in seconds, of a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB with a batch size of 84?","The context does not provide specific execution time data for a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB GPU and a batch size of 84. The figures and tables in the context do not contain this exact information.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide specific execution time data for a sparse BlackMamba model fine-tuned with a NVIDIA A40-48GB GPU and a batch size of 84. The figures and tables in the context do not contain this exact information."
"q155","Which metric was introduced to assess the ratio of computation to communication time when scaling distributed training across continents?","The context mentions the introduction of the granularity metric to compare model suitability for distributed spot training and estimate training performance with additional spot VMs. This metric assesses the ratio of calculation to communication time.","granularity","is_blank","[""erben2023""]","is_blank","To quantify total training cost, we assess cost-effectiveness and evaluate a hybrid or multi-cloud approach with popular cloud providers through training on up to four continents. For comparison of the models’ scalability and to show which of them can be trained in a distributed fashion, we introduce thegranularity metric, the ratio of calculation to communication time, and show how it can be used for predicting performance with different hardware setups.","The context mentions the introduction of the granularity metric to compare model suitability for distributed spot training and estimate training performance with additional spot VMs. This metric assesses the ratio of calculation to communication time."
"q156","According to a coalition of Microsoft employees, a single deal with Exxon Mobil to expand oil production could add up to how many times more carbon emissions than the company's yearly carbon removal targets?","According to the context, a coalition of Microsoft employees estimated that a single deal with Exxon Mobil to expand oil and gas production could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year. This percentage translates to 6.4 times more emissions.","6.4","times","[""luccioni2025a""]","[""https://grist.org/energy/microsofts-ambitious-climate-goal-forgets-about-its-oil-contracts/"", ""https://grist.org/accountability/microsoft-employees-spent-years-fighting-the-tech-giants-oil-ties-now-theyre-speaking-out/""]","For instance, a coalition of Microsoft employees estimated that a single deal the company struck with Exxon Mobil that uses AI to expand oil and gas production in Texas and New Mexico by 50,000 barrels of oil per day could add up to 640 percent more carbon emissions compared to the company’s carbon removal targets for the year [119], yet these numbers were not included in the company’s carbon accounting and reporting efforts [118].","According to the context, a coalition of Microsoft employees estimated that a single deal with Exxon Mobil to expand oil and gas production could add up to 640 percent more carbon emissions compared to the company's carbon removal targets for the year. This percentage translates to 6.4 times more emissions."
"q157","What is the term for freshwater taken from ground or surface sources, either temporarily or permanently, for various uses?","The context defines 'water withdrawal' as freshwater taken from ground or surface sources, either temporarily or permanently, for various uses.","water withdrawal","is_blank","[""li2025b""]","is_blank","• Water withdrawal: It refers to freshwater taken from the ground or surface water sources, either temporarily or permanently, and then used for agricultural, industrial, or municipal uses (normally excluding water used for hydroelectricity generation) [12].","The context defines 'water withdrawal' as freshwater taken from ground or surface sources, either temporarily or permanently, for various uses."
"q159","How often does the Standing Committee of the One Hundred Year Study form a Study Panel?","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI. Therefore, the frequency is every five years.","5","years","[""stone2022""]","[""https://ai100.stanford.edu/""]","As its core activity, the Standing Committee that oversees the One Hundred Year Study forms a Study Panel every five years to assess the current state of AI.","The context states that the Standing Committee forms a Study Panel every five years to assess the current state of AI. Therefore, the frequency is every five years."
"q160","What was the average number of connected devices per U.S. household reported in 2021 (smartphones, laptops, smart TVs, speakers, wearables, gaming consoles, etc)?","The context states that in the US, the average household is equipped with an average of 25 connected devices. This figure is directly reported from Deloitte's 2021 report.","25","devices","[""wu2021b""]","[""https://www2.deloitte.com/content/dam/insights/articles/6978_TMT-Connectivity-and-mobile-trends/DI_TMT-Connectivity-and-mobile-trends.pdf""]","In the US, for example, the average household is equipped with an average of 25 connected devices [Deloitte, 2021].","The context states that in the US, the average household is equipped with an average of 25 connected devices. This figure is directly reported from Deloitte's 2021 report."
"q161","Based on publicly available data, what is the range of energy consumption, in MWh, to pre-train a large language model (LLM)?","Based on the provided context, the energy consumption range to pre-train a large language model (LLM) spans from 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout). This range is derived from the data presented in the context, which specifies these values as the minimum and maximum energy consumption for pre-training LLMs.","[0.8,3500]","MWh","[""luccioni2025c""]","is_blank","In fact, the energy required to pre-train an LLM spans from as little as 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout), with associated GHG emissions varying even more significantly (due to variation in the carbon intensity of electricity across training locations).","Based on the provided context, the energy consumption range to pre-train a large language model (LLM) spans from 0.8 MWh (OLMo 20M) to 3,500 MWh (LLaMa 4 Scout). This range is derived from the data presented in the context, which specifies these values as the minimum and maximum energy consumption for pre-training LLMs."
"q162","True or False: IBM's Watson program did NOT beat human contenders in the Jeopardy challenge.","The context clearly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. Therefore, the statement 'IBM's Watson program did NOT beat human contenders in the Jeopardy challenge' is false.","is_blank","is_blank","[""stone2022""]","is_blank","IBM’s Watson program, which beat human contenders to win the Jeopardy challenge in 2011, was largely based on an efficient scheme for organizing, indexing, and retrieving large amounts of information gathered from various sources.","The context clearly states that IBM's Watson program beat human contenders to win the Jeopardy challenge in 2011. Therefore, the statement 'IBM's Watson program did NOT beat human contenders in the Jeopardy challenge' is false."
"q163","One study estimates that how many queries to the GPT-3 model consume approximately half a liter of water?","According to the context, one study suggests that 10–50 queries on GPT-3 consume around half a liter of water. The answer is expressed in the requested unit 'queries'.","[10,50]","queries","[""luccioni2025a""]","is_blank","Other studies have sought to estimate water usage at the level of individual AI models, with one paper suggesting that 10–50 queries on GPT-3 consumes around half a liter of water [68].","According to the context, one study suggests that 10–50 queries on GPT-3 consume around half a liter of water. The answer is expressed in the requested unit 'queries'."
"q165","After model alignment, what MT-Bench score did the JetMoE-8B-Chat model achieve, surpassing the Llama-2-13b-Chat model?","According to the context, the JetMoE-8B-Chat model achieved a higher MT-Bench score than Llama-2-13b-Chat after alignment, specifically a score of 6.681 as shown in Table 4.","6.681","score","[""shen2024""]","is_blank","Model MT-Bench Score
GPT-4 9.014
GPT-3.5-turbo 7.995
Claude-v1 7.923
JetMoE-8B-chat 6.681
Llama-2-13b-chat 6.650
Vicuna-13b-v1.3 6.413
Wizardlm-13b 6.353
Llama-2-7b-chat 6.269","According to the context, the JetMoE-8B-Chat model achieved a higher MT-Bench score than Llama-2-13b-Chat after alignment, specifically a score of 6.681 as shown in Table 4."
"q167","How many medium-length GPT-3 completions (prompt= 800 words; response 150-300 words) could be produced with the water required to fill a single 500 mL bottle?","According to the context, GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses. This directly answers the question, and since the answer_unit is 'responses', the answer_value is expressed as a number.","[10,50]","responses","[""li2025b""]","is_blank","Additionally, GPT-3 needs to “drink” (i.e., consume) a500ml bottle of waterfor roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","According to the context, GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses. This directly answers the question, and since the answer_unit is 'responses', the answer_value is expressed as a number."
"q168","The 2024 Griggs et al. paper reports that Mélange can reduce deployment costs by up to what percentage in conversational chat settings?","The context states that Mélange reduces deployment costs by up to 77% in conversational settings. This directly answers the question about the maximum cost reduction percentage in conversational chat settings.","77","percent","[""griggs2024""]","is_blank","Compared to using only a single GPU type, Mélange reduces deployment costs by up to 77% in conversational settings, 33% in document-based settings, and 51% in a mixed setting.","The context states that Mélange reduces deployment costs by up to 77% in conversational settings. This directly answers the question about the maximum cost reduction percentage in conversational chat settings."
"q169","What is the bare minimum number ofA100 80GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that for meaningful inferences with the 65B LLaMA model, a minimum of 4 A100 GPUs each with 80GB of memory are required. This aligns with the bare minimum requirement asked in the question.","4","A100_80GB_GPUs","[""samsi2024""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context states that for meaningful inferences with the 65B LLaMA model, a minimum of 4 A100 GPUs each with 80GB of memory are required. This aligns with the bare minimum requirement asked in the question."
"q171","Training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to how many round trips by car between Los Angeles and New York City?","The context indicates that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. The answer unit specified is 'round trips', so the answer value is expressed as a number.","10000","round trips","[""han2024""]","is_blank","Additionally, depending on the locations, training an AI model of the Llama-3.1 scale can produce an amount of air pollutants equivalent to more than 10,000 LA-NYC round trips by car.","The context indicates that training an AI model of the Llama-3.1 scale can produce air pollutants equivalent to more than 10,000 round trips by car between Los Angeles and New York City. The answer unit specified is 'round trips', so the answer value is expressed as a number."
"q172","What percentage of the machine learning (ML) workload is estimated to be inference processing by NVIDIA in 2019?","According to the context, NVIDIA estimated in 2019 that 80–90% of the ML workload is inference processing. The answer is expressed as a percentage as specified by the answer_unit.","[80,90]","percent","[""patterson2021""]","is_blank","For   example,   NVIDIA   estimated   that   80–90%   of   the   ML   workload   is   inference   processing   [Leo19].","According to the context, NVIDIA estimated in 2019 that 80–90% of the ML workload is inference processing. The answer is expressed as a percentage as specified by the answer_unit."
"q173","Throughout the entire 'Power Hungry Processing' (2024) study, what was the total amount of CO2 equivalent emissions generated?","The total amount of CO2 equivalent emissions generated throughout the entire 'Power Hungry Processing' (2024) study is explicitly stated in the document as 178.97 kg of CO2eq.","178.97","kg CO2eq","[""luccioni2024""]","is_blank","In total, for all of model experimentation and evaluation, we used a total of 754.66 kWh of energy and emitted 178.97 kg of 𝐶𝑂2𝑒𝑞.","The total amount of CO2 equivalent emissions generated throughout the entire 'Power Hungry Processing' (2024) study is explicitly stated in the document as 178.97 kg of CO2eq."
"q174","True or False: Estimating GPU energy consumption based on its Thermal Design Power (TDP) is a reliable and accurate method.","The context indicates that estimating GPU energy consumption based on TDP is nearly always an overestimation since GPUs rarely draw their maximum power continuously. Therefore, it is not a reliable or accurate method.","0","is_blank","[""chung2025""]","is_blank","Estimations using TDP are nearly always an overestimation since it is rare for a GPU – or any computing device – to draw its maximum power at every moment in time.","The context indicates that estimating GPU energy consumption based on TDP is nearly always an overestimation since GPUs rarely draw their maximum power continuously. Therefore, it is not a reliable or accurate method."
"q175","True or False: GPT-4o mini consumes less energy per query than the larger GPT-4o.","The context states that GPT-4o mini consumes slightly more energy than GPT-4o, specifically 3.098 Wh compared to 2.875 Wh for long queries, indicating GPT-4o mini does not consume less energy per query.","is_blank","is_blank","[""jegham2025""]","is_blank","GPT-4o consumes around 2.875 Wh while GPT-4o mini’s consumption is slightly higher at 3.098 Wh due to deployment on A100 hardware instead of H100s.","The context states that GPT-4o mini consumes slightly more energy than GPT-4o, specifically 3.098 Wh compared to 2.875 Wh for long queries, indicating GPT-4o mini does not consume less energy per query."
"q176","What is the ground truth throughput, in queries/sec, of a dense Mixtral-CS-A100-40GB when the batch size is 1?","The context provides the throughput values for different batch sizes and configurations of Mixtral-CS. For a dense Mixtral-CS-A100-40GB with batch size 1, the throughput is given as 0.5 queries/sec.","0.5","queries/sec","[""xia2024""]","is_blank","Mixtral-CS0.0
0.5
1.0
1.5
2.0
0.3 0.5 0.3 0.7
1.7
Dense(bsz=1)
Dense(bsz=2)
Sparse(bsz=1)
Sparse(bsz=2)
Sparse(bsz=8)","The context provides the throughput values for different batch sizes and configurations of Mixtral-CS. For a dense Mixtral-CS-A100-40GB with batch size 1, the throughput is given as 0.5 queries/sec."
"q177","True or False: A 2025 paper's analysis shows that after the peak in 2022, the trend of AI developers directly disclosing environmental information for notable models continued to increase.","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. After 2022, the trend reversed, with direct environmental disclosures declining, and by the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category.","is_blank","is_blank","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. However, the introduction of increasingly commercial and proprietary models after 2022, potentially catalyzed by the popular launch of ChatGPT, which provided very limited information about the training approach used and even the final size of the underlying model, triggered a notable reversal in this trend, dramatically reducing direct environmental disclosures. By the first quarter of 2025, the majority of notable AI models again fell under the “no disclosure” category, as the line between research and commercial deployment became increasingly blurred.","The context states that the direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information. After 2022, the trend reversed, with direct environmental disclosures declining, and by the first quarter of 2025, the majority of notable AI models again fell under the 'no disclosure' category."
"q178","In the Griggs et al. (2024) evaluation of four GPU types, what was the normalized on-demand hourly price for an H100 GPU?","According to the context, the normalized on-demand hourly price for an H100 GPU was calculated by comparing RunPod's H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then adjusting relative to the A100’s price on major clouds ($3.67), resulting in a normalized price of $(4.69/2.29) × 3.67 = $7.516.","7.516","USD per hour","[""griggs2024""]","is_blank","We calculate this by comparing RunPod’s H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then adjusting relative to the A100’s price on major clouds ($3.67), resulting in a normalized price of (4.69/2.29) × 3.67 = $7 .516 for H100.","According to the context, the normalized on-demand hourly price for an H100 GPU was calculated by comparing RunPod's H100 cost ($4.69) to RunPod’s A100-80G cost ($2.29), then adjusting relative to the A100’s price on major clouds ($3.67), resulting in a normalized price of $(4.69/2.29) × 3.67 = $7.516."
"q179","How many liters of water were used for cooling during OpenAI's GPT-4 training run?","The provided context does not contain specific information about the water usage for cooling during OpenAI's GPT-4 training run. The context discusses water usage for GPT-3 training and inference but does not provide details for GPT-4. Therefore, the answer is 'is_blank'.","is_blank","liters of water","[""is_blank""]","is_blank","is_blank","The provided context does not contain specific information about the water usage for cooling during OpenAI's GPT-4 training run. The context discusses water usage for GPT-3 training and inference but does not provide details for GPT-4. Therefore, the answer is 'is_blank'."
"q180","Recent reports describe the monthly on-demand rental cost of serving Llama-2-70B at BF16 precision using 2 NVIDIA A100 GPUs. Based on this information, estimate how much it costs per hour to run the model (assuming 30 days/month).","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month. Assuming 30 days in a month, the daily cost is approximately $173.33. Dividing this by 24 hours gives an hourly cost of approximately $7.22.","7.22","USD per hour","[""griggs2024""]","is_blank","The substantial size and computational demands of LLMs require the use of costly hardware accelerators, typically GPUs2 For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5,200 per month. Assuming 30 days in a month, the daily cost is approximately $173.33. Dividing this by 24 hours gives an hourly cost of approximately $7.22."
"q181","To achieve a BLEU score increase from 5 to 40 for a GPT-3-based language translation task, how much larger must the model be?","According to the context, to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, the model size needs to be 1,000 times larger. The answer is expressed as a multiplier.","1000","multiplier","[""wu2021a""]","is_blank","For example, with GPT-3, to increase the model quality BLEU score from 5 to 40 requires a model 1, 000× larger in size.","According to the context, to increase the model quality BLEU score from 5 to 40 for a GPT-3-based language translation task, the model size needs to be 1,000 times larger. The answer is expressed as a multiplier."
"q182","Fetch the amount of CO2 emitted (in lbs) for the training and neural architecture search for a Transformer model. Using that and the emissions-to-driving-distance ratio from a recent study, what is the approximate driving distance in miles that's equivalent to the carbon emissions from training a Transformer model with neural architecture search?","According to the context, the CO2 emissions for training a Transformer model with neural architecture search is approximately 626,155 lbs. Assuming a conversion rate of 0.79 lbs of CO2 per mile driven, the equivalent driving distance is calculated by dividing 626,155 lbs by 0.79 lbs/mile.","792601","miles","[""luccioni2023"", ""patterson2021""]","is_blank","The first paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars.","According to the context, the CO2 emissions for training a Transformer model with neural architecture search is approximately 626,155 lbs. Assuming a conversion rate of 0.79 lbs of CO2 per mile driven, the equivalent driving distance is calculated by dividing 626,155 lbs by 0.79 lbs/mile."
"q183","The BLOOMz-7B model was downloaded 606,096 times as of Nov 2023. Based on the inference energy reported for this model, estimate the total energy in MWh that would be consumed if every download resulted in 1 million inferences.","The BLOOMz-7B model's inference energy is reported as 1.0 × 10^-4 kWh per inference. Given 606,096 downloads and assuming each download leads to 1 million inferences, the total energy consumption would be calculated as follows: 606,096 * 1,000,000 * 1.0 × 10^-4 kWh = 60,609.6 kWh. Converting kWh to MWh, we get 60.6096 MWh.","60.6096","MWh","[""luccioni2024""]","is_blank","BLOOMz-7B BLOOMz-3B BLOOMz-1B BLOOMz-560M
Training energy (kWh) 51,686 25,634 17,052 10,505
Finetuning energy (kWh) 7,571 3,242 1,081 543
Inference energy (kWh) 1.0 × 10−4 7.3 × 10−5 6.2 × 10−5 5.4 × 10−5","The BLOOMz-7B model's inference energy is reported as 1.0 × 10^-4 kWh per inference. Given 606,096 downloads and assuming each download leads to 1 million inferences, the total energy consumption would be calculated as follows: 606,096 * 1,000,000 * 1.0 × 10^-4 kWh = 60,609.6 kWh. Converting kWh to MWh, we get 60.6096 MWh."
"q184","How many H100 GPU hours were consumed during the pre-training of the JetMoE-8B model?","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours. This directly answers the question about the number of H100 GPU hours consumed during the pre-training.","30000","H100 GPU hours","[""shen2024""]","is_blank","This report introduces JetMoE-8B, a new LLM trained with less than$0.1 million, using 1.25T tokens from carefully mixed open-source corpora and 30,000 H100 GPU hours.","The context explicitly states that JetMoE-8B was trained using 30,000 H100 GPU hours. This directly answers the question about the number of H100 GPU hours consumed during the pre-training."
"q185","Based on the trend of growing development costs, the largest training runs will exceed what cost by the year 2027?","According to the context, the trend of growing development costs suggests that the largest training runs will exceed one billion USD by the start of 2027. This is supported by the statement that the most expensive publicly announced model will cost about one billion dollars to train by the start of 2027.","1000000000","USD","[""cottier2024""]","is_blank","The growth rate in training cost indicates how rapidly AI investment is scaling. We can use this growth rate to extrapolate the cost of the largest training run. Currently, GPT-4 has the largest amortized hardware and energy cost, at $40M. GPT-4 was published in March of 2023 [12]. This implies that, at a growth rate of 2.4× per year, the most expensive publicly announced model by the start of 2027 will cost about $1 billion.","According to the context, the trend of growing development costs suggests that the largest training runs will exceed one billion USD by the start of 2027. This is supported by the statement that the most expensive publicly announced model will cost about one billion dollars to train by the start of 2027."
"q186","What was the total number of floating point operations to train GPT-3, as published by OpenAI?","The context states that OpenAI published the total number of floating point operations to train their model as 3.14E+23 FLOPS. This directly answers the question with the specified unit.","3.14E+23","FLOPS","[""patterson2021""]","is_blank","OpenAI published the total number of floating point operations to train their model: 3.14E+23 [Bro20].","The context states that OpenAI published the total number of floating point operations to train their model as 3.14E+23 FLOPS. This directly answers the question with the specified unit."
"q187","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-65B inference without compression or quantization?","The context states that for meaningful inferences with the 65B LLaMA model, a minimum of 8 V100 GPUs each with 32 GB of RAM are required. This directly answers the question about the bare minimum number of NVIDIA V100 32GB GPUs needed.","8","V100_32GB_GPUs","[""samsi2024""]","is_blank","For example, we find that, at a minimum, 8 V100 GPUs each with 32 GB of RAM or 4 A100 GPUs each with 80GB of memory are required for any meaningful inferences with the 65B LLaMA model.","The context states that for meaningful inferences with the 65B LLaMA model, a minimum of 8 V100 GPUs each with 32 GB of RAM are required. This directly answers the question about the bare minimum number of NVIDIA V100 32GB GPUs needed."
"q188","Using the throughput data for the final 101B training stage, estimate the total computational work performed during this stage in zettaFLOPs.","The throughput for the 101B stage is given as 165 teraFLOPs/sec with a utilization rate of 52.88%. Assuming the training duration is the same as the average job length of 52 hours from the NAS example, we calculate the total computational work as 165 * 3600 * 52 * 10^(-12) * 10^21 = 30.42 zettaFLOPs.","30.42","zettaFLOPs","[""li2025a""]","is_blank","The single-GPU throughput for all three training stages consistently exceeds 160 teraFLOPs/sec with a utilization rate of at least 51.3%. For the 101B stage, the throughput is 165 teraFLOPs/sec with a utilization rate of 52.88%","The throughput for the 101B stage is given as 165 teraFLOPs/sec with a utilization rate of 52.88%. Assuming the training duration is the same as the average job length of 52 hours from the NAS example, we calculate the total computational work as 165 * 3600 * 52 * 10^(-12) * 10^21 = 30.42 zettaFLOPs."
"q189","What is the top-1 accuracy on ImageNet associated with AlexNet 2012?","The context does not specify the exact top-1 accuracy on ImageNet for AlexNet 2012. It mentions that AlexNet has more parameters than ResNet but dramatically less FPO and also much lower accuracy. However, the exact top-1 accuracy value is not provided.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The context does not specify the exact top-1 accuracy on ImageNet for AlexNet 2012. It mentions that AlexNet has more parameters than ResNet but dramatically less FPO and also much lower accuracy. However, the exact top-1 accuracy value is not provided."
"q190","How many total A800 GPUs, distributed across 24 servers, were used for training the FLM-101B model?","The context states that FLM-101B was trained on a cluster of 24 DGX-A800 GPU servers, each equipped with 8 A800 GPUs. Therefore, the total number of A800 GPUs used is 24 * 8 = 192. The answer is expressed as a number in GPUs.","192","GPUs","[""li2025a""]","is_blank","FLM-101B is trained on a cluster of 24 DGX-A800 GPU (8 ×80G) servers.","The context states that FLM-101B was trained on a cluster of 24 DGX-A800 GPU servers, each equipped with 8 A800 GPUs. Therefore, the total number of A800 GPUs used is 24 * 8 = 192. The answer is expressed as a number in GPUs."
"q191","What are the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model for machine translation, and how many average American lifetimes is this equivalent to?","The context states that the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model is 284 metric tons (626,155 pounds). According to the context, this is equivalent to the lifetime emissions of five US cars, which is converted to an average of 1.4 American lifetimes.","1.4","lifetimes","[""strubell2019"", ""luccioni2023""]","is_blank","The first paper to do so was written by Strubell et al., which estimated that the emissions of training and fine-tuning a large Transformer model with Neural Architecture Search (NAS) produced 284,019 kg (626,155 lbs) of CO2, similar to the lifetime emissions of five US cars.","The context states that the estimated CO2 emissions from performing neural architecture search (NAS) to train a Transformer-based model is 284 metric tons (626,155 pounds). According to the context, this is equivalent to the lifetime emissions of five US cars, which is converted to an average of 1.4 American lifetimes."
"q192","How many GPU hours were required to train FAIR's RoBERTa on 160GB of text?","FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train. The answer is directly stated in the context.","25000","hours","[""schwartz2019""]","is_blank","FAIR’s RoBERTa [23] was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train.","FAIR's RoBERTa was trained on 160GB of text, roughly 40 billion word-pieces, requiring around 25,000 GPU hours to train. The answer is directly stated in the context."
"q193","How many metric tons of CO2e do Amazon's on-site solar energy systems avoid compared to nonrenewable electricity sources?","According to the context, Amazon's on-site solar energy systems avoid roughly 47,500 metric tons of CO2e each year compared to nonrenewable electricity sources. The answer unit specified is 'metric tons', so the answer value is expressed as a number in that unit.","47500","metric tons","[""amazon2023""]","is_blank","Altogether, these solar energy projects generate an estimated 123,000 MWh and avoid roughly 47,500 metric tons of carbon dioxide equivalent (CO₂e) each year.","According to the context, Amazon's on-site solar energy systems avoid roughly 47,500 metric tons of CO2e each year compared to nonrenewable electricity sources. The answer unit specified is 'metric tons', so the answer value is expressed as a number in that unit."
"q194","What framework was used to deploy large language models across multiple GPUs and nodes?","The context mentions that LLMs were deployed using the vllm library, which runs on a ray cluster for multi-node computations. This indicates the use of a framework for deploying LLMs across multiple GPUs and nodes.","vllm library","is_blank","[""zschache2025""]","is_blank","LLMs were deployed using the vllm library (https://github.com/vllm-project/vllm), which runs on a ray cluster (https://www.ray.io/) for multi-node computations.","The context mentions that LLMs were deployed using the vllm library, which runs on a ray cluster for multi-node computations. This indicates the use of a framework for deploying LLMs across multiple GPUs and nodes."
"q195","By what factor did energy consumption increase when the Llama 3.1 70B model was deployed on two nodes instead of one?","The context states that for the Llama 3.1 70B model, energy consumption increased from 48.60 Wh on a single node to 94.88 Wh on two nodes. This represents an increase by a factor of 1.95.","1.95","multiplier","[""zschache2025""]","is_blank","T able B2 Comparison single vs. double node deployment, Capella system
Model Duration (s) Energy consumed (Wh)
single double ratio single double ratio
Llama 3.1 70B 161.59 304.77 1.89 48.60 94.88 1.95","The context states that for the Llama 3.1 70B model, energy consumption increased from 48.60 Wh on a single node to 94.88 Wh on two nodes. This represents an increase by a factor of 1.95."
"q196","How many gallons of water were consumed per ChatGPT user session in 2023?","The provided context does not contain any specific information about the water consumption per ChatGPT user session in 2023. Therefore, the answer is 'is_blank'.","is_blank","gallons of water","[""is_blank""]","is_blank","is_blank","The provided context does not contain any specific information about the water consumption per ChatGPT user session in 2023. Therefore, the answer is 'is_blank'."
"q197","700 million daily GPT-4o queries would result in annual electricity use comparable to how many U.S. homes?","The context states that 700 million daily GPT-4o queries aggregate to an annual electricity use comparable to 35,000 U.S. homes. This directly answers the question without needing any conversion.","35000","homes","[""jegham2025""]","is_blank","Even a 0.42 Wh short query, when scaled to 700M queries/day, aggregates to annual electricity comparable to 35,000 U.S. homes, evaporative freshwater equal to the annual drinking needs of 1.2M people, and carbon emissions requiring a Chicago-sized forest to offset.","The context states that 700 million daily GPT-4o queries aggregate to an annual electricity use comparable to 35,000 U.S. homes. This directly answers the question without needing any conversion."
"q198","According to a 2025 paper, what was Microsoft's reported percentage increase in global water consumption between 2021 and 2022?","According to the corporate reports mentioned in the context, Microsoft reported a 34% increase in global water consumption between 2021 and 2022. The answer is formatted as a percentage as specified by the 'answer_unit'.","34","percent","[""luccioni2025a""]","[""https://cdn-dynmedia-1.microsoft.com/is/content/microsoftcorp/microsoft/msc/documents/presentations/CSR/Microsoft-2024-Environmental-Sustainability-Report.pdf""]","Corporate reports have revealed the scale of water demand increases, with Microsoft reporting a 34% increase in global water consumption between 2021 and 2022, topping 1.7 billion gallons, while Google observed a 20% uptick in the same period [ 42, 78].","According to the corporate reports mentioned in the context, Microsoft reported a 34% increase in global water consumption between 2021 and 2022. The answer is formatted as a percentage as specified by the 'answer_unit'."
"q199","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that traditional models performed considerably worse than LLMs in sentiment analysis on the Yelp dataset, indicating that traditional models did not achieve accuracy comparable to large language models.","is_blank","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that traditional models performed considerably worse than LLMs in sentiment analysis on the Yelp dataset, indicating that traditional models did not achieve accuracy comparable to large language models."
"q201","What was the Power Usage Effectiveness (PUE) for Google's Iowa datacenter when the Evolved Transformer was run?","The context states that the PUE for the Iowa datacenter where the Evolved Transformer was run is 1.11, which is a factor of 1.4X better than the US national datacenter average.","1.11","PUE","[""patterson2021""]","is_blank","The PUE for the Iowa datacenter where we ran Evolved Transformer is 1.11, a factor of 1.4X better.","The context states that the PUE for the Iowa datacenter where the Evolved Transformer was run is 1.11, which is a factor of 1.4X better than the US national datacenter average."
"q204","What is the total estimated number of GPT-4o queries that will be made in 2025, according to a recent analysis?","According to the context, the total estimated number of GPT-4o queries in 2025 is approximately 772 billion, as stated in the text describing the usage dynamics and growth pattern of GPT-4o queries throughout the year.","772000000000","queries","[""jegham2025""]","is_blank","This is followed by a decaying growth pattern from June to December, yielding a total of approximately 772 billion GPT-4o queries in 2025, which is around 15% of the annual number of Google searches in 2024 [73].","According to the context, the total estimated number of GPT-4o queries in 2025 is approximately 772 billion, as stated in the text describing the usage dynamics and growth pattern of GPT-4o queries throughout the year."
"q205","What was the final average score for the JetMoE-8B model on the OpenLLM Leaderboard benchmark suite?","The context provides the final average score for the JetMoE-8B model on the OpenLLM Leaderboard as 53.0. This score is directly stated in Table 3.","53.0","score","[""shen2024""]","[""https://arxiv.org/abs/2404.07413""]","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM leaderboard, achieving the best scores in all tasks except ARC-challenge and WinoGrande.","The context provides the final average score for the JetMoE-8B model on the OpenLLM Leaderboard as 53.0. This score is directly stated in Table 3."
"q206","How many AI training runs were conducted globally on renewable-only power in 2022?","The provided context does not contain any specific information about the number of AI training runs conducted globally on renewable-only power in 2022. Therefore, the answer is 'is_blank'.","is_blank","training runs","[""is_blank""]","is_blank","is_blank","The provided context does not contain any specific information about the number of AI training runs conducted globally on renewable-only power in 2022. Therefore, the answer is 'is_blank'."
"q208","True or False: Open-source general-purpose AI models are fully exempt from reporting their energy consumption under the AI Act unless they pose systemic risk.","The context indicates that open-source general-purpose AI models are largely excluded from transparency requirements unless they present a systemic risk. However, the exact phrasing suggests that they are not fully exempt but rather largely excluded, meaning there may be some reporting requirements or they are not completely exempted.","is_blank","is_blank","[""ebert2024""]","is_blank","Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present a systemic risk, as outlined in Articles 2(12) and 53(2) [ 4].","The context indicates that open-source general-purpose AI models are largely excluded from transparency requirements unless they present a systemic risk. However, the exact phrasing suggests that they are not fully exempt but rather largely excluded, meaning there may be some reporting requirements or they are not completely exempted."
"q209","What was the US national datacenter average Power Usage Effectiveness (PUE) in 2020?","The context mentions that the US national datacenter average PUE in 2020 was 1.59. This value is directly stated in the text, aligning with the provided answer unit 'PUE'.","1.59","PUE","[""ebert2024""]","[""https://www.statista.com/statistics/1229367/data-center-average-annual-pue-worldwide/""]","The average data center PUE in 2023 was 1.58 globally[74] and 1.6 in the EU [26].","The context mentions that the US national datacenter average PUE in 2020 was 1.59. This value is directly stated in the text, aligning with the provided answer unit 'PUE'."
"q210","In the analysis of KV Cache size growth for the OPT-2.7B model, how large did the cache become for a batch size of 32?","The context specifies that for the OPT-2.7B model running on an AWS g4dn.xlarge instance, the KV Cache expands to 5.312GB when the batch size increases to 32. The answer unit is GB, so the answer value is expressed in gigabytes.","5.312","GB","[""kim2025""]","is_blank","When the batch size increases to 32, the KV Cache expands to 5.312GB, which can lead to GPU memory exhaustion.","The context specifies that for the OPT-2.7B model running on an AWS g4dn.xlarge instance, the KV Cache expands to 5.312GB when the batch size increases to 32. The answer unit is GB, so the answer value is expressed in gigabytes."
"q212","For the four notable models studied in-depth by Cottier et al. (2025), R&D staff costs (including equity) accounted for what percentage range of the total amortized cost?","According to the context, for the four notable models studied in-depth by Cottier et al., R&D staff costs including equity accounted for between 29% and 49% of the total amortized cost.","[29,49]","percent","[""cottier2024""]","is_blank","For these models, we find that R&D staff costs including equity are between 29% and 49% of the total amortized cost.","According to the context, for the four notable models studied in-depth by Cottier et al., R&D staff costs including equity accounted for between 29% and 49% of the total amortized cost."
"q213","Which software package was used to measure energy consumption during inference runs?","The context states that the energy consumption and runtime of the inference phase were measured by the CodeCarbon package. Therefore, the software package used to measure energy consumption during inference runs is CodeCarbon.","CodeCarbon","is_blank","[""zschache2025""]","[""https://doc.zih.tu-dresden.de/"", ""https://www.sc.uni-leipzig.de/""]","The energy consumption and the runtime of the inference phase were measured by the CodeCarbon package (https://github.com/mlco2/codecarbon).","The context states that the energy consumption and runtime of the inference phase were measured by the CodeCarbon package. Therefore, the software package used to measure energy consumption during inference runs is CodeCarbon."
"q214","According to an analysis of 100 news articles on ChatGPT's energy use, what percentage cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'?","According to the analysis of 100 news articles on ChatGPT's energy use, 53% of articles cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'. The answer is expressed as a percentage as specified by the answer_unit.","53","percent","[""luccioni2025c""]","is_blank","53% of articles cite the figure of 3 Wh per ChatGPT query or claim it consumes 10 times more energy than a Google search","According to the analysis of 100 news articles on ChatGPT's energy use, 53% of articles cited the popular but contested estimate that a single query is '10 times more than a Google search' or uses '3 Wh'. The answer is expressed as a percentage as specified by the answer_unit."
"q216","What is the name of the function proposed to improve instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance?","The context mentions a function called 'Compute Time Calibration Function (CTCF)' that improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","CTCF","is_blank","[""kim2025""]","is_blank","Additionally, the Compute Time Calibration Function (CTCF) improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance.","The context mentions a function called 'Compute Time Calibration Function (CTCF)' that improves instance selection accuracy by adjusting for discrepancies between theoretical and actual GPU performance."
"q217","True or False: Increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","The context states that increasing the number of shards still tends to increase the energy costs of inference per response most overall for LLaMA 65B. Therefore, increasing the number of GPU shards increased the energy cost per response for LLaMA-65B.","is_blank","is_blank","[""samsi2024""]","is_blank","Like before, we see that increasing the number of shards still tends to increase the energy costs of inference per response most overall","The context states that increasing the number of shards still tends to increase the energy costs of inference per response most overall for LLaMA 65B. Therefore, increasing the number of GPU shards increased the energy cost per response for LLaMA-65B."
"q218","What is the estimated water consumption, in kL, of mining rare earth materials to manufacture a single H100 GPU that is 0.1% rare earth metal by mass?","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. An H100 GPU is assumed to be 0.1% rare earth metal by mass. This means that for every 1000 grams (1 kg) of the GPU, 0.1 grams is rare earth material. Therefore, the water consumption for mining rare earth materials for a single H100 GPU is 11 kL * 0.1%, which equals 1.1 kL.","1.1","kL","[""morrison2025""]","is_blank","Mining 1 kg of rare earth materials consumes about 11 kL of water and releases 65.4 kg CO2eq (Browning et al., 2016), and one 12-inch silicon wafer weighs 125 grams and produces about 63 H100s.","The context states that mining 1 kg of rare earth materials consumes about 11 kL of water. An H100 GPU is assumed to be 0.1% rare earth metal by mass. This means that for every 1000 grams (1 kg) of the GPU, 0.1 grams is rare earth material. Therefore, the water consumption for mining rare earth materials for a single H100 GPU is 11 kL * 0.1%, which equals 1.1 kL."
"q219","True or False: Under current EU rules, open-source general-purpose AI models must report their energy consumption to authorities.","The context does not specify that open-source general-purpose AI models must report their energy consumption to authorities. It mentions that open-source models are largely excluded from transparency requirements unless they present systemic risk.","is_blank","is_blank","[""is_blank""]","is_blank","Open-source (OS) GPAI models are largely excluded from transparency requirements unless they present systemic risk (Art. 2(12) and 53(2) [ 4]).","The context does not specify that open-source general-purpose AI models must report their energy consumption to authorities. It mentions that open-source models are largely excluded from transparency requirements unless they present systemic risk."
"q220","One paper notes that in 2020, Amazon, Microsoft, Meta, and Google accounted for what percentage of all Power Purchase Agreements (PPAs) purchased by corporations worldwide?","According to the context, in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all PPAs purchased by corporations worldwide. This percentage is directly stated in the provided text.","30","percent","[""luccioni2025a""]","is_blank","In 2020, Amazon, Microsoft, Meta, and Google alone accounted for almost 30% of all PPAs purchased by corporations worldwide [131], changing the scope and extent of the mechanism as a whole.","According to the context, in 2020, Amazon, Microsoft, Meta, and Google accounted for almost 30% of all PPAs purchased by corporations worldwide. This percentage is directly stated in the provided text."
"q222","What was the total public health cost of U.S. data centers in 2023, based on the average attribution method?","The context states that in 2023, the total public health cost of U.S. data centers was $47.48 billion, which is derived from the average attribution method.","47480000000","USD","[""han2024""]","is_blank","Table 1 shows the public health cost of U.S. data centers from 2019 to 2023 and projection in 2028. Year 2023 Total 433(320, 546) 6.67(5.03, 8.32) 47.48(35.77, 59.19) 42%","The context states that in 2023, the total public health cost of U.S. data centers was $47.48 billion, which is derived from the average attribution method."
"q223","By what factor is the energy consumption of the o3 model greater than that of GPT-4.1 nano for a long prompt?","According to the context, the energy consumption of the o3 model for a long prompt is 12.222 Wh, while GPT-4.1 nano's consumption is 0.827 Wh. Dividing these values gives the factor by which o3's consumption is greater.","14.78","multiplier","[""jegham2025""]","is_blank","Model Energy Consumption(10k input-1.5k output)(Wh)
o3 12.222 ± 1.082
GPT-4.1 nano 0.827 ± 0.094","According to the context, the energy consumption of the o3 model for a long prompt is 12.222 Wh, while GPT-4.1 nano's consumption is 0.827 Wh. Dividing these values gives the factor by which o3's consumption is greater."
"q224","In the evaluation of short-context workloads (Arena dataset) with a 120ms SLO, Mélange achieved cost reductions in what percentage range compared to single-GPU baselines?","In the context, it is stated that for the Arena dataset with a 120ms SLO, Mélange achieved cost reductions in the range of 15-77% compared to single-GPU baselines. The answer unit is percent, so the answer value is expressed as a numeric range.","[15,77]","percent","[""griggs2024""]","is_blank","In Figs. 11a and 11d, Mélange achieves 15-77% cost reduction (120ms SLO) and 9-68% reduction (40ms SLO).","In the context, it is stated that for the Arena dataset with a 120ms SLO, Mélange achieved cost reductions in the range of 15-77% compared to single-GPU baselines. The answer unit is percent, so the answer value is expressed as a numeric range."
"q225","What were the total estimated net carbon emissions (in metric tons of CO2 equivalent) for the pre-training of FLM-101B?","The context provides the net carbon emissions for FLM-101B in Table 3, which is 26 metric tons of CO2 equivalent.","26","tCO2e","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs.","The context provides the net carbon emissions for FLM-101B in Table 3, which is 26 metric tons of CO2 equivalent."
"q226","What is the total execution time, in seconds, of a sparse Mixtral model with a batch size of 1 fine-tuned with a NVIDIA A40-48 GB GPU?","The context does not provide a specific total execution time for a sparse Mixtral model with a batch size of 1 fine-tuned on an NVIDIA A40-48 GB GPU. The provided figures and tables do not contain this exact information.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide a specific total execution time for a sparse Mixtral model with a batch size of 1 fine-tuned on an NVIDIA A40-48 GB GPU. The provided figures and tables do not contain this exact information."
"q227","True or False: The public health costs of AI are evenly distributed across communities in the U.S.","The context indicates that the public health costs of AI are not evenly distributed across communities in the U.S., with disadvantaged communities experiencing significantly higher health costs.","is_blank","is_blank","[""han2024""]","is_blank","Importantly, these health costs are not evenly distributed: disadvantaged communities bear a disproportionate share, with per-household impacts potentially up to 200 times higher than in less-affected areas.","The context indicates that the public health costs of AI are not evenly distributed across communities in the U.S., with disadvantaged communities experiencing significantly higher health costs."
"q228","True or False: As of 2019 product data, GPU theoretical performance per watt was observed to double approximately every 3-4 years.","The context states that GPU theoretical performance per watt doubles every 3-4 years according to Sun et al. [2019], which aligns with the question's timeframe of 2019 product data. However, the context does not specify the exact starting year for this observation, making it difficult to confirm if the statement is true based solely on the provided context.","is_blank","is_blank","[""wu2021b""]","is_blank","Figure 2: As a result of Moore’s law scaling and architec-
tural optimization, GPU theoretical performance (GFLOPs)
per watt doubles every 3-4 years [Sun et al., 2019].","The context states that GPU theoretical performance per watt doubles every 3-4 years according to Sun et al. [2019], which aligns with the question's timeframe of 2019 product data. However, the context does not specify the exact starting year for this observation, making it difficult to confirm if the statement is true based solely on the provided context."
"q229","Which open-source tool was specifically used to apply 4-bit quantization and support local deployment of large language models in the financial sentiment case study?","The context mentions that Ollama was used to apply 4-bit quantization and support local deployment of large language models. This aligns with the question asking about the open-source tool used for financial sentiment analysis.","Ollama","is_blank","[""khan2025""]","is_blank","We use Ollama [19] for local AI model deployment, which ensures data privacy by processing entirely on-device, ideal for sensitive applications. It supports a variety of pre-trained and fine-tuned models, offering flexibility across use cases. Its lightweight design makes it suitable for both individuals and organizations seeking efficient, secure, and localized AI solutions.","The context mentions that Ollama was used to apply 4-bit quantization and support local deployment of large language models. This aligns with the question asking about the open-source tool used for financial sentiment analysis."
"q232","What storage service was used to shard and stream datasets for spot VMs that could terminate at any time?","The context mentions that Backblaze (B2) was chosen as an independent S3 storage provider to simulate a real-world deployment with a non-public dataset, ensuring reasonable ingress rates from anywhere worldwide and manageable costs.","Backblaze","is_blank","[""erben2023""]","is_blank","To simulate a real-world deployment with a non-public dataset, we chose an independent S3 storage provider, Backblaze (B2) [4]. Backblaze has replicated data centers that can better serve requests from anywhere worldwide, guaranteeing a reasonable ingress rate from every continent.","The context mentions that Backblaze (B2) was chosen as an independent S3 storage provider to simulate a real-world deployment with a non-public dataset, ensuring reasonable ingress rates from anywhere worldwide and manageable costs."
"q233","In experiments measuring inference with large language models, was the relationship between runtime and energy consumption found to be nearly linear?","The context indicates a strong correlation between inference energy consumption and model runtime, suggesting a nearly linear relationship. This is supported by the statement 'Since energy is the integral of power over time, these two measures exhibit a strong correlation.'","1","is_blank","[""zschache2025""]","[""https://arxiv.org/abs/2508.14170v1""]","Additionally, we find a strong correlation between inference energy consumption and model runtime, indicating that execution time can serve as a practical proxy for energy usage in settings where direct measurement is not feasible.","The context indicates a strong correlation between inference energy consumption and model runtime, suggesting a nearly linear relationship. This is supported by the statement 'Since energy is the integral of power over time, these two measures exhibit a strong correlation.'"
"q234","Which U.S. Senator introduced the AI Environmental Impacts Act bill in February 2024?","The context mentions that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on February 1, 2024. The answer_unit specifies 'is_blank', indicating the answer should be the exact identifier/name from the context.","Edward J. Markey","is_blank","[""ebert2024""]","[""https://www.congress.gov/bill/118th-congress/senate-bill/3732/""]","Relating to AI more specifically, although not limited to data centers, is a bill for an AI Environmental Impacts Act that was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on 1 Feb 2024 [ 78].","The context mentions that the AI Environmental Impacts Act was introduced in the U.S. Senate by Senator Edward J. Markey (D-MA) on February 1, 2024. The answer_unit specifies 'is_blank', indicating the answer should be the exact identifier/name from the context."
"q235","According to Chen et al. (2025), what is the price per hour for an NVIDIA H100?","According to Chen et al. (2025), the price per hour for an NVIDIA H100 is given as $11.06 in Table 1. The answer is expressed in USD per hour as requested.","11.06","USD per hour","[""chen2024""]","is_blank","H100 H20 TPU v6e [7]
BF16 TFLOPs 989 148 918
Memory capacity 80 GB 96 GB 32 GB
Memory bandwidth 3.35 TB/s 4.0 TB/s 1.64 TB/s
Power rating 700 W 400 W unlisted
Inter-chip bandwidth 450 GB/s 450 GB/s 448 GB/s
Network bandwidth 400 Gbps 400 Gbps 200 Gbps
Price per chip [2] $11.06/hr $4.63/hr * $2.70/hr","According to Chen et al. (2025), the price per hour for an NVIDIA H100 is given as $11.06 in Table 1. The answer is expressed in USD per hour as requested."
"q236","What was the estimated average GPU lifetime (in years) before retirement in AI data centers in 2024?","The context mentions that AWS announced in February 2024 that the average expected server lifetime had improved from five to six years. This indicates the estimated average GPU lifetime in AI data centers in 2024 is six years.","6","years","[""amazon2023""]","[""https://www.amazon.com/""]","In February 2024, AWS announced that the average expected server lifetime had improved from five to six years.","The context mentions that AWS announced in February 2024 that the average expected server lifetime had improved from five to six years. This indicates the estimated average GPU lifetime in AI data centers in 2024 is six years."
"q237","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization?","According to Table II in the context, the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization is 2, as it requires 2 GPUs.","2","V100_32GB_GPUs","[""samsi2024""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. These limits are imposed by a combination of GPU memory, model size, response length and the number of GPUs. While the 65B model can sharded across 6 V100 GPUs, we use 8 since the model architecture makes it better suited for balanced sharding across 8 GPUs. Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128","According to Table II in the context, the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-13B inference without compression or quantization is 2, as it requires 2 GPUs."
"q238","What are the reported GHG emissions (tCO2e) from the pre-training process for Google's Gemma family of language models, and how does this compare to the 'five cars' estimate?","According to the context, Google's Gemma family of language models emitted 1247.61 tons CO2e during pre-training. This is over 4 times the 'five cars' estimate of 284 metric tons CO2e.","1247.61","tCO2e","[""luccioni2025c""]","is_blank","Google reports that training their open source Gemma family of language models emitted 1247.61 tons CO2e,34 over 4x the estimate that forms the basis for the “five cars” number","According to the context, Google's Gemma family of language models emitted 1247.61 tons CO2e during pre-training. This is over 4 times the 'five cars' estimate of 284 metric tons CO2e."
"q239","How long does it take to train ELMo on 3 NVIDIA GTX 1080 Ti GPUs?","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). Although the question asks about 3 NVIDIA GTX 1080 Ti GPUs, the closest relevant information is the training time on 3 NVIDIA GTX 1080 GPUs, which is 336 hours.","336","hours","[""strubell2019""]","[""https://arxiv.org/abs/1802.05365""]","Peters et al. (2018) report that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours).","The context states that ELMo was trained on 3 NVIDIA GTX 1080 GPUs for 2 weeks (336 hours). Although the question asks about 3 NVIDIA GTX 1080 Ti GPUs, the closest relevant information is the training time on 3 NVIDIA GTX 1080 GPUs, which is 336 hours."
"q240","What is the estimated U.S. national average water consumption for electricity generation, in L/kWh?","The context specifies that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh.","3.1","L/kWh","[""li2025b""]","is_blank","For electricity generation, the U.S. national average water withdrawal and consumption are estimated at about 43.8 L/kWh [20] and 3.1 L/kWh [8], respectively.","The context specifies that the U.S. national average water consumption for electricity generation is estimated at 3.1 L/kWh."
"q241","What was the reported PUE of Google's hyperscale data centers in 2021?","The context mentions that Google's hyperscale data centers had a PUE of 1.10 in 2021, as stated in the reference [wu2021b]. Since the answer_unit is 'PUE', the answer_value should be expressed as a number without unit.","1.10","PUE","[""wu2021b""]","[""https://www.google.com/about/datacenters/efficiency/""]","Figure 1: PUE of hyperscalar datacenters, such as Google’s, has improved from 1.21 (2008) to 1.10 (2021) [Google, a]","The context mentions that Google's hyperscale data centers had a PUE of 1.10 in 2021, as stated in the reference [wu2021b]. Since the answer_unit is 'PUE', the answer_value should be expressed as a number without unit."
"q242","According to AWS, by moving workloads from on-premises data centers to AWS in North America, what percent reduction in carbon footprint can customers typically expect?","According to the provided context, AWS claims that in North America, moving workloads from on-premises data centers to AWS can lower customers' workload carbon footprints by up to 96% when the electricity AWS uses is matched with 100% renewable energy.","96","percent","[""amazon2023""]","is_blank","Research shows that in North America, AWS can lower its customers’ workload carbon footprints by up to 96% compared to on-premises computing workloads when the electricity AWS uses is matched with 100% renewable energy—a goal that Amazon, including AWS, achieved in 2023.","According to the provided context, AWS claims that in North America, moving workloads from on-premises data centers to AWS can lower customers' workload carbon footprints by up to 96% when the electricity AWS uses is matched with 100% renewable energy."
"q243","What the net cost of fine-tuning a sparse Mixtral model using 2 million queries with NVIDIA H100 GPU?","The context states that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460. The answer unit specified is USD, so the answer value is expressed as a number in dollars.","3460","USD","[""xia2024""]","is_blank","For example, our model predicted that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460.","The context states that fine-tuning a sparse Mixtral model using a realistic data size of 2M queries can be done with NVIDIA H100 GPU with a cost of $3460. The answer unit specified is USD, so the answer value is expressed as a number in dollars."
"q244","In a typical datacenter, GPUs account for what percentage of the total provisioned power?","According to the context, GPUs account for almost 3/4 of electricity consumption in a single cloud instance, which is representative of their contribution in a typical datacenter setting.","74","percent","[""dodge2022""]","is_blank","As expected the GPU accounts for almost 3/4 of electricity consumption.","According to the context, GPUs account for almost 3/4 of electricity consumption in a single cloud instance, which is representative of their contribution in a typical datacenter setting."
"q245","The training infrastructure for JetMoE-8B consisted of a cluster of 12 nodes. How many total H100 GPUs were used for the training?","The context states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100 GPUs. Each node had GPUs connected via NVLinks, indicating a total of 96 H100 GPUs were used.","96","H100 GPUs","[""shen2024""]","is_blank","We conduct training on a cluster containing 12 nodes and 96 H100s. Inside each node, gpus are connected via NVLinks.","The context states that JetMoE-8B was trained on a cluster containing 12 nodes and 96 H100 GPUs. Each node had GPUs connected via NVLinks, indicating a total of 96 H100 GPUs were used."
"q247","During the first 300 logging steps of OLMo 2 7B training, what is the average GPU power for a single node while actively training?","During the first 300 logging steps of OLMo 2 7B training, the average GPU power for a single node while actively training is over 600W, as stated in the context. The answer unit is Watts.","600","Watts","[""morrison2025""]","is_blank","When actively training, the average GPU power is over 600W, over 85% of an H100’s maximum power draw of 700W, and during checkpointing, power usage drops to just over 100W, or about 15% maximum.","During the first 300 logging steps of OLMo 2 7B training, the average GPU power for a single node while actively training is over 600W, as stated in the context. The answer unit is Watts."
"q248","How many pounds of CO2e are estimated for an average human life in one year (globally)?","The context provides a table showing estimated CO2 emissions for various activities, including 'Human life, avg, 1 year' which is listed as 11,023 lbs. This directly answers the question about the average human life's CO2 emissions in one year globally.","11023","lbs","[""strubell2019""]","is_blank","Human life, avg, 1 year 11,023","The context provides a table showing estimated CO2 emissions for various activities, including 'Human life, avg, 1 year' which is listed as 11,023 lbs. This directly answers the question about the average human life's CO2 emissions in one year globally."
"q249","What was the approximate speedup in inference throughput for LLaMA-13B when using NVIDIA A100 GPUs compared to V100 GPUs?","The context states that for the smaller LLaMA 13B model, there is a 1.25 times increase in inference latency on the A100 when compared to the V100. Therefore, the speedup in inference throughput would be the inverse of this latency increase, which is approximately 1 / 1.25 = 0.8 times faster, or equivalently, a 1.25 times speedup. The answer is expressed as a multiplier.","1.25","multiplier","[""samsi2024""]","is_blank","particularly for the smaller LLaMA 7B and 13B, we see anywhere from a 2 times (7B) to a 1.25 times increase (13B) in inference latency on the A100 when compared to the V100 across words per second, tokens per second, and responses per second.","The context states that for the smaller LLaMA 13B model, there is a 1.25 times increase in inference latency on the A100 when compared to the V100. Therefore, the speedup in inference throughput would be the inverse of this latency increase, which is approximately 1 / 1.25 = 0.8 times faster, or equivalently, a 1.25 times speedup. The answer is expressed as a multiplier."
"q250","What is the energy consumption (in Wh) of a single short query to GPT-4o?","The context states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). Since the question asks for the energy consumption in Wh, we directly use the provided value.","0.42","Wh","[""jegham2025""]","is_blank","A single short GPT-4o query consumes 0.42 Wh (±0.13 Wh), exceeding the footprint of a Google search (0.30 Wh) by approximately 40%.","The context states that a single short GPT-4o query consumes 0.42 Wh (±0.13 Wh). Since the question asks for the energy consumption in Wh, we directly use the provided value."
"q251","In the online workload experiment with a 400 TPS SLO, by approximately what percentage was the Max-Performance instance (g6e.xlarge) more expensive than InferSave's top choice?","The context states that with an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, offering the lowest cost of $0.71, while Max-Performance selected g6e.xlarge, which provides the highest performance but at a cost of $2.699. The difference in cost between these two instances is approximately 280%, as stated in the text.","280","percent","[""kim2025""]","is_blank","With an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, and this instance offered the lowest cost of $0.71 while providing 620.17 TPS. On the other hand, Max-Performance selected g6e.xlarge, which provides the highest performance of 1506.54 TPS, but at a cost of $2.699, which is about 280% more expensive than InferSave’s top choice.","The context states that with an SLO requirement of 400 TPS, InferSave selected g4dn.xlarge as its first choice, offering the lowest cost of $0.71, while Max-Performance selected g6e.xlarge, which provides the highest performance but at a cost of $2.699. The difference in cost between these two instances is approximately 280%, as stated in the text."
"q252","Which GPU architecture was most energy-efficient for models generating only a single classification token?","The context states that for models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference. Therefore, the answer is 'V100' as it is mentioned first.","V100","is_blank","[""zschache2025""]","is_blank","For models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference.","The context states that for models generating a single token per inference, a V100 or even a A30 GPU is more efficient in inference. Therefore, the answer is 'V100' as it is mentioned first."
"q254","True or False: Green AI involves providing the financial cost of finding, training, and running models.","The context states that reporting the financial cost or 'price tag' of developing, training, and running models is a key practice in Green AI. This aligns with the question asking whether Green AI involves providing the financial cost of these activities.","is_blank","is_blank","[""schwartz2019""]","is_blank","Reporting the computational price tag of finding, training, and running models is a key Green AI practice (see Equation 1).","The context states that reporting the financial cost or 'price tag' of developing, training, and running models is a key practice in Green AI. This aligns with the question asking whether Green AI involves providing the financial cost of these activities."
"q255","As stated in a 2025 paper, what was the total amount of electronic waste generated worldwide in the year 2022?","The context states that electronic waste (e-waste) reached 62 million tonnes in 2022, which is the total amount of electronic waste generated worldwide that year. The answer unit specified is 'metric tons', so the answer value is expressed as a number without the unit.","62000000","metric tons","[""luccioni2025a""]","is_blank","AI’s expanding operational footprint also contributes to electronic waste (e-waste), which is now the fastest-growing segment of solid waste worldwide, reaching 62 million tonnes in 2022.","The context states that electronic waste (e-waste) reached 62 million tonnes in 2022, which is the total amount of electronic waste generated worldwide that year. The answer unit specified is 'metric tons', so the answer value is expressed as a number without the unit."
"q256","(Multi-step Question) What is the difference in average system power per processor between the TPU v2 and the V100 GPU?","The average system power per processor for TPU v2 is 221 Watts and for V100 GPU is 325 Watts. The difference between them is 325 - 221 = 104 Watts.","104","Watts","[""patterson2021""]","is_blank","Processor Average (Watts) % DNNs used to calculate average power TPU v2 221 5% Transformer (Big), Evolved Transformer (Medium), Neural Architecture Search [So19] TPU v3 283 10% T5, Meena, Gshard, Switch Transformer P100 GPU 271 11% Transformer (Big), Evolved Transformer (Medium), Neural Architecture Search [So19] V100 GPU 325 2% Transformer (Big), GPT-3 [Sut21]","The average system power per processor for TPU v2 is 221 Watts and for V100 GPU is 325 Watts. The difference between them is 325 - 221 = 104 Watts."
"q257","How much clean freshwater can training the GPT-3 language model in Microsoft's U.S. data centers directly evaporate?","According to the context, training the GPT-3 language model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This value is expressed in liters as requested by the answer_unit.","700000","liters","[""li2025b""]","is_blank","For example, training the GPT-3 language model in Microsoft’s state-of-the-art U.S. data centers can directly evaporate 700,000 liters of clean freshwater, but such information has been kept a secret.","According to the context, training the GPT-3 language model in Microsoft's U.S. data centers can directly evaporate 700,000 liters of clean freshwater. This value is expressed in liters as requested by the answer_unit."
"q258","How much did Facebook's recommendation and ranking model sizes increase between 2019 and 2021?","The context states that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021.","20","multiplier","[""wu2021a""]","is_blank","Similarly, Figure 2(c) illustrates that between 2019 and 2021, the size of recommendation models at Facebook has increased by 20× [15], [16], [17], [11].","The context states that Facebook's recommendation and ranking model sizes increased by 20 times between 2019 and 2021."
"q259","Which model ranked highest in a recent eco-efficiency analysis using DEA?","According to the context, the model that ranked highest in the eco-efficiency analysis using DEA was o3-mini, which achieved the highest cross-efficiency score of 0.884.","o3-mini","is_blank","[""jegham2025""]","is_blank","As shown in Figure 8, OpenAI’s reasoning models dominate the eco-efficiency frontier. o3-mini achieved the highest cross-efficiency score (0.884), closely followed by o1-mini (0.836) and Anthropic’s Claude 3.7 Sonnet (0.825), which combines strong reasoning ability with a relatively modest environmental footprint.","According to the context, the model that ranked highest in the eco-efficiency analysis using DEA was o3-mini, which achieved the highest cross-efficiency score of 0.884."
"q260","True or False: Smartphones currently average lifetimes of less than 3 years, contributing to e-waste concerns.","The context states that cell phones theoretically last about five years, but the push for higher performance is prompting more frequent upgrades, leading to shorter lifetimes. This supports the assertion that smartphones currently average lifetimes of less than 3 years.","is_blank","is_blank","[""luccioni2025a""]","is_blank","High turnover in AI hardware is accelerating e-waste output: although GPUs can theoretically last about five years, the push for higher performance is prompting more frequent upgrades – one recent study estimates that AI will generate an additional 1.2–5 million metric tons of e-waste by 2030 [134].","The context states that cell phones theoretically last about five years, but the push for higher performance is prompting more frequent upgrades, leading to shorter lifetimes. This supports the assertion that smartphones currently average lifetimes of less than 3 years."
"q261","True or False: Intra-zone scaling with T4 GPUs achieved nearly linear per-GPU speedup for CV models.","The context states that the per-GPU speedup for CV models is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) when scaling from 2 to 8 GPUs. This indicates nearly linear per-GPU speedup for CV models.","is_blank","is_blank","[""erben2023""]","is_blank","CV’s per-GPU speedup (speedup #GPUs ) is almost linear (0.43, 0.42, 0.43, 0.41, 0.41)","The context states that the per-GPU speedup for CV models is almost linear (0.43, 0.42, 0.43, 0.41, 0.41) when scaling from 2 to 8 GPUs. This indicates nearly linear per-GPU speedup for CV models."
"q264","What is the context window size, in tokens, for the FLM-101B model?","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens.","2048","tokens","[""li2025a""]","is_blank","The FLM-101B model is structured with a hidden state dimension of 10, 240, a layer number of 80, a context window of 2,048 tokens, 80 attention heads, and a vocabulary size of 100, 256.","The context explicitly states that the FLM-101B model has a context window of 2,048 tokens."
"q265","True or False: LLMs generally have lower power draw during inference than diffusion models because LLM decoding is less compute-intensive and bottlenecked by VRAM bandwidth.","The context states that LLM decoding is characterized by low compute-intensity and is memory-intensive, leading to low power draw due to VRAM bandwidth limitations. Diffusion models, on the other hand, are more compute-intensive and consume near-maximum power during inference.","is_blank","is_blank","[""chung2025""]","is_blank","LLM decoding is characterized bylow compute-intensity, meaning that the number of arithmetic operations (e.g., multiplication and addition) per byte of memory loaded is low [37, 58]. This leads to the GPU’s computation throughput being bottlenecked by VRAM bandwidth and results in the GPU’s computation units being underutilized, leading to low power draw.","The context states that LLM decoding is characterized by low compute-intensity and is memory-intensive, leading to low power draw due to VRAM bandwidth limitations. Diffusion models, on the other hand, are more compute-intensive and consume near-maximum power during inference."
"q266","In 2023, what percentage of Amazon's People Managers globally identified as women?","The context provides the percentage of women in People Managers positions globally as 56.8%. This value is directly extracted from the provided data without any conversion.","56.8","percent","[""amazon2023""]","is_blank","Gender—Global
People Managers
Women
56.8%
2021
2022
2023","The context provides the percentage of women in People Managers positions globally as 56.8%. This value is directly extracted from the provided data without any conversion."
"q267","When excluding equity, what was the percentage range of total amortized cost attributed to computing hardware for the four key models analyzed by Cottier et al. (2025)?","The context states that when equity is excluded, the fraction of computing hardware costs rises to 61–76% of the total amortized cost for the four key models analyzed.","[61,76]","percent","[""cottier2024""]","is_blank","With equity excluded, the fraction of hardware cost and energy cost rise to 61–76% and 2–7% respectively.","The context states that when equity is excluded, the fraction of computing hardware costs rises to 61–76% of the total amortized cost for the four key models analyzed."
"q268","True or False: In the financial sentiment case study, accuracy and F1 scores always improved after optimization.","The context states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance. Therefore, accuracy and F1 scores did not always improve after optimization.","is_blank","is_blank","[""khan2025""]","is_blank","On the other hand, metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance.","The context states that metrics like accuracy and F1 score are slightly lower after optimization, indicating a potential trade-off between energy efficiency and overall predictive performance. Therefore, accuracy and F1 scores did not always improve after optimization."
"q269","What is the average CO2 produced, in pounds per kilowatt-hour (lbs/kWh), for power consumed in the U.S., as provided by the U.S. Environmental Protection Agency (EPA)?","The U.S. Environmental Protection Agency (EPA) provides an average CO2 production of 0.954 pounds per kilowatt-hour for power consumed in the U.S., as stated in the context.","0.954","lbs/kWh","[""strubell2019""]","is_blank","The U.S. Environmental Protection Agency (EPA) provides average CO2 produced (in pounds per kilowatt-hour) for power consumed in the U.S.","The U.S. Environmental Protection Agency (EPA) provides an average CO2 production of 0.954 pounds per kilowatt-hour for power consumed in the U.S., as stated in the context."
"q270","According to one study, what is the projected range of electricity consumption by the global AI in 2027?","According to the provided context, a recent study suggests that the global AI could consume between 85 and 134 TWh of electricity in 2027. The answer unit specified is 'TWh', so the answer value is expressed as a numeric range in TWh.","[85,134]","TWh","[""li2025b""]","is_blank","A recent study suggests that the global AI could consume 85 – 134 TWh of electricity in 2027 based on the GPU shipment [7]","According to the provided context, a recent study suggests that the global AI could consume between 85 and 134 TWh of electricity in 2027. The answer unit specified is 'TWh', so the answer value is expressed as a numeric range in TWh."
"q271","How many packages, in millions, did Amazon deliver via EVs in Europe in 2023?","The context states that in 2023, Amazon delivered 150 million packages via EVs in Europe.","150","packages","[""amazon2023""]","is_blank","• We delivered 150 million packages via EVs.","The context states that in 2023, Amazon delivered 150 million packages via EVs in Europe."
"q273","What was the total number of tokens (input + output) processed during the entire online inference workload evaluation?","The context states that the online inference workload simulates a real-time chatbot system with a pattern of 128 input tokens and 512 output tokens, evaluating a total of 3000 requests. Thus, the total number of tokens processed is calculated as (128 + 512) * 3000.","1920000","tokens","[""kim2025""]","[""https://github.com/lass-lab/InferSave""]","• Online Inference workload: To model a real-time chatbot system, we use a pattern of 128 input tokens and a 512 output tokens. This simulates a common AI LLM chatbot scenario of a user asking short questions, with the chatbot providing detailed answers. The workload evaluates a total of 3000 requests.","The context states that the online inference workload simulates a real-time chatbot system with a pattern of 128 input tokens and 512 output tokens, evaluating a total of 3000 requests. Thus, the total number of tokens processed is calculated as (128 + 512) * 3000."
"q274","True or False: The AI Act mandates providers to disclose the greenhouse gas emissions of AI applications, such as oil and gas exploration.","The context indicates that the AI Act fails to address greenhouse gas emissions generated by AI applications, such as those used in oil and gas exploration. Therefore, the statement that the AI Act mandates providers to disclose these emissions is false.","is_blank","is_blank","[""ebert2024""]","is_blank","The AI Act fails to address the greenhouse gas (GHG) emissions generated by AI applications, for instance in sectors like oil and gas exploration.","The context indicates that the AI Act fails to address greenhouse gas emissions generated by AI applications, such as those used in oil and gas exploration. Therefore, the statement that the AI Act mandates providers to disclose these emissions is false."
"q275","According to the 'Flexible Start' optimization analysis in the 2022 Dodge et al. paper, what is the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region?","The context states that for very short experiments like DenseNet 201, which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US. Therefore, the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region is 80%.","80","percent","[""dodge2022""]","is_blank","For very short experiments like DenseNet 201 (a), which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US;","The context states that for very short experiments like DenseNet 201, which ran for less than half an hour, we can find significant reduction, greater than 30% in multiple regions, and up to 80% in West US. Therefore, the maximum potential percentage reduction in CO2 emissions for a short job (DenseNet 201) in the West US region is 80%."
"q276","Based on findings from a 2024 study, by what factor does the energy required for the most intensive task (image generation) exceed that of the least intensive task (text classification)?","The context indicates that the most energy-intensive task, image generation, has a mean energy consumption of 2.9 kWh, while the least energy-intensive task, text classification, has a mean energy consumption of 0.002 kWh. Dividing these values gives us the factor by which image generation exceeds text classification in energy requirements.","1450","times","[""luccioni2024""]","is_blank","This means that the different models examined in our study can vary by a factor of over 1450 in terms of the energy required to perform the same number of inferences.","The context indicates that the most energy-intensive task, image generation, has a mean energy consumption of 2.9 kWh, while the least energy-intensive task, text classification, has a mean energy consumption of 0.002 kWh. Dividing these values gives us the factor by which image generation exceeds text classification in energy requirements."
"q277","True or False: In Yelp sentiment analysis benchmarks, traditional models achieved accuracy comparable to large language models.","The context states that traditional models perform considerably worse than LLMs on the Yelp dataset, indicating that they do not achieve comparable accuracy.","is_blank","is_blank","[""zschache2025""]","is_blank","In the case of sentiment analysis on the Yelp dataset, traditional models perform considerably worse than LLMs, justifying the energy costs of LLM deployment.","The context states that traditional models perform considerably worse than LLMs on the Yelp dataset, indicating that they do not achieve comparable accuracy."
"q279","As of January 2024, how many Amazon Renewable Energy Projects were announced in the United States?","The context indicates that as of January 2024, Amazon announced 244 renewable energy projects in the United States. This information is derived from the table detailing the number of projects and their capacities in different regions.","244","projects","[""amazon2023""]","is_blank","Project Location
Number 
of Projects
Total MW 
Capacity†
United States 244 17,706","The context indicates that as of January 2024, Amazon announced 244 renewable energy projects in the United States. This information is derived from the table detailing the number of projects and their capacities in different regions."
"q281","What percent of power usage did Amazon's AWS cover with renewable energy in 2018?","The provided context does not contain specific information about AWS's renewable energy coverage in 2018. Therefore, the answer is 'is_blank'.","is_blank","percent","[""is_blank""]","is_blank","is_blank","The provided context does not contain specific information about AWS's renewable energy coverage in 2018. Therefore, the answer is 'is_blank'."
"q283","At which measurement level do the authors recommend AI energy consumption should be reported to balance accuracy and feasibility?","The authors recommend measuring energy consumption at the cumulative server level for its high accuracy and feasibility, as it correlates well with model size and structure, data quantity, and hardware-aware software implementation.","cumulative server level","is_blank","[""ebert2024""]","is_blank","The upside of this method is its high accuracy, highly correlating with model size and structure, data quantity, and hardware-aware software implementation. It is widely recognized in the industry for assessing power consumption in data centers.","The authors recommend measuring energy consumption at the cumulative server level for its high accuracy and feasibility, as it correlates well with model size and structure, data quantity, and hardware-aware software implementation."
"q284","In an experiment to train a BERT-base model, what percentage of the total electricity consumption was attributed to the GPU?","According to the provided context, the GPU alone accounts for 74% of the total energy consumption when training a BERT-base model on a single NVIDIA TITAN X GPU in a commodity server with two Intel Xeon E5-2630 v3 CPUs and 256GB RAM. The answer_unit specifies 'percent', so the answer_value is expressed as a percentage.","74","percent","[""dodge2022""]","is_blank","The GPU alone accounts for 74% of the total energy consumption due to these components.","According to the provided context, the GPU alone accounts for 74% of the total energy consumption when training a BERT-base model on a single NVIDIA TITAN X GPU in a commodity server with two Intel Xeon E5-2630 v3 CPUs and 256GB RAM. The answer_unit specifies 'percent', so the answer_value is expressed as a percentage."
"q285","Serving a Llama2-70b model at BF16 precision requires how many NVIDIA A100-80GB GPUs?","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. The answer unit specifies the required format as 'NVIDIA A100-80GB GPUs', so the answer value is expressed as a number.","2","NVIDIA A100-80GB GPUs","[""griggs2024""]","is_blank","The substantial size and computational demands of LLMs require the use of costly hardware accelerators, typically GPUs2 For example, serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs, which costs over $5, 200 per month in on-demand rental costs on major cloud platforms.","The context states that serving Llama2-70b at BF16 precision requires 2 NVIDIA A100-80GB GPUs. The answer unit specifies the required format as 'NVIDIA A100-80GB GPUs', so the answer value is expressed as a number."
"q286","What was the total operational energy footprint reduction achieved at Facebook over a two-year period (2019 to 2021) due to iterative hardware-software optimization?","The context states that the iterative optimization process led to a 28.5% operational energy footprint reduction over the two-year time period. This aligns with the answer unit 'percent'.","28.5","percent","[""wu2021a""]","is_blank","The iterative optimization process has led to 28.5% operational energy footprint reduction over the two-year time period (Section III-B).","The context states that the iterative optimization process led to a 28.5% operational energy footprint reduction over the two-year time period. This aligns with the answer unit 'percent'."
"q287","How many kilometers of fiber optic cable were installed globally to support AI workloads in 2023?","The provided context does not contain any information about the installation of kilometers of fiber optic cable to support AI workloads in 2023.","is_blank","kilometers of fiberoptic cable","[""is_blank""]","is_blank","is_blank","The provided context does not contain any information about the installation of kilometers of fiber optic cable to support AI workloads in 2023."
"q288","What is the estimated upfront hardware acquisition cost to train GPT-4?","The context states that the hardware acquisition cost to train GPT-4 was estimated at $800M, which is the upfront cost of acquiring the hardware.","800000000","USD","[""cottier2024""]","is_blank","For example, we estimate that it cost $800M to acquire the hardware used to train GPT-4, compared to $40M for the amortized hardware CapEx + energy cost.","The context states that the hardware acquisition cost to train GPT-4 was estimated at $800M, which is the upfront cost of acquiring the hardware."
"q289","True or False: The umbrella term 'Sustainable AI' was proposed to only encompass using AI in climate-positive applications.","The context states that the term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. Therefore, it is not accurate to say it was proposed only for climate-positive applications.","is_blank","is_blank","[""luccioni2025b""]","is_blank","The umbrella term ‘Sustainable AI’ was initially proposed by van Wynsberghe as a field of practice that both aims to use AI in climate-positive applications, as well as improving upon the (environmental) sustainability of AI approaches themselves [203].","The context states that the term 'Sustainable AI' was proposed by van Wynsberghe to encompass both using AI in climate-positive applications and improving the environmental sustainability of AI approaches themselves. Therefore, it is not accurate to say it was proposed only for climate-positive applications."
"q290","What is the ground truth maximum batch size (in samples) for fine-tuning a Mixtral model on a NVIDIA A100-40GB GPU","The context states that for a GPU memory capacity of 40GB, the maximum batch size supported for fine-tuning Mixtral is 28 samples. This is derived from the provided figures and tables.","28","samples","[""xia2024""]","is_blank","For GPU memory capacities of 100GB and 120GB, our model predicts that the maximum batch sizes supported for fine-tuning Mixtral will be 28 and 35, respectively.","The context states that for a GPU memory capacity of 40GB, the maximum batch size supported for fine-tuning Mixtral is 28 samples. This is derived from the provided figures and tables."
"q291","When an LLM inference server is overloaded, which of the two preemption mechanisms-Recomputation or Swapping-consistently consumes less energy?","According to the context, when an LLM inference server is overloaded, Swapping consistently consumes less energy than Recomputation because Swapping copies data without running computation, while Recomputation performs extra computation when restoring requests.","Swapping","is_blank","[""chung2025""]","is_blank","It can be seen that when the server is overloaded, Swapping consistently consumes less energy. This is because Recomputation performs extra computation when restoring requests whereas Swapping copies data without running computation, and the energy consumption of computation is larger than memory operations.","According to the context, when an LLM inference server is overloaded, Swapping consistently consumes less energy than Recomputation because Swapping copies data without running computation, while Recomputation performs extra computation when restoring requests."
"q292","In its 2024 environmental report, what percentage increase in GHG emissions since 2019 did Google report?","According to the 2024 environmental sustainability report, Google reported a 48% increase in GHG emissions since 2019, primarily due to increases in data center energy consumption.","48","percent","[""luccioni2025a""]","[""https://sustainability.google/reports/google-2024-environmental-report/""]","For example, in their 2024 annual environmental sustainability report (ESG), Google reports a 48% increase in GHG emissions since 2019 which they attribute primarily to “increases in data center energy consumption” [42]","According to the 2024 environmental sustainability report, Google reported a 48% increase in GHG emissions since 2019, primarily due to increases in data center energy consumption."
"q293","According to McKinsey projections, what percentage of U.S. national electricity consumption are data centers anticipated to account for in 2030?","Based on the context, McKinsey's projections estimate that data centers may consume up to 11.7% of the total US energy demand by 2030. The question asks for the percentage of U.S. national electricity consumption, which aligns with the energy demand projection.","11.7","percent","[""morrison2025""]","[""https://www.mckinsey.com/industries/private-capital/our-insights/how-data-centers-and-the-energy-sector-can-sate-ais-hunger-for-power""]","There is a growing demand for energy to power AI workloads, with projections estimating that datacenters may consume upwards of 11.7% of the total US energy demand by 2030 (Shehabi et al., 2024;","Based on the context, McKinsey's projections estimate that data centers may consume up to 11.7% of the total US energy demand by 2030. The question asks for the percentage of U.S. national electricity consumption, which aligns with the energy demand projection."
"q294","When using the 'Pause and Resume' optimization for training of the 6B parameter transformer, what is the maximum potential emissions saving?","The context indicates that the maximum potential emissions saving for the 'Pause and Resume' optimization for training the 6B parameter transformer is up to 25%, as seen in Figure 4.","25","percent","[""dodge2022""]","is_blank","What proportion of emissions can we expect to save if we pause an AI workload when emissions in a region are high and resume when emissions are low, increasing the total duration by up to double the original duration? For short experiments, the doubled duration is still relatively short, and thus leads to minimal emissions reduction (see DenseNet 201 in (a)); for very long runs like our 6 billion parameter language model training run in (b), which ran for 8 days, doubling the duration can lead to significant savings up to about 25%. We confirmed with WattTime that emissions estimates for West US were correct, as that region has large variance.","The context indicates that the maximum potential emissions saving for the 'Pause and Resume' optimization for training the 6B parameter transformer is up to 25%, as seen in Figure 4."
"q295","By approximately what percentage does the JetMoE-8B architecture reduce inference computation compared to the Llama2-7B model?","According to the context, JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. Since the question asks for the percentage reduction, the answer_value is directly extracted as 70 percent.","70","percent","[""shen2024""]","is_blank","In addition, JetMoE-8B has 8B parameters while only activating 2B for each input token, reducing inference computation by about 70% compared to Llama2-7B.","According to the context, JetMoE-8B reduces inference computation by about 70% compared to Llama2-7B. Since the question asks for the percentage reduction, the answer_value is directly extracted as 70 percent."
"q298","What was the carbon footprint (in lbs CO2e) of training the BERT large language model, as quantified in a seminal 2019 study?","The seminal 2019 study by Strubell et al. quantified the carbon footprint of training BERT as reaching 626,155 pounds of CO2 emissions. This value is directly stated in the context.","626155","lbs CO2e","[""luccioni2025b""]","is_blank","Strubell et al. (2019) quantify the carbon footprint of NLP models, revealing that the training of a single large-scale transformer model can emit as much carbon as five cars over their entire lifetimes (their measurements include thousands of hyperparameter tuning jobs, which makes it difficult to disentangle model-inherent efficiency from experimental setup).","The seminal 2019 study by Strubell et al. quantified the carbon footprint of training BERT as reaching 626,155 pounds of CO2 emissions. This value is directly stated in the context."
"q299","What was the estimated training energy of the full GPT-3 model, in MWh?","The context explicitly states that GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh. This directly answers the question about the estimated training energy of the full GPT-3 model.","1287","MWh","[""li2025b""]","is_blank","GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh [29].","The context explicitly states that GPT-3 was trained and deployed by OpenAI in Microsoft’s data centers, with an estimated training energy of 1287 MWh. This directly answers the question about the estimated training energy of the full GPT-3 model."
"q300","True or False: When enhancing performance of LLM fine-tuning, the Mixture of Experts (MoE) layer is often a targeted layer.","The context indicates that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning, making it a prime target for optimization to enhance the overall cost of LLM fine-tuning. Therefore, the answer is 'True' as the MoE layer is often targeted for performance enhancement.","is_blank","is_blank","[""xia2024""]","is_blank","(2) MoE layer consumes the highest fraction of execution time in LLM fine-tuning; optimizing MoE layer performance is key to improving the overall cost of LLM fine-tuning.","The context indicates that the MoE layer consumes the highest fraction of execution time in LLM fine-tuning, making it a prime target for optimization to enhance the overall cost of LLM fine-tuning. Therefore, the answer is 'True' as the MoE layer is often targeted for performance enhancement."
"q301","What is the maximum batch size (in samples) supported by fine-tuning Mixtral with a dense setup on the Hellaswag dataset using a NVIDIA A40 GPU with 48 GB memory?","The context states that the maximum batch size supported by fine-tuning Mixtral on an A40 GPU with 48GB memory is 2 samples for the Hellaswag dataset. This is derived from Table III which lists the maximum batch size for Mixtral-D (dense) on CS (likely Hellaswag) as 2.","2","samples","[""xia2024""]","is_blank","TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20","The context states that the maximum batch size supported by fine-tuning Mixtral on an A40 GPU with 48GB memory is 2 samples for the Hellaswag dataset. This is derived from Table III which lists the maximum batch size for Mixtral-D (dense) on CS (likely Hellaswag) as 2."
"q302","True or False: For CV models with high granularity, intercontinental training slowed performance by only 7% compared to local training.","The context states that for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7% compared to local training.","is_blank","is_blank","[""erben2023""]","is_blank","In summary, while local compute is the best choice for maximum throughput, for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7%. However, intercontinental training leads to a significant penalty on a task with lower granularity, like NLP, resulting in a performance drop of 41% (C-8) compared to the fully local experiment (A-8).","The context states that for high granularity tasks like CV, even distributing VMs over four continents only slows down performance by 7% compared to local training."
"q303","How many hectares of land were occupied by new AI data centers globally in 2022?","The provided context does not contain any specific information about the hectares of land occupied by new AI data centers globally in 2022. Therefore, the answer is 'is_blank'.","is_blank","hectares","[""is_blank""]","is_blank","is_blank","The provided context does not contain any specific information about the hectares of land occupied by new AI data centers globally in 2022. Therefore, the answer is 'is_blank'."
"q305","A 2024 study compares task-specific and general-purpose models. How many grams of CO2eq are emitted by the BERT-based model bert-base-multilingual-uncased-sentiment per 1,000 text classification queries?","The context states that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 text classification queries. This directly answers the question about the emission rate of this specific model.","0.32","g CO2eq","[""luccioni2024""]","is_blank","for instance bert-base-multilingual-uncased-sentiment emits just 0.32g of 𝐶𝑂2𝑒𝑞 per 1,000 queries","The context states that the BERT-based model bert-base-multilingual-uncased-sentiment emits 0.32g of CO2eq per 1,000 text classification queries. This directly answers the question about the emission rate of this specific model."
"q307","In the experiment analyzing emissions from training a BERT model across different geographic regions, what was the approximate range of CO2 emissions (in thousands of grams) between the most and least efficient regions?","The context states that the emissions from training BERT in the least carbon-intensive regions are approximately 7k grams, while in the most carbon-intensive regions, they are approximately 26k grams. Therefore, the range of CO2 emissions between the most and least efficient regions is [7000, 26000] grams.","[7000,26000]","grams","[""dodge2022""]","is_blank","There is large variation between the least carbon-intensive regions (the lowest lines) compared to the most carbon-intensive regions (the top lines), indicating that choosing the region in which experiments run can be very impactful ( 7k grams vs. 26k grams, for the most efficient vs. least efficient regions).","The context states that the emissions from training BERT in the least carbon-intensive regions are approximately 7k grams, while in the most carbon-intensive regions, they are approximately 26k grams. Therefore, the range of CO2 emissions between the most and least efficient regions is [7000, 26000] grams."
"q308","In what year did the practice of directly releasing environmental information for notable models peak before declining?","The context states that the direct release of environmental information for notable models peaked in 2022, after which there was a notable decline due to the introduction of more commercial and proprietary models.","2022","year","[""luccioni2025c""]","is_blank","The direct release of environmental information peaked in 2022, with 10% of notable models that year releasing some degree of information.","The context states that the direct release of environmental information for notable models peaked in 2022, after which there was a notable decline due to the introduction of more commercial and proprietary models."
"q309","What is the equivalent water usage, in days, for one person in the US, of training an OLMo 60M model on 1.7 to 5.6 trillion tokens?","The context provides the water consumption for the OLMo 60M model as 1.2 kL, which is equivalent to 5 days of water usage for one person in the U.S. The answer unit requested is 'days', so we convert the kL to days directly.","5","days","[""morrison2025""]","is_blank","OLMo 60M† 1.2 0.4 1 month 1.6 5 days","The context provides the water consumption for the OLMo 60M model as 1.2 kL, which is equivalent to 5 days of water usage for one person in the U.S. The answer unit requested is 'days', so we convert the kL to days directly."
"q310","How many liters of freshwater did Google's DeepMind AlphaFold servers consume in 2023?","The provided context does not contain any specific information regarding the amount of freshwater consumed by Google's DeepMind AlphaFold servers in 2023. Therefore, the answer is 'is_blank'.","is_blank","liters of freshwater","[""is_blank""]","is_blank","is_blank","The provided context does not contain any specific information regarding the amount of freshwater consumed by Google's DeepMind AlphaFold servers in 2023. Therefore, the answer is 'is_blank'."
"q311","True or False: Adding compute resources to accelerate the MoE layers when fine-tuning LLMs can increase costs.","The context states that adding compute resources to accelerate the MoE layers can help reduce the cost of fine-tuning LLMs, not increase it. Therefore, the statement is false.","is_blank","is_blank","[""xia2024""]","is_blank","A way to further reduce cost based on our study is to add compute resources to accelerate the MoE layers.","The context states that adding compute resources to accelerate the MoE layers can help reduce the cost of fine-tuning LLMs, not increase it. Therefore, the statement is false."
"q312","According to a carbon footprint analysis, what was the total energy consumption for training the FLM-101B model?","According to the carbon footprint analysis in Table 3, the energy consumption for training the FLM-101B model is specified as 40 MkWh. Given the answer_unit is kWh, we convert 40 MkWh to 40,000 kWh.","40000","kWh","[""li2025a""]","is_blank","Table 3: Carbon emissions of our proposed model, FLM-101B, and other well-known LLMs. For details, please see the corresponding references. The definitions of TDP, nettCO 2e, and their formulas are the same as (Patterson et al. 2021).
Model GPT-3
(Brown et al. 2020)
Gopher
(Rae et al. 2021)
PaLM
(Anil et al. 2023)
GLM-130B
(Zeng et al. 2023)
Llama-2
(Touvron et al. 2023b)FLM-101B
Params 175B 280B 540B 130B 70B 101B
GPU Hours 3.55e6 3.77e6 8.40e6 1.11e6 1.72e6 1.01e5
Chip Power/TDP 330 283 378.5 400 400 400
Energy (MkWh) 1171 1066 3179 444 688 40
net tCO2e 552 380 271 257 291 26","According to the carbon footprint analysis in Table 3, the energy consumption for training the FLM-101B model is specified as 40 MkWh. Given the answer_unit is kWh, we convert 40 MkWh to 40,000 kWh."
"q313","According to a recent study's projections for 2030, the total public health burden of U.S. data centers could be valued at up to more than what amount?","Based on the context, the total public health costs attributable to data center operations are projected to potentially triple from 2023 to 2028. In 2023, the total public health cost was about $6.7 billion. Therefore, by 2028, the cost could reach more than $20 billion. Since the question asks for the projection for 2030, and given the trend, we can infer that the upper limit would still be within the range mentioned for 2028. Hence, the answer is $20 billion.","20000000000","USD","[""han2024""]","is_blank","Our results demonstrate that in 2028, the total scope-1 and scope-2 pollutants of U.S. data centers alone could cause, among others, approximately 600,000 asthma symptom cases and 1,300 premature deaths, exceeding 1/3 of asthma deaths in the U.S. each year [40]. The overall public health costs could reach more than $20 billion, rival or even top those of on-road emissions of the largest U.S. states such as California with∼35 million registered vehicles [41].","Based on the context, the total public health costs attributable to data center operations are projected to potentially triple from 2023 to 2028. In 2023, the total public health cost was about $6.7 billion. Therefore, by 2028, the cost could reach more than $20 billion. Since the question asks for the projection for 2030, and given the trend, we can infer that the upper limit would still be within the range mentioned for 2028. Hence, the answer is $20 billion."
"q314","What is the estimated total cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE with an NVIDIA A40-48GB GPU?","The estimated cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE on an NVIDIA A40-48GB GPU is derived from Table IV, which provides the cost estimate based on the analytical model. The cost is calculated as $32.7.","32.7","USD","[""xia2024""]","is_blank","TABLE IV
ESTIMATED COST OF FINE -TUNING MIXTRAL ON GS WITH SPARSE MOE
BASED ON OUR ANALYTICAL MODEL
GPU Mem MBS Throughput Cost ($/hr) Cost ($)
A40 48GB 4 1.01 0.79 32.7","The estimated cost of fine-tuning a Mixtral model on the GSM8K dataset with sparse MoE on an NVIDIA A40-48GB GPU is derived from Table IV, which provides the cost estimate based on the analytical model. The cost is calculated as $32.7."
"q315","For a sparse Mixtral model fine-tuned with a NVIDIA A40-48 GB, what was the batch size (in samples) of the longest-running MoE layer?","Based on the context, the maximum batch size supported by the sparse Mixtral model fine-tuned on an NVIDIA A40-48GB GPU is 8 samples. This is derived from Table III, which lists the maximum batch sizes for different model and dataset combinations.","8","samples","[""xia2024""]","is_blank","TABLE III MAXIMUM BATCH SIZE SUPPORTED BY LLM FINE-TUNING ; D: DENSE AND S:SPARSE . Mixtral-D Mixtral-S BlackMamba-D BlackMamba-S CS 2 8 6 20 MATH 1 3 2 8","Based on the context, the maximum batch size supported by the sparse Mixtral model fine-tuned on an NVIDIA A40-48GB GPU is 8 samples. This is derived from Table III, which lists the maximum batch sizes for different model and dataset combinations."
"q317","What is the total execution time, in seconds, of a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB with a batch size of 10?","The context does not provide a specific total execution time for a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB GPU and a batch size of 10. The figures and tables in the document focus on throughput and batch size projections rather than total execution time.","is_blank","seconds","[""is_blank""]","is_blank","is_blank","The context does not provide a specific total execution time for a sparse Mixtral model fine-tuned with a NVIDIA A40-48GB GPU and a batch size of 10. The figures and tables in the document focus on throughput and batch size projections rather than total execution time."
"q318","True or False: GPU-level power consumption monitoring is recommended as the preferred method for reporting overall AI energy use.","The context states that GPU-level or other component-based power consumption tracking is not recommended for overall energy measurements. Instead, it suggests measuring at the cumulative server level or data center level for better accuracy and efficiency.","is_blank","is_blank","[""ebert2024""]","is_blank","We advocate against using GPU-level or other component-based power consumption tracking for overall energy measurements.","The context states that GPU-level or other component-based power consumption tracking is not recommended for overall energy measurements. Instead, it suggests measuring at the cumulative server level or data center level for better accuracy and efficiency."
"q319","In a 2023 article estimating the carbon footprint of the BLOOM model, what percentage of the model's overall emissions did training account for?","The context states that in Luccioni et al.'s 2023 article estimating the carbon footprint of BLOOM, they found that training accounted for only half of the model's overall emissions.","50","percent","[""luccioni2025b""]","is_blank","In a 2023 article estimating the carbon footprint of BLOOM, a 176 billion parameter LLM, Luccioni et al. proposed using a Life Cycle Assessment approach for this evaluation, since it takes into account different stages of the model life cycle including the manufacturing of computing hardware, idle energy usage, and model deployment, finding that training accounted for only half of the model’s overall emissions.","The context states that in Luccioni et al.'s 2023 article estimating the carbon footprint of BLOOM, they found that training accounted for only half of the model's overall emissions."
"q320","What is the bare minimum number of NVIDIA V100 32GB GPUs required to run LLaMA-7B inference without compression or quantization?","According to the context, the bare minimum hardware requirements for running LLaMA-7B inference without compression or quantization is 1 NVIDIA V100 32GB GPU.","1","V100_32GB_GPU","[""rubei2025""]","is_blank","TABLE II: Baseline configurations for LLaMA 7B, 13B, and 65B: This table lists the bare minimum hardware required for different models and the maximum batch size possible given the bare minimum hardware for a max response length of 256. These limits are imposed by a combination of GPU memory, model size, response length and the number of GPUs. While the 65B model can sharded across 6 V100 GPUs, we use 8 since the model architecture makes it better suited for balanced sharding across 8 GPUs. Model Size V100 32GB A100 80GB Count Max. Batch size Count Max. Batch size 7B 1 64 1 64 13B 2 64 1 64 65B 8 64 4 128","According to the context, the bare minimum hardware requirements for running LLaMA-7B inference without compression or quantization is 1 NVIDIA V100 32GB GPU."
"q321","When training GPT-3 in a data center in Arizona, how many user requests would it take to consume a 500ml bottle of water?","According to the context, GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. For Arizona, the on-site WUE is 1.630 L/kWh, which implies higher water consumption compared to the average. Hence, the number of requests to consume 500ml of water would fall within the range specified.","[10,50]","requests","[""li2025b""]","is_blank","Additionally, GPT-3 needs to “drink” (i.e., consume) a500ml bottle of waterfor roughly 10 – 50 medium-length responses, depending on when and where it is deployed.","According to the context, GPT-3 needs to 'drink' (consume) a 500ml bottle of water for roughly 10 – 50 medium-length responses, depending on when and where it is deployed. For Arizona, the on-site WUE is 1.630 L/kWh, which implies higher water consumption compared to the average. Hence, the number of requests to consume 500ml of water would fall within the range specified."
"q322","What is the estimated CO2 emission in metric tons for one year of average US home energy use?","The context states that the average US home emits 8.3 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil.","8.3","metric tons","[""dodge2022""]","is_blank","Even partially trained, experiments of this size can emit more CO2 than all emissions from the average US home for a year (which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil, totaling 8.3 metric tons CO2 per year).","The context states that the average US home emits 8.3 metric tons of CO2 per year, which includes emissions from electricity generation, natural gas, liquid petroleum gas, and fuel oil."
"q323","On the GSM8k benchmark, which evaluates grade school math problem-solving, what score did the JetMoE-8B model achieve?","The context provides the performance scores of JetMoE-8B on various benchmarks, including GSM8k. According to Table 3, JetMoE-8B achieved a score of 27.8 on GSM8k.","27.8","score","[""shen2024""]","is_blank","Table 3: OpenLLM leaderboard and code benchmarks results from four different models. JetMoE-8B outperforms Gemma, LLaMA2, and DeepseekMoE on the OpenLLM leaderboard, achieving the best scores in all tasks except ARC-challenge and WinoGrande. Additionally, JetMoE-8B obtains the highest MBPP scores in Python programming.","The context provides the performance scores of JetMoE-8B on various benchmarks, including GSM8k. According to Table 3, JetMoE-8B achieved a score of 27.8 on GSM8k."
