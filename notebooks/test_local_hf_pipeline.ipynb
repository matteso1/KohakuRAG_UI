{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Local HF Pipeline - End-to-End Test\n",
    "\n",
    "This notebook validates that the **fully local** KohakuRAG pipeline works\n",
    "without any network calls. It tests:\n",
    "\n",
    "1. Local embeddings (`LocalHFEmbeddingModel` via sentence-transformers)\n",
    "2. Local LLM chat (`HuggingFaceLocalChatModel` via transformers)\n",
    "3. Full RAG pipeline: index documents, retrieve, and answer\n",
    "\n",
    "**Prerequisites:**\n",
    "- Kernel: `kohaku-gb10` (or your project venv)\n",
    "- Dependencies installed: `pip install -r local_requirements.txt`\n",
    "- Vendored packages installed: `pip install -e vendor/KohakuVault && pip install -e vendor/KohakuRAG`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step1-header",
   "metadata": {},
   "source": [
    "## Step 1 - Verify imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import sentence_transformers\n",
    "\n",
    "print(f\"torch:                {torch.__version__}\")\n",
    "print(f\"transformers:         {transformers.__version__}\")\n",
    "print(f\"sentence-transformers: {sentence_transformers.__version__}\")\n",
    "print(f\"CUDA available:       {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU:                  {torch.cuda.get_device_name(0)}\")\n",
    "print()\n",
    "\n",
    "import kohakurag\n",
    "import kohakuvault\n",
    "print(f\"kohakurag:  {kohakurag.__file__}\")\n",
    "print(f\"kohakuvault: {kohakuvault.__file__}\")\n",
    "print(\"\\nAll imports OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step2-header",
   "metadata": {},
   "source": [
    "## Step 2 - Test local embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kohakurag.embeddings import LocalHFEmbeddingModel\n",
    "\n",
    "# Use a small, fast model for testing\n",
    "embedder = LocalHFEmbeddingModel(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "print(f\"Embedding model loaded: BAAI/bge-base-en-v1.5\")\n",
    "print(f\"Embedding dimension:    {embedder.dimension}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "test_texts = [\n",
    "    \"Solar panels convert sunlight into electricity.\",\n",
    "    \"Photovoltaic cells generate power from solar radiation.\",\n",
    "    \"The capital of France is Paris.\",\n",
    "]\n",
    "\n",
    "vecs = await embedder.embed(test_texts)\n",
    "print(f\"Embedding shape: {vecs.shape}\")\n",
    "print(f\"Dtype:           {vecs.dtype}\")\n",
    "\n",
    "# Cosine similarity (vectors are already normalized)\n",
    "sim_01 = float(np.dot(vecs[0], vecs[1]))\n",
    "sim_02 = float(np.dot(vecs[0], vecs[2]))\n",
    "print(f\"\\nSimilarity (solar vs photovoltaic): {sim_01:.4f}  (should be high)\")\n",
    "print(f\"Similarity (solar vs Paris):         {sim_02:.4f}  (should be low)\")\n",
    "assert sim_01 > sim_02, \"Semantic similarity check failed!\"\n",
    "print(\"\\nEmbedding sanity check PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step3-header",
   "metadata": {},
   "source": [
    "## Step 3 - Test local LLM chat\n",
    "\n",
    "This loads a local HF model for generation. The default is `Qwen/Qwen2.5-7B-Instruct`.\n",
    "\n",
    "**Note:** If this is too large for your GPU, change `LLM_MODEL_ID` to a smaller model\n",
    "like `Qwen/Qwen2.5-1.5B-Instruct` or `TinyLlama/TinyLlama-1.1B-Chat-v1.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3-config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the LLM model - adjust if needed for your hardware\n",
    "LLM_MODEL_ID = \"Qwen/Qwen2.5-7B-Instruct\"  # change to smaller model if OOM\n",
    "LLM_DTYPE = \"bf16\"  # \"bf16\", \"fp16\", or \"auto\"\n",
    "\n",
    "print(f\"Will load: {LLM_MODEL_ID} ({LLM_DTYPE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kohakurag.llm import HuggingFaceLocalChatModel\n",
    "\n",
    "chat = HuggingFaceLocalChatModel(\n",
    "    model=LLM_MODEL_ID,\n",
    "    dtype=LLM_DTYPE,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.0,  # greedy for reproducibility\n",
    ")\n",
    "print(f\"LLM loaded: {LLM_MODEL_ID}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await chat.complete(\n",
    "    \"What is 2 + 2? Answer with just the number.\",\n",
    "    system_prompt=\"You are a helpful assistant. Be concise.\",\n",
    ")\n",
    "print(f\"LLM response: {response!r}\")\n",
    "assert \"4\" in response, f\"Expected '4' in response, got: {response}\"\n",
    "print(\"LLM sanity check PASSED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step4-header",
   "metadata": {},
   "source": [
    "## Step 4 - Full RAG pipeline (in-memory)\n",
    "\n",
    "This test creates a small document set, indexes it into an in-memory store,\n",
    "then runs retrieval + answer generation. No database file needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-docs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents about sustainable AI\n",
    "documents = [\n",
    "    {\n",
    "        \"id\": \"doc1\",\n",
    "        \"title\": \"Energy Efficiency in Data Centers\",\n",
    "        \"text\": (\n",
    "            \"Modern data centers consume approximately 1-2% of global electricity. \"\n",
    "            \"Liquid cooling systems can reduce energy usage by up to 40% compared to \"\n",
    "            \"traditional air cooling. Google reported a PUE (Power Usage Effectiveness) \"\n",
    "            \"of 1.10 across its fleet in 2023.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc2\",\n",
    "        \"title\": \"Carbon Footprint of LLM Training\",\n",
    "        \"text\": (\n",
    "            \"Training GPT-3 (175B parameters) was estimated to emit approximately \"\n",
    "            \"552 tonnes of CO2. Smaller models like Llama-2-7B require roughly \"\n",
    "            \"30x less compute. Techniques such as mixed-precision training and \"\n",
    "            \"gradient checkpointing can further reduce energy consumption by 20-30%.\"\n",
    "        ),\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"doc3\",\n",
    "        \"title\": \"Renewable Energy for AI Workloads\",\n",
    "        \"text\": (\n",
    "            \"Microsoft committed to being carbon negative by 2030. Their Azure \"\n",
    "            \"data centers in Sweden run on 100% renewable energy. Solar-powered \"\n",
    "            \"inference clusters have shown 15% cost savings in regions with \"\n",
    "            \"high solar irradiance.\"\n",
    "        ),\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(documents)} test documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-index",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kohakurag.types import NodeKind, StoredNode\n",
    "from kohakurag.pipeline import RAGPipeline\n",
    "from kohakurag.datastore import InMemoryNodeStore\n",
    "\n",
    "# Build StoredNodes with embeddings\n",
    "nodes = []\n",
    "all_texts = [doc[\"text\"] for doc in documents]\n",
    "all_vecs = await embedder.embed(all_texts)\n",
    "\n",
    "for doc, vec in zip(documents, all_vecs):\n",
    "    node = StoredNode(\n",
    "        node_id=f\"{doc['id']}:p0:s0\",\n",
    "        parent_id=f\"{doc['id']}:p0\",\n",
    "        kind=NodeKind.PARAGRAPH,\n",
    "        title=doc[\"title\"],\n",
    "        text=doc[\"text\"],\n",
    "        metadata={\"document_id\": doc[\"id\"]},\n",
    "        embedding=vec,\n",
    "        child_ids=[],\n",
    "    )\n",
    "    nodes.append(node)\n",
    "\n",
    "# Create in-memory store and index\n",
    "store = InMemoryNodeStore()\n",
    "await store.upsert_nodes(nodes)\n",
    "print(f\"Indexed {len(nodes)} nodes into in-memory store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble pipeline with local components\n",
    "pipeline = RAGPipeline(\n",
    "    store=store,\n",
    "    embedder=embedder,\n",
    "    chat_model=chat,\n",
    "    top_k=3,\n",
    ")\n",
    "print(\"Pipeline assembled (local embedder + local LLM + in-memory store)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-retrieve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval only\n",
    "question = \"How much CO2 does training a large language model produce?\"\n",
    "\n",
    "result = await pipeline.retrieve(question, top_k=3)\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Retrieved {len(result.matches)} matches:\\n\")\n",
    "for i, match in enumerate(result.matches):\n",
    "    print(f\"  [{i+1}] score={match.score:.4f}  {match.node.title}\")\n",
    "    print(f\"      {match.node.text[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step4-answer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test full QA (retrieve + generate)\n",
    "answer = await pipeline.answer(question)\n",
    "\n",
    "print(f\"Question: {answer['question']}\\n\")\n",
    "print(f\"Response:\\n{answer['response']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step5-header",
   "metadata": {},
   "source": [
    "## Step 5 - Structured QA (JSON output)\n",
    "\n",
    "This tests the `run_qa` method with the same prompt templates used in production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "system_prompt = (\n",
    "    \"You must answer strictly based on the provided context snippets. \"\n",
    "    \"Do NOT use external knowledge. Respond in JSON with keys: \"\n",
    "    \"explanation, answer, answer_value, ref_id.\"\n",
    ")\n",
    "\n",
    "user_template = \"\"\"Question: {question}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Additional info: {additional_info_json}\n",
    "\n",
    "Return STRICT JSON:\n",
    "- explanation: 1-2 sentences\n",
    "- answer: short answer\n",
    "- answer_value: numeric/categorical value or \"is_blank\"\n",
    "- ref_id: list of document ids used\n",
    "\n",
    "JSON Answer:\"\"\"\n",
    "\n",
    "structured_result = await pipeline.run_qa(\n",
    "    question=\"What PUE did Google report for its data centers?\",\n",
    "    system_prompt=system_prompt,\n",
    "    user_template=user_template,\n",
    "    additional_info={\"answer_unit\": \"PUE ratio\"},\n",
    "    top_k=3,\n",
    ")\n",
    "\n",
    "print(f\"Answer:       {structured_result.answer.answer}\")\n",
    "print(f\"Answer value: {structured_result.answer.answer_value}\")\n",
    "print(f\"Ref IDs:      {structured_result.answer.ref_id}\")\n",
    "print(f\"Explanation:  {structured_result.answer.explanation}\")\n",
    "print(f\"\\nRaw LLM output:\\n{structured_result.raw_response[:500]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "step6-header",
   "metadata": {},
   "source": [
    "## Step 6 - Offline validation\n",
    "\n",
    "Confirm no network calls were made by unsetting API keys and re-running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "step6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clear any API keys to prove we're fully local\n",
    "for key in [\"OPENROUTER_API_KEY\", \"OPENAI_API_KEY\", \"JINA_API_KEY\"]:\n",
    "    os.environ.pop(key, None)\n",
    "\n",
    "# Re-run a query - should work without any API keys\n",
    "offline_answer = await pipeline.answer(\n",
    "    \"What percentage of global electricity do data centers use?\"\n",
    ")\n",
    "print(f\"Offline response:\\n{offline_answer['response']}\")\n",
    "print(\"\\nOFFLINE VALIDATION PASSED - no API keys needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "If all cells above ran successfully, your local HF pipeline is working:\n",
    "\n",
    "| Component | Provider | Model |\n",
    "|-----------|----------|-------|\n",
    "| Embeddings | `LocalHFEmbeddingModel` | `BAAI/bge-base-en-v1.5` |\n",
    "| LLM | `HuggingFaceLocalChatModel` | Configured above |\n",
    "| Vector store | `InMemoryNodeStore` | (in-memory, no DB needed) |\n",
    "\n",
    "To use with the full production pipeline (KVaultNodeStore + pre-indexed docs),\n",
    "set `llm_provider = \"hf_local\"` and `embedding_model = \"hf_local\"` in your config."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kohaku-gb10",
   "language": "python",
   "name": "kohaku-gb10"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}